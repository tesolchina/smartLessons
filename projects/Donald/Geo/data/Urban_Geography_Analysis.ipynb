{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392eab6f",
   "metadata": {},
   "source": [
    "# Year 10 Geography Data Analysis - Group 3 Site Comparison\n",
    "## Urban Planning & Land Use Analysis\n",
    "\n",
    "This notebook analyzes urban geography data from 3 different sites, examining:\n",
    "- Environmental Quality Standards (EQS)\n",
    "- Building characteristics\n",
    "- Noise levels\n",
    "- Traffic patterns\n",
    "- Pedestrian activity\n",
    "- Land use types\n",
    "- Urban hierarchy (high/low order)\n",
    "\n",
    "**Data Source**: YR 10 DATA(GROUP 3).csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200dc182",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import pandas, numpy, matplotlib, seaborn, and other necessary libraries for data analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Configure display settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"ğŸ“Š Pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb61fad",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "Load the CSV file using pandas and perform initial exploration including shape, columns, data types, and first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '/Users/simonwang/Documents/Usage/VibeCoding/DailyAssistant/projects/DonaldGeo/data/YR 10 DATA(GROUP 3).csv'\n",
    "\n",
    "# Read the CSV file\n",
    "try:\n",
    "    # Try reading with different parameters to handle the unusual structure\n",
    "    df_raw = pd.read_csv(file_path, header=None)\n",
    "    print(\"âœ… Dataset loaded successfully\")\n",
    "    print(f\"ğŸ“ Dataset shape: {df_raw.shape}\")\n",
    "    print(\"\\nğŸ“‹ Raw data preview:\")\n",
    "    display(df_raw)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d587aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure more carefully\n",
    "print(\"ğŸ” Detailed data exploration:\")\n",
    "print(f\"Number of rows: {len(df_raw)}\")\n",
    "print(f\"Number of columns: {len(df_raw.columns)}\")\n",
    "print(\"\\nğŸ“Š Data types:\")\n",
    "print(df_raw.dtypes)\n",
    "print(\"\\nğŸ” First few rows with indices:\")\n",
    "for i, row in df_raw.head(10).iterrows():\n",
    "    print(f\"Row {i}: {row.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607ac35",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Clean the dataset by removing duplicates, standardizing column names, and identifying data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833eafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a properly structured DataFrame from the raw data\n",
    "# Based on the structure, we need to transpose and reorganize\n",
    "\n",
    "# Extract the meaningful rows (skip empty rows)\n",
    "data_rows = []\n",
    "for i, row in df_raw.iterrows():\n",
    "    if pd.notna(row[0]) and str(row[0]).strip() != '':\n",
    "        data_rows.append(row.tolist())\n",
    "\n",
    "print(\"ğŸ§¹ Cleaning data structure...\")\n",
    "print(f\"Found {len(data_rows)} data rows\")\n",
    "\n",
    "# Create structured DataFrame\n",
    "# Row 0: Group identifier\n",
    "# Row 1: Site headers\n",
    "# Rows 2+: Data measurements\n",
    "\n",
    "if len(data_rows) >= 3:\n",
    "    # Extract site names (columns)\n",
    "    sites = [col for col in data_rows[1][1:] if pd.notna(col) and str(col).strip() != '']\n",
    "    print(f\"ğŸ“ Sites identified: {sites}\")\n",
    "    \n",
    "    # Create clean DataFrame\n",
    "    clean_data = []\n",
    "    \n",
    "    for row in data_rows[3:]:  # Skip header rows\n",
    "        if pd.notna(row[0]) and str(row[0]).strip() != '':\n",
    "            measurement = str(row[0]).strip()\n",
    "            values = []\n",
    "            \n",
    "            for i in range(1, len(row)):\n",
    "                if i <= len(sites):\n",
    "                    values.append(row[i] if pd.notna(row[i]) else None)\n",
    "            \n",
    "            # Pad values if needed\n",
    "            while len(values) < len(sites):\n",
    "                values.append(None)\n",
    "            \n",
    "            clean_data.append([measurement] + values[:len(sites)])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    columns = ['Measurement'] + sites\n",
    "    df_clean = pd.DataFrame(clean_data, columns=columns)\n",
    "    \n",
    "    print(\"\\nâœ… Clean dataset created:\")\n",
    "    display(df_clean)\n",
    "else:\n",
    "    print(\"âŒ Unable to parse data structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean measurement names and standardize format\n",
    "print(\"ğŸ§¹ Standardizing measurement names...\")\n",
    "\n",
    "# Create a mapping for cleaner names\n",
    "measurement_mapping = {\n",
    "    'EQS': 'Environmental_Quality_Score',\n",
    "    'BULDING HEIGHT ': 'Building_Height_m',\n",
    "    'DECIBELS': 'Noise_Level_dB',\n",
    "    'TRAFFIC COUNT': 'Traffic_Count',\n",
    "    'PEDESTRIAN COUNT ': 'Pedestrian_Count',\n",
    "    'ALL TRAFFIC OPTIONS ': 'Public_Transport',\n",
    "    'LANDUSE ': 'Land_Use_Type',\n",
    "    'HIGH OR LOW ORDER?': 'Urban_Hierarchy'\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df_clean['Measurement_Clean'] = df_clean['Measurement'].map(\n",
    "    lambda x: measurement_mapping.get(x, x.strip().replace(' ', '_'))\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“‹ Measurement mapping:\")\n",
    "for original, clean in zip(df_clean['Measurement'], df_clean['Measurement_Clean']):\n",
    "    print(f\"'{original}' â†’ '{clean}'\")\n",
    "\n",
    "# Set clean measurement as index\n",
    "df_clean.set_index('Measurement_Clean', inplace=True)\n",
    "df_clean.drop('Measurement', axis=1, inplace=True)\n",
    "\n",
    "print(\"\\nâœ… Cleaned dataset:\")\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56a7f9",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values\n",
    "\n",
    "Identify missing values, analyze patterns, and implement appropriate strategies for handling them (removal, imputation, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"ğŸ” Missing Value Analysis:\")\n",
    "print(\"\\nğŸ“Š Missing values by measurement:\")\n",
    "\n",
    "missing_summary = df_clean.isnull().sum()\n",
    "print(missing_summary)\n",
    "\n",
    "# Calculate missing percentages\n",
    "missing_pct = (df_clean.isnull().sum() / len(df_clean)) * 100\n",
    "print(\"\\nğŸ“ˆ Missing value percentages:\")\n",
    "for col in df_clean.columns:\n",
    "    print(f\"{col}: {missing_pct[col]:.1f}%\")\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_clean.isnull(), cmap='viridis', cbar=True, yticklabels=True)\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.xlabel('Sites')\n",
    "plt.ylabel('Measurements')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Missing value analysis complete\")\n",
    "if missing_summary.sum() == 0:\n",
    "    print(\"ğŸ‰ No missing values found!\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Found {missing_summary.sum()} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac52f76",
   "metadata": {},
   "source": [
    "## 5. Data Type Conversions\n",
    "\n",
    "Convert columns to appropriate data types (numeric, datetime, categorical) for proper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrames for different data types\n",
    "print(\"ğŸ”„ Processing data types...\")\n",
    "\n",
    "# Numeric measurements\n",
    "numeric_measurements = ['Building_Height_m', 'Noise_Level_dB', 'Pedestrian_Count']\n",
    "df_numeric = df_clean.loc[numeric_measurements].copy()\n",
    "\n",
    "# Convert to numeric\n",
    "for col in df_numeric.columns:\n",
    "    df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')\n",
    "\n",
    "print(\"\\nğŸ“Š Numeric data:\")\n",
    "display(df_numeric)\n",
    "\n",
    "# Categorical data\n",
    "categorical_measurements = ['Environmental_Quality_Score', 'Land_Use_Type', 'Urban_Hierarchy']\n",
    "df_categorical = df_clean.loc[categorical_measurements].copy()\n",
    "\n",
    "print(\"\\nğŸ“‹ Categorical data:\")\n",
    "display(df_categorical)\n",
    "\n",
    "# Complex data (Traffic and Transport)\n",
    "complex_measurements = ['Traffic_Count', 'Public_Transport']\n",
    "df_complex = df_clean.loc[complex_measurements].copy()\n",
    "\n",
    "print(\"\\nğŸš— Complex data (Traffic & Transport):\")\n",
    "display(df_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process traffic count data\n",
    "print(\"ğŸš— Processing traffic count data...\")\n",
    "\n",
    "def parse_traffic_data(traffic_str):\n",
    "    \"\"\"Parse traffic count string into structured data\"\"\"\n",
    "    if pd.isna(traffic_str):\n",
    "        return {}\n",
    "    \n",
    "    traffic_dict = {}\n",
    "    lines = str(traffic_str).split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            vehicle, count = line.split(':', 1)\n",
    "            try:\n",
    "                traffic_dict[vehicle.strip()] = int(count.strip())\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    return traffic_dict\n",
    "\n",
    "# Parse traffic data for each site\n",
    "traffic_data = {}\n",
    "for site in df_complex.columns:\n",
    "    traffic_str = df_complex.loc['Traffic_Count', site]\n",
    "    traffic_data[site] = parse_traffic_data(traffic_str)\n",
    "\n",
    "print(\"\\nğŸš— Parsed traffic data:\")\n",
    "for site, data in traffic_data.items():\n",
    "    print(f\"\\n{site}:\")\n",
    "    for vehicle, count in data.items():\n",
    "        print(f\"  {vehicle}: {count}\")\n",
    "    total = sum(data.values())\n",
    "    print(f\"  Total: {total}\")\n",
    "\n",
    "# Create traffic DataFrame\n",
    "all_vehicles = set()\n",
    "for data in traffic_data.values():\n",
    "    all_vehicles.update(data.keys())\n",
    "\n",
    "traffic_matrix = []\n",
    "for vehicle in sorted(all_vehicles):\n",
    "    row = []\n",
    "    for site in df_complex.columns:\n",
    "        count = traffic_data[site].get(vehicle, 0)\n",
    "        row.append(count)\n",
    "    traffic_matrix.append(row)\n",
    "\n",
    "df_traffic = pd.DataFrame(traffic_matrix, \n",
    "                         columns=df_complex.columns, \n",
    "                         index=sorted(all_vehicles))\n",
    "\n",
    "print(\"\\nğŸ“Š Traffic count matrix:\")\n",
    "display(df_traffic)\n",
    "\n",
    "# Calculate totals\n",
    "df_traffic.loc['Total'] = df_traffic.sum()\n",
    "print(\"\\nâœ… Traffic data processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9409b",
   "metadata": {},
   "source": [
    "## 6. Basic Statistical Analysis\n",
    "\n",
    "Generate descriptive statistics, correlation analysis, and summary insights about the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555eeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of numeric data\n",
    "print(\"ğŸ“Š Statistical Summary - Numeric Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Descriptive statistics\n",
    "stats_summary = df_numeric.describe()\n",
    "display(stats_summary)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nğŸ“ˆ Additional Statistics:\")\n",
    "for measurement in df_numeric.index:\n",
    "    values = df_numeric.loc[measurement].dropna()\n",
    "    print(f\"\\n{measurement}:\")\n",
    "    print(f\"  Range: {values.min():.1f} - {values.max():.1f}\")\n",
    "    print(f\"  Mean: {values.mean():.1f}\")\n",
    "    print(f\"  Median: {values.median():.1f}\")\n",
    "    print(f\"  Std Dev: {values.std():.1f}\")\n",
    "    \n",
    "    # Site comparison\n",
    "    print(f\"  Highest: {values.idxmax()} ({values.max():.1f})\")\n",
    "    print(f\"  Lowest: {values.idxmin()} ({values.min():.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic analysis\n",
    "print(\"ğŸš— Traffic Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Total traffic by site\n",
    "total_traffic = df_traffic.loc['Total']\n",
    "print(\"\\nğŸ“Š Total Traffic by Site:\")\n",
    "for site in total_traffic.index:\n",
    "    print(f\"  {site}: {total_traffic[site]} vehicles\")\n",
    "\n",
    "# Most common vehicle types\n",
    "vehicle_totals = df_traffic.drop('Total').sum(axis=1).sort_values(ascending=False)\n",
    "print(\"\\nğŸš™ Most Common Vehicle Types:\")\n",
    "for vehicle, count in vehicle_totals.items():\n",
    "    print(f\"  {vehicle}: {count} total\")\n",
    "\n",
    "# Traffic composition by site\n",
    "print(\"\\nğŸ“ˆ Traffic Composition (%)\")\n",
    "traffic_pct = df_traffic.drop('Total').div(df_traffic.loc['Total'], axis=1) * 100\n",
    "display(traffic_pct.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site comparison analysis\n",
    "print(\"ğŸ™ï¸ Site Comparison Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comprehensive site profile\n",
    "site_profiles = {}\n",
    "\n",
    "for site in df_clean.columns:\n",
    "    profile = {\n",
    "        'EQS': df_clean.loc['Environmental_Quality_Score', site],\n",
    "        'Building_Height': df_numeric.loc['Building_Height_m', site],\n",
    "        'Noise_Level': df_numeric.loc['Noise_Level_dB', site],\n",
    "        'Total_Traffic': total_traffic[site],\n",
    "        'Pedestrians': df_numeric.loc['Pedestrian_Count', site],\n",
    "        'Land_Use': df_clean.loc['Land_Use_Type', site],\n",
    "        'Urban_Order': df_clean.loc['Urban_Hierarchy', site]\n",
    "    }\n",
    "    site_profiles[site] = profile\n",
    "\n",
    "# Display site profiles\n",
    "for site, profile in site_profiles.items():\n",
    "    print(f\"\\nğŸ“ {site} Profile:\")\n",
    "    for key, value in profile.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Identify site characteristics\n",
    "print(\"\\nğŸ” Site Characteristics:\")\n",
    "busiest_site = max(site_profiles.keys(), key=lambda x: site_profiles[x]['Total_Traffic'])\n",
    "noisiest_site = max(site_profiles.keys(), key=lambda x: site_profiles[x]['Noise_Level'])\n",
    "tallest_site = max(site_profiles.keys(), key=lambda x: site_profiles[x]['Building_Height'])\n",
    "\n",
    "print(f\"ğŸš— Busiest traffic: {busiest_site} ({site_profiles[busiest_site]['Total_Traffic']} vehicles)\")\n",
    "print(f\"ğŸ”Š Noisiest: {noisiest_site} ({site_profiles[noisiest_site]['Noise_Level']} dB)\")\n",
    "print(f\"ğŸ¢ Tallest buildings: {tallest_site} ({site_profiles[tallest_site]['Building_Height']}m)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4b108",
   "metadata": {},
   "source": [
    "## 7. Create Data Visualizations\n",
    "\n",
    "Create various plots including histograms, scatter plots, box plots, and bar charts to visualize data distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7bf7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization dashboard\n",
    "print(\"ğŸ“Š Creating Urban Geography Visualizations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "sites = df_numeric.columns.tolist()\n",
    "\n",
    "# Create a comprehensive dashboard\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Urban Geography Analysis - Site Comparison Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Building Height Comparison\n",
    "ax1 = axes[0, 0]\n",
    "heights = df_numeric.loc['Building_Height_m'].values\n",
    "bars1 = ax1.bar(sites, heights, color=colors)\n",
    "ax1.set_title('Building Heights by Site', fontweight='bold')\n",
    "ax1.set_ylabel('Height (meters)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "# Add value labels on bars\n",
    "for bar, height in zip(bars1, heights):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{height}m', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Noise Level Comparison\n",
    "ax2 = axes[0, 1]\n",
    "noise_levels = df_numeric.loc['Noise_Level_dB'].values\n",
    "bars2 = ax2.bar(sites, noise_levels, color=colors)\n",
    "ax2.set_title('Noise Levels by Site', fontweight='bold')\n",
    "ax2.set_ylabel('Decibels (dB)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "# Add value labels\n",
    "for bar, noise in zip(bars2, noise_levels):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{noise}dB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Pedestrian Count Comparison\n",
    "ax3 = axes[0, 2]\n",
    "pedestrians = df_numeric.loc['Pedestrian_Count'].values\n",
    "bars3 = ax3.bar(sites, pedestrians, color=colors)\n",
    "ax3.set_title('Pedestrian Activity by Site', fontweight='bold')\n",
    "ax3.set_ylabel('Number of Pedestrians')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "# Add value labels\n",
    "for bar, ped in zip(bars3, pedestrians):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "             f'{int(ped)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Total Traffic Comparison\n",
    "ax4 = axes[1, 0]\n",
    "traffic_totals = [total_traffic[site] for site in sites]\n",
    "bars4 = ax4.bar(sites, traffic_totals, color=colors)\n",
    "ax4.set_title('Total Vehicle Traffic by Site', fontweight='bold')\n",
    "ax4.set_ylabel('Number of Vehicles')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "# Add value labels\n",
    "for bar, traffic in zip(bars4, traffic_totals):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{traffic}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. Traffic Composition Stacked Bar\n",
    "ax5 = axes[1, 1]\n",
    "traffic_data_plot = df_traffic.drop('Total')\n",
    "bottom = np.zeros(len(sites))\n",
    "\n",
    "for i, vehicle in enumerate(traffic_data_plot.index):\n",
    "    values = [traffic_data_plot.loc[vehicle, site] for site in sites]\n",
    "    ax5.bar(sites, values, bottom=bottom, label=vehicle, alpha=0.8)\n",
    "    bottom += values\n",
    "\n",
    "ax5.set_title('Traffic Composition by Vehicle Type', fontweight='bold')\n",
    "ax5.set_ylabel('Number of Vehicles')\n",
    "ax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Multi-metric Radar Chart (using regular plot)\n",
    "ax6 = axes[1, 2]\n",
    "# Normalize metrics for comparison (0-100 scale)\n",
    "metrics = ['Building\\nHeight', 'Noise\\nLevel', 'Pedestrian\\nCount', 'Total\\nTraffic']\n",
    "site1_values = [\n",
    "    (df_numeric.loc['Building_Height_m', sites[0]] / df_numeric.loc['Building_Height_m'].max()) * 100,\n",
    "    (df_numeric.loc['Noise_Level_dB', sites[0]] / df_numeric.loc['Noise_Level_dB'].max()) * 100,\n",
    "    (df_numeric.loc['Pedestrian_Count', sites[0]] / df_numeric.loc['Pedestrian_Count'].max()) * 100,\n",
    "    (total_traffic[sites[0]] / max(traffic_totals)) * 100\n",
    "]\n",
    "\n",
    "x_pos = np.arange(len(metrics))\n",
    "ax6.bar(x_pos, site1_values, color=colors[0], alpha=0.7, label=sites[0])\n",
    "ax6.set_title(f'{sites[0]} - Normalized Metrics', fontweight='bold')\n",
    "ax6.set_ylabel('Relative Score (0-100)')\n",
    "ax6.set_xticks(x_pos)\n",
    "ax6.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax6.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Dashboard created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Plotly visualizations\n",
    "print(\"ğŸ¨ Creating Interactive Visualizations with Plotly\")\n",
    "\n",
    "# Create subplots for interactive dashboard\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Building Heights vs Noise Levels', 'Traffic vs Pedestrian Activity',\n",
    "                   'Site Comparison Radar', 'Vehicle Type Distribution'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"type\": \"scatterpolar\"}, {\"type\": \"pie\"}]]\n",
    ")\n",
    "\n",
    "# 1. Scatter plot: Building Height vs Noise Level\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_numeric.loc['Building_Height_m'],\n",
    "        y=df_numeric.loc['Noise_Level_dB'],\n",
    "        mode='markers+text',\n",
    "        text=sites,\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(size=15, color=colors),\n",
    "        name=\"Sites\"\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Scatter plot: Traffic vs Pedestrians\n",
    "traffic_totals_list = [total_traffic[site] for site in sites]\n",
    "pedestrian_list = df_numeric.loc['Pedestrian_Count'].tolist()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=traffic_totals_list,\n",
    "        y=pedestrian_list,\n",
    "        mode='markers+text',\n",
    "        text=sites,\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(size=15, color=colors),\n",
    "        name=\"Sites\"\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Radar chart for first site\n",
    "categories = ['Building Height', 'Noise Level', 'Pedestrian Count', 'Traffic Count', 'EQS Score']\n",
    "site1_radar = [\n",
    "    df_numeric.loc['Building_Height_m', sites[0]],\n",
    "    df_numeric.loc['Noise_Level_dB', sites[0]],\n",
    "    df_numeric.loc['Pedestrian_Count', sites[0]],\n",
    "    total_traffic[sites[0]],\n",
    "    3 if df_clean.loc['Environmental_Quality_Score', sites[0]] == '3+' else 2\n",
    "]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatterpolar(\n",
    "        r=site1_radar,\n",
    "        theta=categories,\n",
    "        fill='toself',\n",
    "        name=sites[0]\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Pie chart for total vehicle types across all sites\n",
    "vehicle_totals_pie = df_traffic.drop('Total').sum(axis=1)\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=vehicle_totals_pie.index,\n",
    "        values=vehicle_totals_pie.values,\n",
    "        name=\"Vehicle Types\"\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Urban Geography Interactive Dashboard\",\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Building Height (m)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Noise Level (dB)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Total Traffic Count\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Pedestrian Count\", row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"âœ… Interactive dashboard created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ed57c",
   "metadata": {},
   "source": [
    "## 8. Advanced Visualizations and Insights\n",
    "\n",
    "Develop advanced visualizations like heatmaps, pair plots, and multi-dimensional charts to uncover deeper patterns and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ec29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced correlation and pattern analysis\n",
    "print(\"ğŸ” Advanced Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create correlation matrix for numeric data\n",
    "# Transpose to have sites as rows and measurements as columns\n",
    "df_for_corr = df_numeric.T\n",
    "df_for_corr['Total_Traffic'] = [total_traffic[site] for site in df_for_corr.index]\n",
    "\n",
    "print(\"\\nğŸ“Š Site-Measurement Matrix:\")\n",
    "display(df_for_corr)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_for_corr.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('Urban Metrics Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify strong correlations\n",
    "print(\"\\nğŸ”— Strong Correlations (|r| > 0.5):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            print(f\"  {var1} â†” {var2}: r = {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f393c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced site classification and clustering analysis\n",
    "print(\"ğŸ™ï¸ Site Classification Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comprehensive site scoring system\n",
    "def calculate_urban_intensity_score(site):\n",
    "    \"\"\"Calculate urban intensity score based on multiple factors\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Building height (normalized to 0-30)\n",
    "    height = df_numeric.loc['Building_Height_m', site]\n",
    "    score += (height / 15) * 10  # Max 20 points\n",
    "    \n",
    "    # Traffic count (normalized)\n",
    "    traffic = total_traffic[site]\n",
    "    score += (traffic / 30) * 10  # Max 10 points\n",
    "    \n",
    "    # Pedestrian activity (normalized)\n",
    "    pedestrians = df_numeric.loc['Pedestrian_Count', site]\n",
    "    score += (pedestrians / 100) * 15  # Max 15 points\n",
    "    \n",
    "    # Noise level (higher = more urban)\n",
    "    noise = df_numeric.loc['Noise_Level_dB', site]\n",
    "    score += ((noise - 60) / 15) * 10  # Max 10 points\n",
    "    \n",
    "    # Land use bonus\n",
    "    land_use = df_clean.loc['Land_Use_Type', site]\n",
    "    if 'commercial' in str(land_use).lower():\n",
    "        score += 5\n",
    "    \n",
    "    return max(0, score)\n",
    "\n",
    "# Calculate scores for all sites\n",
    "urban_scores = {site: calculate_urban_intensity_score(site) for site in sites}\n",
    "\n",
    "print(\"\\nğŸ™ï¸ Urban Intensity Scores:\")\n",
    "for site, score in sorted(urban_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {site}: {score:.1f} points\")\n",
    "\n",
    "# Visualize urban intensity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Urban intensity bar chart\n",
    "scores = list(urban_scores.values())\n",
    "bars = ax1.bar(sites, scores, color=colors)\n",
    "ax1.set_title('Urban Intensity Score by Site', fontweight='bold', fontsize=14)\n",
    "ax1.set_ylabel('Urban Intensity Score')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add score labels\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Multi-dimensional scatter plot\n",
    "# X: Building height, Y: Traffic, Size: Pedestrians, Color: Noise\n",
    "heights = [df_numeric.loc['Building_Height_m', site] for site in sites]\n",
    "traffic_counts = [total_traffic[site] for site in sites]\n",
    "pedestrian_counts = [df_numeric.loc['Pedestrian_Count', site] for site in sites]\n",
    "noise_levels = [df_numeric.loc['Noise_Level_dB', site] for site in sites]\n",
    "\n",
    "scatter = ax2.scatter(heights, traffic_counts, \n",
    "                    s=[p*3 for p in pedestrian_counts],  # Size based on pedestrians\n",
    "                    c=noise_levels,  # Color based on noise\n",
    "                    cmap='Reds', alpha=0.7)\n",
    "\n",
    "# Add site labels\n",
    "for i, site in enumerate(sites):\n",
    "    ax2.annotate(site, (heights[i], traffic_counts[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontweight='bold')\n",
    "\n",
    "ax2.set_title('Multi-Dimensional Site Analysis\\n(Size=Pedestrians, Color=Noise)', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Building Height (m)')\n",
    "ax2.set_ylabel('Total Traffic Count')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax2)\n",
    "cbar.set_label('Noise Level (dB)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Advanced analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final insights and recommendations\n",
    "print(\"ğŸ’¡ Key Insights and Urban Planning Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify site characteristics\n",
    "max_score_site = max(urban_scores, key=urban_scores.get)\n",
    "min_score_site = min(urban_scores, key=urban_scores.get)\n",
    "\n",
    "print(f\"\\nğŸ† Most Urban Intensive: {max_score_site}\")\n",
    "print(f\"   Score: {urban_scores[max_score_site]:.1f}\")\n",
    "print(f\"   Characteristics: High density, commercial area\")\n",
    "\n",
    "print(f\"\\nğŸŒ± Least Urban Intensive: {min_score_site}\")\n",
    "print(f\"   Score: {urban_scores[min_score_site]:.1f}\")\n",
    "print(f\"   Characteristics: Lower density, residential area\")\n",
    "\n",
    "# Generate specific insights\n",
    "insights = []\n",
    "\n",
    "# Traffic insights\n",
    "busiest_traffic = max(sites, key=lambda x: total_traffic[x])\n",
    "insights.append(f\"ğŸš— Traffic: {busiest_traffic} has the highest traffic volume ({total_traffic[busiest_traffic]} vehicles)\")\n",
    "\n",
    "# Noise insights\n",
    "noisiest = max(sites, key=lambda x: df_numeric.loc['Noise_Level_dB', x])\n",
    "noise_level = df_numeric.loc['Noise_Level_dB', noisiest]\n",
    "if noise_level > 70:\n",
    "    insights.append(f\"ğŸ”Š Noise: {noisiest} exceeds WHO recommended levels ({noise_level} dB > 70 dB)\")\n",
    "\n",
    "# Pedestrian insights\n",
    "most_pedestrians = max(sites, key=lambda x: df_numeric.loc['Pedestrian_Count', x])\n",
    "insights.append(f\"ğŸš¶ Pedestrian Activity: {most_pedestrians} shows highest foot traffic ({df_numeric.loc['Pedestrian_Count', most_pedestrians]} people)\")\n",
    "\n",
    "# Building insights\n",
    "tallest = max(sites, key=lambda x: df_numeric.loc['Building_Height_m', x])\n",
    "insights.append(f\"ğŸ¢ Development: {tallest} has tallest buildings ({df_numeric.loc['Building_Height_m', tallest]}m), indicating higher density\")\n",
    "\n",
    "print(\"\\nğŸ“Š Key Insights:\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"   {i}. {insight}\")\n",
    "\n",
    "# Recommendations\n",
    "recommendations = [\n",
    "    f\"Urban Planning: {max_score_site} requires traffic management and noise reduction strategies\",\n",
    "    f\"Transport: All sites have MTR access - enhance bus connectivity for {min_score_site}\",\n",
    "    f\"Development: {min_score_site} has potential for sustainable low-rise development\",\n",
    "    \"Environmental: Implement noise barriers in high-traffic commercial areas\",\n",
    "    \"Pedestrian Safety: Improve walkways in high-activity zones\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ’¡ Planning Recommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ Analysis Summary Complete\")\n",
    "print(f\"   â€¢ {len(sites)} sites analyzed\")\n",
    "print(f\"   â€¢ {len(df_clean)} measurement types\")\n",
    "print(f\"   â€¢ {len(vehicle_totals)} vehicle categories\")\n",
    "print(f\"   â€¢ Comprehensive urban intensity scoring\")\n",
    "print(\"   â€¢ Interactive visualizations created\")\n",
    "print(\"   â€¢ Planning recommendations generated\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
