# Moderating top-down policy impact and supporting EAP curricular renewal: Exploring the potential of diagnostic assessment

Janna D. Fox\*

School of Linguistics and Language Studies, Carleton University, 215 Paterson Hall, 1125 Colonel By Drive, Ottawa, ON K1S 5B6, Canada

# Abstract

A Canadian university required a new admissions policy in order to increase the fairness of procedures for second language (L2) international students, admitted provisionally to the university with mandatory language course requirements. This necessitated reform of the English for Academic Purposes (EAP) curriculum, but there was no agreement between EAP teachers and university policy makers on how to proceed. Thus, policy makers imposed a new policy, top-down, which allowed L2 students to use external proficiency tests (e.g., TOEFL, IELTS) for placement in levels of the EAP program. This study examined the role that diagnostic assessment played in moderating policy impact and supporting EAP curricular renewal. Applying a mixed method approach to investigate policy impact over time, qualitative data were drawn from the $( N = 9$ ) EAP teachers throughout the 2007e2008 academic year and examined in relation to quantitative data drawn from the $n = 2 6 1$ ) L2 students, who were affected by the new policy. Diagnostic assessment was used to develop individual learning profiles, which the EAP teachers drew on both individually and as a group to inform their teaching. The results of this study suggest that on-going diagnostic assessment provided the impetus for building consensus, encouraging innovation and supporting curricular renewal, thereby moderating policy impact.

$©$ 2008 Elsevier Ltd. All rights reserved.

Keywords: curricular reform; diagnostic assessment; policy impact; learning profiles

Policy makers1 within a Canadian university intended to increase the fairness of their university’s admission procedures for second language (L2) international students, who needed to know their English for Academic Purposes (EAP) requirements before arriving in Canada. This necessitated curricular reform, but there was no agreement within the EAP program, or between the EAP teachers and the university’s policy makers, on how to proceed.

In the absence of agreement, the policy makers imposed a new policy top-down, which allowed L2 students to use external proficiency tests (e.g., TOEFL, IELTS) for placement in the EAP program. This study was occasioned by that policy decision and had two purposes: first, to examine the impact of the new policy on language teaching, and academic performance within the EAP program; and, second, to explore the potential of on-going diagnostic assessment in moderating the impact of the new policy and supporting EAP curricular renewal.

In the section below a brief summary of background is provided to situate the study within the troubling context of cutbacks and reconsiderations that currently characterizes English as a Second Language (ESL) and EAP programs in Canadian universities. It is followed by a more detailed description of the present study’s context, including the policy change which was its catalyst. The theoretical background that informed the use of diagnostic assessment as a tool to moderate policy impact and encourage curricular renewal is then briefly summarized followed by the study itself. Although data were drawn from other key stakeholders (e.g., students affected by the policy, admissions staff, program coordinators, degree program professors, etc.), this paper focuses primarily on EAP teachers’ accounts in order to examine the potential of diagnostic assessment in moderating top-down policy impact and supporting teacher innovation and curricular renewal.

# 1. Background

Tertiary-level EAP programs are in a particularly perilous position in the current educational climate in Canada. There are a number of reasons for this. First, concerns regarding cost, pressures for space, and an overall educational agenda driven by accountability have increased the scrutiny of policy makers, who are looking for programs to cut. Given the nature of EAP programs and the fact that teachers working within them do not generally have academic appointments and may teach up to 20 hours a week, there is often little evidence of program effectiveness from either accumulated statistics or published research. Indeed, in Canada, job descriptions in most universities limit the scope of work for EAP teachers to teaching alone.

The vulnerability of language programs has been exacerbated by the fact that many questions have been raised about EAP approaches in the research literature (Fox, Cheng, Berman, Song & Myles, 2006). For example, a number of researchers (Hansen, 2000; Freedman, 1999) have questioned the value of a ‘generalizable’ academic discourse (i.e., that is specific to university study, but generalizable across disciplines). They argue that academic language is embedded within disciplinary cultures. Other researchers have challenged EAP approaches because they unreflectively perpetuate dominant/hegemonic western academic cultures and dismiss other academic cultures and traditions (Pennycook, 1998). Still others have raised questions about the pedagogical approaches that have characterized EAP teaching, arguing that these approaches have under-theorized and inadequately considered curricular issues of coverage, complexity, and learning needs (Samuda, 2001). And, within EAP itself there is little agreement on what should be the appropriate emphasis in instruction (Fox et al., 2006) e Sustained content? Critical thinking? Skills/strategies? Acculturation?

Further, particularly in larger Canadian universities, language support has tended to develop in an ad hoc rather than strategic manner. As one coordinator of an ESL program in a large Canadian university noted, ‘‘I have no idea what other ESL support there is. I have no idea what’s going on elsewhere in the university. I sometimes meet teachers from ESL programs in my own university that I had no idea existed!’’ The ad hoc nature of language support has in many contexts reinforced an egg-crate model of education (Lortie, cited in Hargreaves, 1989), with EAP teachers limited to teaching within their individual classrooms, without sufficient collaboration or interaction to support curricular renewal or professional development. This model is often implicit in discussions of teacher resistance (Markee, 1993) to curricular renewal and led Hargreaves (1989:54) to his incisive criticism of the culture of teaching: ‘‘Teachers it seems are present-oriented, conservative and individualistic. They tend to avoid long-term planning and collaboration with their colleagues, and to resist involvement in whole school decision-making’’. Such was unfortunately the case for some of the teachers within the full-time pre-university ESL program in the university where the present study took place. After several failed attempts to renew this program, it was closed after 25 years of operation by university policy makers in July 2008. Rancor over program direction and curricular reform, financial concerns of the university, and the lack of evidence of program effectiveness, contributed to its vulnerability and ultimate demise.

At the same time that policy makers closed the pre-university ESL program, they revised e top-down e the policies affecting admission to the university’s EAP program. Within this EAP program, there had also been on-going debate over potential curricular reform and recognition of the need for a new admission policy for L2 international students. But, as Farrell (2000:90) points out, ‘‘Educational change of any consequence is inherently and necessarily intensely political and conflictual. It involves what people care most about . strongly held beliefs, values and visions.’’ As the need for a new policy became more urgent, discussions between policy makers and the EAP program became increasingly more difficult. In the end the policy makers acted alone.

Instituted in the 1990s, the EAP program had responded to the needs of large numbers of non-English speaking, academically qualified immigrants in Canada, who wanted to access university courses, but could not attain the required language proficiency scores on tests such as the TOEFL. To increase access, the university implemented a policy of gradual admission, which allowed academically qualified students with language proficiency below that required for full admission, to begin degree programs on a part-time basis while they upgraded their language skills by studying EAP. Students were admitted to the EAP program by taking an in-house placement test, the Canadian Academic English Language (CAEL) Assessment. Band 40 (of nine criterion/curriculum-referenced band scores) was identified as the minimum threshold required for access to the EAP program and the gradual admission option.

Throughout the 1990s the program flourished, but over time the clientele changed. Fewer immigrant students were enrolled in EAP from within Canada, and many more international students were seeking access to degree programs through the EAP option. The problem for academically qualified international students was that access to the EAP program depended on taking the local placement test after arriving in Canada. The test was not widely available abroad. Increasingly, L2 students who were eligible for the gradual admission option on the basis of their academic backgrounds and who believed their English was sufficient for the EAP program option, arrived in Canada, took the placement test, but ended up studying full time pre-university ESL (in some cases, for extended periods of time and at considerable expense) as they attempted to raise their English to the level required for the EAP program.

When discussions with the EAP program did not lead to consensus, the policy makers applied the following logic in defining the new EAP admission policy:

1) International students need to know in advance of their arrival in Canada if they are eligible for the EAP option, and how many EAP courses they will be required to take;   
2) Proficiency test scores are used for admission to degree programs based on cut-scores derived from concordance tables for internationally available standardized tests, e.g., IELTS and TOEFL (PBT, CBT and iBT);   
3) Therefore, concordance tables can be used to establish cut-score comparability across these tests for placement in different levels of the EAP program.

Further, because pre-university ESL was no longer being offered, but students in the highest level of the pre-university ESL program had typically managed to acquire sufficient English over time and enter the EAP program, the policy makers also decided to lower the threshold requirements for access to the EAP option.

When the new policy was implemented, both policy makers and EAP program personnel were concerned about the potential for negative impact on language teaching and learning. Their concern prompted the present study. It received the full support of both policy makers and language teachers alike because of the research approach, namely to use diagnostic assessment to provide on-going, systematic, and comprehensive information to both groups about the learning progress of students admitted on the basis of the new policy.

This study reports on the ‘uptake’ by the EAP teachers of the diagnostic assessment provided to them on an on-going basis during the first year of policy implementation. Although the language teachers considered here routinely engaged in formative and summative assessment, as Alderson (2007: 38) points out, diagnostic assessment ‘‘is a much neglected area.’’ He notes that although diagnostic assessment is frequently mentioned, it is little understood and often confused with other assessment practices such as placement testing: ‘‘Much clearer thinking is needed in our field to define what we need to know in order to be able to provide adequate diagnoses for learners, on which useful feedback and advice can be given to assist with language development’’. In recent years, clear thinking with regard to diagnostic assessment has been increasingly evident, for example, in the work of Abbott (2007) or Jang (2005), on sub-component skills in reading; Moore and Morton (2005) on university writing tasks, sources and rhetorical functions; Read (2000) and Alderson (2005), on grammatical and lexical knowledge and use; Stansfield and Winke (2008), on language aptitude testing. Other work of relevance is found in Buck and Tatsuoka (1998) on attributes of listening; or Do¨rnyei (2005) on self-assessment. Growing interest in diagnostic assessment has been accompanied by recognitions of issues within the field of EAP and the potential of new directions (Hyland & Hamp-Lyons, 2002), increased use of individual learning profiles (Shohamy, 1992) and learning checklists (Banerjee & Wall, 2006).

Alderson (2007: 38) also emphasized the importance of complementing our understanding of diagnostic assessment by carefully examining teacher accounts of assessment, ‘‘for insight into what can be diagnosed .what facilitates and enhances learning.’’ Rea-Dickins (2006), reporting on a study of teachers’ classroom assessment practices, argued for assessment dialogues in supporting teacher innovation and learning. Similarly, Colby-Kelly and Turner (2007) argue for the usefulness of an assessment bridge, which fuses assessment to teaching and learning. They found that teacher justifications of on-going assessment were linked to learner progress. There is also evidence from the curricular reform literature that such teachers’ accounts may be ‘‘leverage points’’ (Fox, 2004: 1) for teacher innovation and change. As Foucault (1969: 209) surmised, educational change involves ‘‘transformation in social practice, perhaps also in neighboring practices and their mutual articulation.’’ Lemke (1995: 31) following Foucault, noted that ‘‘discursive change is cultural change [which] requires that a social community change its ways of speaking and doing.’’ Thus, curriculum itself may be viewed as a kind of rhetorical accomplishment (Fox, 2004), and curricular renewal may be observed in changes in the ways a community speaks about what it does and engages in doing it. In the present study, teachers’ accounts and use of diagnostic assessment were explored in order to address the following research questions:

1) What was the impact of the top-down policy change on language teaching and academic performance? 2) Was there evidence in the teachers’ accounts that diagnostic assessment moderated impact and supported curricular renewal?

# 2. Method

In order to address the research questions, a mixed method, concurrent nested design (Cresswell, Clark, Gutmann & Hansen, 2003) was applied, in which qualitative and quantitative data were collected simultaneously in relation to each of the research questions. As Cresswell et al. (2003: 229) point out, ‘‘unlike the traditional triangulation design, a nested design has a predominant method that guides the project.’’ In the present study, priority was accorded to the qualitative accounts of the EAP teachers, collected in meetings, e-mail exchanges, and from semi-structured interviews over the year. Analysis of the interviews was considered in relation to analysis of the quantitative survey data, drawn from: diagnostic test performance, student academic performance over the term, background data provided by the university registrar, and self-assessment questionnaire data provided by the students themselves. The interview data from the teachers allowed for the identification of emergent themes in characterizing the impact of the policy change. These were examined in relation to the quantitative analysis of the survey data. Thus, the design was iterative: ‘‘characterized by a time sequence in which study results achieved a greater depth and richness through successive iterations of the study and reformulations of study findings’’ (Tashakkori & Teddlie, 1998: 23). In answering the research questions that guided the study, quantitative data were given less priority. They were collected alongside the qualitative data but embedded, or nested, within the predominant method and continuously evaluated in relation to the qualitative data analysis.

# 2.1. Participants

Four EAP teachers (two teaching basic level classes; and two teaching both intermediate and advanced classes) were interviewed at intervals throughout the year. The 9 EAP teachers in the program, their Coordinator, and Director also participated in meetings and interacted with the researcher throughout the period of the study. $n = 2 6 1$ students who were registered in the EAP program in the academic year 2007e2008, participated in the quantitative component of the study. Table 1 provides additional demographic information about these students.

# 2.2. Analysis

Interview data (and data collected at meetings, through e-mail communication, informal chats, etc.) were analyzed recursively using a modified grounded theory approach (Charmaz, 2006). Interpretations were recorded in the form of memos, which were checked on an on-going basis in relation to both newly collected and previously considered data. Findings were summarized for and reported to the teachers repeatedly during the year, which resulted in more data for analysis. When no further gaps remained, memos were organized in higher-order categories (Strauss & Corbin, 1998; Charmaz, 2006). In order to insure coding consistency, randomly selected interviews were separately coded and analyzed by three independent researchers. Overall coding consistency was calculated at $r = . 9 5$ . Findings from these data were examined in relation to the results of quantitative analysis, which included descriptive statistics, correlation, factor, and multiple regression analysis of the 261 students’ placement test results, academic performance and selfassessment questionnaires (see below).

Table 1 Demographic information for EAP student population $( n = 2 6 1$ ).   

<html><body><table><tr><td>Variables</td><td>Percentages</td></tr><tr><td>1. Gender</td><td></td></tr><tr><td>a. Female</td><td>41%</td></tr><tr><td>b. Male</td><td>59%</td></tr><tr><td>2. Age</td><td></td></tr><tr><td>a. 17-19</td><td>18.3%</td></tr><tr><td>b. 20-22</td><td>54.2%</td></tr><tr><td>c. 23-25</td><td>18.3%</td></tr><tr><td>d. 26+</td><td>9.2%</td></tr><tr><td>3. Most commonly spoken first languages</td><td></td></tr><tr><td>a. Chinese (including Mandarin &amp; Cantonese)</td><td>51.6%</td></tr><tr><td>b. Arabic</td><td>13.9%</td></tr><tr><td>c. Farsi</td><td>8.2%</td></tr><tr><td>d. Persian e. Other</td><td>2.5%</td></tr><tr><td></td><td>23.8%</td></tr><tr><td>4. Student status</td><td></td></tr><tr><td>a. Undergraduate student</td><td>92.5%</td></tr><tr><td>b. Graduate student</td><td>5.0%</td></tr><tr><td>c. Other</td><td>2.5%</td></tr></table></body></html>

# 2.3. Instruments

# 2.3.1. The placement test (establishing the diagnostic baseline)

There were two initial constraints on the research study. First, the study was approved in August for implementation in early September and offered only a limited window of access to students, whose programs could not be disrupted by the study; and, second, there were no readily available, appropriate EAP diagnostic tests. Thus, the decision was made to use the locally available placement test to describe the baseline skills and strategies of the students affected by the new policy. The test was standardized, validated and most importantly, criterion-referenced to the incremental developmental hierarchy of skills and strategies within the EAP program. Before administering the test, it was systematically analyzed for sub-component skills using test specifications provided by the developer, relating them to the research literature and validating them through verbal protocol analysis of the verbal accounts of tasks and items on the test, provided by 3 researchers and 4 test takers. Specific details are provided in the section below.

2.3.1.1. Developing the placement test as a diagnostic tool. First, sub-component skills that have served as useful diagnostic indicators of academic language development were identified in the research literature and summarized in lists [Appendix A]. Such diagnostic indicators are available for academic reading, for example, in the work of Jang (2005), Banerjee and Wall (2006) and, Abbott (2007). Next, sub-component skill indicators (e.g., retrieve key details, match verbal information with visual information, etc.) that repeated across these sources were clustered together under category headings that defined them (e.g., skim text for overview, draw inference, etc.). Subsequently, the test specifications of the placement test were analyzed in relation to the categories and added to the lists. In some cases, categories did not perfectly map onto the placement test specifications. When this happened, they were listed separately. In the final synthesis, placement test specifications were highlighted in relation to categories and sub-component skill clusters that had been identified, and related to items/tasks on the test [Appendix A].

In order to validate the item - sub-component skill relationships, 3 researchers and 4 students provided verbal accounts while taking the placement test. Although there was no universal agreement across the respondents, there were noticeable trends in response, as was the case in the work of Jang (2005) and Abbott (2007). For example, one reading comprehension question on the test asked students to explain a term used in the reading e ‘‘greenisms’’. The four distracters were: a) officers and members of environmental organizations; b) beliefs or ideologies characterized by environmental concerns; c) investment in environmental causes; d) laws that protect the environment. The researchers and two of the four test takers correctly identified b) as the answer, explaining, ‘‘well here it’s because of the ism’’ (test taker 1). One of the test takers who incorrectly answered said, ‘‘I don’t know this word. It’s new so I’ll have to just guess a), because the article is a lot about organizations.’’ (test taker 3) Thus, this item was identified as evidence of the sub-component ability to comprehend words on the basis of their linguistic parts.

2.3.1.2. Marking consistency. The tests were marked for overall scores and sub-skill scores in reading, listening, and writing by two trained raters, monitored for consistency on an on-going basis by the testing unit within the university. Two independent researchers marked the tests for diagnostic information regarding specific sub-component skills. In order to ensure consistency, $n = 3 0$ randomly drawn tests were separately marked by the researchers. Inter-rater reliability coefficients were calculated with Pearson’s r-values ranging from .90 (for reading) to .93 (for writing), indicating an acceptable degree of scoring consistency. Speaking was assessed by the teachers themselves according to a negotiated rubric and recorded on the learning profiles by the teachers.

# 2.3.2. The learning profile

[Appendix B] initially provided EAP teachers with specific information about the baseline sub-component abilities of each of the students they were teaching in relation to classes, levels and the incremental, developmental criteria, which defined the program itself. The learning profile was the most important instrument developed for the study. Its design was shaped by consensus, as the teachers considered what information would be most useful in informing their teaching. In addition to the information from the placement test, self-assessment questionnaires and student background, the learning profile included: criterion-referenced developmental analysis, targeted instruction recommendations for students with under-developed capability in sub-component skill areas, and summaries of teachers’ actions and learning outcomes including on-going teacher-developed diagnostic test results. [See Appendix B for a sample learning profile].

# 3. Findings and discussion

Findings relating to the research questions are discussed separately within a chronological framework that reflects their emergence in the analysis of the data. For example, although comments regarding the misplacement of students were made throughout the period of the study, they were far more evident at the beginning and thus are discussed first in relation to the first research question.

3.1. Research question one: what was the impact of the top-down policy change on language teaching and academic performance?

Qualitative analysis resulted in the identification of five key categories, which were further validated by quantitative analysis, namely, evidence of: 1) misplacement; 2) negative impact of the new policy on teaching; 3) teacher uptake of diagnostic information (pro-active or positive); 4) teacher uptake of diagnostic information (passive or negative); and 5) student motivation vs. proficiency in academic success.

# 3.1.1. Evidence of misplacement

By mid-September, at the beginning of the 2007e2008 academic year, EAP teachers were reporting initial negative impact from the new policy. Table 2 provides an overview of the distribution of students in the EAP program by level at the beginning of the year. The first substantive category identified from analysis of the teachers’ accounts related to the diverse abilities they were facing in their classes. These responses were categorized as evidence of misplacement.

The teachers reported that the new policy had resulted in an exceedingly problematic mix of abilities within EAP class levels: ‘‘There are huge differences in ability within the class e particularly in writing and comprehension of written and oral instructions’’ (Teacher 1). Another teacher commented, ‘‘Overall the problem is that there are extreme differences in ability within the class e much less consistency in terms of ability’’ (Teacher 3). Her comments were very similar to those of another teacher who remarked, ‘‘well the most obvious thing was the huge discrepancy between people at the top and people at the bottom of the class’’ (Teacher 4). The concerns expressed by the teachers were most acute at the basic level of the program: ‘‘This [the new admission policy] muddied the boundaries of what an introductory/basic EAP class should be’’ (Teacher 3).

The teachers’ reports were confirmed by the quantitative analysis of the Sub-component skills assessment undertaken at the beginning of the year (Fig. 1).

Table 2 Participants by EAP Level.   

<html><body><table><tr><td>Level</td><td> Number</td></tr><tr><td>EAP 1300 (basic)</td><td>79</td></tr><tr><td>EAP 1500(intermediate)</td><td>110</td></tr><tr><td>EAP 1900 (advanced)</td><td>72</td></tr><tr><td>Total</td><td>261</td></tr></table></body></html>

Although writing (Fig. 1) presented the most coherent pattern of increasing skill across the levels of the program, it was still the case that $2 5 \%$ of the students studying at the advanced (1900) EAP level had skills in writing that were comparable to students at the basic (1300) EAP level. Of concern as well was the extreme range of writing skills within the class levels. Based on scores out of 100 on the diagnostic writing scale, students ranged from 10 to 60 at the basic level; 15 to 65 at the intermediate level; and 30 to 80 at the advanced level in writing. At the basic level, for example, students with minimal capability in writing (scoring 10e20 on the diagnostic writing scale) had been placed in the same class as students with considerable capability in writing (scoring 50e60). The situation at the advanced level was equally troubling. Students with only 30 on the diagnostic writing scale had been placed in the same classes with students who attained 80 on the writing scale. In addition, there were large numbers of outliers (see the scatter at the top of Fig. 1), particularly at the intermediate level, indicating that the students’ writing proficiency had been underestimated by the tests. The patterns for reading, listening and speaking were equally concerning.

Further confirmation of particular problems at the basic level was available from the analysis of skill comparability across the tests used for placement.

As discussed above, test scores set for admission to EAP levels were derived from concordance tables. Fig. 2 presents analysis of the sub-component writing scale in relation to placement by test at admission. Although the tests presented a stair-step pattern of increasing ability from lower to higher proficiency, it is important to note the differences between IELTS-placed students and those placed by TOEFL iBT. IELTS-placed students were considerably lower as a group than either the TOEFL iBT or the local/CAEL-placed students e particularly at the basic level of the program. This pattern was repeated across all of the sub-component skills, suggesting problems with the concordances. On the whole, IELTS-tested students presented a profile of skill competency that was lower than either the TOEFL or the CAEL groups.

![](img/f144267c2b4fa3df3a84481639e391e6f3e841efccdea7eca56e5d7e04cd6af3.jpg)  
Fig. 1. Diagnostic Writing Scores by EAP Level $( N = 2 6 1$ ).

![](img/f94863f95680ed9c78e89bfd605ed1eefb0928250f7f532d60d9b18fb0fc4205.jpg)  
Fig. 2. Test at admission and EAP placement by total scores on the Diagnostic Writing Assessment $N = 2 6 1$ ).

# 3.1.2. Evidence of negative impact on teaching

By early October there was considerable evidence from interviews and meetings with the teachers of negative impact from the new policy. These responses were summarized by the category negative impact on teaching. Teachers reported that the range of abilities within their classes was undermining both the effectiveness of their teaching and the quality of their students’ learning. Their responses were documented in numerous memos that summarized specific ways in which teaching had been undermined by the new policy: ‘‘I have to work harder and they have to work harder. It [the range of ability] disadvantages not only the lower people, but also the higher ones, because I’m not able to move ahead as fast as I should if there hadn’t been such a divide between those who can and those who can’t’’ (Teacher 2). Another teacher reported similarly, ‘‘I feel I disadvantage my stronger students; I feel I am teaching below the level required. I have to repeat myself. I have to ‘teach down’ e to spend so much more time on housekeeping e protocols, etiquette, discipline e [this is] one of my most frustrating teaching experiences.’’ (Teacher 3) Yet another teacher reported, ‘‘Because so many of them were struggling, it was a catch 22 e because they were so weak they didn’t (or couldn’t) do their homework; because they were weak they arrived late, things are getting worse and worse for them as the term progresses. It is very frustrating e for them and for me’’ (Teacher 4).

Numerous memos were also recorded with regard to changing group dynamics within the EAP classes. Teachers reported that students tended to group themselves by ability: ‘‘usually the best students sat together; and the worst sat together’’ (Teacher 8). Another teacher remarked, ‘‘It [the class] divided along language lines and ability so I had to keep trying to mix them up, so the same ability group was not always working together . so the lower students were not always clustering together.’’ She observed that her strategy of ‘‘mixing ability groups or putting weak students with strong ones’’ was in fact a useful one. ‘‘It did work quite well if weak students got friendly with another group of stronger students who could help them.’’ Many times, however, the combinations she tried didn’t work. ‘‘Eventually in most cases, I just let them do what they wanted. So I had groups of very weak students working together. I knew it wasn’t the best strategy for them, but I could only do so much’’ (Teacher 1).

In addition, teachers’ evaluation practices appear to have been somewhat undermined by the presence of such diverse abilities within a class. Many teachers expressed uncertainty over how to evaluate students with such different abilities in the same class. ‘‘I know how de-motivating failure is, so how do I evaluate such a diverse group of students? The low ability students . some of them are really working hard. Should I reflect ‘effort’ in the marks I give them or apply a strict standard? I want to be fair to all the students (high and low). But I haven’t figured out how to do this’’ (Teacher 4). Another teacher expressed fear that program standards would be undermined: ‘‘It’s going to create a domino effect throughout the program. I think the teachers will fail only a certain percentage of students. That means the teachers will pass weak students up to the next level, but those students will be far below the level expected. In the long-term, this will really be destabilizing’’ (Teacher 7). Mullen (2008) also reports on uncertainty regarding best practices in evaluation and the undermining of standards in her discussion of another Canadian university’s EAP program where the existing placement test was replaced by TOEIC as a result of central policy decision-making.

Having found evidence of negative impact on teaching and learning as a result of the new admission policy, the focus turned to the second research question.

3.2. Research question two: was there evidence in the teachers’ accounts that diagnostic assessment moderated impact and supported curricular renewal?

In early September formal and informal meetings took place with the program Director, Coordinator and EAP teachers. It was agreed that the placement test would be administered to all of the EAP students during the first week of class. Although the potential benefits of more diagnostic information were universally recognized, there were issues relating to how diagnostic information (and the results of the placement test) would be communicated. Through negotiation with the teachers, it was decided that information should be summarized in an individual learning profile for each student in the program. Some teachers were concerned that a student might be negatively affected if they learned, for example, that their language ability was significantly lower than that of other students in their class. It was therefore agreed that the learning profiles would be provided to the teachers and they would individually decide how best to use the information. Their interpretation and use of the diagnostic information was monitored in relation to research question two.

Brown (1995: 249) identifies the delphi technique, ‘‘a meeting or series of meetings where the task is to reach consensus,’’ as a useful approach in building agreement ‘‘on the goals and objectives of the curriculum’’. Application of the delphi technique produced initial positive outcomes in the present study. The task of reaching consensus on the design of the learning profile to report on the results of the diagnostic assessment resulted in key, substantive and non-threatening discussions of the programs’ goals and objectives. As one teacher put it:

We always seemed to get our backs up when we tried to talk about the program. It always came down to hurting someone’s feelings. It was all so personal. If you suggested something or disagreed, people thought you were somehow criticizing them personally . but, negotiating the learning profile allowed us to identify what was really important to know about our students and ultimately we agreed on that. And, I think that has created a new sense of what we share. (Teacher 8)

Negotiation and use of the learning profile provided a neutral site in which to collaborate on the definition of program goals, document outcomes of new teaching approaches, and potentially support curricular renewal.

By mid-October initial diagnostic findings were summarized on individual learning profiles and shared with the teachers. They formed the basis of on-going discussions throughout the year [Appendix B].

3.2.1. Evidence of teacher uptake of diagnostic assessment (pro-Active/positive)

Teachers were provided with diagnostic information contained in their students’ individual learning profiles, but precisely how they should use it was left to each teacher to decide. Once the flow of diagnostic information began, actions undertaken by the teachers were categorized under the category teacher uptake (pro-active/positive).

The following e-mail was circulated after the first group meeting in which the individual learning profiles and statistics on the overall program were shared and discussed:

Subject: Learning Profiles, Explanations and Statistics

# Colleagues:

I’ve put together a version of the profile that I think could be used with my students, possibly as part of a class activity. I’ve included:   
1. diagnostic assessment skill scores (this is what the students have been waiting for) and level means (for comparison to others, at the same level of the EAP program);   
2. self-assessment accuracy (the indicator of whether students are being too hard on themselves/too optimistic) and an explanation;   
3. recommended priorities for language development (an edited version of that section of the profile so that students can better understand and apply it);   
4. my own mid-term comments (to tie the diagnostic assessment into their current situation in the course for the students). (e-mail, October 14, Teacher 6)

Teacher 6 invited response from the other teachers, and there was an animated exchange extending over several days as the teachers discussed how they would communicate information to their students. Five teachers actively participated in this e-mail exchange.

By early November, most of the teachers were reporting that that they had talked to each of their students individually about the diagnostic information in their learning profiles. At the November EAP meeting, as a group, the teachers began to talk about focusing their instruction to address specific needs and enhance development in areas of strength. Several teachers reported on new classroom activities they had undertaken to develop specific subcomponent skills. At the end of the meeting, one of the teachers commented, ‘‘I think we’re all targeting certain things. I know I’m directing my students’ focus more precisely’’ (Teacher 1). Teachers also reported developing their own diagnostic tests, drawing on the sub-component skill analyses that were undertaken for the placement test. ‘‘I’m really looking closely now within these areas. I give a short diagnostic test each week. This helps me see if progress is being made.’’ (Teacher 4) When asked where she was finding these tests, she reported, ‘‘Well some are available on the internet for the Academic Word List for example and I just adapt them, you know? I’m saving all of this, of course and will use and improve them as I learn more about the quality of information they are giving me.’’ (Teacher 1)

Following on Teacher 1’s observation, there was evidence that some of the teachers were beginning to change their overall approach to teaching: ‘‘I find myself doing things I would never have thought of doing before . like knowing that [name of student] had these problems, basic problems, with word attack skills? I’ve never worked on such a micro level before with a student. But it’s amazing, I see changes in her understanding . no one had ever pointed out these features of the language to her before. I would never have guessed that this would work with her, and I wouldn’t have felt, you know, comfortable doing this because it goes against the basic philosophy of how we teach.’’ (Teacher 3) Another teacher reported, ‘‘The biggest change is in the range of what I do and how I do it. I had always worked with groups in my EAP classes, but my groups are so much more specific now. We still do general EAP using a thematic approach, but I also use the diagnostic information e so, for example, I actually worked with two students last week for an hour on basic pronunciation contrasts e imagine!’’ (Teacher 5)

By the end of the first term, mid-December, a number of teachers were reporting systematic and comprehensive approaches to the use of the diagnostic information contained in the learning profiles. For example, Teacher 4 reported that she had actively incorporated more diagnostic assessment on her own in her course and had used her Teaching Assistant (TA) in a new way. ‘‘I used all of the diagnostic information available to identify areas where individual students needed more support. After reviewing the diagnostic information with me, the TA interviewed each student looking at the results with them. Based on negotiations, but guided by the diagnostic informatione the TA set up groups which met every week.’’ These groups targeted specific areas for focused language development. She explained, ‘‘We worked on reading strategies e if that was the focus e or vocabulary, or sentence combining e a whole lot of things to support their learning.’’

She pointed out that the students in her classes put in extra time, outside of class as a result. She reported that most of her students were enthusiastic about the extra group activities, but ‘‘some were not motivated.really only a few were not motivated.’’ She accounted for the high level of participation as an outcome of her approach. Using the evidence accumulated from diagnostic assessments in their learning profiles, linking this to their own self-assessment, and relating both to their desire to improve their English quickly and efficiently so that they could study full time, proved to be a powerful motivating factor. ‘‘They started out by setting goals with the TA and then reported on how things were going at regular intervals and we adjusted or fine-tuned as necessary. At the end we looked again at what the goals had been, what they had done, and how much impact there had been.’’

Teacher 4 used other resources at the university as well to support this strategy. ‘‘I also involved the peer support resources and linked my students to peer helpers in the library.’’ When asked if she had observed any differences in her students as a result of the increased use of diagnostic assessment, she replied: ‘‘I saw a marked difference in my students as a result. They participated in class discussions more actively, asked questions; their research presentations were really quite impressive. They told me their reading was improving. I saw evidence of this in better writing.’’

# 3.2.2. Evidence of teacher uptake of diagnostic assessment (passive or negative)

Although most teachers in the program actively applied the information from the diagnostic assessment, two did not. As one teacher reported, ‘‘I handed the reports to my students and gave them time in class to review and discuss them. I really haven’t followed up with them since then’’ (Teacher 8). One teacher did not share the information with students in any direct way: ‘‘I thought the information might be more hurtful than helpful. I liked having the information for my own use e don’t get me wrong e it was helpful to see how my students and my course compared statistically to the other students in the program, but I don’t think it influenced what I did in the course or how I did it’’ (Teacher 9).

Teacher 3 commented: ‘‘I know what you are trying to do and I really appreciate the diagnostic information, and I’m using the learning profiles, but in some cases it’s really depressing to see how little knowledge of English some of my students actually have. They just don’t belong at this level e well, they should never have been allowed to enter the program in the first place’’ Teacher 6 was more negative: ‘‘I think there will be many more failures as a result of the new policy.’’

Using SPSS version 16, differences between test at admission and academic outcomes in the EAP courses (i.e., grades in EAP and degree-related courses) were analyzed with Spearman’s rho and alpha set at $< . 0 5$ . A weak but significant relationship $( r = . 3 3 4 ^ { * } )$ ), was found between IELTS-admitted students and EAP final marks. Further examination of this finding revealed why the relationship was significant.

At the end of the first term, there were significantly more failures at the basic level of the program amongst those students who had been admitted with IELTS scores of 5, than those admitted by virtue of the other tests, confirming not only the reports of the teachers over the term, but also providing additional evidence that the concordances set for IELTS were problematic. Lowering the threshold had had a negative impact that was not moderated by diagnostic assessment (Fig. 3).

Indeed, students admitted to basic level EAP on the basis of an overall IELTS score of 5 were $2 5 \%$ more likely to fail their course than those admitted with 5.5. The lowest mark received amongst those students who were admitted with an IELTS band score of 5.5, was 5 or C. Marks are awarded in this program on the basis of a 12- point scale, ranging from 0 for $\mathrm { F }$ to 12 for $\mathbf { A } +$ , with a 6 or B- required for successful course completion. Although the numbers are small $( n = 2 1$ ), the relationship between lowered entrance scores amongst the group admitted in the Fall of 2007 and higher failure rates was clear. An increased pattern of failure was only evident at the basic level. At the other levels the percentage of failures remained fairly constant. It remains to be determined whether the ‘‘domino effect’’ alluded to by Teacher 7 was at play here as weaker students may have been awarded passing marks. There was some evidence of this: ‘‘I found it difficult to fail her, you know, relatively speaking she was much better than some of the other students in the class’’ (Teacher 6). Overall, the statistics regarding the numbers of students passing or failing were similar to those of previous years, with the exception of the basic level IELTS group.

![](img/d3f86605d86377c777b12c45b7dd7f9aa2ec0b6d833e329c61cdf473cb598f0b.jpg)  
Fig. 3. ESL marks by IELTS band score at admission in EAP basic level courses $( n = 2 1$ ).

Interestingly, there were no significant relationships found between test at admission and performance in degree related courses. There are two key reasons for this: 1) students at the lowest English ability levels typically registered in only their EAP course and did not include a degree-related course in their first term; 2) most students registered in mathematics courses where they typically excelled. As one student explained, ‘‘the content of my mathematics course is what I learned two years ago in high school. It’s really easy for me. I don’t need English to get an A.’’ Another student, when interviewed about her $\mathbf { A } +$ in mathematics reported, ‘‘Yes. I do really well in math. we studied this before in my country.’’ Whenever possible, students strategically chose degree-related courses to improve their grades. Learning did, however, take place, as one student noted: ‘‘I’m really good in math so it’s not really interesting e my course that is . but I am learning a lot about Canadian students and how they relate to professors and I am making some friends. They know I’m good at math and that makes me. Well I can use that, you know. I have a study group with some Canadian students in that class. I learn a lot of English that way.’’ Strategic choices on the part of students played a role in their academic success. However, strategic choices were less important on the whole than the key factor of motivation.

# 3.2.3. Motivation vs. proficiency

Motivation was a key factor in student success at all levels of the program e overriding the negative or positive influence of language proficiency in a number of cases. This was particularly so at the basic levels. For example, Teacher 2 commented: ‘‘His problem was that he didn’t put effort into preparing for things, so for example a writing task being assessed in class, I would give them tables to complete so that all they had to do was synthesize from the table and summarize in class. His issues were more ones of motivation and organization e not language!’’ She compared this student with another in the same class, who had passed in spite of very low language proficiency:

She worked very hard.[she was] so quiet in class and came in with really weak language .IELTS 5 .really low in comparison with the typical student in a basic level. She set goals, and she really went with it . was absolutely determined to succeed and she did . in fact she worked very hard with another weak student and pulled him up too! (Teacher 2)

Although it is beyond the scope of this paper to consider the quantitative analysis in detail, it confirmed the important role that motivation played in students’ academic performance. When questionnaire responses were regressed against marks in both degree-related courses and EAP courses, motivation to work hard was a significant predictor of final grades in both.2

# 4. Conclusion

This paper provides an initial analysis of the impact of policy changes within one Canadian university. The study examined the accounts of EAP teachers for: 1) evidence of impact on their teaching and 2) the potential of diagnostic assessment to moderate impact and support curricular renewal.

Analysis of the teachers’ accounts of policy impact suggested an increased amount of variability within class levels and across skills, and an increased number of misplaced and at-risk students. Their accounts were confirmed by quantitative analysis. Diagnostic assessment appears to have moderated the negative impact of the admission policy to a degree. Of course, in a naturalistic study such as this, it is impossible to tease apart the specific contribution of diagnostic assessment in the success and retention of the EAP students affected by the policy. This is a limitation of the study’s design. However, although teacher uptake varied, most of the teachers actively used the additional diagnostic information about their students to target their students’ specific needs and develop their strengths. There is evidence that the use of on-going diagnostic assessment supported teacher change, innovation and curricular renewal, even in the challenging educational context considered here. Systematically collecting evidence of student development over time in, for example, learning profiles, can also provide evidence to policy makers of program effectiveness. It is important to note that in the context of this study, policy makers are again actively consulting with the EAP program. As a result, students will no longer be admitted to the basic EAP course on the basis of an overall IELTS average of 5.

The findings of this study prompt a number of observations. The results suggest that diagnostic assessment leading to targeted instruction that focuses increased attention on individual student needs and strengths is both efficient and effective in supporting language development, and works well in tandem with traditional EAP approaches. This is consistent with the findings of Green and Weir (2004: 488) who emphasized the desirability of precisely targeted formal instruction resulting from ‘‘detailed diagnostic information.’’ This study also suggests the important role that students’ motivation plays in their academic success or failure. And yet, motivation is not assessed as part of the admission process. Taking a more diagnostic approach would provide information to both teachers and learners on the role that factors other than language proficiency play in academic success.

One important outcome of this study has been growing expertise amongst the EAP teachers in the development and use of individual learning profiles and diagnostic assessment. Individualized approaches are well documented in the EAP literature (Banerjee & Wall, 2006). Not only are such approaches clearly of benefit to students, they provide evidence of learning over time and verification of program effectiveness for policy makers, who may at times question the necessity of such programs. Negotiation of the learning profile and its on-going use was an important neutral and non-threatening mediating tool in encouraging innovation and renewal. It summarized key diagnostic information, contained recommended actions that might be taken, and documented teacher actions in response. It was ultimately the principal impetus for teacher discussion, collaborative exchange, innovation and curricular renewal.

Finally, there was considerable evidence here of instability in the score concordance relationships between the in-house placement test (CAEL), TOEFL iBT, and IELTS. The language ability of the IELTS group of students was consistently lower across EAP skills and levels. This suggests that concordances published by test developers are not uniformly accurate across tests, scale points and subtests. To be fair to the test developers, ‘‘providing information to test taker and test score users about the abilities of test takers at different score levels has been a persistent problem in educational and psychological measurement’’ (Gomez, Noah, Schedl, Wright, & Yolkut, 2007: 417). Still, test developers should be held to a higher standard and address the simplistic and reductive uptake that characterizes test users’ interpretations of concordance tables. Further, there is considerable misunderstanding on the part of policy makers about test purpose. Increasing numbers of institutions are using proficiency tests for placement purposes (Mullen, 2008), but proficiency tests do not test the incremental developmental changes that define learning within an EAP program. As Chalhoub-Deville and Turner (2000) have argued, best practice in standard setting, the use of language proficiency tests and elaboration of admission policies necessitate the local and situated study of test impact. This study provides one such example.

# Appendix A. Identifying and validating sub-component skills for diagnostic assessment:

1. Summary of sub-component reading skills from the research literature:

# Abbott (2007)

Break down words into smaller parts.   
Scan text for specific items from the question.   
Identify or formulate synonyms/paraphrase of items from text.   
Match verbal info with visual info. Match key vocabulary in text and question.   
Apply knowledge of grammar, syntax, punctuation, etc.   
Local context used to understand a particular word/phrase in text.   
Skim text, identify major points, summarise main context.   
Connections formed between parts of text, info synthesis.   
Draw inferences, conclusions, hypotheses based on text.   
Speculate using background knowledge.   
Discourse format as a clue to answering question.

# Banerjee and Wall (2006)

Skim text for overview.   
Scan and retrieve key items.   
Identify main points in text.   
Reprise main points in text or speech.   
Synthesise info from multiple sources/parts of the text.   
Apply and understand the info.   
Critical interpretation, relating to other sources/knowledge.   
Ability to cope with heavy reading load.   
Understand visual displays of data.   
Synthesise data from multiple sources.   
Able to use visual data to solve textual problems.   
Critical analysis, evaluation of argumentation in text.   
Support or refute argumentation in text.

# Jang (2005)

Use contextual clues to deduce meaning of a specific word/phrase. Draw on background knowledge to deduce word meaning (out of context). Follow logical arguments across successive sentences, by understanding lexical and grammatical linking tools. Skim paragraph for literal meaning of info. Read widely but selectively to recognize implicit info. Infer writer’s purpose and/or major arguments by drawing on background knowledge and implicit info in text. Determine validity of information in text. Discriminate major ideas from supporting details. - Recognise argument structures such as compare/contract, cause/effect, rhetorical structures.

2. Synthesis of Subcomponent Abilities in Reading in relation to CAEL specifications:

2.1 Skim text for overview (Abbott, Banerjee & Wall, Jang)

- Identify main points (Banerjee & Wall)   
- Identify literal meaning of main points (Jang)   
- Use clues from the question to identify main points (Abbott)   
- Discriminate major arguments from supporting details (Jang)   
- Skim text, identify major points, summarise main context (Abbott)   
- CAEL specification: get gist

2.2 Use background knowledge. (Abbott, Banerjee & Wall, Jang)

- .to speculate beyond the text (Abbott) - .for critical interpretation (Banerjee & Wall) - .to deduce word meaning (Jang) - .to infer writer’s purpose and/or arguments (Jang)

2.3 Follow arguments throughout text, broader reading (Jang, Banerjee & Wall, Abbott)

- Apply knowledge of grammar and lexical linking devices (Jang)   
- Synthesise material from multiple sources or parts of text (Banerjee & Wall)   
- Form connections between parts of text, synthesise scattered information (Abbott)   
- Read widely but selectively to recognize implicit information (Jang)   
- CAEL specification: locating supporting info   
- CAEL specification: follow sequences of events   
- CAEL specification: identify logical relations

2.4 Word processing (Abbott, Jang)

- Break words down into smaller parts (Abbott)   
- Identify and formulate synonyms for specific words/phrases (Abbott)   
- Use local context to deduce word meaning (Jang)   
- Use local context to deduce word meaning (Abbott)   
- CAEL specification: comprehending vocabulary in context   
- CAEL specification: identify synonyms and/or definitions

.5 Addressing the question (Abbott, Banerjee & Wall)

- Match key vocabulary in text and question (Abbott)   
- Apply and understand information in text (Banerjee & Wall)   
- Use the discourse format/organization to answer the question (Abbott)   
- CAEL specification: identifying main idea   
- CAEL specification: identifying topic

2.6 Using visual and textual information (Abbott, Banerjee & Wall) - Match verbal and visual information (Abbott) - Use visual data to solve verbal problems (Banerjee & Wall) - Understand visual data displays (Banerjee & Wall) - CAEL specifications: use textual (bold text, italics, headings, etc.) or graphic displays (e.g. figures, tables, etc.) to signal information hierarchies and locate key information

2.7 Critical analysis (Jang, Banerjee & Wall)

- Determine which information is true or not true (Jang) - Reprise main points in text or speech (Banerjee & Wall) - Critical analysis of argumentation in text (Banerjee & Wall) - Support or refute argumentation in text (Banerjee & Wall) - CAEL specification: classify - CAEL specification: arrive at conclusions/inferencing

2.8 Drawing inferences (Abbott, Jang)

- Draw inferences, conclusions and hypotheses based on information within the text (Abbott)   
- Infer writer’s purpose and arguments through implicit information within the text (Jang)   
- CAEL specification: predicting?   
- CAEL specification: arrive at conclusions/inferencing

2.9 Application of language skills (Abbott, Jang)

- Apply knowledge of grammar, syntax, punctuation, etc. (Abbott)   
- Recognise rhetorical argument structures, such as compare/contrast, cause/effect and alternative arguments (Jang)   
- Use the discourse format as a clue for answering the question (Abbott)   
- CAEL specification: display knowledge of a genre/discourse

2.10 Ability to cope with heavy reading load (Banerjee & Wall)

- CAEL specifications: speeded pressure to read two-three academic texts related to a topic

Appendix B. Sample learning profile: diagnostic summary (issued October 13, 2007 with initial teacher commentary)   
Name of Student SHS#2 Assigned Class 1500 (Intermediate EAP) Teacher XXXX Student Score and Class and Level Mean   

<html><body><table><tr><td rowspan="2">Skill</td><td rowspan="2">Student Score</td><td rowspan="2">Class Mean</td><td rowspan="2">1300 Mean</td><td rowspan="2">1500 Mean</td><td rowspan="2">1900 Mean</td><td colspan="3">DEVELOPMENT</td></tr><tr><td></td><td></td><td></td></tr><tr><td>Reading</td><td>43</td><td>64</td><td>55</td><td>60</td><td>69</td><td rowspan="2">Four Skills:</td><td rowspan="2">Balanced NOT Balanced</td><td rowspan="2"></td></tr><tr><td>Listening</td><td>98</td><td>58</td><td>50</td><td>54</td><td>73</td></tr><tr><td>Writing</td><td>65</td><td>47.8</td><td>39</td><td>48</td><td>56</td><td>Academic Expertise&amp;</td><td>Balanced</td><td>NOT Balanced</td></tr><tr><td> Speaking</td><td>70</td><td>65</td><td>59</td><td>56</td><td>62</td><td>Language (EAP 13/10/2007weaknessin reding (see owcoloquial but veyffetive writing with excellent</td><td>V</td><td></td></tr><tr><td>+or-) Accuracy of Self-Assessment</td><td colspan="4">)(13/10/2007) unaware of weakness in reading or strengths in other skills; rated all equally good.(+) (20/11/07) aware, working and improving</td><td></td><td colspan="3">anguage control; listening generated imprint transfers to writing lile uptakefrom reading.</td></tr></table></body></html>

Priorities for writing development: (13/10/2007) some fine tuning appears to be required (text with markup is appended)1needs to work on automatizd expressions in writing (lacks precision) specifically cohesive devices (he wrote 'such'rather than 'such as; to'rather than in relation to;as result'rather than as a result of; beside' rather than besides) Given his use of listening input and his strength in speaking these glitches are probably not as evident when he speaks but stand out in his writing.H seems not to be noticing these when he reads.2)reading is the priority for this studentif he notices'in reading, he may more readily transfer what he notices to his writing

![](img/a06fb2be6baf5a463a8f41e8529f914a48290816226173fe52c24ed8c1313094.jpg)  
Reading:Priorities:\*(10/10/2007)PRIORITY FOCUS This student appears to be generally unaware of his limitations in reading.He is very imprecise with regard to details unable to differentiate main from subordinate ideas does not accurately apply information from the reading.May respond too qui $\mathit { 6 6 }$ -without due consideration.May be a auditory learner reading aloud may improvechunking'and imprintslow him down.He's getting about half of what he should from what he reads.

ACTIONS TAKEN (date/action/observed outcomes):(05/11/2007) Activities to direct student's attention to cohesive phrases in written text TA worked with student on targeted activities (e.g.,sentence combining highlighting and discussion,one-on-one and in small groups)(22/11/2007) in-class writing assignment shows some improvement but will require more work; also noticed and addressed students confusion over less vs.fewer last two years vs.two years ago.

# ACTIONS TAKEN (date/action/observed outcomes):

20/10/2007shared results of initial diagnostic with student and discussed approach; identified additional diagnostic tests and administered them, including asking him to read aloud and discuss information.Very revealing.He does need to slow down and consider what hes reading.05/1i/2007).Worked with $\mathcal { T } \mathcal { A }$ on read-alouds student beginning to question what he doesn't understand while reading; $\mathcal { T } \mathcal { A }$ reports improvement; he still attempts to go to auicklv.Student revorts he has the same issues in his first lanauaae.

Listening:Priorities:\*(13/10/2007)Seems to understand allhe hears.Demonstrates full academic control of new infornation processes it with depth and breadth.

ACTIONS TAKEN (date/action/observed outcomes): 22/11/2007No specific actions taken; continued participation in typical EAP activities.

![](img/8e6632a76224eca4c162060a88093307e3456c0a5087ce9e1783379f491d048d.jpg)  
O 8-k Lgg435-64 ft

# References

Abbott, M. (2007). A confirmatory approach to differential item functioning on an ESL reading assessment. Language Testing, 24, 7e36.   
Alderson, J. C. (2005). Diagnosing foreign language proficiency: The interface between learning and assessment. London: Continuum.   
Alderson, J. C. (2007). In J. Fox, M. Wesche, D. Bayliss, L. Cheng, C. Turner, & C. Doe (Eds.), Language testing reconsidered (pp. 21e39). Ottawa, ON: University of Ottawa Press.   
Banerjee, J., & Wall, D. (2006). Assessing and reporting performances on pre-sessional EAP courses: developing a final assessment checklist and investigating its validity. Journal of English for Academic Purposes, 5(1), 50e69.   
Brown, J. D. (1995). Elements of language curriculum. Boston: Heinle & Heinle.   
Buck, G., & Tatsuoka, D. (1998). Application of the rule-space procedure to language testing: examining attributes of a free response listening test. Language Testing, 15(2), 119e157.   
Chalhoub-Deville, M., & Turner, C. (2000). What to look for in ESL admission tests: Cambridge certificate exams, IELTS, and TOEFL. System, 28, 523e539.   
Charmaz, K. (2006). Constructing grounded theory: a practical guide through qualitative analysis. Thousand Oaks, CA: Sage.   
Colby-Kelly, C., & Turner, C. (2007). AFL research in the L2 classroom and evidence of usefulness: taking format assessment to the next level. Canadian Modern Language Review, 64(1), 9e38.   
Creswell, J. W., Clark, P., Gutmann, V. L., & Hanson, W. E. (2003). Advanced mixed methods research designs. In A. Tashakkori, & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioralresearch (pp. 209e240). Thousand Oaks, CA: Sage.   
Do¨rnyei, Z. (2005). The Psychology of the language learner: Individual differences in second language acquisition. Mahway, NJ: Erlbaum.   
Farrell, J. (2000). Why is educational reform so difficult? Similar descriptions, different prescriptions, failed explanations. Curriculum Inquiry, 30, 83e103.   
Foucault, M. (1969). The archaeology of knowledge. New York, NY: Harper & Row.   
Fox, J. (2004). Curriculum design: does it make a difference? Contact, Special Research Symposium Issue, 30(2), 1e5.   
Fox, J., Cheng, L., Berman, R., Song, X., & Myles, J. (2006). Costs and benefits: English for Academic Purposes (EAP) instruction in Canadian universities. In J. Fox, & C. Doe (Eds.), A monograph on English for Academic Purposes in Canadian Universities, Carleton Papers in Applied Language Studies, XXIII (pp. 1e108).   
Freedman, A. (1999). Beyond the text: towards understanding the teaching and learning of genres. Teaching issues. TESOL Quarterly, 4, 764e767.   
Gomez, P., Noah, A., Schedl, M., Wright, C., & Yolkut, A. (2007). Proficiency descriptors based on a scale-anchoring study of the new TOEFL iBT reading test. Language Testing, 24(3), 417e444.   
Green, A., & Weir, C. (2004). Can placement tests inform instructional decisions? Language Testing, 21(4), 467e494.   
Hansen, J. (2000). Interactional conflicts among audience, purpose, and content knowledge in the acquisition of academic literacy in an EAP course. Written Communication, 7, 27e52.   
Hargreaves, A. (1989). Curriculum and assessment reform. Toronto: OISE Press.   
Hyland, K., & Hamp-Lyons, L. (2002). Issues and directions. Journal of English for Academic Purposes, 1, 1e12.   
Jang, E. (2005). A validity narrative: Effects of reading skills diagnosis on teaching and learning in the context of NG TOEFL. Unpublished doctoral dissertation. Urbana-Champaign, IL: University of Illinois at Urbana-Champaign.   
Lemke, J. (1995). Textual politics: Discourse and social dynamics. London: Taylor & Francis.   
Markee, N. (1993). The diffusion of innovation in language teaching. Annual Review of Applied Linguistics, 13, 229e243.   
Moore, T., & Morton, J. (2005). Dimensions of difference: a comparison of university writing and IELTS writing. Journal of English for Academic Purposes, 4, 43e46.   
Mullen, A. (2008). Impact of using a proficiency test as a placement tool: The case of TOEIC. Unpublished doctoral dissertation. Quebec, Canada: Laval University.   
Pennycook, A. (1998). Borrowing others’ words: text, ownership, memory, and plagiarism. In V. Zamel, & R. Spack (Eds.), Negotiating academic literacies (pp. 265e292). Mahwah, NJ: Lawrence Erlbaum.   
Rea-Dickins, P. (2006). Currents and eddies in the discourse of assessment: a learning-focused interpretation. International Journal of Applied Linguistics, 16(2), 163e188.   
Read, J. (2000). Assessing vocabulary. Cambridge: Cambridge University Press.   
Samuda, V. (2001). Guiding relationships between form and meaning during task performance: the role of the teaching. In M. Bygate, P. Skehan, & M. Swain (Eds.), Researching pedagogic tasks: Second language learning, teaching and testing (pp. 119e140). Essex, UK: Longman.   
Shohamy, E. (1992). Beyond performance testing: a diagnostic feedback testing model for assessing foreign language learning. Modern Language Journal, 76(4), 513e521.   
Stansfield, C., & Winke, P. (2008). Testing aptitude for second language learning. In E. Shohamy, & N. Hornberger (Eds.), The encyclopedia of language and education, volume 7, language testing and assessment (pp. 81e96). Springer.   
Strauss, A., & Corbin, J. (1998). Basics of qualitative research: Techniques and procedures for developing grounded theory. Thousand Oaks, CA: Sage.   
Tashakkori, A., & Teddlie, C. (1998). Mixed methodology: Combining qualitative and quantitative approaches. Thousand Oaks, CA: Sage.