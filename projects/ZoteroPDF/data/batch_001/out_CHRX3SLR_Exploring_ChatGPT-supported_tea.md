# Exploring ChatGPT-supported teacher feedback in the EFL context

Jining Han a,\* , Mimi Li b

a Faculty of Education, Southwest University, No.2 Tiansheng Road, 400715, Chongqing, China   
b Department of Literature and Languages, Texas A&M University-Commerce, David Talbot Hall of Languages 141, 2200 Campbell St, Commerce,   
TX, 75428, USA

# A R T I C L E I N F O

# A B S T R A C T

Keywords:   
ChatGPT   
Teacher feedback   
Writing feedback   
AI   
Language education

This study investigates the ChatGPT-supported teacher feedback in the Chinese tertiary EFL context and explores an innovative AI-aided writing pedagogy by integrating ChatGPT into teacher writing feedback provisions to alleviate the challenges of teacher feedback in a large class, which were reported in previous research. The participants of this study were four in structors and 102 students from two undergraduate classes in the world language education program. The students completed two writing tasks: an argumentative essay and an expository essay; then, the instructors provided detailed feedback on their essays based on the ChatGPT feedback. Two prompts were provided to ChatGPT after the training: 1) corrective feedback drawing on Ferris’s (2006)15 types of common errors and 2) holistic rhetorical feedback. Afterwards, the teachers adapted the ChatGPT feedback and shared the detailed individualized writing feedback with each student. We closely examined the types and features of ChatGPTsupported teacher feedback and how EFL students incorporate this feedback into their writing revisions. The findings indicate that the ChatGPT-supported teacher feedback addressed diverse error categories and included helpful comments on the overall rhetoric. Moreover, the students incorporated more of the feedback into their revisions across tasks, which reflects their deeper engagement with the feedback content. This study notes the importance of an “AI $^ +$ Teacher” model that leverages the analytical strengths of AI while maintaining essential teacher‒student interactions. This new approach of ChatGPT-supported teacher feedback has great potential in L2 writing feedback provision and will shed novel light on the writing pedagogy with the aid of AIin the digital era.

# 1. Introduction

The advent of artificial intelligence (AI) in educational settings has significantly transformed the landscape of language learning and teaching, especially in the domain of writing instruction. With the integration of AI-mediated tools such as automated writing evaluation (AWE) and ChatGPT, educators have harnessed the capabilities of natural language processing (NLP) to provide immediate and personalized feedback to learners to enhance their writing proficiency. Compared with AWE tools, ChatGPT feedback can address not only grammatical accuracy but also can help develop students’ critical thinking and analytical skills in writing (Dwivedi et al., 2023; Essel et al., 2024; Herbold et al., 2023). The adaptability of ChatGPT makes it an impactful resource to foster interactive language education. In Chinese tertiary educational settings, professors in research institutions have direct access to ChatGPT with the adherence to relevant regulations regarding the use of AI, whereas students do not. Therefore, the integration of ChatGPT into teacher writing feedback provisions can simultaneously help alleviate the challenge of providing teacher feedback for a large class and utilize the affordances of AI. Specifically, in Chinese universities, an EFL instructor often teaches several large classes, each of which has up to 100 students (Yang et al., 2006), which makes it almost impossible to provide detailed writing feedback for each student and impact the writing feedback on the language and writing development of the students. Thus, the current study explores an innovative feedback approach, called ChatGPT-supported teacher feedback, in the Chinese tertiary EFL context. ChatGPT was first trained on how to provide detailed feedback on students’ essays; then, the teachers adapted ChatGPT feedback before sharing the writing feedback with the students (see the detailed explanation in the Methods section). This study specifically examines how ChatGPT can serve as a companion for teachers to provide tailored and constructive feedback and how Chinese EFL students incorporate ChatGPT-supported teacher feedback into their revisions. This work contributes to the ongoing discourse on optimizing AI tools for L2 writing pedagogy in the digital era.

# 2. Literature review

This study is undergirded by sociocultural theory and informed by previous research on AI-mediated writing feedback.

# 2.1. Theoretical framework

Mediation (Vygotsky, 1978), which is the central concept of sociocultural theory, refers to the roles of tools and artifacts in facilitating human activities. In ChatGPT-supported teacher feedback, ChatGPT acts as a mediator between teacher and feedback process. It can provide immediate and effective responses to students’ writing based on prompts that the teacher inputs. The teacher engages with the ChatGPT output and tailors it to the students’ needs. Moreover, the construct of scaffolding (Bruner, 1985) supports the innovative approach of ChatGPT-supported teacher feedback. ChatGPT can scaffold the teacher in effectively performing feedback tasks that would have been difficult to independently accomplish. Meanwhile, teachers can bridge the gap between current abilities and potential developmental level of the students through appropriate guidance and scaffolding, as reflected by the incorporation of ChatGPT-supported teacher feedback into the writing revisions of the students. This study also uses the Zone of Proximal Development (ZPD) to illustrate how ChatGPT-supported teacher feedback can bridge the gap between current abilities and potential growth of students through appropriate guidance and collaborative support. The potential development level of a student largely depends on collaborative efforts with others, particularly the feedback from teachers. In short, the constructs of mediation and scaffolding inform our research design. The mediation of ChatGPT facilitates the provision of writing feedback for EFL teachers, including both corrective feedback and rhetorical feedback. Additionally, ChatGPT-supported teacher feedback scaffolds the writing revisions of students.

# 2.2. Automated writing evaluation (AWE)

AIfeedback constitutes part of ChatGPT-supported teacher writing feedback, where teachers provide detailed writing feedback based on ChatGPT output in response to appropriate prompts. In this section, we synthesize representative studies that use AWE in L2 writing classes. AWE systems, which integrate NLP technology, latent semantic analysis, and AI, can offer learners immediate auto mated feedback, which helps enhance their writing competence (Grimes & Warschauer, 2010; Wilson & Roscoe, 2020). For example, Wilson et al. (2014) used the AWE tool “Criterion” to provide feedback for essays written by 4162 students in grades 4 to 8, which improved their essay quality. Moreover, by automating the analysis of student writing texts and providing feedback, AWE systems significantly reduce the time and effort that teachers need to invest in offering personalized writing feedback (Han & Sari, 2022). Wilson and Czik (2016) revealed that compared to feedback solely from teachers, the AWE system “PEG Writing” reduced the amount of time that teachers spent providing feedback by 2–3 times, and the quantity of AWE-based teacher feedback remained the same as that provided by only the teacher. AWE lessened the grading burden of instructors while providing them more time to provide feedback on the global issues of writing (Barrot, 2023a, 2023b; Wilson & Czik, 2016).Nevertheless, by directly pointing out errors and providing suggestions for modification, AWE systems may limit the capacity of students to transfer writing skills across writing tasks. Stevenson and Phakiti (2014) reported that although AWE might reduce errors in specific writing tasks, students could not demon strate learning transfer across various tasks. Ranalli’s (2021) study examined the engagement of six L2 students using “Grammarly” based on screen captures, interviews, and stimulated recalls and revealed that the participants primarily used feedback for editing instead of aiming for long-term learning. These findings may be attributed to the directly provision from “Grammarly” of highly specific corrections, which made the students make local changes without substantial cognitive engagement. Due to algorithmic constraints, the feedback provided by AWE tended to be formulaic and lacked consideration of specific pedagogical needs (Ranalli, 2018).

# 2.3. Utilizing ChatGPT for writing feedback

Compared with traditional AWE systems, ChatGPT exhibits advantages in writing feedback provision due to its human-like conversational abilities and capacity for knowledge expression. ChatGPT can deliver personalized and comprehensive feedback on student writing (Caldarini, Jaf, & McGarry, 2022). Barrot, 2023a, 2023b expounded on the potential benefits of ChatGPT in L2 writing and highlighted its capacity as a rich information source for language learning and its ability to clarify themes, construct structures, and provide real-time adaptive feedback. The study also highlighted the limitations of ChatGPT, such as the possibility of generating inaccurate or unintelligible responses and its limited ability to tailor content for specific target audiences. Barrot, 2023a, 2023b emphasized the importance of clear instructional guidelines and targeted training for teachers to effectively use ChatGPT in writing instruction. Yan (2023) conducted an exploratory qualitative study on the role of ChatGPT in a university English writing course. The students noted the benefits of using ChatGPT, such as its significant potential for supporting language learning, providing timely feedback, enhancing learning motivation, and improving the writing quality. Moreover, Kohnke et al. (2023) emphasized the trans formative potential of ChatGPT in language education and its ability to promote engaging and adaptive language learning. ChatGPT facilitates the feedback process by creating customized prompts and responses that address student writings according to the teacher’s guidelines. The immediate ChatGPT feedback helps students understand and incorporate corrections and suggestions into their re visions, which enhances their learning process.

Comparing teacher feedback and ChatGPT feedback, Steiss et al. (2024) reported that well-trained teachers deliver higher-quality feedback than ChatGPT; however, considering the convenience and overall quality of ChatGPT-generated feedback, ChatGPT feedback is valuable when well-trained teachers are unavailable or when there is a need to evaluate many writing drafts. This research study highlights the potential of using ChatGPT to provide writing feedback and implies a prospectus of teacher-AI collaboration.

In another study, Guo and Wang (2023) reported that ChatGPT generated more feedback than teachers, evenly distributed attention across three areas of focus (i.e., content, organization, and language), and tended to provide direct revision guidance. Teachers provide personalized, engaging feedback that accounts for individual abilities and needs of students and often deliver query-based feedback. The teacher participants commented on ChatGPT’s ability to evaluate student writing, specificity of the feedback and limitations of ChatGPT feedback in terms of length, reading difficulty, and relevance. Their study suggested the importance of teacher agency. Instead of having ChatGPT directly provide feedback for students, teacher feedback can be facilitated by ChatGPT output. To bridge the gap that no previous research examined the collaboration of instructors and AIin providing writing feedback, we implemented an innovative feedback approach (i.e., ChatGPT-supported teacher feedback) in a Chinese EFL course to explore the ChatGPT-supported teacher feedback and students’ incorporation of the feedback into their revisions. This study is informed by two research questions.

RQ1: What are the types and features of ChatGPT-supported feedback provided by teachers across two writing tasks? RQ2: How do students incorporate ChatGPT-supported teacher feedback into their revisions regarding two writing task

# 3. The study

# 3.1. Context and participants

The present study, which was part of a larger research project on ChatGPT-supported teacher feedback, explored the types and features of ChatGPT-supported teacher feedback and the incorporation of EFL students of that feedback into their revisions. At a large public university in southwestern China, we used ChatGPT to help instructors provide writing feedback in the Language Teaching Methodology course offered in English to undergraduate students majoring in World Language Education in the spring of 2023. Each class was taught by two instructors (a senior faculty member and a junior faculty member). The curriculum covered various facets of language instruction, such as teaching methods, educational theories, instructional strategies, and intercultural communication, with a particular emphasis on improving English composition skills. In total, 102 students (47 from one class and 55 from the other) participated in this study after they had participated in the study orientation and signed the informed consent forms.

# 3.2. Study design and procedure

We implemented two types of ChatGPT-supported teacher feedback based on the contingent needs of the students: 1) indirect corrective feedback drawing on Ferris’ (2006) 15 types of common errors specific to EFL learners (the instructor changed the direct feedback of ChatGPT to coded indirect feedback in the feedback provision) and 2) holistic rhetorical feedback, which is commonly not included in the traditional EFL teacher feedback. Indirect feedback refers to teachers’ indications of the types of errors and locations of errors, which enable students to identify and correct language mistakes on their own (Ferris et al., 2013). Indirect feedback was adopted in this study because research has shown that indirect feedback promotes the autonomous reflection and problem-solving abilities of students and consequently helps students become independent learners (Ferris, 2010). In addition, indirect feedback has been proven to be an effective strategy for learning content words such as verbs, nouns, and articles (Ferris et al., 2013; Ferris & Roberts, 2001). Specifically, there are two categories of indirect feedback: uncoded indirect feedback and coded indirect feedback. Uncoded indirect feedback refers to instances where the teacher merely marks an error by underlining, circling, or placing a tally in the margin, without explicitly indicating the type of error (Bitchener et al., 2005). Coded indirect feedback, which our study used, refers to instances where the teacher identifies the exact location of an error and specifies the type of error and reportedly enhances the problem solving, reflection, self-learning, and learning outcomes of students. (Ferris, 2010; Han, 2024). The inclusion of rhetorical feedback was inspired by Aljasir (2021) and Carter and Thirakunkovit (2019), who noted that rhetorical feedback enhanced the ability of students to efficiently provide clear arguments, substantiate major points with evidence, maintain logical flow, and organize ideas. Accordingly, we developed two prompts for ChatGPT and trained it to provide feedback according to the prompts: 1) “please point out all errors on sentence structure, word choice, verb tense, noun endings (singular/plural), verb form, punctuation, articles/determiners, word form, spelling, run-ons, pronouns, subject‒verb agreement, fragments, idioms, and informal text” (hereinafter “Instruction 1”) and 2) “please provide feedback on the rhetoric within the article” (hereinafter “Instruction 2”).

The first author provided extensive training to the four instructors on how to provide coded indirect feedback and holistic feedback based on the ChatGPT output. Specifically, he guided the instructors to practice implementing ChatGPT-supported teacher feedback using a few sample papers. They ran the ChatGPT prompts, reviewed the ChatGPT output and provided constructive feedback on the sample papers based on the ChatGPT feedback. Then, accounts were registered for the four teachers to use ChatGPT. The students were required to complete two writing tasks: an argumentative essay entitled “The Importance of Learning a Second Language for Chinese Language Majors” and an expository essay on “The Impact of Technology on Language Education”. Prior to their writing, the teachers provided explicit instructions on how to craft a 350-word essay using digital devices, which included all required components of an essay. Detailed assessment criteria were also provided to the students; for example, the argumentative essay was evaluated based on its thesis robustness, evidence, counterargument, grammatical accuracy, citation of sources, and overall coherence and organization. The essays were composed during class sessions (i.e., $4 5 \mathrm { { m i n } }$ per session) and submitted through a learning management system (LMS). Two instructors, one from each class, graded half of the submitted essays.

The innovative method of ChatGPT-supported teacher feedback includes three stages: 1) the teacher copied the essay of a student into ChatGPT and input Instruction 1; 2) the instructor closely reviewed the ChatGPT-generated feedback to provide coded indirect feedback on the students’ drafts by adding in-text annotations in MS Word; 3) the instructor input Instruction 2 for rhetorical feedback from ChatGPT, made modifications if needed, and appended the holistic feedback to the end of the student’s essay. Subsequently, the two main types of feedback were shared with students via the LMS, and the students were required to submit their revised essays based on the ChatGPT-supported feedback within five days. Table 1 shows the detailed procedure and collected data in the larger research project.

# 3.3. Data analysis

Although 102 students participated in the study and completed their writing assignments in class, seven students did not submit their revised drafts for both writing tasks. Therefore, we excluded the essays of these seven students from our analysis. We analyzed 95 drafts and 95 revised papers for each of the two tasks.

To address RQ1, we conducted a detailed analysis of the ChatGPT-supported teacher feedback and identified the total number of intext comments and the number and percentage of comments regarding each type of language error. For example, in Task 1, the instructor annotated 1176 errors, 220 of which were related to word choice, which accounted for $1 8 . 7 1 \%$ of all errors. We also conducted a content analysis of the rhetorical feedback on 190 essays from the two writing tasks. We enabled the categories of holistic comments to emerge from the data and identified eight types, as displayed in Table 2. Then, we calculated the number of each rhetorical feedback type in each essay and compared the overall rhetorical feedback in the two writing tasks. To answer RQ2, we adapted Ferris’ (1997) method for scoring revisions, where we adjusted Ferris’s seven-point scale to a three-point scale for Instruction 1 regarding the error feedback, including no change, an unsuccessful revision, and a successful revision. For Instruction 2 regarding holistic rhetoric feedback, the three-point scale includes no change, partial modification, and full modification. The distinctions among these types enable us to understand the scaffolding effects of ChatGPT-supported teacher feedback on the students’ revisions. One researcher coded all revisions of the students, and a colleague with a certification in English teaching randomly selected $3 0 \%$ of the papers for coding. The kappa values for Task 1 and Task 2 revisions were 0.903 and 0.853, respectively, with an average kappa value of 0.885.

Table 1 Research procedures and collected data.   

<html><body><table><tr><td>Timeline</td><td>Procedure</td><td>Data collected</td></tr><tr><td colspan="3">Researchers preparation</td></tr><tr><td colspan="3">Week 1</td></tr><tr><td rowspan="3">Week 2</td><td>1. Teacher training 2. Two classes of students are trained.</td><td></td></tr><tr><td>1. Students complete the first writing assignment in class</td><td>1. 1st draft</td></tr><tr><td>2. Teachers provide ChatGPT-supported teacher feedback</td><td>2. ChatGPT-supported teacher feedbacke</td></tr><tr><td>Week 3</td><td>1. Students receive feedback from their teachers</td><td>3. Class observations</td></tr><tr><td rowspan="5">Week 4</td><td colspan="3"></td></tr><tr><td colspan="3">2. Students revise the writing assignment. 1. Students submit the revised writing assignment</td></tr><tr><td colspan="3"></td></tr><tr><td>2. Students complete the 2nd writing assignment in the class</td><td colspan="2">2. 2nd draft 3. Class observations</td></tr><tr><td>3. Teachers provide ChatGPT-supported teacher feedback</td><td colspan="2">4. 1st revision scores</td></tr><tr><td colspan="3"></td></tr><tr><td>Week 5</td><td colspan="3">1. Students receive feedback from their teachers</td></tr><tr><td>Week 6</td><td colspan="3">2. Students revise the writing assignment. 1. Students submit the revised writing assignment</td></tr><tr><td></td><td colspan="3"></td></tr><tr><td>Week 7</td><td></td><td colspan="2">1. 2nd revision 2. 2nd revision scores</td></tr><tr><td>Week 8</td><td>1. Semi-structured interviews with teachers. 1. Semi-structured interviews with students</td><td colspan="3">1. Teacher interviews 1. Student interviews</td></tr></table></body></html>

Note: The interviews, each of which was conducted in Chinese and lasted approximately $3 0 \ \mathrm { m i n }$ , aimed to explore the teachers’ and students perceptions of ChatGPT-supported teacher feedback. The teachers’ perceptions of ChatGPT-supported teacher feedback were the focus of another work of the co-authors.

Table 2 Types of rhetorical feedback: definitions and examples.   

<html><body><table><tr><td></td><td>Codes</td><td>Operationalized definition</td><td>Illustrative examples</td></tr><tr><td>1</td><td>Cohesion and coherence</td><td>Any evaluative or commentary focusing on the logical connection and flow between sentences, paragraphs, and sections of a writing assignment.</td><td>However, the writing contains some issues with coherence.</td></tr><tr><td>2</td><td>Comprehensibility of message</td><td>Any evaluative commentary or inquiry concerning the clarity, understandability, and ease of interpretation of the message conveyed in a writing assignment..</td><td>The text is not clear enough.</td></tr><tr><td>3</td><td>Content development</td><td>Any evaluative or constructive commentary focusing on the creation, evidence, examples, expansion, arguments, and improvement of the essay&#x27;s content.</td><td>The article could be improved by providing more. specific examples and evidence to support the arguments made.</td></tr><tr><td>4</td><td>Mechanics (spelling, punctuation, capitalization)</td><td>Any evaluative or commentary that focuses on the accuracy and appropriateness of spelling, punctuation, and capitalization in a writing assignment.</td><td>However, there are a few errors in punctuation that could be corrected.</td></tr><tr><td>5</td><td>Organization</td><td>Any evaluative or commentary that addresses the structural layout and logical sequence of content in a writing assignment.</td><td>The organization of the article could be improved.</td></tr><tr><td>6</td><td>Sentence structure/grammar</td><td>Any evaluative or commentary on the syntactic arrangement of words within a sentence to convey meaning effectively, governed by the rules of the language.</td><td>There are some grammatical errors in the article. that could be corrected to improve.</td></tr><tr><td>7</td><td>Word choice</td><td>Any evaluative or commentary focusing on the appropriateness, precision, and impact of the word used in a writing assignment.</td><td>There are some issues with word choice that could. be improved.</td></tr><tr><td>8</td><td>Writing style</td><td>Any evaluative or commentary that addresses the author&#x27;s stylistic choices in a writing assignment. This includes feedback on the tone, voice, and formality of the writing style..</td><td>The author uses informal language in some places, which can detract from the overall professionalism. of the article.</td></tr></table></body></html>

# 4. Results

We present the findings of this study regarding the two research questions.

4.1. RQ 1: what are the types and features of ChatGPT-supported feedback provided by teachers across two writing tasks?

The instructors provided indirect feedback on Ferris’s 15 error types according to the ChatGPT output. Examples 1 and 2 in Table 3 are illustrative. Moreover, the instructors provided the overall feedback on the rhetoric of the students’ essays based on the ChatGPT output, as shown in example 3.

Table 4 presents an overall picture of ChatGPT-supported teacher feedback regarding different error types in each writing task. As displayed in Table 4, Writing Task 1 received 1176 instances of feedback with an average of 12.38 instances per essay. Writing   
Task 2 received 665 instances with an average of 7 feedback points per essay. In Task 1, the three most frequent addressed types of   
errors in ChatGPT-supported teacher feedback were word choice $( n = 2 2 0$ , $1 8 . 7 1 \%$ ), sentence structure $\mathbf { \tilde { \Delta } } n = 1 6 7$ , $1 4 . 2 \% )$ ), and   
punctuation $( n = 1 2 8$ , $1 0 . 8 8 \%$ ). The four least frequently addressed types of errors were fragments $( n = 7$ , $0 . 6 \%$ , idioms $( n = 1 9$ ,

Table 3 Illustrative examples of ChatGPT outputs and teacher feedback.   

<html><body><table><tr><td>Examples</td><td>ChatGPT output</td><td>Teacher feedback</td></tr><tr><td>Example 1 Verb tense Example 2 Word choice</td><td>ChatGPT output: &quot;had studied&quot;.</td><td>Verb tense: check the tense of &quot;had study&quot;..</td></tr><tr><td>Example 3 Rhetorical feedback</td><td>&quot;Communication and communication&quot; seem repetitive. Perhaps you meant &quot;communication and collaboration&quot;.. The article effectively presents the importance of learning a second foreign language, particularly for students majoring in Chinese, in the context of globalization and increasing global interactions. The author provides various arguments, including cognitive benefits, career opportunities, and cultural exchange, to support their claim.</td><td>&quot;Communication and communication&quot; seem repetitive. Your article is well structured and organized, with clear and concise paragraphs that focus on specific points. You use relevant examples to illustrate their arguments, which helps to make the article more</td></tr><tr><td></td><td>The article is well-structured and organized, with clear and concise paragraphs that focus on specific points. The author also uses relevant examples to illustrate their arguments, which helps to make the article more persuasive. One area where the article could be improved is in the use of sources to support the arguments presented. While the author mentions experimental results that show the cognitive benefits of learning a second language, they do not provide specific references or sources for these results. Adding this information would make the article more credible and convincing. Overall, the article effectively presents the benefits of learning a second foreign language, particularly for students majoring in Chinese, and provides compelling arguments to support this claim.</td><td>persuasive. (1) One area where your article could be improved is in the use of sources to support the arguments presented, and (2) while you mention experimental results that show the cognitive benefits of learning a second language, you do not provide specific references or sources for these results. Adding this information would make your article more credible and convincing.e</td></tr></table></body></html>

Table 4 Distribution of error types across two writing tasks.   

<html><body><table><tr><td>Feedback in terms of error types</td><td colspan="2">1st writing task N = 95</td><td colspan="2">2nd writing task N = 95</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>Sentence structure</td><td>167</td><td>14.2.</td><td>71</td><td>10.68</td></tr><tr><td>Word choice</td><td>220</td><td>18.71</td><td>145</td><td>21.8</td></tr><tr><td>Verb tense</td><td>105</td><td>8.93</td><td>38</td><td>5.71</td></tr><tr><td> Noun endings (singular/plural)</td><td>55</td><td>4.68</td><td>40</td><td>6.02</td></tr><tr><td>Verb form</td><td>74</td><td>6.29</td><td>35</td><td>5.26</td></tr><tr><td>Punctuation</td><td>128</td><td>10.88</td><td>38</td><td>5.71</td></tr><tr><td>Articles/determiners</td><td>92</td><td>7.82</td><td>38</td><td>5.71</td></tr><tr><td>Word form</td><td>117</td><td>9.95</td><td>54</td><td>8.12</td></tr><tr><td> Spelling</td><td>46</td><td>3.91</td><td>72</td><td>10.83</td></tr><tr><td>Run-ons</td><td>32</td><td>2.72</td><td>41</td><td>6.17</td></tr><tr><td>Pronouns</td><td>32</td><td>2.72</td><td>8.</td><td>1.2</td></tr><tr><td>Subject-verb agreement</td><td>44</td><td>3.74</td><td>32</td><td>4.8</td></tr><tr><td>Fragments</td><td>7</td><td>0.6</td><td>29</td><td>4.36</td></tr><tr><td> Idiom</td><td>19</td><td>1.62</td><td>4.</td><td>0.6</td></tr><tr><td>Informal</td><td>38</td><td>3.23</td><td>20</td><td>3</td></tr><tr><td>Total</td><td colspan="2">1176</td><td colspan="2">665</td></tr></table></body></html>

$1 . 6 2 \% )$ ), pronouns $( n = 3 2$ , $2 . 7 2 \%$ ) and run-ons $( n = 3 2$ , $2 . 7 2 \%$ , which were tied. In Task 2, this pattern only slightly changed, where the three most common types of errors became word choice $( n = 1 4 5$ , $2 1 . 8 \%$ , spelling $( n = 7 2$ , $1 0 . 8 3 \% )$ ), and sentence structure $( n =$ 71, $1 0 . 6 8 \%$ . The three least frequent error types were idioms $( n = 4$ , $0 . 6 \%$ , pronouns $( n = 8 , 1 . 2 \% )$ , and informal language $( n = 2 0$ , $3 \%$ ). The proportion of nine types of errors decreased. The sentence structure, verb tense, verb form and punctuation errors decreased from $1 4 . 2 \%$ , $8 . 9 3 \%$ , $6 . 2 9 \%$ and $1 0 . 8 8 \%$ of all errors in Task $1 { - } 1 0 . 6 8 \%$ , $5 . 7 1 \%$ , $5 . 2 6 \%$ , and $5 . 7 1 \%$ of all errors in Task 2, respectively. Additionally, the percentages of article/determiner, word form, pronoun, idiom, and informal language errors decreased from $7 . 8 2 \%$ , $9 . 9 5 \%$ , $2 . 7 2 \%$ , $1 . 6 2 \%$ and $3 . 2 3 \% - 5 . 7 1 \%$ , $8 . 1 2 \%$ , $1 . 2 \%$ , $0 . 6 \%$ , and $3 \%$ , respectively. Conversely, the proportions of six types of errors increased: word choice from $1 8 . 7 1 \%$ to $2 1 . 8 \%$ , noun endings (singular/plural) from $4 . 6 8 \%$ to $6 . 0 2 \%$ , spelling from $3 . 9 1 \%$ to $1 0 . 8 3 \%$ , run-on sentences from $2 . 7 2 \%$ to $6 . 1 7 \%$ , subject‒verb agreement from $3 . 7 4 \%$ to $4 . 8 \%$ , and fragments from $0 . 6 \%$ to $4 . 3 6 \%$ . We conducted a Chi-square test on the number of errors in two writing tasks, which resulted in a $\chi ^ { 2 }$ value of 115.275 and a $p$ -value of $5 . 2 7 2 \times$ $1 0 ^ { - 1 8 }$ (see Table 6). These results indicate a statistically significant difference in the distribution of error types between 1st and 2nd writing tasks. Possible reasons will be addressed in the discussion section.

Moreover, we analyzed the feedback provided on 190 essays across two different writing tasks, as shown in Table 5. Comprehensibility of message was the most frequent feedback type in Task 1 at 95 $( 2 1 \% )$ but decreased to $1 2 \%$ in Task 2. Cohesion and Coherence decreased from 78 $( 1 7 \% )$ in Task 1 to 49 $( 1 1 \% )$ in Task 2, which is in parallel with the shift from message comprehensibility to more form-focused feedback. Both Content development and Organization maintained high importance across tasks, and content development slightly decreased from 95 $( 2 1 \% )$ in Task 1 to 67 $( 1 5 \% )$ in Task 2, whereas organization remained steady at approximately $1 8 \%$ . Sentence Structure/Grammar increased from 33 $( 7 \% )$ in Task 1 to 68 $. 1 5 \% )$ in Task 2, which suggests a greater emphasis on the grammatical accuracy in Task 2. Word Choice more than doubled from 17 $( 4 \% )$ in Task 1 to 50 $( 1 1 \% )$ in Task 2, which reflects the increased attention to language precision in Task 2. Feedback on mechanics increased from 12 $( 3 \% )$ to 36 $( 8 \% )$ . Writing style feedback remained consistent at approximately $1 0 \%$ , which indicates a sustained focus on the formal aspects of writing. We performed Chi-square analyses to analyze the instances of rhetorical feedback across two writing tasks, which resulted in $\chi ^ { 2 }$ of 61.45 and a $p$ -value less than 0.001 (see Table 6). These results indicate a statistically significant difference in the distribution of rhetorical feedback types across two writing tasks.

.2. RQ2: how do students incorporate ChatGPT-supported feedback into revisions regarding the two writing tasks?

We found that most errors addressed in ChatGPT-supported teacher feedback were successfully corrected. As shown in Table 7,

Table 5 Distribution of rhetorical feedback types across two writing tasks.   

<html><body><table><tr><td>Feedback types</td><td>Task 1 (N = 95)</td><td>Task 2 (N = 95)</td></tr><tr><td>Cohesion and coherence</td><td>78 (17%)</td><td>49 (11%)</td></tr><tr><td>Comprehensibility of message</td><td>95 (21%)</td><td>57 (12%)</td></tr><tr><td>Content development</td><td>95 (21%)</td><td>67 (15%)</td></tr><tr><td>Mechanics (spelling, punctuation, capitalization)</td><td>12 (3%)</td><td>36 (8%)</td></tr><tr><td>Organization</td><td>85 (18%)</td><td>84 (18%)</td></tr><tr><td>Sentence structure/grammar</td><td>33 (7%)</td><td>68 (15%)</td></tr><tr><td>Word choice</td><td>17 (4%)</td><td>50 (11%)</td></tr><tr><td>Writing style</td><td>45 (10%)</td><td>48 (10%)</td></tr><tr><td>Total</td><td>460</td><td>459</td></tr></table></body></html>

Table 6 Chi-square tests on the number of errors and instances of rhetorical feedback across two writing tasks.   

<html><body><table><tr><td>Task1 vs. Task2</td><td>x2</td><td>df</td><td>N</td><td>p</td></tr><tr><td>Distribution of error types</td><td>115.275</td><td>14</td><td>1841</td><td>&lt;0.001</td></tr><tr><td>Distribution of rhetorical feedback types</td><td>61.45</td><td>7</td><td>919</td><td>&lt;0.001</td></tr></table></body></html>

Note: $p < 0 . 0 0 1$ indicates that the p-values are less than 0.001.

$6 4 . 5 4 \%$ and $6 3 . 9 1 \%$ of the changes in Task 1 and Task 2 were successful revisions, respectively. Moreover, we detected a decrease in the instances of no change, from $2 3 . 5 5 \%$ in Task $1 - 2 0 . 3 \%$ in Task 2. In Task 1, only $9 . 4 7 \%$ of the modifications were classified as full revisions, $2 5 . 2 6 \%$ were categorized as partial revisions, and $6 5 . 2 6 \%$ were classified as no change. Table 8 shows the students’ re sponses to rhetorical feedback. The revision of rhetorical feedback considerably shifted in Task 2: no changes decreased to $3 5 . 7 9 \%$ and partial revisions decreased to $7 . 3 7 \%$ . However, the instances of unsuccessful revisions increased from $1 1 . 9 \%$ in Task $1 { - } 1 5 . 7 9 \%$ in Task 2. There was a change in revision of the rhetorical feedback, where full revisions increased to $5 6 . 8 4 \%$ .

We performed Chi-square analyses to investigate whether there were significant differences among the three ratings in task 1 and task 2. For the students’ revisions based on Ferris’ 15 error categories, there is a significant difference among the three ratings (no change, unsuccessful revision, and successful revision) in Task 1 and Task 2, with statistically more successful revisions identified. In the case of the students’ revisions based on rhetorical feedback, there is a significant difference among the three ratings (no change, partial modification, and comprehensive modification) in each task (see Table 9). Interestingly, statistically more “no change” instances were identified in task 1, but statistically more “comprehensive modification” instances were discovered, which clearly revealed the better engagement and performance of the students in task 2.

We further examined the revisions in terms of error categories across the two tasks, as shown in Table 10. In Task 1, four error categories had successful revision ratios of more than $7 0 \%$ : word choice $( 8 2 . 3 \% )$ , sentence structure $( 7 6 . 6 \% )$ , spelling $( 7 6 . 1 \% )$ , and word form $( 7 3 . 5 \% )$ . Notably, $8 9 . 6 \%$ of word choice errors were modified, and $8 2 . 3 \%$ of these modifications were successful. Similarly, $9 1 \%$ of the sentence structure errors were modified, $7 6 . 6 \%$ of the errors were successfully corrected and only $9 \%$ of the errors remained unmodified. The other six error categories had percentages of successful revisions of $6 0 { - } 7 0 \%$ : verb form $( 6 8 . 9 \% )$ , informal $( 6 5 . 8 \% )$ , verb tense $( 6 5 . 7 \% )$ , pronouns $( 6 5 . 6 \% )$ , noun endings $( 6 3 . 6 \% )$ , and articles/determiners $( 6 2 . 0 \% )$ ). Only two error categories had a low successful revision ratio (under $3 0 \%$ ): punctuation $( 2 1 . 9 \% )$ and fragments $( 1 4 . 3 \% )$ .

In Task 2 (see Table 11), there was an overall improvement in the correction of errors, where successful modification rates exceeded $5 0 \%$ for 13 types of errors. Although the success rates for correcting punctuation and fragment errors were below $5 0 \%$ $4 2 . 1 \%$ and $4 8 . 3 \%$ , respectively), these rates increased compared with those of Task 1. The percentages of successful revisions for other types of errors were above $5 0 \%$ . Additionally, six error categories exhibited successful revision rates above $7 0 \%$ : noun endings $( 8 5 \% )$ , verb forms $( 7 1 . 4 \% )$ , spelling $( 7 6 . 4 \% )$ , subject‒verb agreement $( 8 1 . 3 \% )$ , idioms $( 1 0 0 \% )$ , and informal $( 8 0 . 0 \% )$ , which indicates a signifi cant shift from the results in Task 1. In Task 2, although the successful revision of spelling errors maintained a similar percentage to that in Task 1 (above $7 0 \%$ ), the percentages of successful revisions for the other five categories improved, which suggests that under the teacher’s guidance, students began to focus on errors that were poorly addressed in Task 1. However, there were notable declines in the success of sentence structure and word choice revisions: $5 4 . 9 \%$ and $5 8 \%$ , respectively, with particularly high rates of no change $( 2 6 . 8 \%$ and $2 3 . 4 \%$ , respectively). The interviews that we conducted in the larger project can help explain these discrepancies, which will be addressed in the Discussion section.

# 5. Discussion

In this section, we first interpret our findings in this study in terms of the types of ChatGPT-supported teacher feedback and stu dents’ incorporation of the feedback. Then, we highlight the effectiveness of this innovative feedback approach based on the multi faceted data that we collected in the larger project. First, the total quantity of corrective feedback decreased from 1176 to 665 instances across the two tasks, possibly due to a general decrease in number of errors made by the students in Task 2. Second, the strategies of the teachers for ChatGPT-supported feedback may partially explain the findings of the feedback on different error types. For example, two instructors mentioned their use of deletion and condensing techniques to provide feedback (e.g., recurring errors were condensed into one) in the post-task interview. As Instructor Wei stated, “for recurring spelling mistakes in a text, it’s not necessary to point out each one; condensing them into a single piece of feedback for the student will suffice”. Third, by interviewing the students, we learned that they tended to pay more attention to the drafts in Task 2; for example, Jia said, “When I was writing for the first time, I didn’t pay much attention to the need for revisions. When I realized that I had to make revisions, I became more careful during my second attempt at writing”, and Wang stated that “if I have to correct every mistake, then I need to be more careful with my first draft to reduce the number of errors”. These excerpts well explain the fewer instances of errors addressed in the ChatGPT-supported teacher feedback. Interestingly, the number of spelling errors significantly increased in Task 2. According to the classroom observations, the teachers asked the students to bring laptops for in-class writing prior to the implementation of Task 1. Most students used laptops with Microsoft

Table 7 Overall statistics for students’ revisions based on Ferris’ 15 error categories.   

<html><body><table><tr><td>Rating</td><td colspan="2">Task 1</td><td colspan="2">Task 2</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>No change</td><td>277</td><td>23.55</td><td>135</td><td>20.3</td></tr><tr><td>Unsuccessful revision</td><td>140</td><td>11.9</td><td>105</td><td>15.79</td></tr><tr><td>Successful revision</td><td>759</td><td>64.54</td><td>425</td><td>63.91</td></tr><tr><td>Total</td><td>1176</td><td></td><td>665</td><td></td></tr></table></body></html>

Table 8 Overall statistics for students’ revisions based on rhetorical feedback.   

<html><body><table><tr><td>Rating</td><td colspan="2">Task 1</td><td colspan="2">Task 2</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>No change</td><td>62</td><td>65.26</td><td>34</td><td>35.79</td></tr><tr><td> Partial modification</td><td>24</td><td>25.26</td><td>7.</td><td>7.37</td></tr><tr><td>Comprehensive modification</td><td>9</td><td>9.47</td><td>54</td><td>56.84</td></tr><tr><td>Total</td><td>95</td><td></td><td>95</td><td></td></tr></table></body></html>

Table 9 Results of the Chi-square test for the three ratings in Tasks 1 and 2.   

<html><body><table><tr><td></td><td></td><td>x?</td><td>df</td><td>N</td><td>p</td></tr><tr><td>Students&#x27; revisions based on Ferris&#x27; 15 error categories</td><td>Task1</td><td>539.332</td><td>2 </td><td>1176</td><td>&lt;0.001</td></tr><tr><td></td><td>Task2</td><td>281.805</td><td>2</td><td>665</td><td>&lt;0.001</td></tr><tr><td>Students revisions based on rhetorical feedback.</td><td>Task1</td><td>47.137</td><td>2</td><td>95</td><td>&lt;0.001</td></tr><tr><td></td><td>Task2</td><td>35.137</td><td>2</td><td>95</td><td>&lt;0.001</td></tr></table></body></html>

Note: $p < 0 . 0 0 1$ indicates that the p values are less than 0.001.

Table 10 Revisions for different error categories in Task 1.   

<html><body><table><tr><td></td><td>Error type</td><td> no change</td><td>%</td><td> unsuccessful revision</td><td>%</td><td>successful revision</td><td>%</td><td>No.</td></tr><tr><td>1</td><td>Sentence structure</td><td>15</td><td>9.0%</td><td>24</td><td>14.4%</td><td>128</td><td>76.6%</td><td>167</td></tr><tr><td>2</td><td>Word choice</td><td>16</td><td>7.3%</td><td>23</td><td>10.5%</td><td>181</td><td>82.3%</td><td>220</td></tr><tr><td>3</td><td>Verb tense</td><td>20</td><td>19.0%</td><td>16</td><td>15.2%</td><td>69</td><td>65.7%</td><td>105</td></tr><tr><td>4</td><td> Noun endings (singular/plural)</td><td>13</td><td>23.6%</td><td>7</td><td>12.7%</td><td>35</td><td>63.6%</td><td>55</td></tr><tr><td>5</td><td>Verb form</td><td>12</td><td>16.2%</td><td>11</td><td>14.9%</td><td>51</td><td>68.9%</td><td>74</td></tr><tr><td>6</td><td>Punctuation</td><td>87</td><td>68.0%</td><td>13</td><td>10.2%</td><td>28</td><td>21.9%</td><td>128</td></tr><tr><td>7</td><td> Articles/determiners</td><td>26</td><td>28.3%</td><td>9</td><td>9.8%</td><td>57</td><td>62.0%</td><td>92.</td></tr><tr><td>8</td><td>Word form</td><td>25</td><td>21.4%</td><td>6</td><td>5.1%</td><td>86</td><td>73.5%</td><td>117</td></tr><tr><td>9</td><td>Spelling</td><td>9</td><td>19.6%</td><td>2</td><td>4.3%</td><td>35</td><td>76.1%</td><td>46</td></tr><tr><td>10</td><td>Run-ons</td><td>14</td><td>43.8%</td><td>5</td><td>15.6%</td><td>13</td><td>40.6%</td><td>32</td></tr><tr><td>11</td><td>Pronouns</td><td>7</td><td>21.9%</td><td>4</td><td>12.5%</td><td>21</td><td>65.6%</td><td>32</td></tr><tr><td>12</td><td>Subject-verb agreement</td><td>17</td><td>38.6%</td><td>6</td><td>13.6%</td><td>21</td><td>47.7%</td><td>44</td></tr><tr><td>13</td><td>Fragments</td><td>3</td><td>42.9%</td><td>3</td><td>42.9%</td><td>1</td><td>14.3%</td><td>7</td></tr><tr><td>14</td><td>Idiom</td><td>5</td><td>26.3%</td><td>6</td><td>31.6%</td><td>8</td><td>42.1%</td><td>19</td></tr><tr><td>15</td><td>Informal</td><td>8</td><td>21.1%</td><td>5</td><td>13.2%</td><td>25</td><td>65.8%</td><td>38</td></tr></table></body></html>

Table 11 Revisions for different error categories in Task 2.   

<html><body><table><tr><td></td><td>Error type</td><td> no change</td><td>%</td><td>unsuccessful revision</td><td>%</td><td>successful revision</td><td>%</td><td>No.</td></tr><tr><td>1</td><td>Sentence structure</td><td>19</td><td>26.8%</td><td>13</td><td>18.3%</td><td>39</td><td>54.9%</td><td>71</td></tr><tr><td>2</td><td>Word choice</td><td>34</td><td>23.4%</td><td>27</td><td>18.6%</td><td>84</td><td>58.0%</td><td>145</td></tr><tr><td>3</td><td>Verb tense</td><td>7</td><td>18.4%</td><td>9</td><td>23.7%</td><td>22</td><td>57.9%</td><td>38</td></tr><tr><td>4</td><td> Noun endings (singular/plural)</td><td>2</td><td>5.0%</td><td>4</td><td>10.0%</td><td>34</td><td>85.0%</td><td>40</td></tr><tr><td>5</td><td>Verb form</td><td>8</td><td>22.9%</td><td>2</td><td>5.7%</td><td>25</td><td>71.4%</td><td>35</td></tr><tr><td>6</td><td>Punctuation</td><td>13</td><td>34.2%</td><td>9</td><td>23.7%</td><td>16</td><td>42.1%</td><td>38</td></tr><tr><td>7</td><td> Articles/determiners</td><td>8.</td><td>21.1%</td><td>4.</td><td>10.5%</td><td>26</td><td>68.4%</td><td>38</td></tr><tr><td>8</td><td>Word form</td><td>11</td><td>20.4%</td><td>10</td><td>18.5%</td><td>33</td><td>61.1%</td><td>54</td></tr><tr><td>9</td><td>Spelling</td><td>14</td><td>19.4%</td><td>3</td><td>4.2%</td><td>55</td><td>76.4%</td><td>72</td></tr><tr><td>10</td><td>Run-ons</td><td>7</td><td>17.1%</td><td>7</td><td>17.1%</td><td>27</td><td>65.9%</td><td>41</td></tr><tr><td>11</td><td>Pronouns</td><td>1</td><td>12.5%</td><td>3</td><td>37.5%</td><td>4</td><td>50.0%</td><td>8</td></tr><tr><td>12</td><td>Subject-verb agreement</td><td>2</td><td>6.3%</td><td>4</td><td>12.5%</td><td>26</td><td>81.3%</td><td>32</td></tr><tr><td>13</td><td>Fragments</td><td>6</td><td>20.7%</td><td>9</td><td>31.0%</td><td>14</td><td>48.3%</td><td>29</td></tr><tr><td>14</td><td>Idiom</td><td>0</td><td>0%</td><td>0</td><td>0%</td><td>4</td><td>100%</td><td>4</td></tr><tr><td>15</td><td>Informal</td><td>3</td><td>15.0%</td><td>1</td><td>5.0%</td><td>16</td><td>80.0%</td><td>20</td></tr></table></body></html>

Word, which features an automatic correction function that reduces spelling errors. In the second writing assignment, teachers did not remind them to bring laptops, and most students used tablets or smartphones for writing instead. The lack of an automatic correction function in the portable devices increased the number of spelling errors, and more errors were addressed in the ChatGPT-supported teacher feedback.

Moreover, different genres regarding the two writing tasks (i.e., argumentative and expository essays) might account for the differences that we identified across the tasks. Argumentative essays require students to develop a position on a particular issue and support it with evidence and reasoning (Chuang & Yan, 2022; Taylor et al., 2019), whereas expository essays aim to clearly and concisely explain a topic or provide information (Jeong, 2017; Zhang & Li, 2021). In other words, the complex structure of argumentative essays requires more in-depth feedback on content organization and the quality of the arguments. Since expository essays typically focus on explaining concepts or providing information, the ChatGPT-supported teacher feedback emphasized that the in formation was clear, logically organized, and effectively communicated. For example, word choice and sentence structure errors were prominent in both genres, but argumentative essays presented more issues related to argumentation and logical flow, whereas expository essays presented more issues with clarity and organization.

The finding that most errors were successfully amended suggests that the ChatGPT-supported teacher feedback helps students be more aware of language errors and effectively scaffolds them in writing revisions. Moreover, the students had more complete mod ifications to address the rhetorical feedback in Task 2. This change suggests the active engagement of the students with the feedback content, which led to successful attempts at revision. The students reported the clearness and helpfulness of the ChatGPT-supported teacher feedback compared with the traditional teacher feedback that they received in the post-task interviews, as shown in the following excerpts.

Excerpt 1: “Previously, teachers would just grade my essay, and I had no idea what specifically needed to be revised or where exactly I went wrong. However, the feedback from both ChatGPT and the teacher can clearly tell me where the mistakes are and which parts need improvement” (post-task interview with Student 1).

Excerpt 2: “Usually, teachers would just mark the errors, for example, by underlining them or circling them with a red pen. Sometimes, I couldn’t quite understand what exactly was wrong, whether it was a word choice error or a problem with my sentence structure, so it was difficult to make corrections. This method, where the teacher clearly tells me the type of error, makes it easier for me to revise” (post-task interview with Student 2).

Against the fact that EFL students received unclear teacher feedback on errors occurring in their writing, as reported in Zheng and Yu (2018) and Cheng et al. (2021), the students in this study received very clear and comprehensive error feedback, which they largely incorporated into their revisions. Moreover, the holistic rhetorical feedback generated by ChatGPT greatly helped instructors when they commented on the global issues. As one instructor stated,

I believe that providing holistic rhetorical feedback on the content is very important for students, but it is too time-consuming. Previously, I would just give a grade, and if time allowed, I would write a few sentences to tell the students where they could improve. I did not have time to praise the students’ work or engage in a dialogue with them. However, the content generated by ChatGPT can help me solve this problem. By adding and deleting, I can quickly provide high-quality rhetorical feedback to the students.

Thus, our study clearly indicates that ChatGPT-supported teacher feedback can ameliorate the situation where EFL teachers barely effectively address global issues of writing (Barrot, 2023a, 2023b; Mizumoto & Eguchi, 2023; Steiss et al., 2024).

Interestingly, our findings reveal that most errors related to the sentence structure and word choice, which were considered untreatable (Ferris & Roberts, 2001), were successfully revised in both writing tasks. This discrepancy echoes Han’s (2024) finding that computer-mediated coded WCF facilitates the successful revision of regularly considered untreatable errors such as word choice errors. These findings may indicate that EFL learners, who are now immersed in computer-mediated learning environments, can readily access online resources such as corpora and AI tools to transform previously untreatable errors into treatable ones (Mueller & Jacobsen, 2016; Ngo & Chen, 2024; Sataka, 2020). In other words, technological advancements such as AI have made certain error types more amendable than they used to be.

# 6. Conclusion

This study comprehensively examined the use of ChatGPT-supported teacher feedback in a Chinese tertiary EFL setting. The findings underscore the utility of ChatGPT in augmenting the traditional feedback process through the implementation of AI-teacher collaboration. The common understanding is that the performance of ChatGPT cannot currently compete with trained teachers (Steiss et al., 2024) and it can be difficult for all students to receive detailed constructive writing feedback from trained teachers in the EFL contexts. Thus, the collaboration of teachers and AI has emerged as a feasible solution to effective writing pedagogy, as clearly shown in this study.

A notable reduction in error rates across successive writing tasks suggests that ChatGPT-supported teacher feedback positively contributes to student writing skills, and the students could better address specific error types and revise the overall quality of their papers (e.g., adding examples to support one’s argument). Additionally, the study confirms that the integration of AI-powered feedback can effectively supplement human instruction and facilitate a more nuanced and efficient feedback process. The findings of our study resonate with recent studies utilizing AI-powered feedback (Han & Sari, 2024; Kao & Reynolds, 2024; Shi & Aryadoust, 2024). For example, feedback based on AI technologies such as “Criterion” combined with teacher feedback was more effective than teacher feedback alone in reducing errors in grammar and mechanics (Han & Sari, 2024). Additionally, Kao and Reynolds (2024)

applied AWE to provide feedback on students’ academic writings, and the results revealed that students highly trusted AI-powered feedback concerning grammar rules and lexical choices.

The present study suggests the need for a hybrid model of instruction that synergizes the analytical capabilities of AI with the pedagogical expertise of educators in the future. In response to the broad debate on the use of AI in language education, our position is that instead of eliminating the use of AI, we can make full use of AI with instructors’ agency. The “AI + Teacher” model can reduce the workload of teachers and enrich the educational landscape by offering a more adaptive and individualized learning experience. This study points to several targeted areas for future investigations. First, we implemented ChatGPT-supported teacher feedback in the Chinese EFL context in this study; future research can examine this new feedback approach in other instructional settings. Second, we focused on how students incorporated the ChatGPT-supported teacher feedback into their revisions but did not examine their per ceptions of this innovative feedback approach. Future studies can closely examine the attitudes of students toward and their per spectives on ChatGPT-supported teacher feedback in comparison to traditional teacher feedback. Third, in another study, we explored how instructors adapted ChatGPT outputs for teacher feedback and their positive perceptions of ChatGPT-supported teacher feedback. The perspectives of teachers, who are important stakeholders in this new feedback approach, deserve continued exploration. Fourth, the impacts of ChatGPT-supported feedback on specific writing outcomes and long-term writing performance of EFL/ESL students warrant closer examination.

In conclusion, our study reveals that ChatGPT can help teachers provide writing feedback, reduce their workload and offer im mediate personalized responses. In our study, ChatGPT-supported teacher feedback well addressed both global issues and local issues in the students’ writing, and the students incorporated the majority of the feedback into their writing revisions with increased per centages of incorporation across tasks. ChatGPT-supported teacher feedback, as reflected in the “AI $^ +$ Teacher” model, introduces innovation to the L2 writing pedagogy and deserves more inquiries in the coming years.

# Funding

This work was supported by Southwest University [grant number SWUPilotPlan002].

# Conflict of interest statement

We declare that we have no conflicts of interest.

# CRediT authorship contribution statement

Jining Han: Writing – review & editing, Writing – original draft, Resources, Methodology, Formal analysis, Data curation, Conceptualization. Mimi Li: Writing – review & editing, Validation, Supervision, Methodology, Conceptualization.

# References

Aljasir, N. (2021). Matches or mismatches? Exploring shifts in individuals’ beliefs about written corrective feedback as students and teachers-to-be. Journal of Teaching and Teacher Education, 9(1), 1–10. https://doi.org/10.12785/jtte/090101   
Barrot, J. S. (2023a). Using automated written corrective feedback in the writing classrooms: Effects on L2 writing accuracy. Computer Assisted Language Learning, 36 (4), 584–607. https://doi.org/10.1080/09588221.2021.1936071   
Barrot, J. S. (2023b). Using ChatGPT for second language writing: Pitfalls and potentials. Assessing Writing, 57, Article 100745. https://doi.org/10.1016/j. asw.2023.100745   
Bitchener, J., Young, S., & Cameron, D. (2005). The effect of different types of corrective feedback on ESL student writing. Journal of Second Language Writing, 14(3), 191–205. https://doi.org/10.1016/j.jslw.2005.08.001   
Bruner, J. (1985). Models of the learner. Educational Researcher, 14(6), 5–8. https://doi.org/10.3102/0013189X014006005   
Caldarini, G., Jaf, S., & McGarry, K. (2022). A literature survey of recent advances in chatbots. Information, 13(1), 41. https://doi.org/10.3390/info13010041   
Carter, T., & Thirakunkovit, S. (2019). A comparison of L1 and ESL written feedback Preferences: Pedagogical applications and theoretical implications. Journal of Response to Writing, 5(2), 139–174. https://scholarsarchive.byu.edu/journalrw/vol5/iss2/7.   
Cheng, X., Zhang, L., & Yan, Q. (2021). Exploring teacher written feedback in EFL writing classrooms: Beliefs and practices in interaction. In Language teaching research. Advance online publication. https://doi.org/10.1177/13621688211057665.   
Chuang, P. L., & Yan, X. (2022). An investigation of the relationship between argument structure and essay quality in assessed writing. Journal of Second Language Writing, 56, Article 100892. https://doi.org/10.1016/j.jslw.2022.100892   
Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., … Wright, R. (2023). “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71, Article 102642. https://doi.org/10.36227/techrxiv.21789434.v1   
Essel, H. B., Vlachopoulos, D., Essuman, A. B., & Amankwa, J. O. (2024). CHATGPT effects on cognitive skills of undergraduate students: Receiving instant responses from ai-based conversational large language models (llms). Computers and Education: Artificial Intelligence, 6, Article 100198. https://doi.org/10.1016/j. caeai.2023.100198   
Ferris, D. R. (1997). The influence of teacher commentary on student revision. Tesol Quarterly, 31(2), 315–339. https://doi.org/10.2307/3588049   
Ferris, D. (2006). Does error feedback help student writers? New evidence on the short- and long-term effects of written error correction. In K. Hyland, & F. Hyland (Eds.), Feedback in second language writing: Contexts and issues (pp. 81–104). Cambridge, UK: Cambridge University Press.   
Ferris, D. R. (2010). Second language writing research and written corrective feedback in SLA: Intersections and practical applications. Studies in Second Language Acquisition, 32(2), 181–201. https://doi.org/10.1017/s0272263109990490   
Ferris, D. R., Liu, H., Sinha, A., & Senna, M. (2013). Written corrective feedback for individual L2 writers. Journal of Second Language Writing, 22(3), 307–329. https:// doi.org/10.1016/j.jslw.2012.09.009   
Ferris, D., & Roberts, B. (2001). Error feedback in L2 writing classes: How explicit does it need to be? Journal of Second Language Writing, 10(3), 161–184. https://doi. org/10.1016/s1060-3743(01)00039-x   
Grimes, D., & Warschauer, M. (2010). Utility in a fallible tool: A multi-site case study of automated writing evaluation. The Journal of Technology, Learning, and Assessment, 8(6), 4–43. https://ejournals.bc.edu/index.php/jtla/article/view/1625/1469.   
Guo, K., & Wang, D. (2023). To resist it or to embrace it? Examining ChatGPT’s potential to support teacher feedback in EFL writing. Education and Information Technologies, 29(7), 8435–8463. https://doi.org/10.1007/s10639-023-12146-0   
Han, J. (2024). Students’ responses to computer-mediated coded feedback and the factors influencing those students’ responses: A multiple-case study in the cfl setting. Language, Learning and Technology, 28(1), 1–23. https://hdl.handle.net/10125/73547.   
Han, T., & Sari, E. (2022). An investigation on the use of automated feedback in Turkish EFL students’ writing classes. Computer Assisted Language Learning, 37(4), 961–985. https://doi.org/10.1080/09588221.2022.2067179   
Herbold, S., Hautli-Janisz, A., Heuer, U., Kikteva, Z., & Trautsch, A. (2023). A large-scale comparison of human-written versus ChatGPT-generated essays. Scientific Reports, 13(1), Article 18617. https://doi.org/10.1038/s41598-023-45644-9   
Jeong, H. (2017). Narrative and expository genre effects on students, raters, and performance criteria. Assessing Writing, 31, 113–125. https://doi.org/10.1016/j. asw.2016.08.006   
Kao, C. W., & Reynolds, B. L. (2024). Timed Second Language writing performance: Effects of perceived teacher vs perceived automated feedback. Humanities and Social Sciences Communications, 11, 1012. https://doi.org/10.1057/s41599-024-03522-3   
Kohnke, L., Moorhouse, B., & Zou, D. (2023). ChatGPT for language teaching and learning. RELC Journal, 54(2), 537–550. https://doi.org/10.1177/ 0033688223116286   
Mizumoto, A., & Eguchi, M. (2023). Exploring the potential of using an AI language model for automated essay scoring. Research Methods in Applied Linguistics, 2(2), Article 100050. https://doi.org/10.1016/j.rmal.2023.100050   
Mueller, C. M., & Jacobsen, N. D. (2016). A comparison of the effectiveness of EFL students’ use of dictionaries and an online corpus for the enhancement of revision skills. ReCALL, 28(1), 3–21. https://doi.org/10.1017/S0958344015000142   
Ngo, T., & Chen, H. (2024). The effectiveness of corpus use in ESL/EFL writing: A meta-analysis. In Language teaching research. Advance online publication. https:// doi.org/10.1177/13621688241260183.   
Ranalli, J. (2018). Automated written corrective feedback: How well can students make use of it? Computer Assisted Language Learning, 31(7), 653–674. https://doi. org/10.1080/09588221.2018.1428994   
Ranalli, J. (2021). L2 student engagement with automated feedback on writing: Potential for learning and issues of trust. Journal of Second Language Writing, 52, Article 100816. https://doi.org/10.1016/j.jslw.2021.100816   
Sataka, Y. (2020). How error types affect the accuracy of L2 error correction with corpus use. Journal of Second Language Writing, 50, Article 100757. https://doi.org/ 10.1016/j.jslw.2020.100757   
Shi, H., & Aryadoust, V. (2024). A systematic review of AI-based automated written feedback research. ReCALL, 36(2), 187–209. https://doi.org/10.1017/ s0958344023000265   
Steiss, J., Tate, T., Graham, S., Cruz, J., Hebert, M., Wang, J., Moon, Y., Tseng, W., Warschauer, M., & Olson, C. B. (2024). Comparing the quality of human and ChatGPT feedback of students’ writing. Learning and Instruction, 91, Article 101894. https://doi.org/10.1016/j.learninstruc.2024.101894   
Stevenson, M., & Phakiti, A. (2014). The effects of computer-generated feedback on the quality of writing. Assessing Writing, 19, 51–65. https://doi.org/10.1016/j. asw.2013.11.007   
Taylor, K. S., Lawrence, J. F., Connor, C. M., & Snow, C. E. (2019). Cognitive and linguistic features of adolescent argumentative writing: Do connectives signal more complex reasoning? Reading and Writing, 32, 983–1007. https://doi.org/10.1007/s11145-018-9898-6   
Vygotsky, L. S. (1978). Mind in society: Interaction between learning and development. Cambridge. MA: Harvard University Press.   
Wilson, J., & Czik, A. (2016). Automated essay evaluation software in English Language Arts classrooms: Effects on teacher feedback, student motivation, and writing quality. Computers & Education, 100, 94–109. https://doi.org/10.1016/j.compedu.2016.05.004   
Wilson, J., Olinghouse, N. G., & Andrada, G. N. (2014). Does automated feedback improve writing quality? Learning Disabilities: A Contemporary Journal, 12(1), 93–118. https://files.eric.ed.gov/fulltext/EJ1039856.pdf.   
Wilson, J., & Roscoe, R. D. (2020). Automated writing evaluation and feedback: Multiple metrics of efficacy. Journal of Educational Computing Research, 58(1), 87–125. https://doi.org/10.1177/0735633119830764   
Yan, D. (2023). Impact of ChatGPT on learners in a L2 writing practicum: An exploratory investigation. Education and Information Technologies, 28(11), 13943–13967. https://doi.org/10.1007/s10639-023-11742-4   
Yang, M., Badger, R., & Yu, Z. (2006). A comparative study of peer and teacher feedback in a Chinese EFL writing class. Journal of Second Language Writing, 15(3), 179–200. https://doi.org/10.1016/j.jslw.2006.09.004   
Zhang, X., & Li, W. (2021). Effects of n-grams on the rated L2 writing quality of expository essays: A conceptual replication and extension. System, 97, Article 102437. https://doi.org/10.1016/j.system.2020.102437   
Zheng, Y., & Yu, S. (2018). Student engagement with teacher written corrective feedback in EFL writing: A case study of Chinese lower-proficiency students. Assessing Writing, 37, 13–24. https://doi.org/10.1016/j.asw.2018.03.001