# Friend or foe? Google Translate in language for academic purposes

Michael Groves a,1 , Klaus Mundt b,\*

a Centre for English Language Education, University of Nottingham, Malaysia Campus, Jalan Broga, Semenyih, Selangor, 43500, Malaysia b Centre for English Language Education, School of Education, The University of Nottingham, Dearing Building, Jubilee Campus, Wollaton Road, Nottingham, NG8 1BB, United Kingdom

# a r t i c l e i n f o

# a b s t r a c t

Article history: Available online 2 October 2014

Keywords:   
Language for academic purposes   
Machine translation   
IT in SLA   
Academic literacy

A recent development in digital technology, machine translation (MT), is improving in its ability to translate with grammatical and lexical accuracy, and is also becoming increasingly available for students of language for academic purposes. Given the acceptance of other digital technology for teaching and learning, it seems likely that machine translation will become a tool students will rely on to complete their assignments in a second language. This would have implications for the community of practice of academic language teaching. In this study students were asked to submit an essay in their first language and this was then translated into English through a web-based translation engine. The resulting English text was analysed for grammatical error. The analysis found that the translation engine was far from able to produce error-free text – however, judging in relation to international testing standards, the level of accuracy is approaching the minimum needed for university admission at many institutions. Thus, this paper sets out to argue, based on the assumption that MT will continue to improve, that this technology will have a profound influence on the teaching of Languages for Academic Purposes, and with imaginative use, will allow this influence to be positive for both the students and their instructors.

$©$ 2014 Elsevier Ltd. All rights reserved.

# 1. Introduction

An expert writer of academic English writes at a number of levels. The writer needs to take well-formed words, and use them to construct well-formed sentences, and then link these sentences together into clear, coherent and cohesive paragraphs. Beyond this, the writer then needs to align the writing with the generic expectations of the intended audience (Swales, 1998), as well as the stylistic conventions of the discipline that are commonly attached to specific genres (Biber, 2006). So the writer aligns the writing with the expectations of the communities of practice and enquiry in the field in which the writing takes place (Hyland, 2004).

Technological developments have long helped writers of academic English with many of these issues, either directly or indirectly. For example, corpus analysis has made much stronger and more robust descriptions of academic English as a genre, as well as the sub-genres within it than traditional linguistic work possibly could have (Peacock, 2002; Reid, 1992; Silver, 2003). Another example of beneficial uses of digital technology is that the wide availability of published papers on the Web has allowed students and researchers to examine the style and structure of vast swathes of academic writing. Further, freely available tools, such as Lextutor (www.lextutor.ca) and concordance tools have allowed students of academic English to examine their own interlanguage through various lenses and improve it (Cobb, 2010). This is well documented in various threads in the literature (Adolphs, 2006; Bruce, 2010).

In addition, new technologies have even entered the toolkit of all academic writers with little or no fanfare in the EAP literature. The spellchecker has enabled non-native and native speakers alike to write English that is less prone to error, and the inclusion of an effective autocorrect facility removes many mistakes without the user even being aware of their existence. Some word processors also have grammar checkers and the more recent development of the ‘format consistency checker’, signified by blue underlining in Microsoft Word. However, the success of these features has been less impressive (Vernon, 2000).

On the other hand, there is another technology that could overtake and replace these features – web-based machine translation (MT), most noticeably Google Translate. This is a free, web-based service that translates between a variety of languages. It is also available on mobile devices as an app. Google Translate is a statistics-based translation tool, which means that the system calculates probabilities of various translations of a phrase being correct, rather than aiming for word-for-word translation. It also has a level of interactivity with the end user, with users being able to correct the original translation, and this information being absorbed into the database.

Machine translation by means of computers dates back to punch card systems in the 1940s. Since then it has experienced several setbacks and significant advances (Hutchins, 2000), and with developments in Artificial Intelligence might well be poised to reach a degree of significant sophistication (Newton, 1992). Early machine translations were often of poor, even impenetrable, quality (Komeli, Hendavalan, & Rahimi, 2011), and are still far from perfect. However, the use of MT tools is becoming more and more widespread. Examples include the socio-political empowerment of minority language communities in Canada (Bowker, 2008), the use of spoken machine translation for non-English speakers in the British healthcare system (Somers & Lovel, 2006) and screening of the gist of news reports by US intelligence agencies (Koehn, 2010). Despite the fact that the quality of the translation is often regarded as poor in comparison to human translations, the use of MT is now reaching a much wider audience than before (Hutchins, 2006), and the development of more sophisticated MT options is receiving more substantial attention from policymakers (Bellos, 2012).

Several MT applications are available, both as professional software and web-based freeware, that can cater to different clients and specific purposes (Austermühl, 2001). This study focusses on only one of those, namely Google Translate, since, while a comparison of the quality of different MT tools would be interesting, it goes beyond the remit of this paper.

Once MT has become an effective tool, it has the potential to have a profound impact on the field of language teaching. After all, why would a potential student go to the effort and expense of learning a foreign language if she is able to produce an acceptable L2 text from her own L1 writing, instantly and with no financial cost? It is therefore vital to understand the ability of MT to create sophisticated academic text, and the present study takes a step towards that goal.

The aim of this study is to examine text which has been translated from Malay (Bahasa Melayu) and written Chinese through the Google Translate engine into English. The study will only investigate the grammatical accuracy of the translation, without examining the larger discoursal and epistemological features of the writing. It aims to discover if Google Translate has the ability to produce stretches of grammatically correct, communicative English.

Studies on this topic are scarce, probably due to the lack of imminent implications for current EAP practice. However, it would be short-sighted to ignore this emerging technology and the changes it may bring with it. The present paper’s contribution to considerations related to academic language teaching and learning is to highlight these possible changes and to hopefully trigger more specific discussion on the benefits and the threats MT can bring, so EAP remains well prepared to address the needs of its community of practice.

# 2. Technological advances and their use in EAP

Technological advances have always played a significant role in second language teaching and acquisition, and they have generally been accepted as valuable tools in the classroom, for autonomous practice, for tutor–student communication and for research. The fact alone that most EAP courses these days seem to provide additional learning resources for their students on online platforms indicates that digital technology has firmly established itself as an integral and valued part of EAP.

In the course of this development, the face of the classroom has been altered as well. For instance, traditional learner dictionaries have largely given way to electronic dictionaries accessed through mobile phones, tablet computers or laptops, which, in turn, allow students to access other applications as well (Mehta, 2012). The quality of these dictionaries and other learning aids seems questionable on occasion, so the EAP practitioner’s role has been extended to that of quality assurance, or at the very least to that of a guide to reliable online resources. After all, in the plethora of resources available to the student, there are significant differences in quality and reliability, and it seems part of the EAP tutor’s remit to stay abreast of recent developments so as to be able to recommend appropriate ones to their students (Chapelle, 2003; Mehta, 2012).

It would seem that the willingness with which useful tools have been accepted into EAP practice may lead to other tools finding their way into the classroom as well; some of these, for example established data-driven tools, such as concordancers, have since their inception been used extensively in teaching, research and materials development (Oakey, 2010). Other technological applications may not receive such a warm welcome. Given the availability of online resources to the students in the classroom, it is more than likely that students will not stop at the use of online dictionaries. It seems inevitable that students will also utilise applications that will allow them not only to understand the meaning of single lexical items but also to render entire stretches of text into their L1 or into English. Unless the use of online technology is banned from the classroom, it seems that students’ use of online translation tools is inevitable – and this may be cause for concern.

This paper does not adopt the position of one of the specific streams of thought discussed by Chapelle (2003, p.10), namely the technologist, the social pragmatist or the critical analyst position. Rather, it places itself in a more neutral analytical space. Thus we do not advocate the growing influence of technology as a welcome and positive development, but neither do we perceive technology as inextricably connected to ideology and commerce. In that sense, this study takes place from a perspective that could be termed ‘analytical realism’ – namely the investigation of research data to illuminate the space between technology and language teaching/learning without the ideological bias of the three schools of thought mentioned above.

# 2.1. Mechanics and quality of machine translation

According to Cancedda, Dymetman, Foster, and Goutte (2009), ideas and ambitious plans for MT stretch back to the 1940s. Traditionally, MT had been based on the idea of parsing and generations – that is, the computer understanding the underlying meaning of a text in the source language, and then applying a new set of rules belonging to the target language. However, by the 1980s this emphasis had shifted to statistics-based translation practise, and this is still the dominant paradigm.

Currently, the reliability of MT is still questionable, and it seems advisable not to rely on it as an absolute communication tool, but, if used, to ensure that the content of a translation is verified. In that, as Scherf (1992) points out, MT remains a device of support, not one that eliminates the necessity to acquire a degree of proficiency in a language the writer wants to communicate in. In addition, Sheppard (2011) acknowledges the usefulness of Google Translate, especially when the cost is compared to the use of professional translation services, but describes the many flaws of the translated text, from syntactic problems to issues of style and identity. She also points out that there is a major issue with confidentiality, since the text is stored on the Google servers – which, in the academic context, would have serious ethical implications. Other empirical studies in the use of MT have come to similar conclusions. Costa-jussá, Farrús, and Pons (2012) found that in the field of medicine, the translation program was unable to produce acceptable translations all of the time. They also noted a major difference between the Romance and Germanic languages, for which the translation program could produce acceptable, as defined by the researchers, translations up to $8 0 \%$ of the time, and other languages, such as Basque, for which these could only be produced $2 7 \%$ of the time. Meanwhile, Kirchoff, Turner, Axelrod, and Saavedra (2011) found that when health literature was translated between Spanish and English, the quality was unacceptable unless post-editing by a human translator took place. They also found that the most common error types were morphological and in word sense. Both of the above studies came to the conclusion that MT was only truly effective when used with a human post-editor.

One of the reasons that MT is not yet sufficiently developed to be authoritative in producing accurate renditions of a text is that it is not able to evaluate the context of a stretch of text that a human translator or writer will take into account. Kaltenbacher (2000) identifies this as the cause for a relatively large number of syntactical errors in MT, in particular across languages in which the grammar differs significantly. This finds support in a recent study by van Rensburg, Snyman, and Lotz (2012). In this study it was found that Google translations of academic texts between English and Afrikaans were no match for those created by human translators, indicating that MT is not yet sufficiently sophisticated to create acceptable target texts.

In the case of Google Translate, the most widely accessible translation tool that is currently available, Bellos (2012) points out that, due the enormous amount of data it relies on, Google Translate is an exceptionally well-developed translation tool that quite frequently produces acceptable target texts. However, he also points out that the language pairs that Google Translate handles rely heavily on what translations have been produced prior to a new translation attempt. This means that Google Translate draws cross references between two corpora – that of the source language and that of the target language. The more similar texts exist in both languages, the more precise the translation can be because it can utilise recurring linguistic bundles in both languages. This in turn means that Google Translate is probably much more reliable when it comes to translations between the dominant language English and one that is also frequently used for the same purpose. In other words, it seems more likely to produce an accurate translation between English and French academic texts, both languages that have a long tradition of academic genres and a long history of translated seminal works, than to produce an accurate translation between English and a language that has not had such an extensive academic text production and exchange.

Austermühl (2011) discusses the issue of the quality of MT in some depth and from a number of perspectives. He described the idea that MT is not aiming for ‘high quality’ translation. Instead, it is aiming for ‘usable’ translation. He also suggests that the perceived rise in the quality of machine translation is in fact due to a lowering of expectations, suggesting that the foreseeable future lies in machines and humans working together in order to create polished stretches of the target language. However, he does not take into account the situation of many students studying in a medium where English is not their first language – they understand that their own English is not of a high quality, and may believe that using a free translation service such as Google is the better option for them, despite the drawbacks.

On the other hand, Google, with some justification, claim great success for their translation engine. In a recent blog post (Google, 2012), they claim that they translate in any one day the same amount of text found in 1 million books. Put another way, the output of the translation engine in one day is equivalent to the output of all the world’s professional translators in a year – thus becoming responsible for the majority of the world’s translations. However, it is not clear whether this translation is receptive (in order to understand something in a foreign language) or productive (in order to generate something in a foreign language). If a user is using the tool to read something in a foreign language, there will be a far greater tolerance of error in terms of sentence structure. This is because the emphasis will be only on personal understanding of the text. However, if the user is expecting the program to create text to be read by the reader of the target language, there is likely to be a greater need, or at least expectation of, grammatical accuracy.

This is the reason why monitoring developments in the accuracy of Google Translate is so interesting and certainly a valuable aspect of language teaching. It seems likely that one of three things will happen. Either English (in one of its ‘standard’ forms or in a lingua franca form) will be used increasingly as the primary language of communication in academia, which means that there will be a growing demand for EAP. Or there will be a stronger reliance on local languages to deepen or establish local academic traditions and identities. In the latter case, the use of communication aids is likely to become more widespread, which in turn would mean that Google Translate would become increasingly reliable making it more attractive to EAP learners. In a third scenario, a different language would take over from English as an international lingua franca, again increasing the likelihood of machine translation being used.

# 3. Methodology

# 3.1. Analytical framework

What is arguably of the greatest interest to the EAP community of practice is whether or not Google Translate is at present able to accurately render a source text into the target language of English. To assess this, two approaches come to mind. One proposed by Colina (2009) investigates the issue from a perspective of translation quality. This approach has been previously employed by van Rensburg et al. (2012) to investigate the quality of translations by Google Translate of different academic genres from Afrikaans into English and from English into Afrikaans. However, van Rensburg, Snyman and Lotz’s study focussed on the translation competence of Google Translate, and therefore on the overall quality of the translation, while the present study is more interested in the linguistic accuracy of the translation product. Hence, Colina’s assessment model does not seem appropriate for the present purposes.

We will instead make use of the taxonomy of error types introduced by Ferris, Liu, Sinha, and Senna (2013). This seems more suitable, as we intend to treat the translation like we would treat student writing in English, for this is how a text would be treated in EAP contexts. Ferris et al.’s taxonomy (see Table 1 for a more detailed description) allows for detailed error analysis and thus a measurable assessment of the translation’s linguistic accuracy. This will in turn allow for an assessment of its acceptability as an EAP student text. It will also allow for conclusions to be drawn regarding which areas already show a degree of sophistication and which the translation software at present does not seem to be able to cope with.

Empirical research that subjects Google translations to analysis of specific linguistic features, to date, is hard to find. Most studies seem concerned with an assessment of the translation quality and efficiency. That is why most studies seem content with more general findings and comparisons between human and machine translations (e.g., Aiken & Balan, 2011; Garcia, 2010); yet, it has been demonstrated that Google Translate seems to produce more accurate results when working with European languages, while it seems to struggle with Asian languages (Aiken & Balan, 2011). Research that investigates MT from the perspective of English language acquisition and EAP is, to our knowledge, not available, and more intricate frameworks that look at sentence level accuracy, that is those that would lend themselves to be used as marking/feedback criteria for second language text production, as utilised in the present study, do not seem to have been used in MT research.

Table 1 Error coding (adapted from Ferris et al., 2013).   

<html><body><table><tr><td>Code</td><td>Title</td><td>Example from translations</td></tr><tr><td>VT</td><td>Verb tense</td><td>one that measured the level of ability in several ways</td></tr><tr><td>VF</td><td>Verb phrase</td><td>the individual who failed the exam ignored or looked down upon by society</td></tr><tr><td>WF</td><td>Word form</td><td>A student test detects only the ability to say yes or memory</td></tr><tr><td>ART</td><td>Article</td><td> same way to learn the memorizing</td></tr><tr><td>PL</td><td>Plural</td><td>Examination, especially in Malaysia plays an important role</td></tr><tr><td>AGR</td><td>Agreement</td><td>Activities such as off-site is very dominant</td></tr><tr><td>PREP</td><td>Preposition</td><td>Abuse and misunderstanding among students on examinations should be eliminated</td></tr><tr><td>wo</td><td>Word order</td><td>students will focus on such topics only</td></tr><tr><td>ww</td><td>Wrong word</td><td>Support parents and teachers are required so that they can be overcome</td></tr><tr><td>wc</td><td>Word choice</td><td>Examination is considered something very high</td></tr><tr><td>COM</td><td>Comma</td><td>Learning aspects such as, music and art, can not be measured.</td></tr><tr><td>SP</td><td> Spelling</td><td>How can the ideological principles Specifically implement them?</td></tr><tr><td>AP</td><td>Apostrophe</td><td>Third, teachers and students too expect students exam results.</td></tr><tr><td>Ss</td><td>Sentence structure</td><td>This result is that parents do not ignore and less affection on them.</td></tr><tr><td>mw</td><td>Missing word</td><td>First, the examination has been highly beneficial to students but also students to study a topic that will be tested only on the exam</td></tr><tr><td>REF</td><td>Pronoun reference unclear</td><td>and students will focus on such topics</td></tr><tr><td>PRO RO</td><td>Pronoun incorrect Run on</td><td>Teachers will also place high expectations on him I believe that in order to test the ability of the method to detect the candidates more harm</td></tr><tr><td></td><td></td><td>than good, in other words, the examination system is not a good way to test students&#x27; abilities In addition, people who have a bias to the students who got poor marks from students who</td></tr><tr><td>FRAG</td><td>Fragment</td><td>get higher scores.</td></tr><tr><td>UNCLEAR</td><td> Unclear</td><td>College entrance examination system for screening system, especially in the eyes of their talents</td></tr></table></body></html>

Of course, we expressly recognise here that EAP deals with much more than sentence level accuracy. Indeed it would seem that the focus of EAP lies elsewhere, namely on discourse features, considerations of sociolinguistic competence with regard to different disciplines and the fostering of autonomous learning strategies (Alexander, Argent, & Spencer, 2008; Bruce, 2010). However, we argue that linguistic accuracy will always factor in the clarity of student writing. It is for this reason that the present study investigates linguistic accuracy in such detail, so as to be able to make inferences about the translation’s overall success. It is also in the areas of lexical and grammatical accuracy that we anticipate the most impressive advances in translation technology in the near future because it seems easier to establish equivalencies at word, collocational and sentence level across languages based on corpus data than to establish metadiscoursal equivalencies, provided they even exist in a language pair. Thus, it seems appropriate, given the present state-of-the-art of machine translation, to investigate text at this basic level.

# 3.2. Study procedure

The students involved were enrolled on a pre-university foundation course at a UK branch campus in Malaysia. The course balances input in the area of EAP and also more content-based social science modules. The emphasis of the EAP modules is on discourse level content, rather than sentence level grammar.

After ethical approval was granted, the students were asked to write a short essay in their first language with the title, “Exams are the best way to assess students: Discuss”. The essays were written in their own time and then emailed to the researchers, who translated them through the Google Translate interface. The students were asked to write as they would at school – not to try and apply what they had learned in previous EAP classes. This was done so that the students would not try to ‘hybridise’ their writing, using, for example, organisational patterns they had been taught as part of EAP, but not in their first language. This exercise took place in August 2013. In total five scripts were translated, made up of 1,523 words in Malay and 744 words in Chinese.

Certain words were not translated by the program. In such cases a native speaker of the source language was consulted. If it was a matter of a clear typographical error in the original, then the original was changed to the correct version and the text was re-translated. The three instances where this did not solve the issue probably illustrate lexical limitations in the program. The translations were then independently coded for errors by two researchers, and an agreed count of errors was arrived at. Agreement between both researchers was 92 per cent on average. Both researchers are experienced tutors of EAP, both in the UK and overseas. Both hold Masters degrees in a related field and are actively engaged in assessing EAP courses.

The errors were coded using an adapted model from Ferris et al. (2013), which can be seen in Table 1. This framework was chosen because it was based on EAP corrective feedback – in other words, areas that experienced EAP practitioners deemed important when correcting student writing. Therefore the focus was on the communicative effect of the writing.

Table 1 shows the coding categories, and examples from the translated scripts. The framework has been changed slightly. In the original there were two categories, one for ‘run on sentence’ and the other for ‘comma splice’. This model only uses ‘run on sentence’ for the sake of simplicity, as, arguably, the two are quite similar. In addition, a category of ‘unclear’ was added, in case the intended meaning was impenetrable.

# 4. Results

The results were mixed. Table 2 shows the number of errors in each script.

It is clear from the data presented in Table 2 that the translation program was more accurate when translating Malay than Chinese, with roughly half the number of errors per 1,000 words, (64.3 and 134.4, respectively). It is worth noting that the number of errors is lower than the number of sentences in Script 2, meaning that the translation program was able to translate some sentences without apparent error.

Table 3 shows the aggregated counts between the two languages that were measured. It clearly shows that the program is more able to produce grammatical, correct English from Malay than from Chinese. In addition, it shows that the Malay scripts were averaging fewer than one error per sentence, thus suggesting that the program is able to produce intermittent stretches of grammatically correct English. One possible reason for this is the relative strength of English in Malaysia – there is a larger number of documents in both Malay and English online – leading to a larger pool of potential translations for the Google engine.

Table 2 Errors per script.   

<html><body><table><tr><td>Script</td><td>Languagee</td><td>Words</td><td>Sentences</td><td>Errors</td><td>Errors per 100 words</td><td>Errors per sentence</td></tr><tr><td>1</td><td> Malay</td><td> 555</td><td>33</td><td>40</td><td>7.20721</td><td>1.21212</td></tr><tr><td>2</td><td> Malay</td><td>460</td><td>42</td><td>28</td><td>6.08696</td><td>0.66667</td></tr><tr><td>3</td><td> Malay</td><td>508</td><td>28</td><td>30</td><td>5.90551</td><td>1.07143</td></tr><tr><td>4</td><td>Chinese</td><td>443</td><td>11</td><td>64</td><td>14.447</td><td>5.81818</td></tr><tr><td>5</td><td>Chinese</td><td>301</td><td>12</td><td>36</td><td>11.9601</td><td>3</td></tr></table></body></html>

Table 3 Aggregated error counts by language.   

<html><body><table><tr><td>Source language</td><td>Total words</td><td>Total sentences</td><td>Total errors</td><td>Errors per 1000 words</td><td>Errors per sentence</td></tr><tr><td> Malay</td><td>1523</td><td>103</td><td>98</td><td>64.34668</td><td>0.951456</td></tr><tr><td>Chinese</td><td>744</td><td>23</td><td>100</td><td>134.4086</td><td>4.347826</td></tr></table></body></html>

In terms of the classifications of errors counted, Table 4 shows some interesting results. Firstly, the most common forms of error were to do with word choice, followed by sentence structure and missing words. This would suggest that the program struggles with the subtle distinctions of meaning between words in the target language as well as the original language. In addition, it seems to be unable to parse certain structures of the original language effectively and may be left to translate word for word, which can lead to a lack of clarity in the text produced

Table 5 provides evidence from the scripts of instances where Google Translate struggled to render the text appropriately, illustrating clear limitations. The sample error categories chosen here are arguably those that can significantly affect reader comprehension; and, thus, the lack of translation accuracy can have a significant impact on the quality of the target text.

However, it is also interesting to note some of the areas where the program did relatively well. There were virtually no spelling errors – this is not surprising, given the dichotomous nature of spelling accuracy, and the lack of ambiguity involved. What is more interesting is the fact that areas like pronoun use, preposition use and article use were handled well by the translation software. These are areas with which EFL learners often struggle. Also, for example the article as it is used in English does not exist in Malay or Chinese. It would thus seem that the software was here able to compensate such syntactical gaps between the languages.

It is also illuminating to look at some of the sentences that the program produced which are not only grammatically correct but also written in a convincing academic style, as illustrated in Table 6.

These sentences clearly show the ability of the program to translate relatively long stretches into clear and formal English. However, also apparent is a certain cultural imprint in the discourse the writers produced with the help of the translation engine. For example, emphasis on the importance of parental support to enhance examination results is more likely to be seen in a Confucian educational context than a Socratic one. In addition, the Chinese writer of script 4 started the essay with a discussion of the Tang Dynasty, and how their influence was still apparent in modern-day China. This supports the idea that the program is at present unable to translate “beyond the sentence” – in other words, to deal in a meaningful way with concepts that may be culturally or generically inappropriate to the target field of writing.

Overall, then, the Google translation service was able to produce stretches of clear and accurate English. However, this accuracy was patchy, and at times led to a breakdown of clarity.

# 5. Discussion

Even though Google Translate was able to translate a number of stretches of Malay or Chinese into grammatically correct English, it was far from producing overall convincing results. There were errors in all areas of Ferris et al.’s (2013) taxonomy, and the majority of these were to do with sentence structure and word choice. The scripts, while communicatively relatively clear, were riddled with error. The translation program, then, is not yet able to be used as a substitute for a professional translator or language proficiency. The level of error is far too high to create what could be called a polished or professional standard of language.

Table 4 Error classifications.   

<html><body><table><tr><td>Student scripte</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>Grand total</td></tr><tr><td>Agreement</td><td>1</td><td>1</td><td>1</td><td>2</td><td>1</td><td>6</td></tr><tr><td>Apostrophe</td><td>1</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Article</td><td>1</td><td></td><td>1</td><td>3</td><td>1</td><td>6</td></tr><tr><td>Comma</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>5</td></tr><tr><td>Fragment</td><td></td><td>1</td><td></td><td>2</td><td>3</td><td>6</td></tr><tr><td>Missing word</td><td>4</td><td>1</td><td>5</td><td>10</td><td>6</td><td>26</td></tr><tr><td>Plural</td><td>4</td><td>2</td><td>3</td><td>4</td><td></td><td>13</td></tr><tr><td> Pronoun incorrecte</td><td>1</td><td></td><td>1</td><td></td><td></td><td>2</td></tr><tr><td>Run on</td><td></td><td>1</td><td>1</td><td>6</td><td></td><td>:</td></tr><tr><td>Sentence structure</td><td>5</td><td>8</td><td>1</td><td>6</td><td>3</td><td>23</td></tr><tr><td>Unclear</td><td></td><td></td><td>1</td><td>5</td><td>3</td><td>9</td></tr><tr><td>Verb phrase</td><td></td><td></td><td></td><td>2 </td><td>3</td><td>5</td></tr><tr><td>Verb tense</td><td>2</td><td>1</td><td>5</td><td>3</td><td>4</td><td>15</td></tr><tr><td>Word choice</td><td>4</td><td>1</td><td>3</td><td>3</td><td>1</td><td>12</td></tr><tr><td>Word form</td><td></td><td>3</td><td></td><td>3</td><td>2</td><td>8</td></tr><tr><td>Word order</td><td>2</td><td>1</td><td>2</td><td>2</td><td>1</td><td>8</td></tr><tr><td>Wrong word</td><td>7</td><td>6</td><td>5</td><td>10</td><td>7</td><td>35</td></tr><tr><td>Proposition</td><td>2</td><td>1</td><td></td><td>1</td><td></td><td>4</td></tr><tr><td>Pronoun reference unclear</td><td>5</td><td></td><td></td><td></td><td></td><td>5</td></tr><tr><td>Spelling</td><td></td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Grand total</td><td>40</td><td>28</td><td>30</td><td>64</td><td>36</td><td>198</td></tr></table></body></html>

Table 5 Samples of flawed translations.   

<html><body><table><tr><td>Error category</td><td>Example</td><td>Script</td></tr><tr><td rowspan="2">Word choice</td><td>Therefore, this forum would like to discuss the disadvantages of a comprehensive examination. NB: The purpose statement of the text</td><td>3</td></tr><tr><td>... such a large population base in the kingdom of exam system is a more equitable and appropriate educational detection system. NB: One might speculate that &#x27;kingdom&#x27; could be replaced by &#x27;realm&#x27; and &#x27;detection&#x27; might be replaced</td><td>4</td></tr><tr><td rowspan="3">Sentence structure</td><td>by &#x27;assessment&#x27; - however, the software opted for other choices. From educational institutions to the religious institution, one that measured the level of ability in several</td><td>2</td></tr><tr><td>ways, for example, examinations. NB: While there is more than one issue in this sentence, the main one seems to be that it is an incomplete</td><td></td></tr><tr><td>structure without a main clause. .. some talents and abilities is not possible in an exam in which can be tested. NB: Apart from the issue in subject-verb agreement, the use of the relative clause affects the</td><td>5</td></tr><tr><td rowspan="3">Unclear</td><td>sentence structure overall.</td><td>3</td></tr><tr><td>However, no doubt slip showing the performance of the examination and an important achievement as a measure of credibility in the workplace and so on.</td><td></td></tr><tr><td>In the extreme case, but even common sense are also ignored, focusing on university students lack knowledge of life news uncommon, students lose your wallet cry by reminding parents to go first to know of cases is not just a special case of an alarm</td><td>4</td></tr></table></body></html>

Table 6 Examples of accurate translations.   

<html><body><table><tr><td>Example</td><td>Script</td></tr><tr><td>For example, the teacher will inform the students about the topics that will be tested in the exam and students will focus on such topics only.</td><td>1</td></tr><tr><td>There are times when students pay bribes to teachers to pass exams.</td><td>1</td></tr><tr><td>Finally, moral support from parents can give students a passion for outstanding results in examinations.</td><td>1</td></tr><tr><td>Intelligence or excellence of a person can not be assessed through exams.</td><td>2</td></tr><tr><td>Therefore, people should change their perspective and open their minds to look from another angle.</td><td>2</td></tr><tr><td>Overall, the authorities should recognize the seriousness of this problem.</td><td>3</td></tr><tr><td>On the other hand, the emperor also made use of the examination system to complete his statist ideology [...]</td><td>4</td></tr><tr><td>[...] in other words, the examination system is not a good way to test students&#x27; abilities.</td><td>4</td></tr><tr><td>[...] the stage is a dancer&#x27;s platform to express their talents and abilities</td><td>5</td></tr></table></body></html>

However, it is important to bear in mind that, while universities would normally expect their students to strive for excellence in their skills of expression, they are also pragmatic – understanding that grammatical accuracy supports communicative competence, but does not dictate it. In their public descriptors, IELTS defines a 6.0 in their grammatical category as “makes some errors in grammar and punctuation but they rarely reduce communication” (IELTS, n.d.). It defines 8.0 as “the majority” of sentences without error – showing that even at a very high level, IELTS would tolerate certain levels of grammatical error. Therefore, it is important to see these results not through the lens of a native speaker. Rather, we should see them in comparison with an academic writer who is on the margins of acceptable competence. The question is whether the translate program can perform as well as, or better, than a student who has just managed to “scrape though IELTS”.

The answer is – from a grammatical point of view – no longer unequivocally ‘no’, but also not yet definitely ‘yes’. There is clearly a long way to go before the program can create long stretches of accurate text, and a large number of errors still feature in the translations. The evidence presented in this paper shows that there is a difference in accuracy when Google Translate is used in Malay and Chinese, and it is not unreasonable to assume that other languages will show different levels of accuracy. However, it would be short-sighted to deny the future possibility that this or other translation programs will be able to translate between languages to a level with greater accuracy than a large number of students who are studying in their second language. Therefore, and again from a purely grammatical point of view, such students would be better off translating their work through Google. A student would have no incentive to go through the long and frustrating process of writing in a foreign language and then proofreading the text.

However, this approach would not necessarily help the student in the long term. Google Translate is (currently) only able to translate at a lexico-grammatical level. The program is unable to help students align their writing to the norms and expectations of the wider discourse community. The translation program operates at a level of shallow literacy, not “deep literacy” (Davies, 2007). There are many other issues – from basic organisation and avoidance of fallacy to hedging and the epistemological balance of academic writing for different disciplines – which Google Translate is simply unable to deal with in its present form. Thus, as commonly agreed in the literature (e.g., Austermühl, 2001; Kirchoff et al., 2011; Koehn, 2010; Quah, 2006), the use of MT necessitates post-editing to achieve truly high-quality outcomes.

It is also important to bear in mind that there were certain limitations in this study. Chief among these is the limited amount of data analysed. It is important to analyse the output of a larger amount of translated text in order to discover more widely generalisable patterns. In addition, a framework specifically designed for this analysis would have allowed more precise analysis of the output. For example, a framework where the type of sentence structure errors could have been coded would have provided a more nuanced analysis.

# 6. Implications and perspectives

It seems counterintuitive to the EAP practitioner to welcome machine translation tools into their territory, as these could be perceived to undermine the very act of teaching and learning a foreign language. The act of translation and bilingual teaching is not very common in EAP, where student cohorts usually consist of members with a wide variety of cultural and linguistic backgrounds. So the rapid developments in machine translation and their potential use by EAP learners might indeed be perceived as a threat to what is currently held as best practice in EAP teaching.

Certainly, such concerns are not unfounded, as the sanctioned use of translation tools may undermine the actual language acquisition process or even the need to learn another language in the first place, potentially leaving examinations that forbid the use of such devices the main incentive for students to actually learn the language. Reading and writing could be conducted in L1, as translation services could well extend into the area of electronic books so that entire passages could be rendered into an acceptable version in another language.

Before examining these issues in detail, it is worth considering the ways in which Google Translate may be used. Firstly, we can divide the use into short and long stretches of text. With the short use, writers treat the translation engine much like they would use a dictionary – entering one or two words into the engine. The longer use involves entering stretches of text, from paragraphs to whole essays.

The next distinction is between use for production and use for reception. In use for reception, a learner would take a text in the target language, and translate it for his/her own understanding. For example, a Chinese student at a UK university might take a journal article in English, and translate it into Chinese for easier comprehension. In use for production, the same student would write an essay in Chinese and then use the translation engine to change it into an English essay for submission.

For short use, the translation work functions very much like a dictionary – the weakness being that it only offers one translation at a time, making the translation of an English word like “bank” highly problematic, whether it is translated into or out of English (cf. Austermühl, 2001). A longer translation however, is likely to work better, translating for comprehension – since, as long as the student is able to understand, the grammatical errors are a distraction, not a major cause of failure (Austermühl, 2001). It is at the point when the translation engine is used to create large stretches of text for assessment that there will be an issue with grammatical accuracy – and this may seriously affect the outcomes, given the likely impact on the evaluation of the text. As we have seen, such an approach would be highly problematic for the student. While they might produce a text with tolerable grammatical accuracy, their writing would be flawed in a number of other areas. This is likely to include the mechanics of referencing and citation, organisation, construction of the argument, and such like.

On the other hand, it seems highly likely that the grammatical accuracy of the translations will only improve in the future. Simply by adding more texts to its database, Google is increasing the probability of each translation being correct. Thus, as we move into the future, the grammatical accuracy of the translations will increase. This then leaves the EAP teaching community with two choices: either to deny or to embrace this new technology. Obstructing it would, in our opinion, not be constructive. Technologies are adopted or rejected by the users, and any authorities attempting to regulate their use in order to conform to a previously held world view is bound to fail. This has been seen again and again, from the use of mobile phones in schools to issues with IP and file sharing.

Certainly, a point to be made in favour of banning the use of tools such as Google Translate from EAP is that one of the traditional core functions of presessional EAP is to develop and assess the students’ ability to cope with the language demands of an English-speaking academic environment. Thus, one could argue that a translated text is no longer the student’s own work. This is an argument not to be dismissed lightly. However, EAP seems to be experiencing a gradual shift towards deep disciplinary literacy (as illustrated in practice, for instance, by the continuing rise of English for Specific Academic Purposes) and away from the over-generalised teaching and assessment of allegedly ‘universal’ academic language (e.g., through ad hoc essays based on topics claimed to be of ‘general interest’ – a debate that goes far beyond the parameters of this paper). It would thus seem that the intellectual achievement of demonstrating the ability to create a deep, logical and critical argument (e.g., as usually manifested in written assignments as mode of assessment on EAP courses) may outweigh points such as grammatical accuracy in assessing a student’s suitability to become a member of their intended academic community. This is a point that may not be palatable to many EAP practitioners with a more traditional view of what should be assessed – but we must concede that the text (translated or not) is still produced and owned by the student, and is, thus, their achievement; after all, in academic practice, we seem to be content to find translations of academic research and philosophy into English just as acceptable as reading the original without questioning the ownership of the text.

Whichever attitudes are adopted by the EAP teaching community, the scenario that seems to unfold is still a way off, however. Machine translation will not replace language acquisition in the near future. Indeed, the teaching and learning of academic language will not become superfluous, as academic language is the means of communication and exchange between researchers worldwide, which involves interpersonal communication, coupled with crucial non-universal interpersonal and intercultural communicative competences that will be hard to develop for machines. Further, it would seem, as

Brown and Duguid (2000) point out, that the speed at which technological advances are developed will not coincide with the speed at which these advances are integrated into teaching.

However, should the profession embrace the use of machine translation, it will bring a large number of exciting potential developments. Firstly, the use of translation will allow a move away from the sentence level work needed in a large number of EAP classrooms. Students will be able to express themselves, even if they have a very limited level of English. This will allow EAP instructors to shift the emphasis away from the low level mechanics of English, and begin to focus on the deep literacies. Concepts such as academic identities and construction of knowledge will be able to be embedded earlier in the curriculum, leading to a more solid adoption of them by the students.

In addition, the translation engines might even be used as one of the growing number of tools in teaching the language. Strategic students would want to check the output of the translations – it is unlikely that they would want to place blind faith in this output. As pointed out previously, machine translation in its current state is only able to produce texts of limited quality. Thus, while they would not need to be able to generate such grammatically complex sentences, students will need to be able to check it for accuracy, cohesion and quality of translation. This process could be exploited in the classroom to enhance teaching and learning.

Further, even if MT were to reduce the numbers of those who want to acquire the English language as a means of communication in their academic endeavours, it seems far less likely that MT will be able to cope with specific discourse features across languages. It will not insert linking devices into a translation. It will not add hedging devices where the writer’s L1 text is too assertive for the English context, and it will not rearrange text to fulfil the expectations of Anglophone academic tutors and publishers. While probably considered good style, for example, in commonly elaborate and ‘elegant’ French academic writing (Siepmann, 2006), to the English reader very elaborate sentence structures may appear convoluted. MT would probably render such structures as 1:1 equivalent structure into the target language, unaware that this may not be appropriate. Thus, even if MT finds acceptance in EAP practice, students will still require sophisticated knowledge of how academic English works, ‘deep literacy’, as Davies (2007, p. 51) calls it, so as to be able to verify the suitability of their translated texts and to appropriately amend them where necessary. This means that MT is not likely to threaten the status of EAP; and it will not replace human input (Quah, 2006). Instead it may change the face of EAP – and that is why a good awareness of the tools at our students’ disposal is of such crucial significance. It will allow us to stay abreast of technological developments and the needs effective EAP teaching must address.

There is a parallel to be drawn with the teaching of mathematics and the introduction of the electronic calculator. The calculator did not remove the need for the teaching of maths – instead it allowed students to go further, quicker. It minimised the need for endless practice of long division, and cumbersome and slow aids such as slide rules and logarithmic tables. However, students and teachers use the calculator to assist the students’ fundamental understanding of maths, and their ability to apply it in the real world.

It is for these reasons that we believe that the use of MT may fundamentally change the face of the teaching of English for Academic Purposes. We predict that this will involve a shift away from sentence level grammatical teaching – and a shift towards the deeper understanding of the functions and metafunctions of language as used for academic purposes.

# 7. Conclusion

When considering the implications of Google Translate, we have made three assumptions. The first of these is that the grammatical quality of the translation will continue to improve, as the Google database grows. Secondly, we assume that it will remain free at the point of access for the user – and become available on an increasing number of platforms, from computers to tablets and mobile phones. Our third assumption is that if the technology is available to, and useful for, students, they will use it – whatever their EAP instructors/counsellors advise.

The current study has been exploratory and small-scale in nature. However, with the above assumptions in mind, some tentative conclusions, worthy of further investigation, can be drawn. It is clear that Google Translate has the potential to make a large difference to the EAP teaching community. However, it is not to be compared to robots in factories making the factory workers redundant. As long as we accept this technology and try to work with it, not against it, it has the potential to make the teaching of EAP a much more exploratory and critical activity. This will free students and teachers to examine issues of epistemology, and a deeper understanding of what academic expression actually is.

# References

Adolphs, S. (2006). Introducing electronic text analysis: A practical guide for language and literary studies. London: Routledge.   
Aiken, M., & Balan, S. (2011). An analysis of Google Translate accuracy. Translation Journal, 16(2). (online) Available at http://www.bokorlang.com/journal/ 56google.htm (03.05.14).   
Alexander, O., Argent, S., & Spencer, J. (2008). EAP essentials. Reading, UK: Garnet.   
Austermühl, F. (2001). Electronic tools for translators. Manchester, UK: St. Jerome Publishing.   
Austermühl, F. (2011). Of clouds and crowds: Current developments in translation technology. (online) Available at http://www.t21n.com/homepage/articles. php (30.04.14).   
Bellos, D. (2012). Is that a fish in your ear? London: Penguin.   
Biber, D. (2006). University language: A corpus-based study of spoken and written registers. London: John Benjamins.   
Bowker, L. (2008). Official languages minority communities, machine translation, and translator education: Reflections on the status quo and considerations for the future. TTR – Traduction, Terminologie, Rédaction, 21(2), 15-61. (online) Available at http://www.erudit.org/revue/ttr/2008/v21/n2/037491ar.html? vue¼resume.   
Brown, J. S., & Duguid, P. (2000). The social life of information. Boston: Harvard Business School Press.   
Bruce, I. (2010). Textual and discoursal resources used in the essay genre in sociology and English. Journal of English for Academic Purposes, 9(3), 153-166. (online) Available at http://www.sciencedirect.com/science/article/pii/S1475158510000238.   
Cancedda, N., Dymetman, M., Foster, G., & Goutte, C. (2009). A statistical machine learning primer. In C. Goutte, N. Cancedda, & M. Dymetman (Eds.), Learning machine translation (pp. 1-38). Cambridge, MA: MIT Press.   
Chapelle, C. (2003). English language learning and technology. Lectures on applied linguistics in the age of information and communication technology. Philadelphia, PA: John Benjamins.   
Cobb, T. (2010). Instructional uses of linguistic technologies. Procedia – Social and Behavioral Sciences, 3, 14-23. (online) Available at http://www. sciencedirect.com/science/article/pii/S1877042810013819.   
Colina, S. (2009). Further evidence for a functionalist approach to translation quality evaluation. Target, 21(2), 215-244. (online) Available at http://www.jbeplatform.com/content/journals/10.1075/target.21.2.02col.   
Costa-jussá, M., Farrús, M., & Pons, J. (2012). Machine translation in medicine: A quality analysis of statistical machine translation in the medical domain. Advanced Research in Scientific Areas. (online) Available at www.arsa-conf.com/ (Accessed 08.05.14).   
Davies, C. (2007). What can technology do for/to English? In A. Adams, & S. Brindley (Eds.), Teaching secondary English with ICT (pp. 50-66) Maidenhead: Open University Press.   
Ferris, D., Liu, H., Sinha, A., & Senna, M. (2013). Written corrective feedback for individual L2 writers. Journal of Second Language Writing, 22(3), 307-329. (online) Available at http://www.sciencedirect.com/science/article/pii/S1060374312000811.   
Garcia, I. (2010). Is machine translation ready yet? Target: International Journal on Translation Studies, 22(1), 7-21. (online) Available at http://www.jbeplatform.com/content/journals/10.1075/target.22.1.02gar.   
Google. (2012). Breaking down language barriers. http://googletranslate.blogspot.co.uk/2012/04/breaking-down-language-barriersix-years.html (03.02.13).   
Hutchins, W. J. (2000). The first decades of machine translation: Overview, chronology, sources. In W. J. Hutchins (Ed.), Early years in machine translation (pp. 1-16). Amsterdam: John Benjamins.   
Hutchins, W. J. (2006). Machine translation: History of research and use. In K. Brown (Ed.), Encyclopaedia of languages and linguistics (2nd ed.), (Vol. 7, pp. 375-383). Oxford: Elsevier.   
Hyland, K. (2004). Disciplinary discourses: Social interactions in academic writing. Ann Arbor, MI: University of Michigan Press.   
IELTS (n.d.). IELTS Task 2 Writing band descriptors (public version) (online). Available at http://www.ielts.org/PDF/UOBDs_WritingT2.pdf (03.02.13).   
Kaltenbacher, M. (2000). Aspects of universal grammar in human versus machine translation. In A. Chesterman, N. San Savador, & Y. Gambier (Eds.), Translation in context (pp. 221-230). Amsterdam: John Benjamins.   
Kirchoff, K., Turner, M., Axelrod, A., & Saavedra, F. (2011). Application of statistical machine translation to public health information: A feasibility study. Journal of the American Informatics Association, 18(4), 473-478. (online) Available at http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3128406/.   
Koehn, P. (2010). Statistical machine translation. Cambridge: Cambridge University Press.   
Komeli, Z., Hendavalan, J. A. F., & Rahimi, A. (2011). An investigation of the translation problems incurred by English-to-Persian machine translations: “Padideh, Pars, and Google Softwares”. Procedia – Social and Behavioral Sciences, 28, 1079-1082. (online) Available at http://www.sciencedirect.com/ science/article/pii/S1877042811026346.   
Mehta, N. K. (2012). Mobile phone technology in English Teaching: Causes & concerns. The Modern Journal of Applied Linguistics, 4(2), 82-92. (online) Available at http://www.mjal.org/removedprofiles/2013/Mobile%20Phone%20Technology.pdf.   
Newton, J. (1992). Introduction and overview. In J. Newton (Ed.), Computers in translation – A practical appraisal (pp. 1-13). London: Routledge.   
Oakey, D. (2010). Using corpus and web language data to create EAP teaching materials. In C. Ho, K. Anderson, & A. Leong (Eds.), Transforming literacies and language: Multimodality and literacy in the new media age (pp. 167-184). London: Continuum International.   
Peacock, M. (2002). Communicative moves in the discussion section of research articles. System, 30(4), 479-497. (online) Available at http://www. sciencedirect.com/science/article/pii/S0346251X02000507.   
Quah, C. K. (2006). Translation and technology. Basingstoke: Palgrave Macmillan.   
Reid, J. (1992). A computer text analysis of four cohesion devices in English discourse by native and nonnative writers. Journal of Second Language Writing, 1(2), 79-107. (online) Available at http://www.sciencedirect.com/science/article/pii/106037439290010M.   
van Rensburg, A., Snyman, C., & Lotz, S. (2012). Applying Google Translate in a higher education environment: Translation products assessed. Southern African Linguistics and Applied Language Studies, 3(4), 511-524. (online) Available at http://www.tandfonline.com/doi/abs/10.2989/16073614.2012. 750824#.VCK4yeNdV-4.   
Scherf, W. (1992). Training, talent, and technology. In C. Dollerup, & A. Loddegaard (Eds.), Teaching translation and interpreting (pp. 153-160). Amsterdam: John Benjamins.   
Sheppard, F. (2011). Medical writing in English: The problem with Google Translate. La Presse Médicale, 40(6), 565-566. (online) Available at http://www. em-consulte.com/en/article/293595.   
Siepmann, D. (2006). Academic writing and culture: An overview of differences between English, French and German. Meta: Translators’ Journal, 51(1), 131- 150. (online) Available at http://www.erudit.org/revue/meta/2006/v/n1/012998ar.html.   
Silver, M. (2003). The stance of stance: A critical look at ways stance is expressed and modeled in academic discourse. Journal of English for Academic Purposes, 2(4), 359-374. (online) Available at http://www.sciencedirect.com/science/article/pii/S1475158503000511.   
Somers, J., & Lovel, H. (2006). Can AAC technology facilitate communication for patients with limited English?. (online) Available at http://personalpages. manchester.ac.uk/staff/harold.somers/ESRCfinal.pdf (30.04.14).   
Swales, J. (1998). Genre analysis: English in academic and research settings. Cambridge: Cambridge University Press.   
Vernon, A. (2000). Computerized grammar checkers 2000: Capabilities, limitations, and pedagogical possibilities. Computers and Composition, 17(3), 329- 349. (online) Available at http://ac.els-cdn.com/S8755461500000384/1-s2.0-S8755461500000384-main.pdf?_tid $=$ a3331a86-43e6-11e4-8300- 00000aacb35d&acdnat¼1411562038_854e326556c97a98368b7f20d5a5b368.

Michael Groves holds an MA in Applied Linguistics and ELT from the University of Nottingham. He is currently head of the Centre for English Language Education at the University of Nottingham’s Malaysia campus. He is particularly interested in investigating the development of students’ academic literacy, autonomy, and how these are related.

Klaus Mundt holds a PGCert in Teaching EAP, an MA in Southeast Asian Studies and an MA in Applied Linguistics and ELT and has been involved in language teaching for over a decade. He teaches on the insessional EAP programme and the MA CETI at the University of Nottingham.