# AI or student writing? Analyzing the situational and linguistic characteristics of undergraduate student writing and AI-generated assignments

Larissa Goulart a,\* , Marine Laísa Matte b , Alanna Mendoza a , Lee Alvarado a , Ingrid Veloso a

a Montclair State University, United States b Instituto Federal de Educaçao, ˜ Ciˆencia e Tecnologia Sul-rio-grandense, Brazil

# A R T I C L E I N F O

# A B S T R A C T

Keywords:   
University writing   
Register studies   
Generative AI   
Multidimensional analysis

Since the release of OpenAI’s ChatGPT, universities have faced the issue of whether there is still a place for written assignments in higher education. ChatGPT’s capacity to mimic various written forms raises questions about the necessity of traditional assessments. Given this background, this study explores to what extent AI-generated assignments can replicate the situational and lin guistic features of student-authored assignments. Using a corpus of undergraduate assignments from an English as a Foreign Language (EFL) context, we compare student responses with ChatGPT’s outputs. Employing a register approach, we analyze the situational and linguistic characteristics of texts across three different registers—essays, critiques, and personal narratives. Our methodology follows Biber and Conrad’s (2019) framework, encompassing situational analysis, linguistic analysis, and functional interpretation. The findings aim to inform writing instructors and EFL teachers about the strengths and limitations of AI tools, enhancing their ability to guide students in integrating these technologies into their writing processes.

# 1. Introduction

Since the public release of OpenAI’s ChatGPT in November 2022, university stakeholders have grappled with the question of how ChatGPT can affect university assessment practices, for both short (e.g., essays, critiques, or lab reports) and long writing assignments (e.g., theses and dissertations). ChatGPT’s ability to emulate different forms of written texts has put into question the need for these forms of assessment. From the point of view of instructors and professors, undergraduate student writing traditionally requires students to demonstrate knowledge of their subject matter (Gardner & Nesi, 2013).

In the English as a Foreign Language (EFL) context, generative AI tools have been perceived either as an equalizer of institutional access or as a potential hindrance to the development of critical thinking skills and writing literacy. From the first perspective, some instructors view generative AI as a supportive resource, akin to an additional tutor or editor, which can be particularly beneficial for students from non-English speaking contexts (Alexander et al., 2023). Conversely, others worry that these tools may undermine student agency by potentially replacing students’ original work, thus impeding the development of essential literacy skills. As ChatGPT and similar AI technologies become increasingly prevalent, a pressing question emerges: “Is this the end of written assignments?

This study seeks to answer this question from an innovative perspective: rather than asking whether written assignments are going to be replaced by other assessment practices, we take a step back and examine to what extent AI-generated texts can replicate the situational and linguistic characteristics of student-authored assignments. To do so, we use a corpus of undergraduate assignments from an EFL context and compare them to ChatGPT’s responses to the same prompts. We employ a register approach based on the belief that register is a key predictor of language variation. More specifically, we attempt to answer the following research questions: RQ1) How are situational characteristics similar and different in writing assignments across registers (essays, critiques, and personal narratives) and source (ChatGPT and EFL student) writing?

RQ2) How are linguistic features similar and different in writing assignments across registers (essays, critiques, and personal narratives) and source (ChatGPT and EFL student) writing?

Following Biber and Conrad (2019), in this study, registers are defined as a language variety characterized by the situations in which they are used. That is, essay, critiques, and personal narratives are three different registers that occur in the academic discourse domain. In this study, we follow the three steps of a typical register study: situational analysis, linguistic analysis, and functional interpretation. The first step encompasses a description of the situational characteristics of the register, including elements such as audience, author, purpose, mode, and production circumstances (e.g., edited and revised vs. real-time production), among others. The second step encompasses an analysis of frequently occurring linguistic features in the register. This information is then used to describe the three-way relationship between situational characteristics, frequent linguistic features, and the communicative functions of those features (Biber & Conrad, 2019), resulting in the functional interpretation.

The results of this study can aid writing instructors and English Language teachers better prepare students to incorporate AI tools into their own writing process. In the EFL university context, understanding the differences between AI-generated and studentauthored texts can help instructors guide students in maximizing AI’s strengths while cautioning them about its limitations. More specifically, we believe that in order for teachers to help students make a conscious use of AI in their writing, teachers need to know the extent to which these tools can actually emulate human-writing (in this case, novice academic writers) or not, and how the human-inthe-loop needs to revise the text to create a cohesive piece of writing.

# 2. GenAI’s ability to emulate human writing

The uproar caused by OpenAI’s ChatGPT partly stems from growing concerns that instructors and reviewers might not be able to accurately distinguish between AI-generated and human-authored texts (e.g., Kobis ¨ & Mossink, 2021; Casal & Kessler, 2023; Ma et al., 2023). Table 1 summarizes the results of studies that empirically tested this claim and shows that most studies (Casal & Kessler, 2023; Gao et al., 2023; Ma et al., 2023) focused on the register of academic abstracts. Other academic registers were also the foci of previous studies, such as undergraduate reflective writing and postgraduate essays (Hassoulas et al., 2023), and research articles (Gao et al., 2023). Kobis ¨ and Mossink (2021) explored a different discourse domain, by accounting for human reviewers’ ability to distinguish between human and AI-generated poems. The results of these studies show that, for the register of abstracts, in most cases, human reviewers are not capable of identifying the difference between the two sources of writing, with their accuracy ranging from $3 8 . 9 ~ \%$ (Casal & Kessler, 2023) to $6 8 \%$ (Gao et al., 2023). However, it seems that human reviewers find it easier to identify the source of longer texts, such as research articles, which had an accuracy of classification of $8 6 ~ \%$ (Gao et al., 2023).

The results of these studies offer insights into the linguistic and textual features that stand out in AI-generated texts to human reviewers. Some of the features mentioned by reviewers were writing style and structure (Casal & Kessler, 2023; Hassoulas et al., 2023; Ma et al., 2023), lack of references or the use of inaccurate references (Hassoulas et al., 2023; Ma et al., 2023), and the use of vague language (Casal & Kessler, 2023; Gao et al., 2023) (see Table 1). In sum, the results of these studies indicate that there are situational (e.g., use of references, headers) and linguistic (e.g., vagueness) characteristics that vary between human-authored and AI-generated texts. These papers, however, did not take an empirical approach to account for the presence or absence of these situational features (e. g., references, headers, bullet points, etc.) nor linguistic features (e.g., vagueness), which is one of the goals of the current study (RQ1).

Table 1 Summary of studies investigating human reviewer’s ability to differentiate AI-generated and human-authored texts.   

<html><body><table><tr><td>Study</td><td>GenAI</td><td>Accuracy</td><td>Register</td><td>Indicators</td></tr><tr><td>Kobis and Mossink (2021)</td><td>GPT-2</td><td>50.21 %</td><td>Poems</td><td>Does not survey reviewers</td></tr><tr><td>Ma et al. (2023)</td><td>GPT-3</td><td>66 %</td><td>Abstracts and Wikis in</td><td>Reviewers usually focused on the form features, such as writing style.</td></tr><tr><td></td><td></td><td></td><td>Computer Science and Biomedicine</td><td>GPT abstracts used wrong references and made-up facts. In addition,. GenAI had contradictory sentences</td></tr><tr><td>Casal and Kessler (2023)</td><td>GPT-4</td><td>38.9 %</td><td>Abstracts in Applied Linguistics</td><td>The categories most mentioned by reviewers were: (1) continuity and coherence, (2) specificity or vagueness of details,.</td></tr><tr><td>Hassoulas et al..</td><td>GPT</td><td>23 %</td><td>Undergraduate reflective</td><td>(3) familiarity and voice, and (4) writing quality at the sentence-level. Reviewers indicated that the use of references, structure and layout were.</td></tr><tr><td>(2023)</td><td>3.5</td><td>undergraduate 19 % post-</td><td>writing and postgraduate essays</td><td>the main elements that helped them accurately identify GenAI writing..</td></tr><tr><td>Gao et al. (2023)</td><td>GPT 3</td><td>graduate 68 % abstracts</td><td>Abstracts and research articles</td><td>Reviewers indicated that ChatGPT abstracts were superficial and vague,.</td></tr><tr><td></td><td></td><td></td><td></td><td>Clinical</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>86 % research</td><td></td><td>and sometimes focused on details of original study such as inclusion of</td></tr><tr><td></td><td></td><td>articles</td><td>in Medicine</td><td></td></tr></table></body></html>

Recent studies coming from the field of computational linguistics and computer science have presented Machine Learning (ML) models that seek to accurately classify human-authored and AI-generated texts (e.g., Islam et al., 2023; Katib et al., 2023). These studies usually take a ‘black box’ approach. That is, the main focus is the classification, rather than describing the linguistic features that inform this classification. One exception to this ‘black box’ approach is Theocharopoulos et al. (2023). In this study, the authors developed a ML model that identifies texts as human or AI-generated and also explains the main differences found in these two text sources. Their findings suggest that human-authored texts use more low frequency and specialized words, while AI-generated texts rely more on high-frequency words. Through a descriptive approach, Johansson (2023) compares an AI-generated and human-authored essay written for an English literature class. Her results show that the AI essay is more repetitive, has a higher frequency of boosters, and lower frequency of hedges than the human essay.

Berber-Sardinha (2024) takes a broader focus to examine linguistic variation between AI-generated and human-authored texts. The author compares four registers produced by humans and AI: (1) academic abstracts in applied linguistics and chemistry, (2) conversations, (3) L2 essays, and (4) news articles. In order to describe the linguistic variation among these registers, an additive Multi-Dimensional Analysis (MDA) (see Berber-Sardinha et al., 2019; Goulart & Wood, 2021) was conducted. The results of this study show that AI-generated texts fail to convey register variation, with conversations and academic writing sharing many of the same linguistic characteristics. In addition, AI-generated texts were more informational, lacked narrative features and were less abstract than human-authored texts. Table 2 contains a summary of the three studies mentioned above.

Taken together, the findings of these previous studies show that AI-generated texts are characterized by more vague, abstract, and informational language, as well as by a lack of references, use of boosters, and lack of hedges (see Table 2). The current study seeks to expand on these previous descriptions by taking a closer look at undergraduate writing in EFL contexts. In this study, we investigate whether AI tools can simulate three registers of student writing: essays, critiques, and personal narratives. We compare the situational and linguistic features used by students when writing the same assignments. To do so, we conduct a detailed situational analysis, drawing on several of the features mentioned by studies that looked at reviewer’s perception of AI-generated and human-authored texts. We also conduct an additive MDA. This linguistic analysis approach allows us to obtain a broad picture of the differences be tween the AI-generated and human-authored texts.

# 3. Methods

In this section, we describe the two corpora used in the current study. In addition, we describe the steps of the situational and linguistic analyses conducted for this study.

# 3.1. Corpora

Table 3 details the corpora used in this study. The corpus of undergraduate student writing is composed of texts written by students majoring in English Teaching or Translation at Federal University of Rio Grande do Sul in the south of Brazil. All students included in the corpus were users of English as a Foreign Language (EFL)1 and had taken multiple courses in English linguistics and literature. Out of the 242 texts in the corpus, only 53 contained the original prompts given by the course instructor. Therefore, we selected these 53 texts to compose the corpus used in this study. The prompts varied widely ranging from movie analysis to argumentative essays. These assignments came from three separate registers: essay, critique, and personal narrative. The following are examples of the type of prompts found in the corpus:

Prompt 1: The media has often been accused of perpetuating stereotypes by including only one group of people in advertisements or presenting one class, race, or gender in a specific manner. How does international and national media influence the way Brazil is perceived by others?

Prompt 2: What is language variation? How does it affect your practice as a teacher and/or as a translator? Give examples.

The AI-generated version of the corpus was developed using ChatGPT 3.5 in December 2023. To create the corpus, the prompts in students’ texts were given as an input to ChatGPT. Given that several studies have pointed out the importance of prompt engineering in AI corpora (Gao et al., 2023; Ma et al., 2023), we asked ChatGPT to respond to the prompt as an undergraduate student of English Linguistics or Literature in a Brazilian University. The Gen AI corpus included the same number of texts as the student corpus.

# 3.2. Situational analysis

Situational analysis is a key element in any register study. It includes the description of a register for its non-linguistic charac teristics, including participants, setting, communicative purposes, among others. Based on a survey of previous studies of student and academic writing (e.g., Conrad, 1996; Tasker, 2022; Becker, 2022; Goulart, 2024; Staples, Goulart & Reppen, (in press)), we developed a new situational analysis framework that considers the characteristics of EFL undergraduate written assignments. This framework is

Table 2 Summary of studies describing the lexical and grammatical differences between AI-generated and human-authored texts.   

<html><body><table><tr><td>Study</td><td>GenAI</td><td> Register</td><td>Findings</td></tr><tr><td>Theocharopoulos et al. (2023)</td><td>GPT-3</td><td>Academic Abstracts</td><td>Human-authored texts contained more uncommon and specialized words. (e.g., influenza, treatment, etc.). AI-generated texts used more high-. frequency words</td></tr><tr><td>Johansson (2023)</td><td>GPT-4</td><td>An essay written for an English Literature course</td><td>The AI-generated text was more repetitive and used twice as many. boosters as students. The human-authored text used more hedges..</td></tr><tr><td>Berber-Sardinha (2024)</td><td>GPT-3.5</td><td>Academic abstracts in applied linguistics and chemistry, conversation, L2 essays, and news.</td><td>AI is more informational. Human-authored texts have more features of narrative and abstract style.</td></tr></table></body></html>

Table 3 Number of texts in the corpus.   

<html><body><table><tr><td></td><td>Source</td><td>Total</td><td>Number of words</td><td>Mean Text Length</td></tr><tr><td>Student</td><td>Essay</td><td>23</td><td>11,853</td><td>515.35</td></tr><tr><td></td><td> Personal Narrative</td><td>18</td><td>8185</td><td>454.72</td></tr><tr><td></td><td>Critique</td><td>12</td><td>9372</td><td>781.00</td></tr><tr><td>Total</td><td></td><td>53</td><td>29,410</td><td>554.90</td></tr><tr><td>Gen AI</td><td>Essay</td><td>23</td><td>10,531</td><td>457.87</td></tr><tr><td></td><td> Personal Narrative</td><td>18</td><td>6933</td><td>385.17</td></tr><tr><td></td><td>Critique</td><td>12</td><td>6040</td><td>503.33</td></tr><tr><td>Total</td><td></td><td>53</td><td>23,504</td><td>443,47</td></tr></table></body></html>

described on Table 4.

Audience was annotated as a broad category. Either a text was written for an intended classroom audience (e.g., an essay on media and stereotypes), or it was written for a simulated audience outside of class (e.g., a cover letter to respond to a job advertisement given by the instructor). Explicitness was annotated for presence or absence of explicit statement of purposes (e.g., “In this paper, I will…” or “The goal of this paper is to …”).

Nature of data included three subcategories: presence of primary sources, presence of secondary sources, and use of personal experience. These categories were taken from a recent study of first-year writing (Staples et al., in press). Based on that study, a text was annotated as containing primary sources if the nature of the evidence in the assignments consisted of interviews, surveys, or observations. For secondary sources, we accounted for the use of information from research articles, news articles, and other primary sources in the writing. Finally, personal experience accounts for the use of the authors’ past experiences to support the main idea of the paper. These categories were included in order to better understand the extent to which AI generated assignments could replicate EFL students’ writing when the prompt required the use of personal experiences and primary sources.

Table 4 Situational Analysis Framework for AI-generated and student-authored texts.   

<html><body><table><tr><td>Category</td><td>Description</td></tr><tr><td>Audience</td><td>Class audience</td></tr><tr><td>Explicitness</td><td>Simulated general audience Explicit Statement of Purpose (yes/no)</td></tr><tr><td>Nature of data</td><td>Presence of primary sources (yes/no) Presence of secondary sources (yes/no)</td></tr><tr><td>Textual Layout</td><td>Use of personal experience (yes/no) Headings(yes/no) Abstract (yes/no) Reference list (yes/no)</td></tr><tr><td></td><td>In-text citation (yes/no) Figures (yes/no) Tables (yes/no) Other visuals (yes/no)</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Lists or bullet points (yes/no)</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Table of contents (yes/no)</td></tr><tr><td>Communicative purpose</td><td>Based on Goulart, Biber and Reppen (2022):</td></tr><tr><td></td><td></td></tr><tr><td></td><td>To argue (ARG)</td></tr><tr><td></td><td>To compare (COMP)</td></tr><tr><td></td><td>To explain (EXP)</td></tr><tr><td></td><td>To describe a tangible object (DESC)</td></tr><tr><td></td><td>To narrate a personal experience (PERS)</td></tr><tr><td></td><td>To give a procedural recount (PROC)</td></tr><tr><td></td><td>To narrate (NAR)</td></tr><tr><td></td><td></td></tr><tr><td></td><td>To give personal advice (ADV) To propose (PROP)</td></tr></table></body></html>

Textual Layout accounted for the presence or absence of several textual elements (headings, abstract, reference list, in-text citations, figure, tables, other visuals, lists or bullet points, table of content). These are all characteristics we found to be used in either student or AI generated texts. For example, in our experience with ChatGPT, the generative AI tool consistently replied with lists or bullet points, which is not common in student writing.

Communicative purpose was annotated following Goulart, Biber and Reppen’s (2022) framework of communicative purposes in student writing. This framework includes 9 purposes: to argue, to compare, to explain, to narrate, to propose, to narrate a personal event, to give a procedural recount, to describe a tangible object, and to give personal advice. This framework of communicative purposes was developed to describe the communicative purposes in a group of texts in the British Academic Written English (BAWE) Corpus. Each text in the corpus was annotated for communicative purposes on a scale of 0–3, where 0 meant that the communicative purpose was not present in the text, and 3 meant that a communicative purpose was prevalent in the text. In the current study, only seven purposes were identified in the corpus (to give personal advice, to compare, to narrate, to propose, to argue, to explain, to give personal advice). Three of the authors annotated all texts in the corpus and the average score for communicative purposes for each text was calculated.

# 3.3. Additive multi-dimensional analysis

MDA is a research method initiated by Biber (1988) which typically uses the statistical procedure of exploratory factor analysis (EFA) to group linguistic features into functional layers (known as dimensions). In order to get a broad picture of the linguistic variation between AI-generated and student-authored texts, we conducted an additive MDA. According to Goulart and Staples (2023), the advantage of running an additive MDA is that it allows for comparisons against a range of studies that have also applied the same dimensions to different registers. Over the years, several researchers have used the additive approach to account for language vari ation, and to compare simulated registers with naturally occurring language. Quaglio (2009), for example, used an additive MDA approach to compare the dialogues of the television show "Friends", with naturally occurring conversations. In Goulart and Wood’s (2021) survey of MDAs published since 1984, the authors find that out of the 210 studies published between 1984 and 2020, 115 were additive MDAs, suggesting that these dimensions continue to be relevant to English nowadays. To obtain dimension scores, both corpora used in this study were tagged with the Biber Tagger (Biber, 1988), which includes a tagset of more than 200 linguistic features. After tagging, the frequency of these features was tallied using TagCount (Biber, 1988). This program not only counts the frequency of the linguistic features, but it also outputs the dimension scores for each of Biber’s (1988) original dimensions for each text in the corpus.

A dimension score indicates the extent to which a given text contains the linguistic features that are characteristic of a given dimension. That is, a text with a high positive dimension score for Dimension 1, has a high frequency of the features that load on the positive side of this dimension (see Table 5). Table 5 presents the labels and the linguistic features for Biber’s (1988) Dimensions.

Using the mean scores for each of Biber’s (1988) Dimensions, repeated-measures analyses of variance (ANOVAs) were performed to compare the register and the source of the assignments, and to explore whether there were any significant interactions between register and source. The R statistical software (R Core Team, 2024) was used for these ANOVAs. When significant differences were found among the dimension scores, Post-Hoc Tukey tests were conducted to determine the relationship between source and register.

While both the current study and Berber-Sardinha’s (2024) work employ an additive MDA approach, they diverge significantly in their objectives, corpora, and methodologies. Berber-Sardinha (2024) aims to assess the similarities between AI-generated texts and human-authored texts, with a focus on predicting authorship through dimension scores. In addition, their corpus encompasses a broad range of registers, including academic writing, conversation, L2 essays, and news, providing a wide lens on AI’s textual capabilities. In contrast, the current study is narrowly focused on understanding the linguistic differences between AI and human-generated texts specifically within university registers, including essays, critiques, and personal narratives. In addition, their AI corpus does not match the exact same prompt given to the human generated texts as it would be impossible to give humans a prompt for a naturally occurring conversation. Moreover, while Berber-Sardinha’s analysis primarily measures the predictive potential of dimension scores for authorship identification, our approach follows a traditional register studies framework, incorporating an in-depth situational analysis that compares the contextual and linguistic features of AI-generated texts with those produced by students. Nevertheless, when relevant, we will compare the results of Berber-Sardinha’s (2024) MDA, especially for L2 essays, to that of our MDA.

Table 5 Biber’s (1988) Dimensions.   

<html><body><table><tr><td colspan="2">Dim 1</td></tr><tr><td>Involved</td><td>Private verbs, that-deletion, contractions, present tense, 2nd person pronouns, DO as a pro-verb, analytic negation, demonstrative pronouns, emphatics, 1st person pronouns, pronoun it, BE as a main verb, causative subordination, discourse particles, indefinite pronouns, hedges, amplifiers, sentence relatives, wh-questions, possbility modals, non-phrasal coordination, wh-clauses, final</td></tr><tr><td>Informational production</td><td>prepositions, adverbs, conditional subordinators Word length, prepositions, type/token ratio, attributie adjective, place adverbials, agentles pasives, past participle, present</td></tr><tr><td>Dim 2</td><td> participle</td></tr><tr><td>Narrative Non-Narrative</td><td>Past tense, 3rd person pronouns, perfect aspect, public verbs, synthetic negation, present participial clauses</td></tr><tr><td>Dim 3</td><td>Present tense verbs, attributive adjectives, past participle, word length</td></tr><tr><td>Explicit</td><td>Wh-relative clause on object position, pied piping constructions, wh-relative clause on subject position, phrasal coordination, nominalization</td></tr><tr><td>Situation Dependent Dim 4</td><td>Time adverbials, place adverbials, adverbs</td></tr><tr><td>Overt expression of Persuasion</td><td>Infinitives, prediction modals, suasive verbs, conditional subordination, modals of necessty, split auxiliaries, possbility modals</td></tr><tr><td>Dim 5 Abstract</td><td>Conjuncts, agentless passives, past participial clauses, by-passves, past participle, other adverbial subordinators, predicative adjectives</td></tr></table></body></html>

# 4. Results and discussion

The results and discussion are organized in two sections. The first one presents the results of the situational analysis for both textual features (e.g., audience, nature of data, visuals, etc) and communicative purpose (Section 4.1). The second section presents the results for the additive MDA (Section 4.2).

# 4.1. Situational analysis

After each text was annotated for their situational features, general trends for AI-generated and student-authored registers were calculated. These general trends are described in Table 6. Some of the most salient differences between AI-generated and studentauthored registers are discussed in the next paragraphs.

Audience. Table 6 shows that AI-generated and student-authored registers have the same intended audience for two of the three registers analyzed (essays and personal narratives). In critiques, students tend to use a simulated audience outside of the classroom (e. g., blog posts) more often than GenAI. This might be the result of a classroom discussion that was not captured in the assignment prompt, therefore ChatGPT would not have the information to create an outside audience as well.

Explicitness. This category was included in our analysis based on the belief that ChatGPT would have more explicit statements of purposes, such as ‘In this essay, I will’. However, this did not prove to be true. In fact, both AI-generated and student-authored texts included very few statements of purpose.

Nature of Data. None of the assignments in the corpus included any type of primary data, such as observations, surveys, or in terviews. Secondary data, or the inclusion of arguments and examples from others in one’s texts, varied across assignments. Only essays and critiques used secondary sources, and out of these two, essays showed the greatest difference between AI-generated and student-authored texts (Sts: 18 AI: 8). Students incorporated secondary sources in ${ ^ 2 } / _ { 3 }$ of the assignments in the corpus, while AI only uses sources in slightly over a half of the essays. Understanding the use of sources in assignments can help us understand how AIgenerated and student-authored texts differ. A common criticism of AI is its tendency to hallucinate. In the case of the assignments generated with ChatGPT, we did not find any hallucination of secondary sources, however, we noticed that ChatGPT refers to older sources more than students. ChatGPT’s references were published primarily in the $9 0 \ s$ , while students used information from more recent publications.

Personal experience refers to assignments where the students include a description of an event that they experienced. This char acteristic is especially relevant for personal narratives, which usually include some piece of personal information. Table 6 shows that both AI-generated and student-authored texts use personal experience in almost all personal narratives (Sts: 17, GenAI: 15). An example of such an assignment is a prompt that requires the student to talk about the most important person in their lives. In this assignment, the student narrated a personal experience with their grandmother, while the AI generated a text about a literature teacher, Mrs. Thompson. Excerpts 1 and 2 show a paragraph from the responses of students and ChatGPT to this prompt.

Table 6 General Trends of Situational Characteristics for AI-generated and student-authored texts.   

<html><body><table><tr><td rowspan="2" colspan="2"></td><td colspan="2">Essay (23)</td><td colspan="2">Critique (12)</td><td colspan="2">Personal Narrative (18)</td></tr><tr><td>GenAI</td><td>Student</td><td>GenAI</td><td>Student</td><td>GenAI</td><td>Student</td></tr><tr><td rowspan="2">Audience</td><td>Class</td><td>23</td><td>21</td><td>12</td><td>5</td><td>9</td><td>7</td></tr><tr><td>Simulated</td><td>0</td><td>2</td><td>0</td><td>7</td><td>9</td><td>11</td></tr><tr><td rowspan="3">Explicitness Nature of Data</td><td>Explicit Statement</td><td>2</td><td>1</td><td>1</td><td>2</td><td>9</td><td>8</td></tr><tr><td> Primary</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td> Secondary</td><td>8</td><td>18</td><td>12</td><td>12</td><td>0</td><td>0</td></tr><tr><td rowspan="9">Textual Layout</td><td>Personal</td><td>1</td><td>0</td><td>0</td><td>0</td><td>15</td><td>17</td></tr><tr><td>Heading</td><td>9</td><td>2</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Abstract</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Reference list</td><td>5</td><td>9</td><td>0</td><td>4</td><td>0</td><td>0</td></tr><tr><td>in-text citation</td><td>1</td><td>14</td><td>1</td><td>6</td><td>0</td><td>0</td></tr><tr><td>Figures</td><td>0</td><td>0</td><td>0</td><td>3</td><td>0</td><td>0</td></tr><tr><td>Tables</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Other visuals</td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Lists/bullet points Table of Contentse</td><td>3 0</td><td>0 0</td><td>0 0</td><td>0</td><td>1 0</td><td>1 0</td></tr></table></body></html>

# 4.1.1. Excerpt 1 - Personal Narrative (Student)

She was 55 years old when I was born. And even though she was almost in her sixties, she used to wear red lipstick in her small mouth to go out; in my baptism, there are photos of her holding me with that bright red lipstick, and, because of that, I always thought, looking at those pictures, that she was really edgy for her age.

# 4.1.2. Excerpt 2 - Personal Narrative (GenAI)

Her name was Mrs. Thompson, an unassuming presence in the tapestry of my high school years. Petite in stature but colossal in impact, she was my literature teacher, an architect of words and a maestro of inspiration. Her classroom was not merely a space for learning; it was a sanctuary where ideas blossomed, and dreams took flight.

Textual Layout. Textual layout accounted for the presence or absence of a number of features: headings, abstracts, reference lists, intext citations, figures, tables, other visuals, lists or bullet points, and tables of content. Abstracts, tables, other visuals, and table of contents were not present in neither the AI-generated nor the student-authored texts. Headings (i.e., introduction, literature review, etc.) were only present in essays, with a relatively higher frequency in AI-generated essays (Sts: 2, AI: 9). These headings usually introduce the topic of the next paragraph. For example, in an essay about the influence of media on the perceptions of Brazil in other countries, the student wrote a continuum text, but the AI-generated text included the following headings: Introduction, Cultural Stereotypes and Selective Portrayals, Economic and Social Perspectives, Environmental Narratives, Positive Representations and Nuanced Perspectives, and Conclusion. These headings have the purpose of describing and informing the reader about the topic of the next paragraph. These headings also illustrate a common trend among AI-generated texts: they have a list of facts and information; they do not have a main argument. This issue will be further explored when we look at the communicative purposes of these texts.

Reference lists are another feature that showed considerable differences between AI-generated and student-authored texts. In both essays (Sts: 9, AI: 5) and critiques (Sts: 4, AI: 0), students’ texts included a reference list more often than AI-generated texts. This relates to the finding for the use of secondary sources discussed in previous paragraphs. Since AI-generated texts do not make use of secondary sources as often as students, they also do not include references. Similarly, there are more in-text citations in student-authored texts than in AI-generated texts, this is especially true for essays (Sts: 14, AI:1) and critiques (Sts: 6, AI: 2). These findings suggest that AIgenerated texts usually do not include references to outside sources.

Figures were only used three times by students. Although ChatGPT 3.5 did not integrate the use of images, as Chat GPT 4.0 does. We were interested in observing whether students would include figures in their assignments, which was not the case. For lists and bullet points, AI-generated texts did not make extensive use of bullet points. In fact, only in essays this feature is used (Sts: 0, AI: 3). It seems that prompting ChatGPT to write as a student might have avoided texts written in bullet point format, which are common when asking a direct question to ChatGPT.

![](img/4435a813db472cbc3114c318c99b093fa83db4607713e704bcfeada6c69e3b8e.jpg)  
Fig. 1. Combinations of Communicative Purposes in AI-generated and student-authored Essays.

# 4.1.3. Communicative purpose

As previously mentioned, each text in the corpus was annotated for its communicative purpose on a scale of 0–3 by three anno tators. Figs. 1, 2, and 3 present the amount to which each communicative purpose was used in the text represented by percentages. These figures are divided by text and register, making it possible to compare the communicative purposes articulated by the student and ChatGPT in response to the exact same prompt.

In Fig. 1, we see the communicative purposes used by ChatGPT when writing essays in the left panel, and the communicative purposes articulated by students in the right panel. Fig. 1 shows that ChatGPT shows fewer combinations of purposes than students. AIgenerated texts that are primarily argumentative or explanatory, this is in contrast to student writing, which combines a wider range of purposes, such as to give personal advice and to narrate. Excerpts 3 and 4 illustrate the use of different purposes in student writing in contrast to the AI-generated response to the same prompt. This prompt asked students about language variation and its role in students’ future professional practice as teachers or translators.

# 4.1.4. Excerpt 3 - Essay (Student)

Accents are a distinctive way of pronouncing words in a specific language, and they are associated with specific nations, localities, or social classes (explanation). When you’re in a classroom, applying your knowledge about linguistics, variation is very important because it is situated in the field of sociolinguistics (argumentation). (…) As a professional translator, I was familiar with the saying "language is constantly changing", so I had to put this in context (narrative).

# 4.1.5. Excerpt 4 - Essay (GenAI)

Language is a dynamic and multifaceted tool, constantly evolving and adapting to the diverse needs and contexts of its users. Language variation, the phenomenon encompassing differences in pronunciation, vocabulary, and grammar, among other linguistic elements, is a ubiquitous aspect of human communication (explanation).

In Excerpt 3, the student combines arguments and personal experiences in order to explain the role of language variation in their profession, while in Excerpt 4, ChatGPT provides a textbook definition of language variation. A similar communicative purpose pattern can be observed in the register of critiques, presented in Fig. 2. AI-generated critiques are considerably more informative, making use of the purpose of to explain more than to argue. Additionally, AI-generated critiques tend to propose a solution to a problem (to propose), while students acted as movie/book critics by describing the content of the story (to narrate) and then evaluating it (to argue). Overall, AI-generated essays and critiques tend to be more informative than student-authored essays and critiques, thus, the greater frequency of to explain in AI-generated texts. The explanatory nature of these texts relates to the use of informative headings observed in Table 6. It seems that GenAI assignments tend to follow the format of encyclopedia articles, rather than argumentative or more persuasive texts.

Fig. 3 provides a visual comparison between the communicative purposes used in AI-generated and student-authored personal narratives. The left panel (ChatGPT) shows a broader range of communicative purposes being used than those of essays; however, when compared to the students who were responding to the same prompt, we can see that students combine to narrate to a greater extent than GenAI.

![](img/08d83643a556497a7131ea88cf798d9d1076ac75239a2800b723c7a2e0d3d398.jpg)  
Fig. 2. Combinations of Communicative Purposes in AI-generated and student-authored Critiques.

![](img/ff0d3933d499a6164dbd3de33e13a0c7e9262d6a19094f49c78218fc09c9a733.jpg)  
Fig. 3. Combinations of Communicative Purposes in AI-generated and student-authored Personal narratives.

The situational analysis revealed both similarities and differences between AI-generated and student-authored texts, which are summarized in Table 7. In this table, a $\cdot _ { + } ,$ or ‘-’ sign indicates whether a feature is more or less frequent in a given register or source. For the three registers included in this study, neither ChatGPT nor EFL students used abstracts, tables, or tables of content. Bullet points and figures were present, but not particularly frequent in any of the three registers. AI-generated texts contained more headings than student-authored essays. Another difference between AI-generated and student-authored texts is the use of sources, which had already been mentioned in previous studies (Hassoulas et al., 2023; Ma et al., 2023). We found that AI-generated assignments had fewer references and fewer in-text citations, which also tended to be older than the ones used by students. Specifically for the register of essays, we found that students used the communicative purposes of to narrate and to compare which are not present in the AI-generated essays. For critiques, AI-generated texts have a classroom audience, while student-authored critiques are split between classroom and a simulated audience. The differences between AI-generated and student-authored critiques also extend to communicative purposes, even though GenAI combines more purposes in critiques than essays, these combinations vary considerably from that of students. Students use both to argue and to explain when writing critiques, while ChatGPT uses primarily to explain in combination with other minor purposes. Personal narratives is the register that exhibits fewer differences between AI-generated and student-authored texts. They both use personal experiences and references to the same extent.

Table 7 Summary of situational characteristics of AI-generated and student-authored texts.   

<html><body><table><tr><td></td><td>GenAI</td><td>Student</td></tr><tr><td rowspan="7">Essay</td><td>++ classroom audience</td><td>++ classroom audience</td></tr><tr><td>- - explicit statements of purposes</td><td>- - explicit statements of purposes</td></tr><tr><td>+ - secondary sources</td><td>++ secondary sources</td></tr><tr><td>+ - in-text citations</td><td>++ in-text citations,</td></tr><tr><td>+ - bullet points or lists</td><td>- - bullet points or lists.</td></tr><tr><td>Primarily used to explain and to argue, with some minor instances of to</td><td> Primarily used to explain and to argue, with some minor instances of.</td></tr><tr><td> propose.</td><td>to narrate, to propose, and to compare..</td></tr><tr><td rowspan="8">Critiques</td><td>+ + classroom audience</td><td>+ - classroom audience</td></tr><tr><td>- - explicit statements of purposes</td><td>+ - outside audience</td></tr><tr><td>+ + secondary sources</td><td>- - explicit statements of purposes</td></tr><tr><td>- - reference list</td><td>++ secondary sources</td></tr><tr><td>- - in-text citations.</td><td>+ - reference list</td></tr><tr><td></td><td>+ - in-text citations.</td></tr><tr><td>Primarily used to explain and to argue in combination. Minor purposes included: to narrate, to compare, and to give personal advice.</td><td>Primarily used to explain. Minor purposes included: to argue, to</td></tr><tr><td></td><td>narrate, to propose, and to compare.</td></tr><tr><td rowspan="6">Personal narratives</td><td>+ - classroom audience</td><td>+ - classroom audience</td></tr><tr><td>+ - outside audience</td><td>+ - outside audience</td></tr><tr><td>+ - explicit statements of purposes</td><td>+ - explicit statements of purposes</td></tr><tr><td>+ + personal experience</td><td>+ + personal experience</td></tr><tr><td>Primarily used to explain, to propose, and to give personal advice. Minor</td><td>Primarily used to explain, to propose, and to give personal advice.</td></tr><tr><td>purposes included: to narrate, to argue, and to compare..</td><td>Minor purposes included: to narrate, to argue, and to compare.</td></tr></table></body></html>

In sum, the situational analysis revealed that GenAI is better at reproducing the situational characteristics of more personal reg isters, such as personal narratives. A possible explanation for this is that EFL students use more features of personal involvement in all registers, but only in personal narratives the prompt explicitly asks students to include personal information, and ChatGPT only includes personal information when explicitly stated in the prompt.

# 4.2. Linguistic analysis

This section discusses the results of the Additive MDA using Biber’s (1988) dimensions. Dimension 1, Involved versus Informational Production, is characterized by features of involved production on the positive side, such as private verbs (e.g., believe, feel, think), 1st and 2nd person pronouns, and contractions; and features of informational production on the negative side, such as prepositions, attributive adjectives, and longer words (see Table 5 for a complete list). In Biber’s (1988) MDA, spoken texts (e.g., telephone conversations, face-to-face conversation, spontaneous speeches) had the highest mean dimension scores in Dimension 1, and planned written texts (e.g., academic writing, official documents) the lowest dimension scores in Dimension 1. This means that spoken texts have a high frequency of the linguistic features that load on the positive side of this dimension, while planned written texts have a high frequency of the linguistic features that load on the negative side of this dimension. Table 8 provides the mean and standard deviation for each register (critique, essay, and personal narrative) and source of writing (ChatGPT and student) for Dimension 1. Fig. 4 is a graphical representation of the mean dimension score for each register and source.

The ANOVA showed no significant interaction effects $F ( 2 , 1 0 0 ) = 1 . 1 9$ , $p = . 3 0$ for source and register. There were significant differences among registers (essay, critique, personal narrative), $F ( 2 1 0 0 ) = 1 2 . 0 8 $ , $p = < . 0 0 1$ , $\eta 2 = . 1 9$ . The Post-Hoc Tukey test revealed significant differences between personal narratives $( M { = } { - } 4 . 7 2$ , $S D = 1 7 . 8 $ ) and the two other registers, essays $\scriptstyle ( M = - 1 4 . 2$ , $S D { = } 1 2 . 4 $ ) and critiques $( M { = } { - } 1 3 . 7 $ , $S D { = } 1 5 . 1 { \cdot }$ ), but no significant difference between essays and critiques. In addition, there were significant differences between AI-generated and student-authored texts, $F ( 1 , 1 0 0 ) = 1 7 1 . 1 $ , $p < . 0 0 1$ , $\eta ^ { 2 } { = } . 6 3$ . Post-Hoc analysis also showed a significant difference, with AI-generated texts $M { = } { - } 2 2 . 6$ , $S D { = } 7 . 0 6$ ) having more features of informational discourse than student-authored texts $( M { = } . 8 6$ , $S D { = } 1 2 . 6$ ).

In all three registers, AI-generated texts were considerably more informational than student writing. AI-generated texts have more features that appear in planned and revised writing (e.g., nouns, word length, prepositions). Students use lexico-grammatical features that are commonly associated with spoken interaction (e.g., contractions, private verbs, that deletion, present-tense verbs). Excerpts 5 and 6 illustrate the differences between AI-generated and student-authored texts. Both excerpts are a response to a prompt that asked for a critical analysis of the play ‘Doubt’. In Excerpt 5 features of involved production are underlined. In Excerpt 6, features of informational production are underlined.

# 4.2.1. Excerpt 5 - Critique (Student)

John Patrick Shanley’s play, Doubt: A Parable, revolves around the principal of a catholic school, Sister Aloysius Beauvier. Sister Beauvier suspects that Father Flynn has been harassing the only black student in the school, Donald Miller. (…) At first, she doesn’t have ‘doubts’, but, when Sister Aloysius tells the nun about her doubts, Sister James tries to avoid the topic and pretends that ‘the doubt’ doesn’t exist.

# 4.2.2. Excerpt 6 - Critique (GenAI)

John Patrick Shanley’s "Doubt: A Parable" delves into the intricate interplay of certainty and doubt, power dynamics, and the consequences of moral ambiguity within a Catholic school in the Bronx during the 1960s. The title itself suggests the central theme of uncertainty, a prevailing motif that permeates the entire narrative.

In Excerpt 5, the student retells the plot of the play in an involved manner. Excerpt 6, in contrast, is more impersonal and infor mationally dense. Two factors might contribute to this difference between AI-generated and student-authored texts: AI’s training data and students’ proficiency. ChatGPT’s training data is not publicly available, however, Kublik and Saboo (2022) state that the training data includes online texts, books, and Wikipedia articles, suggesting that the data that informs ChatGPT’s production comes from planned written texts, rather than more informal texts. Therefore, the tool might not be as accurate when replicating personal and conversational types of texts. It is also possible that proficiency plays a role in the linguistic variation observed in Table 8 and Fig. 4. That is, EFL students might make more use of features of conversation because they might not have reached an advanced level of proficiency in academic writing yet.

Table 8 Descriptive statistics for Dimension 1 scores for AI-generated and student-authored texts.   

<html><body><table><tr><td></td><td colspan="2">GenAI</td><td colspan="2">Student</td></tr><tr><td></td><td>Mean</td><td>SD</td><td>Mean</td><td>SD</td></tr><tr><td>Essay</td><td>24.4</td><td>4.14</td><td>3.97</td><td>9.05</td></tr><tr><td> Personal Narrative</td><td>17.2</td><td>8.61</td><td>7.7</td><td>15.8</td></tr><tr><td>Critique</td><td>27.2</td><td>3.02</td><td>0.2</td><td>8.28</td></tr></table></body></html>

![](img/dfe9b81edc610f1e8a23c78a5fa24193c4fefceb72252207f4cd706f7bb61ca0.jpg)  
Fig. 4. Dimension 1 scores for AI-generated and student-authored texts - $F ( 5 1 0 0 ) = 3 9 . 5 3 $ , $p < . 0 0 1$ , $\mathrm { R } ^ { 2 } { = } . 6 6$ . Note. The colored boxes indicate the middle $5 0 ~ \%$ of the data (25th and 75th percentiles). The solid horizontal lines through the boxes indicate the median dimension scores. The red circles indicate the mean dimension scores. Individual data points are marked with the small black dots. The whiskers extending from the boxes show the minimum and maximum dimension scores excluding outliers, which are marked with black dots.

Dimension 1: Involved vs Informational Production   
Table 9 Results of repeated-measures ANOVAs for AI-generated and student-authored texts for Dimension 1.   

<html><body><table><tr><td></td><td>df</td><td>F</td><td>p</td><td></td></tr><tr><td>source</td><td>1</td><td>171.10</td><td>&lt;.001</td><td>.63</td></tr><tr><td>register</td><td>2</td><td>12.08</td><td>&lt;.001</td><td>.19</td></tr><tr><td>Source * register</td><td>2</td><td>1.19</td><td>.30</td><td>.02</td></tr></table></body></html>

The variation between these two sources of writing is more salient for personal narratives. In this register, students are more likely to use features of involved discourse $M = 7 . 7$ , $S D = 1 5 . 8 \mathrm { \AA }$ , than AI-generated texts $( M = - 1 7 . 2 $ , $S D = 8 . 6 1 $ ). This suggests that when asked to write an assignment that requires more personal information, AI-generated texts are more formal than what is usually common for this register.

Fig. 4 highlights another difference between AI-generated and student-authored texts: within register variation. Recent register studies (Biber & Egbert, 2023; Goulart et al., 2021) have addressed variation within registers. These studies show that not all essays (or other registers) written by humans have the same linguistic profile. This is confirmed by Fig. 4, which shows that student-authored registers show considerable internal variation. AI-generated registers, however, appear to be more homogenous. This lack of within-register variation can be explained in light of the combinations of communicative purposes in both sources. The analysis of purpose showed that students combine multiple purposes in the same text more often than GenAI, with AI-generated texts predominantly relying on to explain and to argue purposes. This homogeneity of purpose is reflected in the linguistic profile of these texts.

One of the advantages of running an additive MDA is the ability to compare the results of the analysis with those of previous additive MDAs. Fig. 5 is a visual comparison of the results for Dimension 1 for this study and three other MDAs. The first column shows the results of Biber’s (1988) original MDA. The second column shows the results of Gray’s (2015) MDA of research articles across disciplines, and the third column shows the results of Goulart’s (2024) MDA of undergraduate student writing across disciplinary groups (arts and humanities, social sciences, physical sciences, and life sciences). Fig. 5 shows that AI-generated texts have a similar linguistic profile to that of published academic writing in Biology, Physics and Political Science (Gray, 2015), and to disciplinary writing in the disciplines of life and physical sciences (Goulart, 2024). Student writing, on the other hand, appears to be similar to prepared speeches and general fiction (Biber, 1988), and texts in the discipline of arts and humanities (Goulart, 2024).

![](img/f77e02f0af2de45c6909a90056a9c7b7ba0fd7ea8bd64ae834fdb895ae4c7a3c.jpg)  
Fig. 5. Comparison of Dimension 1 mean scores across Biber (1988), Gray (2015), and Goulart (2024).

Dimension 2, Narrative vs. Non-narrative Concerns, distinguishes between active, and event-oriented writing (positive side) and more expository writing (negative side). Past tense verbs, third-person pronouns, and perfect aspect verbs are some of the linguistic features that cluster on the positive side of this dimension (narrative), while present tense and attributive adjectives cluster on the negative side of this dimension (event-oriented writing). In Biber (1988), general fiction and spontaneous speeches had the highest mean dimension scores for Dimension 2, and broadcasts, official documents, and academic prose had the lowest mean dimension scores for Dimension 2. Table 10 provides the mean and standard deviation for the dimensions score in each register (critique, essay, and personal narrative) and source (ChatGPT and student). Fig. 6 is a graphical representation of the mean dimension score on each register and source.

The repeated-measures ANOVA (see Table 11) showed no significant interactions between register and source, $F ( 2 , 1 0 0 ) = 2 . 2 9$ , $p { = } . 1 0$ . There were significant differences among essay, critique and personal narrative for Dimension 2, $F ( 2 , 1 0 0 ) = 4 . 2 5 $ , $\scriptstyle p = . 0 1$ , $\eta ^ { 2 }$ $= . 0 7 \cdot$ . Post-Hoc Tukey tests revealed significant differences between essays $\scriptstyle \mathbf { M } = - 3 . 8 6$ , $S D { = } 1 . 1 7 $ ) and personal narratives $( M { = } { - } 2 . 7 9$ , $\mathrm { S D } { = } 2 . 5 2 $ ), but not between critiques and these two registers. There were significant differences between AI-generated and studentauthored texts for Dimension 2, $F ( 1 , 1 0 0 ) { = } 1 3 . 4$ , $p { < } . 0 0 1$ , $\eta ^ { 2 } = . 1 2$ . The Post-Hoc Tukey test showed significant differences between AI-generated $\scriptstyle { M = - 3 . 9 3 }$ , $S D { = } 1 . 2 8 $ ) and student-authored texts $( M { = } { - } 2 . 7 1$ , $S _ { Ḋ } \mathrm { Ḋ - } 2 . 1 9 Ḍ Ḍ$ ). Even though both AI-generated and studentauthored texts load on the negative side of this dimension (non-narrative concerns), AI-generated texts are even less narrative than their student-authored counterparts. Similar to Dimension 1, AI-generated texts show less within-register variation, as can be seen in Fig. 6 and Table 10.

Table 10 Descriptive statistics for Dimension 2 scores for AI-generated and student-authored texts.   

<html><body><table><tr><td></td><td>GenAI</td><td></td><td>Student</td><td></td></tr><tr><td></td><td>Mean</td><td>SD</td><td>Mean</td><td>SD.</td></tr><tr><td>Essay</td><td>4.09</td><td>1.31</td><td>3.64</td><td>.98</td></tr><tr><td> Personal Narrative</td><td>3.58</td><td>1.53</td><td>2.01</td><td>3.06</td></tr><tr><td>Critique</td><td>4.15</td><td>.58</td><td>1.98</td><td>1.74</td></tr></table></body></html>

![](img/8f332f8a78e1276465bb57706b26ca08e0891aa2f2091b7f9eb8793059d3c0fd.jpg)  
Fig. 6. Dimension 2 scores for AI-generated and student-authored texts $- F ( 5 1 0 0 ) = 5 . 2 9$ , $p < . 0 0 1$ , $R ^ { 2 } = . 2 0 )$

Dimension 2: Narrative vs Non-Narrative Concerns   

<html><body><table><tr><td></td><td>df</td><td>F</td><td>p</td><td>?</td></tr><tr><td>source</td><td>1</td><td>13.4</td><td>&lt;.001</td><td>.12</td></tr><tr><td>register</td><td>2</td><td>4.25</td><td>.01</td><td>.07</td></tr><tr><td>Source * register</td><td>2</td><td>2.29</td><td>.10</td><td>.04</td></tr></table></body></html>

Dimension 3, Explicit versus Situation-Dependent Reference, distinguishes between context-independent, elaborated reference (positive side), and nonspecific, situation-dependent reference (negative side). Biber (1988) found that features that load on the positive side of Dimension 3 (e.g., wh-relative clauses, pied-piping relative clauses, phrasal coordination, nominalizations) co-occur in official documents, professional letters, and academic prose, and features that load on the negative side of Dimension 3 (e.g., time adverbials, place adverbials, and adverbs) co-occur in broadcasts, telephone conversations, and face-to-face conversations. Table 12 provides the mean and standard deviation for Dimension 3 scores for each register (critique, essay, and personal narrative) and source (ChatGPT and student). Fig. 7 is a graphical representation of the mean dimension score on each register and source.

The results of the repeated-measures ANOVA showed no significant interaction effects between source and registers, $F ( 2 1 0 0 ) = . 0 3$ , $\scriptstyle p = . 9 2 9$ . However, there were significant differences for the main effects. For register, $F ( 2 , 1 0 0 ) = 1 3 . 5 $ , $p < . 0 0 1$ , $\eta ^ { 2 } { = } . 2 1$ , Post-Hoc Tukey tests showed significant differences between personal narratives $\scriptstyle { M = 5 . 1 1 }$ , $S D { = } 5 . 8 3 \mathrm { \Omega }$ ) and essays $\scriptstyle { M = 9 . 8 }$ , $S D { = } 4 . 7 5$ ), but not for critique. For source, $F ( 1 1 0 0 ) = 4 6 . 5 $ , $p < . 0 0 1$ , $\eta ^ { 2 } = . 3 1$ , post-hoc analysis showed that GenAI is more explicit $\scriptstyle { M = 1 0 . 4 }$ , $S D { = } 4 . 4 4 $ ) than student writing $\scriptstyle ( M = 4 . 9 3$ , $S D { = } 4 . 6 2 $ ).

Table 11 Results of repeated-measures ANOVAs for AI-generated and student-authored texts for Dimension 2.   
Table 12 Descriptive statistics for Dimension 3 scores for AI-generated and student-authored texts.   

<html><body><table><tr><td></td><td>GenAI</td><td></td><td>Student</td><td></td></tr><tr><td></td><td>Mean</td><td>SD</td><td>Mean</td><td>SD</td></tr><tr><td>Essay</td><td>12.4</td><td>3.72</td><td>7.24</td><td>4.32</td></tr><tr><td> Personal Narrative</td><td>8.02</td><td>5.36</td><td>2.19</td><td>4.81</td></tr><tr><td>Critique</td><td>10.0</td><td>1.83</td><td>4.58</td><td>2.06</td></tr></table></body></html>

![](img/bb92fcb69cdcc7175e918fb91394199f75175280adc7b5e21eada10a74196a82.jpg)  
Fig. 7. Dimension 3 scores for AI-generated and student-authored texts $- F ( 5 1 0 0 ) = 1 4 . 6 8$ , $p < . 0 0 1$ , $R ^ { 2 } = . 4 2$

Fig. 7 shows that both AI-generated and student-authored texts load on the positive side of Dimension 3 (explicit reference), nevertheless, AI-generated texts contain features of explicit style to a greater extent than student-authored texts. This is in contrast with the results of Berber-Sardinha (2024). In that study, the author finds that AI-generated texts in all registers are less likely to include features of explicit reference than human-authored texts. Looking specifically at essays, the results of Berber-Sardinha (2024) show a small difference between AI-generated ${ ( \mathrm { M } { = } 1 . 3 ) }$ ) and student-authored texts $( \mathrm { M } { = } { } { - } 1 . 7 )$ .

Excerpts 7 and 8 illustrate the differences between AI-generated and student-authored texts for Dimension 3. These excerpts were written in response to an assignment that asked about the importance of language variation for teaching and research. Features that characterize the positive side of Dimension 3 are underlined in both excerpts. In Excerpt 7, we can see that students mainly use nominalizations to refer to the concepts in the prompt (e.g., communication, education, variation), we also see the use of phrasal co ordinators (and), but no wh-relative clauses. In contrast, Excerpt 8, uses phrasal coordination (and), nominalizations (environments, exposure) and wh-relative clauses (who is an avid reader, who does not engage) in order to provide more explicit references to the reader.

# 4.2.3. Excerpt 7 - Essay (Student)

Language variation is an organic process that happens in every language, presented in every person’s speech, every text, and communication. Variation must be a topic of analysis and discussion in the education field to fight linguistic prejudice and stereotypes. Speakers may vary pronunciation, word choice or morphology, and syntax.

# 4.2.4. Excerpt 8 - Essay (GenAI)

Each student is a unique linguistic individual, and factors such as personal experiences, interests, and exposure to different lin guistic environments shape their language use. For instance, a student who is an avid reader may have a richer vocabulary compared to

Dimension 3: Explicit vs Situation-Dependent Reference   
Table 13 Results of repeated-measures ANOVAs for AI-generated and student-authored texts for Dimension 3.   

<html><body><table><tr><td></td><td>df</td><td>F</td><td>p</td><td>?</td></tr><tr><td>source</td><td>1</td><td>46.5</td><td>&lt;.001</td><td>.31</td></tr><tr><td>register</td><td>2</td><td>13.5</td><td>&lt;.001</td><td>.21</td></tr><tr><td>Source * register</td><td>2</td><td>.03</td><td>.929</td><td>.001</td></tr></table></body></html>

a peer who does not engage in reading as frequently.

Dimension 4, Overt Expression of Persuasion, is characterized by features of overt expression of persuasion in the positive side (e.g., infinitive, modals of prediction, suasive verbs, conditional subordination), and no features on the negative side. Biber (1988) found that professional letters, editorials, and romantic fiction had high mean Dimension 4 scores (overt expression of persuasion) and broadcasts, press reviews, and adventure fiction had low mean Dimension 4 scores. Table 14 provides the mean and standard deviation for Dimension 4 scores for each register (critique, essay, and personal narrative) and source (ChatGPT and student). Fig. 7 is a graphical representation of the mean dimension score for each register and source.

The results of the repeated-measures ANOVA showed no significant interaction between register and source, $F ( 2 1 0 0 ) { = } 1 . 8 , p { = } . 1 6 ,$ $\eta ^ { 2 } { = } . 0 3$ , nor were there significant differences among the registers, essays, critiques and personal narratives for Dimension 4, $F ( 2 1 0 0 ) =$ 1.7, $p { = } . 1 7$ , $\eta ^ { 2 } { = } . 0 3$ . However, there were significant differences between AI-generated and student-authored texts, $F ( 1 , 1 0 0 ) = 5 3 . 9 , p$ $= < . 0 0 1$ , $\eta ^ { 2 } { = } . 3 5$ . Post-Hoc Tukey tests revealed significant differences between AI-generated and student-authored texts, with AIgenerated texts containing fewer features of overt persuasion $\scriptstyle \mathbf { M } = - 4 . 6 5$ , $\mathrm { S D } = 1 . 8 2 $ ) than student-authored texts $\mathrm { ( M { = } } \mathrm { - } 1 . 2 6$ , $\mathrm { S D } { = } 2 . 8 8 $ ).

It is interesting to compare the results for Dimension 4 in the present study and in Berber-Sardinha (2024). In Berber-Sardinha (2024) two registers loaded on the positive side of Dimension 4 (Overt Expression of Persuasion): L2 essays and conversations. For L2 essays, the author found that human-authored essays had considerably more features of persuasion $( M { = } 1 . 7 )$ , than AI-generated essays $\scriptstyle ( M = - 2 . 4 )$ . The results of Berber-Sardinha are in contrast to the results of the current study, where both AI-generated and student-authored essays do not contain features of persuasion. It is also interesting to consider within-register variation in this dimension, as the register of critiques varies considerably when written by students but is relatively homogeneous when AI-generated.

Dimension 5, Abstract versus Non-Abstract Information, is characterized by the co-occurrence of conjuncts, agentless passives, past participle, and adverbial subordinators on the positive side, and no features on the negative side. In Biber (1988), academic prose, official documents, and press reviews have a high mean dimension score for Dimension 5, and telephone conversations, romantic fiction, face-to-face conversations have low dimension scores for Dimension 5. Table 16 provides the mean and standard deviation for Dimension 5 scores for each register (critique, essay, and personal narrative) and source (ChatGPT and student). Fig. 9 is a graphical representation of the mean dimension score on each register and source.

Repeated-measures ANOVA showed a significant interaction effect between source and register, $F ( 2 , 1 0 0 ) = 3 7 . 4 ,$ $\scriptstyle p = . 0 2$ , $\eta ^ { 2 } = . 0 6$ Nevertheless, the relative values of the effect size indicate that source (AI-generated vs student-authored), $F ( 1 1 0 0 ) = 2 0 . 9 $ , $p { < } . 0 0 1$ , $\eta ^ { 2 } { = } . 1 5 4$ , was a more important factor in accounting for the language variation observed in Dimension 5. The overall means for AIgenerated vs student-authored texts show that student writing relies more on features of abstract production $\scriptstyle { M = 3 . 4 7 }$ , $S D { = } 3 . 7 5$ ) than AI-generated texts $( M { = } . 7 3$ , $\mathrm { S D } { = } 2 . 6 5$ ).

As shown in Fig. 9, both AI-generated vs student-authored load on the positive side of Dimension 5 (abstract style). Studentauthored texts, however, contain features of abstract style (e.g., conjuncts, agentless passives, past-participle, etc.) to a greater extent than AI-generated texts. This is particularly true for the register of critiques, where we see no overlap in the boxplots.

The results of the linguistic analysis have shown that Dimension 1 (Informational vs Involved Production), Dimension 3 (Explicit vs Situation-Dependent Reference), and Dimension 4 (Overt expression of persuasion) show the greatest linguistic variation between AIgenerated and student-authored texts. The results of the MDA showed that AI-generated texts are more informationally dense, explicit and less narrative, persuasive, and abstract than student-authored texts. This seems to suggest that students draw more on personal references, and the use of involvement features than ChatGPT. Table 18 summarizes the results of the Additive MDA. The $\cdot _ { + } ,$ and ‘-’ signs indicate where in that dimension each register falls.

# 5. Conclusion

This study sought to compare AI-generated texts and student-authored assignments in the context of undergraduate EFL writing, focusing on situational and linguistic characteristics across three registers (essays, critiques, and personal narratives). The findings can offer insights into the strengths and limitations of AI tools like ChatGPT in replicating student writing, which have significant implications for university assessment practices.

The situational analysis revealed that while ChatGPT can mirror personal narratives, it struggled with incorporating certain situational features present in student-authored essays and critiques. Specifically, similar to previous studies that relied on human reviewers (Gao et al., 2024; Ma et al., 2023), the empirical analysis showed that the use of references is a major difference between AI-generated and student-authored texts. In addition, the analysis of purposes showed that although AI can emulate the overall structure of academic writing, it falls short in replicating the complex interplay of communicative purposes that students employ, particularly in more formal registers such as essays and critiques. Although AI-generated texts are becoming increasingly prevalent, especially among EFL contexts, the current findings underscore a significant limitation of these tools. While AI can produce grammatically correct and contextually relevant content with remarkable speed, it still falls short in replicating the effort required for high-quality academic texts. Academic writing demands more than just syntactically correct sentences; it requires engagement with the subject matter, original thought, and the ability to critically analyze and synthesize information. An instructor teaching academic writing could use the results of this study to show their students that advanced EFL writers are more creative in purpose combination and linguistic choices than GenAI, so that students can become critical consumers of GenAI. This discussion might help low proficiency students who are enticed by AI’s ability to produce error free texts realize how creativity plays a role in academic writing.

The linguistic analysis further highlighted these discrepancies, showing that AI-generated texts are more informationally dense, explicit, and less involved than student-authored texts. EFL Students tend to integrate more personal references and features of involvement, making their writing more nuanced and contextually rich. These findings underscore the importance of human elements in writing, such as personal involvement and persuasive features, which AI currently cannot fully replicate.

Table 14 Descriptive statistics for Dimension 4 scores for AI-generated and student-authored texts.   

<html><body><table><tr><td></td><td>GenAI</td><td></td><td>Student</td><td></td></tr><tr><td></td><td>Mean</td><td>SD</td><td>Mean</td><td>SD</td></tr><tr><td>Essay</td><td>-4.39</td><td>1.48</td><td>1.00</td><td>2.94</td></tr><tr><td>Personal Narrative</td><td>3.97</td><td>2.17</td><td>1.54</td><td>2.75</td></tr><tr><td>Critique</td><td>6.17</td><td>.78</td><td>1.34</td><td>3.16</td></tr></table></body></html>

![](img/1c20207ee8b49dda79f2da494d543e0226ee9c46a26921a561f4909ccc58281d.jpg)  
Fig. 8. Dimension 4 scores for AI-generated and student-authored texts - $F ( 5 1 0 0 ) = 1 2 . 2 3 $ , $p < . 0 0 1$ , $R ^ { 2 } = . 3 8$ .

Dimension 4: Overt expression of persuasion   

<html><body><table><tr><td></td><td>df</td><td>F</td><td>p</td><td>?</td></tr><tr><td>source</td><td>1</td><td>53.9</td><td>&lt;.001</td><td>.35</td></tr><tr><td>register</td><td>2</td><td>1.7</td><td>.17</td><td>.03</td></tr><tr><td>Source * register</td><td>2</td><td>1.8</td><td>.16</td><td>.03</td></tr></table></body></html>

Table 15 Results of repeated-measures ANOVAs for AI-generated and student-authored texts for Dimension 1.   
Table 16 Descriptive statistics for Dimension 5 scores for AI-generated and student-authored texts.   

<html><body><table><tr><td></td><td>GenAI</td><td></td><td>Student</td><td></td></tr><tr><td></td><td>Mean</td><td> SD</td><td>Mean</td><td>SD</td></tr><tr><td>Essay</td><td>.73</td><td>2.85</td><td>4.90</td><td>3.65</td></tr><tr><td>Personal Narrative</td><td>.86</td><td>1.62</td><td>1.29</td><td>3.46</td></tr><tr><td>Critique</td><td>.52</td><td>3.58</td><td>4.02</td><td>2.98</td></tr></table></body></html>

Overall, the results of this study suggest that while AI tools like ChatGPT can assist in the writing process, they are not yet capable of replacing human writing, particularly in academic contexts that require a high degree of critical thinking and personal engagement. Writing instructors and English language teachers should be aware of these facts, emphasizing the unique qualities of human writing and guiding students of all proficiency levels on how to effectively use AI as a supplementary tool rather than a replacement. This approach will potentially help students harness the benefits of AI while maintaining the integrity and depth of their academic works. Just as second language writers need to master traditional literacy skills, they also need to effectively navigate the evolving landscape of generative AI tools. As these tools become increasingly widespread, their importance in academic and professional writing will grow, making it crucial for students to learn how to use them strategically.

![](img/810019a1539d6e1790886ecb2be282ad73fe9c1b955248b970a938c5981cb2e2.jpg)  
Fig. 9. Dimension 5 scores for AI-generated and student-authored texts - $F ( 5 1 0 0 ) = 7 . 0 4 $ , $p = < . 0 0 1$ , $R ^ { 2 } { = } . 2 6$

Dimension 5: Abstract versus non-abstract information   

<html><body><table><tr><td></td><td>df</td><td>F</td><td>p</td><td>?</td></tr><tr><td>source</td><td>1</td><td>20.9</td><td>&lt;.001</td><td>.154</td></tr><tr><td>register</td><td>2</td><td>31.1</td><td>.04</td><td>.05</td></tr><tr><td>Source * register</td><td>2</td><td>37.4</td><td>.02</td><td>.06</td></tr></table></body></html>

Table 17 Results of repeated-measures ANOVAs for AI-generated and student-authored texts for Dimension 5.   
Table 18 Summary of additive MDA results for AI-generated and student-authored texts.   

<html><body><table><tr><td></td><td>GenAI</td><td>Student</td></tr><tr><td rowspan="5">Essay</td><td>- - Informational</td><td>+ + Involved</td></tr><tr><td>- - Non-narrative concern</td><td>- Non-narrative concern</td></tr><tr><td>+ + Explicit</td><td>+ Explicit</td></tr><tr><td>- - Lack of persuasion</td><td>- Lack of persuasion</td></tr><tr><td>+ Abstract</td><td>+ + Abstract</td></tr><tr><td rowspan="5">Critiques</td><td>- - - Informational</td><td>+ + Involved</td></tr><tr><td>- - Non-narrative concern</td><td>- Non-narrative concern</td></tr><tr><td>+ + Explicit</td><td>+ Explicit</td></tr><tr><td>- - Lack of persuasion</td><td>- Lack of persuasion</td></tr><tr><td>+ Abstract</td><td>+ + Abstract</td></tr><tr><td rowspan="5">Personal narratives</td><td>- - Informational</td><td>+ + + Involved</td></tr><tr><td>- - Non-narrative concern</td><td>- Non-narrative concern</td></tr><tr><td>+ + Explicit</td><td>+ Explicit</td></tr><tr><td>- - Lack of persuasion</td><td>- Lack of persuasion</td></tr><tr><td>+ Abstract</td><td>+ Abstract</td></tr></table></body></html>

For EFL instructors, this means incorporating AI literacy into the curriculum alongside traditional writing instruction. It is essential for students to understand the benefits of AI, such as improving drafting efficiency, grammar correction, and idea generation, which can enhance their writing productivity. Moreover, the results of this study could be potentially translated into classroom practice to demonstrate that, although ChatGPT generates grammatically correct texts, its outputs are often quite homogeneous and are not as creative as we would expect EFL students to be. However, future studies should focus on the applications of AI in the classroom and students’ perceptions of GenAI texts beyond the linguistic analysis as was presented in the current study. Besides, it would be interesting to investigate how learners at different levels view the use of ChatGPT.

In conclusion, the integration of AI in university assessment practices presents both opportunities and challenges. It is crucial for educators to understand the strengths and limitations of AI-generated texts to better prepare students for a future where AI tools are commonplace. By fostering an informed and strategic use of AI in writing, educators might be able to enhance students’ learning experiences and ensure that written assignments continue to serve as a valuable measure of student knowledge and writing skills.

# CRediT authorship contribution statement

Alanna Mendoza: Validation, Methodology, Data curation, Conceptualization. Ingrid Veloso: Methodology, Conceptualization. Lee Alvarado: Validation, Data curation, Conceptualization. Marine Matte: Writing – review & editing, Validation, Methodology, Formal analysis, Conceptualization. Larissa Goulart: Writing – review & editing, Writing – original draft, Visualization, Validation, Supervision, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization.

# Data Availability

Data will be made available on request.

# References

Alexander, K., Savvidou, C., & Alexander, C. (2023). Who wrote this essay? Detecting AI-generated writing in second language education in higher education. Teaching English with Technology, 23(2), 25–43.   
Becker, K. (2022). Extending our understanding of unpublished academic writing: The creation and analysis of CorGrad (Doctoral dissertation, Iowa State University).   
Berber-Sardinha, T. (2024). AI-generated vs human-authored texts: A multidimensional comparison. Applied Corpus Linguistics, 4(1), Article 100083.   
Berber-Sardinha, T. B., Pinto, M. V., Mayer, C., Zuppardi, M. C., & Kauffmann, C. H. (2019). Adding registers to a previous multi-dimensional analysis. Multidimensional analysis: Research Methods and Current issues, 165–186.   
Biber, D. (1988). Variation across speech and writing. Cambridge University Press.   
Biber, D., & Conrad, S. (2019). Register, genre, and style. Cambridge University Press.   
Biber, D., & Egbert, J. (2023). What is a register?: Accounting for linguistic and situational variation within–and outside of–textual varieties. Register Studies, 5(1), 1–22.   
Casal, J. E., & Kessler, M. (2023). Can linguists distinguish between ChatGPT/AI and human writing?: A study of research ethics and academic publishing. Research Methods in Applied Linguistics, 2(3), Article 100068.   
Conrad, S.M. (1996). Academic discourse in two disciplines: Professional writing and student development in biology and history (Doctoral dissertation, Northern Arizona University).   
Gao, C. A., Howard, F. M., Markov, N. S., Dyer, E. C., Ramesh, S., Luo, Y., & Pearson, A. T. (2023). Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers. NPJ digital Medicine, 6(1), 75.   
Gardner, S., & Nesi, H. (2013). A classification of genre families in university student writing. Applied Linguistics, 34(1), 25–52.   
Goulart, L. (2024). Variation in University Student Writing. John Benjamins.   
Goulart, L., Biber, D., & Reppen, R. (2022). In this essay, I will…: Examining variation of communicative purpose in student written genres. Journal of English for Academic Purposes, 59, 101–159.   
Goulart, L., & Staples, S. (2023). Multidimensional analysis. In Conducting Genre-Based. Research in Applied Linguistics (pp. 127–148). Routledge.   
Goulart, L., & Wood, M. (2021). Methodological synthesis of research using multi-dimensional analysis. Journal of Research Design and Statistics in Linguistics and Communication Science, 6(2), 107, 13.   
Gray, B. (2015). Linguistic variation in research articles. Amsterdam: John Benjamins.   
Hassoulas, A., Powell, N., Roberts, L., Umla-Runge, K., Gray, L., & Coffey, M. (2023). Investigating marker accuracy in differentiating between university scripts written by students and those produced using ChatGPT. Journal of Applied Learning Teaching, 6(2).   
Islam, N., Sutradhar, D., Noor, H., Raya, J.T., Maisha, M.T., & Farid, D.M. (2023). Distinguishing Human Generated Text from ChatGPT Generated Text Using Machine Learning. arXiv. arXiv preprint arXiv:2306.01761.   
Johansson, I.R. (2023). A Tale of Two Texts, a Robot, and Authorship: a comparison between a human-written and a ChatGPT-generated text. Unpublished Undergraduate Thesis. Malmo ¨ University.   
Katib, I., Assiri, F. Y., Abdushkour, H. A., Hamed, D., & Ragab, M. (2023). Differentiating Chat Generative Pretrained Transformer from Humans: Detecting ChatGPTGenerated Text and Human Text Using Machine Learning. Mathematics, 11(15), 3400.   
Kobis, ¨ N., & Mossink, L. D. (2021). Artificial intelligence versus Maya Angelou: Experimental evidence that people cannot differentiate AI-generated from humanwritten poetry. Computers in Human Behavior, 114, Article 106553.   
Kublik, S., & Saboo, S. (2023). GPT-3. The Ultimate Guide to Building NLP Products with OpenAI API. Packt Publishing Ltd.   
Ma, Y., Liu, J., Yi, F., Cheng, Q., Huang, Y., Lu, W., & Liu, X. (2023). AI vs. Human–Differentiation Analysis of Scientific Content Generation. arXiv preprint arXiv: 2301.10416.   
Quaglio. (2009). Television Dialogue: The Sitcom Friends vs. Natural Conversation. Philadelphia: John Benjamins.   
R Core Team (2024). R: A language and environment for statistical computing [Computer software]. 〈https://www.R-project.org/〉.   
Staples, S., Goulart, L., & Reppen, R. (in press). Chapter 3: Situational Analysis. In S. Staples. Understanding Variation in First-Year University Writing: A Corpus-Based Study. Cambridge University Press.   
Tasker, D. G. (2022). A case study of the variety of writing assignments in an undergraduate English department. English for Specific Purposes, 66, 33–62.   
Theocharopoulos, P. C., Anagnostou, P., Tsoukala, A., Georgakopoulos, S. V., Tasoulis, S. K., & Plagianakos, V. P. (2023, July). Detection of fake generated scientific abstracts. In 2023 IEEE Ninth International Conference on Big Data Computing Service and Applications (BigDataService) (pp. 33–39). IEEE.

Linguistics, Corpora, Register Studies, among others. Larissa is currently associate editor of Register Studies. Her forthcoming book, “Variation in University Student Writing,” slated for release in the Fall of 2024 as part of the Studies in Corpus Linguistics series published by John Benjamins, examines how language varies across registers and disciplines in student writing.