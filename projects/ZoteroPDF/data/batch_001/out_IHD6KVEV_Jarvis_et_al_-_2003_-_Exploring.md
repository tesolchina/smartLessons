# Exploring multiple profiles of highly rated learner compositions

Scott Jarvisa,\*, Leslie Grantb , Dawn Bikowskia , Dana Ferrisc

a Department of Linguistics, Ohio University, Gordy Hall 383, Athens, OH 45701, USA b Central Michigan University, Pleasant, MI, USA c California State University, Sacramento, CA, USA

# Abstract

Recent research has come a long way in describing the linguistic features of large samples of written texts, although a satisfactory description of L2 writing remains problematic. Even when variables such as proficiency, language background, topic, and audience have been controlled, straightforward predictive relationships between linguistic variables and quality ratings have remained elusive, and perhaps they always will. We propose a different approach. Rather than assuming a linear relationship between linguistic features and quality ratings, we explore multiple profiles of highly rated timed compositions and describe how they compare in terms of their lexical, grammatical, and discourse features. To this end, we performed a cluster analysis on two sets of timed compositions to examine their patterns of use of several linguistic features. The purpose of the analysis was to investigate whether multiple profiles (or clusters) would emerge among the highly rated compositions in each data set. This did indeed occur. Within each data set, the profiles of highly rated texts differed significantly. Some profiles exhibited above-average levels for several linguistic features, whereas others showed below-average levels. We interpret the results as confirming that highly rated texts are not at all isometric, even though there do appear to be some identifiable constraints on the ways in which highly rated timed compositions may vary.

$©$ 2003 Elsevier Inc. All rights reserved.

Keywords: Multiple profiles; Highly rated compositions; Linguistic features; Cluster analysis

# Introduction

There have been numerous attempts to quantify second language (L2) writing quality in terms of the frequency and distribution of linguistic features that occur in written texts. This seems a worthwhile pursuit, of course, because if a sufficiently predictive relationship can be found between the linguistic features of a text and its quality rating, then this will undoubtedly result in important applications and improved efficiency in writing pedagogy, assessment, and research. So far, unfortunately, the relationship between linguistic features and writing quality has remained rather elusive. Although some studies — which we will review in the following section — have found significant correlations between quality ratings, on the one hand, and certain lexical, grammatical, and discourse features of texts, on the other, such correlations, although significant, tend to be only low to moderate. Additionally, there have been inconsistencies across studies in relation to the specific linguistic predictors of quality ratings that have been reported. What all of this means is that researchers and testers cannot (yet?) confidently determine the quality of learners’ texts through purely objective measures.

One problem may be the approach that has usually been taken. The use of correlation tests (or even tests of differences between groups) assumes a linear relationship between writing quality and linguistic features. In other words, it assumes that there is a single profile of highly rated texts. In this paper, we explore the possibility that there may exist multiple profiles of highly rated texts. Even though this approach may appear to complicate — rather than simplify — our understanding of L2 quality writing, consider past research done on ‘‘good language learners’’ (e.g., Ellis, 1994; Oxford, 1990; Skehan, 1986). We know there is no single ‘‘good language leaner,’’ but rather there exist multiple profiles of good language learners. Narrowing this notion to writing quality, then, the purpose of this study is to explore whether multiple types of highly rated learner compositions can be identified, and, if so, how they compare with one another in terms of selected lexical, grammatical, and discourse features.

# Review of the literature

While we recognize that any type of writing is a complex activity affected by numerous variables (e.g., the writer’s purpose for writing, understanding of audience, understanding of text characteristics, and/or cultural expectations, to name a few), in this study we focus on the use and clustering of a number of linguistic features. To this end, we find ourselves squarely in the ‘‘descriptive approach’’ to discourse analysis (Kaplan & Grabe, 2002), and the features we describe are those found in two data sets that formed the basis for analysis in two previous studies: Ferris (1994) and Grant and Ginther (2000). Both of these data sets had already been tagged for linguistic features using Biber’s classification (1986, 1988; Biber, Conrad, & Reppen, 1998); the features common to both studies are reviewed and analyzed below.

# Text characteristics

General text characteristics include text length, average word length, and diversity of vocabulary (e.g., type/token ratio). These last two text characteristics, taken together, are sometimes referred to as ‘‘lexical specificity.’’ Relating these characteristics to writing quality, then, the general idea is that longer texts and greater lexical specificity are indicative of more sophisticated writing (Biber, 1986, 1988; Biber et al., 1998).

In several studies of timed writing samples, the findings have been consistent: Essays with higher scores are longer than their lower rated counterparts (Carlson, Bridgeman, Camp, & Waanders, 1985; Ferris, 1994; Frase, Faletti, Ginther, & Grant, 1999; Grant & Ginther, 2000; Reid, 1986, 1990), use longer words on average (Frase et al., 1999; Grant & Ginther, 2000; Reid, 1986, 1990; Reppen, 1994), and show more diverse word use than their lower quality counterparts (e.g., Grant & Ginther, 2000; Jarvis, 2002a; Reppen, 1994). However, the means used for producing a text may make a difference in text length. Although some researchers found that email journals resulted in longer writings than paper-and-pencil journals (Gonzalez-Bueno & Perez, 2000), recent work by Biesenbach-Lucas, Meloni, and Weasenforth (2000) found that their subjects wrote more when writing on a word processor than when writing email.

# Lexical features

Lexical features, structures that serve a discourse function (Biber, 1986, 1988), often characterize better quality — or more highly rated — texts (e.g., Cheng & Steffensen, 1996; Connor, 1990; Ferris, 1994; Intaraprawat & Steffensen, 1995). For example, conjuncts (e.g., moreover, however), or logical connectives/connectors (Crismore, Markkanen, & Steffensen, 1993; Longo, 1994), are used to indicate logical relationships between clauses, and they serve an informational or textual role in that they direct readers to notice how the text is organized (Vande Kopple, 1985). Hedges (e.g., sort of) play a different type of metadiscourse role — an interpersonal one (Crismore, 1990; Vande Kopple, 1985). They show uncertainty on the part of the speaker or writer and are most often found in informal discourse (Biber, 1988). Hedges have been called ‘‘validity markers’’ in Vande Kopple’s classification scheme (1985), and such markers can be used by the writer to indicate attitude and perspective (e.g., Crismore et al., 1993). Although students are often advised to decrease their use of unnecessary hedges as a means for improving their writing, researchers such as Hyland (1996a, 1996b) explain that these structures should be used in writing, particularly in scientific writing, because they perform an important pragmatic function. Longo (1994), too, showed that expert writers used hedges to situate their claims whereas student writers did not use hedges for this purpose.

Another class of lexical features, amplifiers (e.g., certainly), indicates a degree of certainty on the part of the writer, according to Chafe (1985), whereas emphatics (e.g., really) show the presence (but not degree) of certainty. These features seem to make the writing more personal for the reader, but the inclusion of such features is not necessarily preferred by the readers, at least with respect to certain types of texts (see Crismore’s 1990 study of social studies texts). In the study mentioned above, Longo (1994) found that expert writers did not use emphatics to the degree that the student writers did in their scientific writings; indeed, the student writers tended to make use of this feature to perform a kind of ‘‘selling’’ of their ideas. Hewings and Hewings (2002), comparing anticipatory $i t$ in student dissertations in business and journal articles in the field of business, found a similar tendency for nonnative student writers — as compared to published writers — to try to ‘‘sell’’ their ideas more.

Demonstratives (this, that, these, and those) are used by writers to reinforce text cohesion, and these structures are often included as one element of cohesion studies (Hinkel, 2001; Jin, 2001). For example, in the study by Jin (2001), demonstrative reference, along with conjunctions and lexical repetition, were analyzed in Chinese graduate student writing. Findings indicated that all students used these cohesion devices across three types of writing and at both proficiency levels examined in the study; however, the more advanced group used more of the ties than did the intermediate group.

Downtoners (e.g., almost) serve to make the verb weaker but can also be used to indicate probability and mark politeness (Biber, 1988; Reppen, 1994). Whereas other researchers (e.g., Crismore et al., 1993) include such markers as emphatic devices that are used to indicate attitude, downtoners were kept separate in this analysis based on Biber’s classification (1988).

As mentioned above, using lexical features demonstrates skill on the part of the writer and an awareness and/or consideration of the reader of the text; however, other factors may play a role as well. For example, findings from past research indicate that the writing medium may play a role in the writer’s decision to incorporate such features. Biesenbach-Lucas et al. (2000) examined differences between texts written as email and by word processor for the use of cohesive features, such as conjuncts and demonstratives. They found that, overall, the two writing formats produced roughly equivalent counts of lexical and grammatical features. There were some differences, however, in the use of demonstrative noun phrases, which occurred more often in word-processed texts, and conjuncts, which unexpectedly occurred more often in email texts.

# Grammatical features

Some of the grammatical features examined in quality essays have included nouns, nominalizations (words ending in morphemes used to mark nouns, such as -ness, -tion, etc.), prepositions, pronouns, adjectives, adverbs and adverbials, articles, and verbs. Biber’s analysis (1988) of numerous texts (although not learner texts) led to the characterization of texts in terms of ‘‘dimensions.’’ For example, academic prose, characterized as highly informational, nonnarrative, explicit, and abstract, is associated with higher frequencies of nouns, prepositions, conjuncts, and passives; at the same time, there are lower frequencies of other features, for example, personal pronouns and contractions.

Studies of second language students’ timed writings have revealed similar distributions of the features mentioned above. For example, Connor (1990) found that highly rated persuasive essays contained more nominalizations, prepositions, passives, and conjuncts than did the essays that were rated low. Similar findings were also reported by Ferris (1994), Frase et al. (1999), and Grant and Ginther (2000).

Adverbs and adverbials, also studied in writing samples, add information to texts like the structures above, but they are primarily used to contribute more information about time and place in discourse that is interpersonal and involved. Similarly, adjectives give additional information, but they are most often associated with heavy noun use and are most often present in descriptive narratives. In a study by McClure, Mir, and Cadierno (1993), older monolingual Spanish- and English-speaking students wrote more descriptive narratives than the younger students in the study. The descriptive nature of their writings was demonstrated, in part, by a higher frequency of adjective use.

Pronouns (first and second personal pronouns), in general, are considered more informal and are of a more personal nature, whereas third person pronouns (including the pronoun it) are more characteristic of academic discourse (Biber, 1988; see also Hewings & Hewings, 2002). However, these generalizations do not hold for certain tasks. For example, in a study by Grabe and Biber (as cited in Grabe & Kaplan, 1996), the researchers found that student essays contained characteristics of academic writing, but also had features of involved/informal writing, such as first person pronoun use. Hyland (2002) also notes how first person pronoun use occurs in published works as a means for authors ‘‘to clearly link themselves with their main contribution’’ (p. 353), but how ESL students go to great lengths to avoid using such pronouns — believing them to be inappropriate for academic writing. In contrast, Reid (1992), in her study of Test of Written English (TWE) essays written by native and nonnative speakers, found that the nonnatives used more pronouns than did the native speakers. Additionally, Grant and Ginther (2000) found that the use of second person pronouns decreased as TWE essay ratings increased, a finding similar to Reppen (1994) and Shaw and Liu (1998), who noted that as writing skill developed, writers shifted from an ‘‘oral’’ style of writing to a ‘‘written’’ style.

Articles give information about the nouns they modify, and their roles vary from first mention of nouns to the marking of nouns already introduced. Writers producing higher rated texts use these structures more often and more appropriately than do those producing lower rated texts, contributing to the cohesion of the writing (e.g., Ferris, 1994; Jarvis, 2002b; Reid, 1992). Master (1987, 1990), who has done considerable work on the article system, has presented a systematic approach for discussing articles at different levels of development (1997; see also the chapter on the determiner system in Master, 1996).

Verbs have been investigated in writing samples in terms of tense, aspect, modal forms, and participials. In higher rated texts, the use of present tense forms, including stative be forms, becomes gradually replaced by more sophisticated verb structures (see, for example, Reppen’s 1994 discussion of verb development in children’s writing at different grade levels). Choice of verb form, however, depends in part on the type of writing being performed. For example, in scientific writing, present tense is used to indicate general truths, and past tense is used to describe methodological steps carried out by scientists and findings that are not (yet) accepted as ‘‘truths.’’ In a study of native and nonnative English speakers’ tense choices in scientific writing, Burrough-Boenisch (2003) found that both groups varied in expected tense usage. Although the researcher explained that the native writers made changes to tense to maintain consistency in their writings, she felt that the nonnative writers’ changes may have been influenced by their first languages; in addition, the author also felt that they may have misunderstood the pragmatics of tense usage.

In terms of clause features, research indicates that higher rated texts use more types of subordination (Grant & Ginther, 2000) as well as passive voice (Connor, 1990; Ferris, 1994; Grant & Ginther, 2000) than do lower rated texts. Although clause use depends in part on the task facing the writer, less mature writers have more difficulty producing these structures accurately and using these structures appropriately than do the more advanced writers. Recent research by Li (2000) suggests that the type of writing done was a major factor in the use of complex sentence structures, but that the decision to use more subordination, for example, co-occurred with an increase in grammatical error. This finding reminds us of what Foster and Skehan (1996) have pointed out: that second language learners have to juggle both complexity and accuracy in any given writing situation. And, to complicate matters, Li found that the structure of the task itself and the interactiveness of the audience appeared to play a role in the nature of email texts generated by second language writers.

Based on the research described above, we see that the research efforts have been to produce a singular description of what constitutes a high quality (or highly rated) text as opposed to a low quality (or lower rated) text. The results indicate that type of writing, task, proficiency level, and means for writing have all played a role in the structures produced, making it nearly impossible to come up with one description of good writing. Like earlier efforts to describe the ‘‘good language learner’’ (Rubin, 1975), we may be at a juncture in our research where we recognize that there are many types of good L2 writing. It is this idea that we explore and attempt to answer in the present study.

# The present study

Stated more precisely, the question we address in this study is whether highly rated timed compositions produced by a group of comparable learners under similar circumstances will fall into multiple significantly different clusters of texts with respect to the use of the following 21 linguistic features:

1. Text length (total number of words)   
2. Mean word length   
3. Diversity of vocabulary (type/token ratio)   
4. Conjuncts (relative frequency)   
5. Hedges (relative frequency)   
6. Amplifiers (relative frequency)   
7. Emphatics (relative frequency)   
8. Demonstratives (relative frequency)   
9. Downtoners (relative frequency)   
10. Nouns and nominalizations (relative frequency)   
11. First and second person pronouns (relative frequency)   
12. Impersonal pronoun it (relative frequency)   
13. Adverbials (relative frequency)   
14. Prepositions (relative frequency)   
15. Definite articles (relative frequency)   
16. Present tense verbs (relative frequency)   
17. Stative verb be (relative frequency)   
18. Passives (relative frequency)   
19. Adverbial subordination (relative frequency)   
20. Relative clauses (relative frequency)   
21. Complementation (relative frequency)

As indicated earlier, both of the data sets we deal with in this study have been analyzed in previous studies (Ferris, 1994; Grant & Ginther, 2000), and had already been tagged for linguistic features using Biber’s classification (1986, 1988; Biber et al., 1998). The 21 linguistic features we selected for analysis in the present study are those that were already common to both data sets. It was important for the purposes of our study to use data sets and linguistic features that had been used in previous studies that assumed a single profile of highly rated timed compositions in order for us to substantiate whether the single-profile (or linear) approach is fundamentally flawed.

Because any emerging usage patterns concerning the selected 21 linguistic features will necessarily be affected by the nature of the writing task, the general proficiency level of the writers, and other background variables such as the learners’ L1s, it is prudent to anticipate that the patterns of linguistic features found in highly rated timed learner compositions will not only be manifold but may also differ from one context to the next. Thus, in order to address the issue of generalizability, we examine two sets of timed assessment essays representing two different contexts. As mentioned, the data sets have been used in previous investigations of writing quality, but this is the first time that they have been examined for the potential that each data set may include more than one type of highly rated text. The first data set consists of $1 6 0 ~ \mathrm { E S L }$ placement compositions written by Arabic-, Chinese-, Japanese-, and Spanish-speaking university-level students (see Ferris, 1994), and the second data set consists of 178 TWE essays written by Arabic, Chinese, and Spanish speakers (see Grant & Ginther, 2000). The data sets were analyzed separately because, given the different contexts in which they were elicited, we did not consider them to be comparable. Nevertheless, in our Discussion section we do draw together the findings of the two separate analyses in order to discuss whether they point to the same conclusions regarding multiple profiles of highly rated timed compositions.

By ‘‘multiple profiles,’’ we mean groups of texts that display within-group similarities with respect to all 21 linguistic features while simultaneously showing significant between-group differences with respect to one or more of these features. For example, one profile of highly rated timed compositions in a given context may be texts that are longer than average, have lower than average mean word length, have lower than average diversity of vocabulary, and so forth. Another profile of highly rated texts in the same context may be texts that are average in length (though significantly shorter than those of the first profile), have above-average mean word length, have above-average diversity of vocabulary, and so forth. The first profile might characterize texts written by learners who use a high number of subordinators and other function words, whereas the latter might characterize texts written by learners who write shorter and simpler sentences but use more precise and more sophisticated vocabulary. Depending on the context of the writing task, both profiles of writing might be equally successful — or equally highly rated. This is the essence of what we have set out to investigate.

Our method of analysis was effectively the same for both data sets. In both cases, after misspellings were corrected, the 21 linguistic features under investigation were tagged and counted with the help of a computer. The tagging may have been affected by grammatical errors in the student texts, but our examination of the tagged texts indicated that the number of tagging errors was low and did not affect the overall results. Counts of the 21 features were obtained for each text, and the counts for Features 4–21 were additionally normed in order to reflect the relative frequencies of each linguistic feature instead of their actual frequencies (see, e.g., Biber, 1988). Next, the normed counts for each text were imported into SPSS 11.0 and subjected to agglomerative hierarchical cluster analysis with between-groups linkage using the squared Euclidean distance measure. A separate cluster analysis was performed on each data set.

Agglomerative hierarchical cluster analysis is a mathematical procedure for classifying cases (e.g., texts) into groups based on their shared similarities across a number of measures (e.g., linguistic features). Agglomerative hierarchical cluster analysis proceeds through several iterations, beginning at a stage where each case constitutes its own cluster. In subsequent iterations, the two closest clusters are combined into a new aggregate cluster, and this reduces the overall number of clusters by one. In the final iteration, the final two clusters are combined into a single aggregate cluster (see, e.g., Toms, Cummings-Hill, Curry, & Cone, 2001).

From the output of cluster analysis, one must determine which iteration — i.e., which number of clusters — best represents the number of groups in the data. There is no perfectly objective way to determine the optimal number of clusters, but this decision is not completely subjective, either. SPSS outputs an agglomeration schedule that shows the relative distance between cases being linked at each iteration. The larger the distance, the less similarity there is between cases that have been clustered together. It is common to select as the optimal solution the number of clusters that one finds just before a large jump in the relative distance coefficient (or fusion coefficient) (e.g., Toms et al., 2001; Yuen, 2000). This is what we did in the present study to establish an optimal cluster solution for each data set.

In our preliminary analyses, we ran cluster analyses using all texts in each data set. This was done to confirm that each data set included multiple clusters of both highly rated and lower rated texts. We did indeed find that several of the clusters included only highly rated or only lower rated texts. The primary purpose of our study is not to explore differences between highly rated and lower rated texts, however, but rather to determine whether highly rated compositions alone are homogeneous in their use of linguistic features. Therefore, in our follow-up analyses, we ran cluster analyses only on the highly rated texts in each data set so that we could better focus on the differences existing just between groups of ‘‘good’’ texts. We encountered one problem at this stage, which was that the first linguistic feature in our analysis (i.e., text length) turned out to be too strong a grouping factor. That is, the cluster analyses in this stage of our analysis identified clusters predominantly in accordance with how long the texts were, irrespective of similarities and differences existing across texts in relation to the other 20 linguistic features. In order to circumvent this problem, we omitted text length from our final cluster analysis of each data set. This was done to allow for a more balanced weighting of the remaining 20 linguistic features in determining how the clusters were formed. The results of our analyses for each data set are presented in the following sections.

# Data Set 1

Data Set 1 consists of 160 university (U.S.) ESL placement compositions written by 40 learners, each from the following four L1 groups: Arabic, Chinese (Mandarin), Japanese, and Spanish. The learners represented a range of proficiency levels, from low intermediate to advanced. The writing task was a $3 5 \mathrm { - m i n }$ timed examination in which learners were asked to respond to a prompt about the effects of culture shock. Each composition was rated holistically on a scale of 1–10 by three separate raters (inter-rater reliability was unavailable), and the sum of the three ratings was used as the composition’s quality score (see Ferris, 1991,

1994). Thus, for this data set, the scores ranged from 6 to 30. The compositions were later entered into the computer, where the linguistic features under investigation were tagged and counted. Linguistic Feature 3 (diversity of vocabulary) was calculated as the number of different words (types) per 100 words of text, whereas Features 4–21 were normed to a relative frequency per 1000 words of text, as determined by the analysis software that was used.

In our preliminary analysis of Data Set 1, we determined that a solution of 12 clusters was optimal. Four of the 12 clusters included only a single text, and three other clusters included a mix of medium to highly rated texts (i.e., texts with quality scores of 13 or higher). The remaining five clusters clearly differentiated between highly rated and lower rated texts: Two of these latter clusters included only texts with quality ratings no higher than 19 and 22, respectively, whereas the other three clusters included only texts with quality ratings no lower than 20, 21, and 22, respectively. On the basis of these results, we determined that a score of 22 was the most objective discriminator between higher and lower rated texts, so we chose this score as the cut-off score.

As mentioned earlier, the final analysis involved a cluster analysis run only on the highly rated texts in each data set. The quality scores for all 160 texts in Data Set 1 ranged from a minimum of 6 to a maximum of 30, but before running our final cluster analysis, we removed all of the texts with scores below 22, and also disqualified one text with a score of 26 (written by a Japanese speaker) and one text with a score of 24 (written by a Spanish speaker) that were found not to cluster with any of the other texts in the data set. This left us with a group of 62 highly rated texts.

In our final analysis of Data Set 1, we found a solution of five clusters (of the 62 highly rated texts) to be optimal. We settled on this solution after observing in the agglomeration schedule output by SPSS that the distance coefficient rose gradually (at approximately 100-point intervals) from one iteration to the next until a large jump of 1077.62 points when it combined five clusters into four. This jump reflected an abrupt discontinuity in the composition of the clusters, so we determined that a five-cluster solution was optimal.

Table 1 gives the make-up of each of the five clusters in terms of the L1 backgrounds of the writers involved. Here, one can see that all four L1s are relatively equally represented within each cluster, such that there is no significant relationship between cluster membership and L1 background (chi-square results: $\chi ^ { 2 } = 1 0 . 0 8 , d f = 1 2 , P = . 6 1 )$ . Even so, Chinese and Japanese speakers do seem to be underrepresented in Clusters 3 and 5.

Figs. 1–5 graphically illustrate the profiles of each cluster in terms of their mean $Z$ scores for each of the 21 linguistic features under investigation. (Note. Linguistic Feature 1 — i.e., text length — was removed from consideration during the cluster analysis, but is shown here to provide a fuller characterization of each cluster.) The $Z$ scores were calculated from all 160 texts in Data Set 1 so as to indicate whether the patterns found in the five clusters of highly rated compositions may be consistently higher or lower than what is found in lower rated compositions. If all five clusters show positive or all five clusters show negative mean $Z$ scores on a particular feature, then this may indicate that the feature is predictive of quality ratings. Figs. 1–5 indicate that there are three linguistic features for which all five clusters show positive mean $Z$ scores: Feature 1 (text length), Feature 3 (diversity of vocabulary), and Feature 7 (emphatics). For all other linguistic features, the various clusters show both positive and negative (i.e., above-average and below-average) mean $Z$ scores.

Table 1 Make-up of the five clusters of highly rated texts in Data Set 1   

<html><body><table><tr><td></td><td>Cluster 1</td><td>Cluster 2</td><td>Cluster 3</td><td>Cluster 4</td><td>Cluster 5</td><td>Total</td></tr><tr><td>Arabic</td><td>2</td><td>6</td><td>2</td><td>1</td><td>3</td><td>14</td></tr><tr><td>Chinese</td><td>1</td><td>6</td><td>0</td><td>6</td><td>1</td><td>14</td></tr><tr><td>Japanese</td><td>2</td><td>7</td><td>0</td><td>3</td><td>1</td><td>13</td></tr><tr><td>Spanish</td><td>3</td><td>7</td><td>2</td><td>5</td><td>4</td><td>21</td></tr><tr><td>Total</td><td>8</td><td>26</td><td>4</td><td>15</td><td>9</td><td>62</td></tr></table></body></html>

Whereas mean $Z$ scores above .5 and below $- . 5$ represent noteworthy departures from the central tendency, one can characterize the profiles of the five clusters in terms of their noteworthy high $\left( Z > . 5 \right)$ and low $( Z < - . 5 )$ use of the various linguistic features, as shown in Table 2. Here, one can see that although there exists some overlap between cluster profiles (especially with respect to Features 1 and 3), each cluster nevertheless exhibits its own unique pattern across the 21 features. The uniqueness of the clusters was also confirmed statistically through a series of oneway ANOVAs using each linguistic feature as the dependent variable and cluster membership as the fixed factor. The ANOVA tests revealed significant differences across clusters for the following seven linguistic features:

Table 2 Profiles of the five clusters in Data Set 1 in terms of high $( + )$ and low $( - )$ use of features   

<html><body><table><tr><td>Features</td><td>Cluster 1</td><td>Cluster 2</td><td>Cluster 3</td><td>Cluster 4</td><td>Cluster 5</td></tr><tr><td>1. Text length</td><td>+</td><td>+</td><td></td><td></td><td>+</td></tr><tr><td>2. Mean word length</td><td></td><td></td><td>+</td><td></td><td></td></tr><tr><td>3. Diversity of vocabulary</td><td>+</td><td>+</td><td></td><td></td><td>+</td></tr><tr><td>4. Conjuncts</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>5. Hedges</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>6. Amplifiers</td><td>+</td><td></td><td></td><td></td><td></td></tr><tr><td>7. Emphatics</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>8. Demonstratives</td><td></td><td></td><td>+</td><td></td><td></td></tr><tr><td>9. Downtoners</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>10. Nouns/nominalizations</td><td></td><td>+</td><td>+</td><td></td><td></td></tr><tr><td>11. First/second person pronouns</td><td>+</td><td></td><td></td><td></td><td></td></tr><tr><td>12. Impersonal pronoun &quot;it&quot;.</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>13. Adverbials</td><td></td><td></td><td></td><td>+</td><td></td></tr><tr><td>14. Prepositions</td><td>+</td><td>+</td><td></td><td></td><td></td></tr><tr><td>15. Definite articles</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>16. Present tense verbs.</td><td></td><td></td><td></td><td>+</td><td></td></tr><tr><td>17. Stative verb &quot;be&quot;.</td><td></td><td></td><td>+</td><td></td><td>+</td></tr><tr><td>18. Passives</td><td></td><td></td><td></td><td></td><td>+</td></tr><tr><td>19. Adverbial subordination</td><td>+</td><td></td><td>+</td><td></td><td></td></tr><tr><td>20. Relative clauses</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>21. Complementation</td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

![](img/72cb37bc11c2a62bdb990165d209e22ad05823421095683132443ba0d455ea40.jpg)  
Fig. 1. Cluster 1 profile (Data Set 1)

![](img/840d244ff46f976511a5ec2c3a6fa9d8a7fceb89722e04b667f5dbfb3f36d8ca.jpg)  
Fig. 2. Cluster 2 profile (Data Set 1)

![](img/e8f70d39283c9826054334c293ab731e56079234a4eaf4d706b5628c6f58ea65.jpg)  
Fig. 3. Cluster 3 profile (Data Set 1)

![](img/d2ef982f4f05c5e48dff3f74cd92dc0ebfde7bf1e3255fc569f278fa4e326eb3.jpg)  
Fig. 4. Cluster 4 profile (Data Set 1)

![](img/1cb6566fe54ece120c040169b3b525cffd972ec28191a5f2ba94d089cff3f51d.jpg)  
Fig. 5. Cluster 5 profile (Data Set 1)

mean word length (Feature 2) $( F = 8 . 6 2 9 $ , $d f = 4 / 5 7$ , $P < . 0 0 1$ ), nouns and nominalizations (Feature 10) $( F = 4 1 . 5 2 1 , ~ d f = 4 / 5 7 ,$ , $P < . 0 0 1$ ), first and second person pronouns (Feature 11) $( F = 1 7 . 1 7 7 , d f = 4 / 5 7 .$ , $P < . 0 0 1$ ), adverbials (Feature 13) $( F = 5 . 2 5 8$ , $d f = 4 / 5 7$ , $P = . 0 0 1$ ),   
$\bullet$ prepositions (Feature 14) $( F = 1 0 . 5 7 3$ , $d f = 4 / 5 7$ , $P < . 0 0 1$ ),   
$\bullet$ present tense verbs (Feature 16) $C = 9 . 6 6 3$ , $d f = 4 / 5 7$ , $P < . 0 0 1$ ), and stative verb be (Feature 17) $( F = 3 . 3 1 8$ , $d f = 4 / 5 7$ , $P = . 0 1 6$ ).

A series of nonparametric Kruskal–Wallis tests confirmed the significant differences across clusters with precisely these seven linguistic features.

# Data Set 2

Data Set 2 consists of 178 TWE essays written by 178 language learners. The 178 participants include 59 Arabic speakers, 60 Chinese speakers, and 59 Spanish speakers. All texts included in this data set had been given a rating of 3, 4, or 5 (on a scale of 1–6, 1 being the lowest possible score) as their official TWE score. The writing task was a 30-min timed essay in response to one of two prompts. Half (29 or 30) of the participants in each L1 group were asked to write about their preferred source of news (see Grant & Ginther, 2000), and the other half were asked to express whether they thought teachers should make learning enjoyable and fun. Each composition was rated holistically by two trained TWE raters following the usual ETS protocol. The final TWE score — or writing quality rating — for each essay was calculated as the mean of the two raters’ ratings for that essay. Inter-rater reliability for this particular set of TWE data was unavailable; however, acceptable levels of inter-rater reliability for TWE ratings fall in the range between .78 and .83, as discussed in the TOEFL Test of Written English Guide (1992). Also, as is standard practice, when raters disagree by two points or more, a third rater is brought in.

After the texts were collected, they were entered into the computer to be tagged and quantified vis-a\`-vis the linguistic features under investigation. Linguistic Feature 3 (diversity of vocabulary) was calculated as the number of different words (types) per the first 50 words of text, whereas Features 4–21 were normed to a relative frequency of occurrence per 200 words of text. The method of calculating vocabulary diversity differed between the two data sets due to differences in the analysis software that was used. Likewise, differences in the software resulted in differences in how relative frequencies were output (i.e., per 1000 words vs. per 200 words) in the two data sets, but this did not have any effect on the results of the cluster analyses.

In our preliminary analysis of Data Set 2, we determined that a solution of seven clusters was optimal for all 178 texts in the data set. All seven of the clusters included multiple texts, and all seven differentiated among texts with different quality scores. Two clusters included texts almost exclusively with ratings of 3–4, one cluster included texts almost exclusively with ratings of 4–5, and the remaining clusters consisted of texts almost exclusively with ratings of 3 (one cluster), 4 (one cluster), and 5 (two clusters).

Our final cluster analysis for this data set included only the 58 highly rated essays that had been given a TWE rating of 5. We chose 5 as the cut-off score because this clearly is a high rating on the TWE, whereas scores of 3 and 4 are not. From the agglomeration schedule output by SPSS, we determined that a solution of three clusters was optimal for the 58 highly rated texts. Table 3 shows the makeup of each of the three clusters in terms of the topics of the essays and the L1s of the writers. A chi-square test revealed a statistically significant relationship $( \chi ^ { 2 } = 7 . 5 0$ , $d f = 2$ , $P = . 0 2 4 ,$ ) between L1 background and cluster membership for the news topic, but did not reveal a significant relationship between L1 background and cluster membership for the teaching topic $( \chi ^ { 2 } = 6 . 6 8 4$ , $d f = 4$ , $P = . 1 5 4 ,$ ). Regarding L1, the patterns seen in Table 3 indicate that, overall, Arabic speakers are relatively underrepresented in Cluster 1, Spanish speakers in Cluster 2, and Chinese speakers in Cluster 3. Regarding topic, Table 3 shows that the texts belonging to Cluster 3 were written exclusively on the topic of teaching. Taken together, these patterns suggest that some profiles of highly rated compositions may be affected by characteristics of the writers’ backgrounds or of the topics they write about, or both.

The profiles of each cluster in terms of their mean $Z$ scores per linguistic feature are shown in Figs. 6–8. As before, the $Z$ scores were calculated from the entire data set of 178 texts in order to show whether the patterns exhibited by the three clusters of well-written texts are consistently above or below average. There are in fact four linguistic features where all three clusters show above-average levels: text length (Feature 1), diversity of vocubulary (Feature 3), downtoners (Feature 9), and adverbials (Feature 13). Additionally, all three clusters show below-average levels with respect to hedges (Feature 5), though the mean Z scores here are only barely below 0 for all three groups.

Table 3 Make-up of the three clusters of highly rated texts in Data Set 2   

<html><body><table><tr><td>Topic</td><td>L1</td><td>Cluster 1</td><td>Cluster 2</td><td>Cluster 3</td><td>Total</td></tr><tr><td>News</td><td>Arabic</td><td>5</td><td>5</td><td>0</td><td>10</td></tr><tr><td></td><td>Chinese</td><td>5</td><td>5</td><td>0</td><td>10</td></tr><tr><td></td><td> Spanish</td><td>10</td><td>0</td><td>0</td><td>10</td></tr><tr><td>Teaching</td><td>Arabic</td><td>5</td><td>3</td><td>1</td><td>9</td></tr><tr><td></td><td>Chinese</td><td>9</td><td>1</td><td>0</td><td>10</td></tr><tr><td></td><td>Spanish</td><td>5</td><td>1</td><td>3</td><td>9</td></tr><tr><td>Total</td><td></td><td>39</td><td>15</td><td>4</td><td>58</td></tr></table></body></html>

![](img/371c4e1abc0c3f4e1bb3910be8b172bc2a80833ec6ce174fa4fb5ea718a792a8.jpg)  
Fig. 6. Cluster 1 profile (Data Set 2)

![](img/97bd13f501602b9c326c0d76272972827bb334f7b32837f186548e6459ee7b7f.jpg)  
Fig. 7. Cluster 2 profile (Data Set 2)

![](img/7b2b13c7e00299c0d619a5d2e29fb08e278be92115724b60648e50a14cf69418.jpg)  
Fig. 8. Cluster 3 profile (Data Set 2)

Table 4 Profiles of the three clusters in Data Set 2 in terms of high and low use of features   

<html><body><table><tr><td>Features</td><td>Cluster 1</td><td>Cluster 2</td><td>Cluster 3</td></tr><tr><td>1. Text length</td><td>+</td><td>+</td><td>+</td></tr><tr><td>2. Mean word length.</td><td></td><td>+</td><td></td></tr><tr><td>3. Diversity of vocabulary.</td><td></td><td>+</td><td>+</td></tr><tr><td>4. Conjuncts</td><td>+</td><td></td><td></td></tr><tr><td>5. Hedges</td><td></td><td></td><td></td></tr><tr><td>6. Amplifiers</td><td></td><td></td><td></td></tr><tr><td>7. Emphatics</td><td></td><td></td><td>+</td></tr><tr><td>8. Demonstratives</td><td></td><td></td><td>+</td></tr><tr><td>9. Downtoners</td><td></td><td></td><td></td></tr><tr><td>10. Nouns/nominalizations</td><td></td><td>+</td><td></td></tr><tr><td>11. First/second person pronouns</td><td></td><td></td><td></td></tr><tr><td>12. Impersonal pronoun *it&quot;</td><td></td><td></td><td></td></tr><tr><td>13. Adverbials</td><td></td><td></td><td></td></tr><tr><td>14. Prepositions</td><td></td><td>+</td><td></td></tr><tr><td>15. Definite articles</td><td></td><td></td><td></td></tr><tr><td>16. Present tense verbs</td><td></td><td></td><td></td></tr><tr><td>17. Stative verb &quot;be&quot;</td><td></td><td></td><td>+</td></tr><tr><td>18. Passives</td><td></td><td></td><td></td></tr><tr><td>19. Adverbial subordination</td><td></td><td></td><td></td></tr><tr><td>20. Relative clauses</td><td></td><td></td><td></td></tr><tr><td>21. Complementation</td><td></td><td></td><td>+</td></tr></table></body></html>

The distinguishing characteristics of the three cluster profiles in terms of their noteworthy high $\left( Z > . 5 \right)$ and low $( Z < - . 5 )$ use of the various linguistic features are summarized in Table 4. Here, one can see clear differences in the clusters’ profiles. There is some overlap among clusters with respect to Features 1 and 3, but the clusters do not share any other distinguishing characteristics; in fact, Clusters 2 and 3 show precisely the opposite tendencies with respect to Features 10, 14, and 21. A series of one-way ANOVAs additionally revealed significant differences across clusters for the following seven linguistic features:

mean word length (Feature 2) $( F = 6 . 0 8 5$ , $d f = 2 / 5 5$ , $P = . 0 0 4 _ { }$ ), nouns and nominalizations (Feature 10) $( F = 7 5 . 2 2 2$ , $d f = 2 / 5 5$ , $P < . 0 0 1$ ), impersonal pronoun it (Feature 12) $( F = 5 . 4 0 5$ , $d f = 2 / 5 5$ , $P = . 0 0 7 $ ), $\bullet$ prepositions (Feature 14) $( F = 1 2 . 7 1 0$ , $d f = 2 / 5 5$ , $P < . 0 0 1$ ), present tense verbs (Feature 16) $( F = 5 . 3 1 1$ , $d f = 2 / 5 5$ , $P = . 0 0 8$ ),

adverbial subordination (Feature 19) $\langle F = 3 . 7 7 9$ , $d f = 2 / 5 5$ , $P = . 0 2 9$ ), and complementation (Feature 21) $( F = 5 . 5 7 1$ , $d f = 2 / 5 5$ , $P = . 0 0 6$ ).

A series of Kruskal–Wallis tests confirmed these results.

# Discussion and conclusions

The results of the present study suggest that the quality of a written text may depend less on the use of individual linguistic features than on how these features are used in tandem. Inasmuch as this is a valid interpretation of our results, it provides a useful explanation for how it is possible to find multiple profiles of quality texts that have been written in the same register, on the same topic, and in the same context. The explanation for this phenomenon — or our interpretation of it — has two dimensions: complementarity and compensation. Complementarity refers to the fact that, although there may indeed be a number of linguistic features that contribute to the overall quality of a written text, high levels of some features may bring about low levels of other features. For example, in Data Set 1 we found that Clusters 2 and 3 exhibited a high use of nouns accompanied by a low use of pronouns, and Clusters 1 and 4 showed the opposite pattern. Whereas the appropriate use of both nouns and pronouns can contribute to the quality of a written text (see Connor, 1990; Ferris, 1994; Frase et al., 1999; Grabe & Kaplan, 1996; Grant & Ginther, 2000), these two features occur largely in complementary distribution, meaning that it is unlikely to find a high use of both features in the same text. Similar patterns can be found in Data Set 2, and together these patterns indicate that there are important constraints on the degree to which different linguistic features can co-occur in a text.

Compensation is the second dimension of the multiple-profiles phenomenon. It refers to the idea that successful writers may be able to compensate for potential deficiencies in their writing by capitalizing on a few of their strengths. For example, one of the apparent deficiencies in the texts written by the writers who belong to Cluster 2 in Data Set 2 is that they used relatively little clausal embedding in any of the following areas: adverbial subordination (Feature 19), relativization (Feature 20), and complementation (Feature 21). However, they seemed to compensate quite well for this deficiency by writing relatively long texts (Feature 1) that used a wide range of vocabulary (Feature 3) and an exceptionally high amount of spatial, temporal, and cause–effect descriptiveness through the use of prepositional phrases (Features 10 and 14). This suggests, again, that there is no single formula for good writing. Instead, writers generating texts that receive high scores have a number of linguistic options for impressing their readers. That is, readers are not necessarily looking for the presence of a particular set of structures in order to rate texts highly.

This does not, however, mean that there are no constraints on the linguistic features that will be found in a highly rated composition. The most consistent pattern that can be seen in our results is that all clusters of highly rated texts in both of our data sets exhibit longer than average text length. Even though highly rated texts obviously can be short, and lower rated texts can be long, such cases are extremely rare in our data. We do not know whether being a good writer makes one write more, whether writing more makes one a better writer, or whether raters are simply biased towards longer texts, but we do know that written texts that are rated highly are nearly always relatively long. Text length therefore appears to be a rather consistent predictor of perceived writing quality, as discussed earlier (Carlson et al., 1985; Ferris, 1994; Frase et al., 1999; Grant & Ginther, 2000; Jarvis, 2002a; Linnarud, 1986; Reid, 1986, 1990).

Similar to predictors, there also appear to be constraints on the differences that can exist between different profiles of highly rated texts. In both of our data sets, we found significant differences across clusters of highly rated texts for seven linguistic features. The seven linguistic features were not the same for both data sets, but four of them were the same: mean word length (Feature 2), nouns and nominalizations (Feature 10), prepositions (Feature 14), and present tense verbs (Feature 16). The inverse of this finding is that we found no significant differences across clusters in either of the data sets with respect to the following eleven linguistic features: text length (Feature 1), diversity of vocabulary (Feature 3), conjuncts (Feature 4), hedges (Feature 5), amplifiers (Feature 6), emphatics (Feature 7), demonstratives (Feature 8), downtoners (Feature 9), definite articles (Feature 15), passives (Feature 18), and relative clauses (Feature 20). Although this latter finding is in need of replication, our tentative interpretation is that (at least some of) these eleven features may represent constraints on the ways in which highly rated texts may vary. That is, from this study, we see that different profiles of highly rated texts can differ from one another in relation to features such as mean word length, nouns and nominalizations, prepositions, and present tense verbs, even though previous studies indicate that mean word length, nouns and nominalizations, prepositions, and present tense verbs are quite often consistently found in highly rated learner compositions (e.g., Frase et al., 1999; Grant & Ginther, 2000; Reid, 1986, 1990). Other features, such as text length, lexical diversity, conjuncts, and so forth, may vary less across highly rated timed writings.

The constraints on the ways in which highly rated texts may vary — if there are any — are presumably even more rigid the more similar the texts are in terms of task, topic, context of writing, and background of the writers. As stated earlier, there were eleven linguistic features for which no significant differences were found in either of the data sets, but when looking at either individual data set — in which task, topic, context of writing, and background of the writers were either held constant or experimentally balanced — one finds 14 linguistic features for which significant differences are not found across clusters of highly rated texts. Again, this confirms what one would naturally assume: that the constraints on quality ratings are more rigid (more numerous) for specific writing tasks than they are for more general types of writing. It also suggests that a high rating on a given composition task may require a rather specific level of use of certain linguistic features, even while allowing texts to vary more or less freely with respect to other features. This interpretation has important, if still somewhat vague, implications concerning the nature of quality writing. We hope that future research will clarify these issues.

Another area in which future research is needed concerns the factors that determine a composition’s profile. Another way of saying this is that we need to learn more about whether factors such as task, topic, context of writing, L1 of the writer, general proficiency level of the writer, and so forth, are predictive of the profile (or cluster) membership of a highly rated text. The factors that we considered in the present study were topic and L1 background. Concerning topic, in Data Set 2 we found some trends that indicated that topic had an effect on cluster membership. The most convincing trend was that Cluster 3 in this data set included texts written on only one topic: teaching. We acknowledge, however, that our statistical tests provided ambiguous evidence concerning the effects of topic. The evidence for the effects of L1 background was also ambiguous in that our chi-square tests revealed a statistically significant relationship between L1 background and cluster membership only in relation to the essays that were written on the news topic in Data Set 2. No significant effects for L1 background were found in Data Set 1. Regardless of the ambiguous results in this study, we know that topic, L1 background, and several other factors can affect a person’s language production, and we hope that future research will clarify any effects that such factors may have on a composition’s quality rating.

Finally, we acknowledge the potential limitations related to the way we conducted this study. These include problems related to the use of computer tagging programs on learner texts, as well as the limitations of characterizing writing proficiency in terms of only 21 linguistic features. Additionally, we acknowledge that our chosen analytical approach, cluster analysis, involves a certain level of subjectivity in decisions about which variables to include, which method of clustering to employ, and the number of clusters to choose as the optimal solution. Accordingly, we recognize that a reanalysis of our data could result in somewhat different results, even though they would likely lead to the same or similar conclusions about the relationship between linguistic features of timed compositions and their quality ratings. In any event, although we do hope that other researchers will replicate our research design in order to corroborate our findings, we also encourage researchers in the future to explore alternative methods that address the potential limitations of our study. Whatever research design one chooses, though, we are convinced that the multiple-profiles approach is the way to go.

# References

Biber, D. (1986). Spoken and written textual dimensions in English: Resolving the contradictory findings. Language, 62, 384–411.

Biber, D. (1988). Variation across speech and writing. Cambridge: Cambridge University Press.   
Biber, D., Conrad, S., & Reppen, R. (1998). Corpus linguistics: Investigating language structure and use. Cambridge: Cambridge University Press.   
Biesenbach-Lucas, S., Meloni, C., & Weasenforth, D. (2000). Use of cohesive features in ESL students’ e-mail and word-processed texts: A comparative study. Computer Assisted Language Learning, 13, 221–237.   
Burrough-Boenisch, J. (2003). Examining present tense conventions in scientific writing in the light of reader reactions to three Dutch-authored discussions. English for Specific Purposes, 22, 5–24.   
Carlson, S., Bridgeman, B., Camp, R., & Waanders, J. (1985). Relationship of admission test scores to writing performance of native and non-native speakers of English (TOEFL Research Rep. No. 19). Princeton, NJ: Educational Testing Service.   
Chafe, W. (1985). Linguistic differences produced by differences between speaking and writing. In D. Olson, N. Torrance, & A. Hildyard (Eds.), Literacy, language, and learning: The nature and consequences of reading and writing (pp. 105–123). Cambridge: Cambridge University Press.   
Cheng, X., & Steffensen, M. (1996). Metadiscourse: A technique for improving student writing. Research in the Teaching of English, 30, 149–181.   
Connor, U. (1990). Linguistic/rhetorical measures for international persuasive students writing. Research in the Teaching of English, 24, 67–87.   
Crismore, A. (1990). Metadiscourse and discourse processes: Interactions and issues. Discourse Processes, 13, 191–205.   
Crismore, A., Markkanen, R., & Steffensen, M. (1993). Metadiscourse in persuasive writing: A study of texts written by American and Finnish university students. Written Communication, 10, 39–71.   
Ellis, R. (1994). The study of second language acquisition. Oxford: Oxford University Press.   
Ferris, D. (1991). Syntactic and lexical characteristics of ESL student writing: A multidimensional study. Unpublished doctoral dissertation, University of Southern California, Los Angeles.   
Ferris, D. (1994). Lexical and syntactic features of ESL writing by students at different levels of L2 proficiency. TESOL Quarterly, 28, 414–420.   
Foster, P., & Skehan, P. (1996). The influence of planning and task type on second language performance. Studies in Second Language Acquisition, 18, 299–323.   
Frase, L., Faletti, J., Ginther, A., & Grant, L. (1999). Computer analysis of the TOEFL Test of Written English (TOEFL Research Rep. No. 64). Princeton, NJ: Educational Testing Service.   
Gonzalez-Bueno, M., & Perez, L. (2000). Electronic mail in foreign language writing: A study of grammatical and lexical accuracy, and quantity of language. Foreign Language Annals, 33, 189–198.   
Grabe, W., & Kaplan, R. (1996). Theory and practice of writing. London: Longman.   
Grant, L., & Ginther, A. (2000). Using computer-tagged linguistic features to describe L2 writing differences. Journal of Second Language Writing, 9, 123–145.   
Hewings, M., & Hewings, A. (2002). ‘‘It is interesting to note that . . .’’: A comparative study of anticipatory ‘it’ in student and published writing. English for Specific Purposes, 21, 367–383.   
Hinkel, E. (2001). Matters of cohesion in L2 academic texts. Applied Language Learning, 12, 111–132.   
Hyland, K. (1996a). Writing without conviction? Hedges in science research articles. Applied Linguistics, 17, 433–454.   
Hyland, K. (1996b). Hedges, boosters and lexical invisibility: Noticing modifiers in academic texts. Language Awareness, 9, 179–197.   
Hyland, K. (2002). Options of identity in academic writing. ELT Journal, 56, 351–358.   
Intaraprawat, P., & Steffensen, M. (1995). The use of metadiscourse in good and poor ESL essays. Journal of Second Language Writing, 4, 253–272.   
Jarvis, S. (2002a). Short texts, best-fitting curves and new measures of lexical diversity. Language Testing, 19, 57–84.   
Jarvis, S. (2002b). Topic continuity in L2 English article use. Studies in Second Language Acquisition, 24, 387–418.   
Jin, W. (2001). A quantitative study of cohesion in Chinese graduate students’ writing: Variations across genres and proficiency levels (ERIC Document Reproduction Service No. ED 452 726).   
Kaplan, R., & Grabe, W. (2002). A modern history of written discourse analysis. Journal of Second Language Writing, 11, 191–223.   
Li, Y. (2000). Linguistic characteristics of ESL writing in task-based e-mail activities. System, 28, 229–245.   
Linnarud, M. (1986). Lexis in composition: A performance analysis of Swedish learners’ written English. Malmo¨, Sweden: CWK Gleerup.   
Longo, B. (1994). Current research in technical communication: The role of metadiscourse in persuasion. Technical Communication, 41, 348–352.   
Master, P. (1987). Generic ‘‘the’’ in Scientific American. English for Specific Purposes, 6, 165–186.   
Master, P. (1990). Teaching the English articles as a binary system. TESOL Quarterly, 24, 461–478.   
Master, P. (1996). Systems of English Grammar: An introduction for language teachers. Upper Saddle River, NJ: Prentice Hall Regents.   
Master, P. (1997). The English article system: Acquisition, function, and pedagogy. System, 25, 215–232.   
McClure, E., Mir, M., & Cadierno, T. (1993). What do you include in a narrative? A comparison of the written narratives of Mexican and American fourth and ninth graders. Monograph Series, 4, 209–224.   
Oxford, R. (1990). Language learning strategies: What every teacher should know. NY: Newbury House/Harper & Row.   
Reid, J. (1986). Using the writer’s workbench in composition teaching and testing. In C. Stansfield (Ed.), Technology and language testing (pp. 167–188). Alexandria, VA: TESOL.   
Reid, J. (1990). Responding to different topic types: A quantitative analysis from a contrastive rhetoric perspective. In B. Kroll (Ed.), Second language writing: Research insights for the classroom (pp. 191–210). Cambridge: Cambridge University Press.   
Reid, J. (1992). A computer text analysis of four cohesion devices in English discourse by native and nonnative writers. Journal of Second Language Writing, 1, 79–107.   
Reppen, R. (1994). Variation in elementary student language: A multi-dimensional perspective. Unpublished doctoral dissertation, Northern Arizona University, Flagstaff.   
Rubin, J. (1975). What the ‘‘good language learner’’ can teach us. TESOL Quarterly, 9, 41–51.   
Shaw, P., & Liu, E. (1998). What develops in the development of second-language writing? Applied Linguistics, 19, 225–254.   
Skehan, P. (1986). Cluster analysis and the identification of learner types. In V. Cook (Ed.), Experimental approaches to second language learning (pp. 81–94). Oxford: Pergamon.   
TOEFL Test of Written English Guide. (1992). Princeton, NJ: Educational Testing Service.   
Toms, M. L., Cummings-Hill, M. A., Curry, D. G., & Cone, S. M. (2001, March). Using cluster analysis for deriving menu structures for automotive mobile multimedia applications. Paper presented at SAE 2001 World Congress, Detroit, Michigan.   
Vande Kopple, W. (1985). Some exploratory discourse on metadiscourse. College Composition and Communication, 36, 82–95.   
Yuen, H. (2000). A cluster-based approach for identifying East Asian economies: A foundation for monetary integration. Department of Economics Working Paper Series, Department of Economics, National University of Singapore.