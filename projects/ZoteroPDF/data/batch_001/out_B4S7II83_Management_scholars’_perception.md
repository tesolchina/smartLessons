# Management scholars’ perception of barriers and bridges to use crowdsourcing in science: qualitative research

Regina Lenart-Gansiniec

To cite this article: Regina Lenart-Gansiniec (20 Feb 2024): Management scholars’ perception of barriers and bridges to use crowdsourcing in science: qualitative research, Studies in Higher Education, DOI: 10.1080/03075079.2024.2312424

To link to this article: https://doi.org/10.1080/03075079.2024.2312424

# Management scholars’ perception of barriers and bridges to use crowdsourcing in science: qualitative research

Regina Lenart-Gansiniec $\textcircled{1}$

Jagiellonian University in Krakow, Krakow, Poland

# ABSTRACT

Crowdsourcing in science is one of the ways of conducting scientific research which responses to the postulates of the democratization, openness, and inclusiveness of science. Although much is known about crowdsourcing in science, less attention is paid to the barriers to the use of crowdsourcing in science and how to overcome them. The purpose of this research is to identify barriers to the use of crowdsourcing in science from the perspective of scholars across management who have little or no experience in organizing crowdsourcing in science initiatives. At the same time, we identify bridges that reduce those barriers. We collected the data using unstructured interviews with 40 junior and senior management scholars who represent different academic ranks. We identified sixteen barriers grouped into six categories: individual, data, knowledge, delegation, quality, and financial barriers. Barriers refer to lack of trust in the crowd, scholars’ reluctance to be open to new things, methodological preferences, concerns about data being shared, data theft, insufficient knowledge of scholars, insufficient crowd knowledge, difficulties in communications, the discomfort of delegating tasks to the crowd, the possibility of the crowd abandoning the tasks, additional work resulting from the need to control and verify the crowd’s work, a potential violation of methodological rigor, the feeling of wasting time, concerns about fees for crowdsourcing in science and the employee evaluation system. We also identified the following two bridges that help eliminate barriers and fosters the use of crowdsourcing in science: autonomy of scholars and university support.

# ARTICLE HISTORY

Received 6 June 2023   
Accepted 25 January 2024

# KEYWORDS

Crowdsourcing in science; higher education   
institutions; barriers; bridges; scholars

# Introduction

Crowdsourcing has been gathering increasing attention in recent years (Brem et al. 2023). The potential of crowdsourcing comes from unlimited access to many heterogeneous sources of knowledge, creating asynchronous, transnational teams consisting of professionals from different industries, scholars, and amateurs interested in performing a specific task (Moghaddam et al. 2023).

Research intensity in the field of crowdsourcing is accompanied by the development of potential possibilities of its application (Brem et al. 2023). It is postulated that crowdsourcing can be a valuable tool for supporting scientific research and creating scientific knowledge. Crowdsourcing provides some response to the challenges faced by researchers when it comes to the need for building interdisciplinary, international research teams, but also to increase the intensity of scientific productivity and transparency of scientific research (Franzoni, Poetz, and Sauermann 2022). In this work, the use of crowdsourcing by scholars is referred to as crowdsourcing in science and it refers to ‘a collaborative online process through which scientists involve a group of self-selected individuals of varying, diverse knowledge and skills, via an open call on the Internet and/or online platforms, to undertake a specified research task or set of tasks’ (Lenart-Gansiniec et al. 2022, 1).

Although previous publications contribute to the recognition and understanding of crowdsourcing in science (Beck et al. 2022; Bücheler and Sieg 2011), ‘the use of crowdsourcing in science is still being tested’ (Bassi et al. 2020, 302). It is increasingly being noticed that many factors may limit the use of crowdsourcing in science (Law et al. 2017; Schlagwein and Daneshgar 2014). However, the existing literature in the area of barriers to crowdsourcing in science focuses on specificities of research fields like STEM (science, technology, engineering, mathematics) and humanities. However, it is recommended that future research should ‘vary by disciplines and disciplinary features’ (Law et al. 2017, 1556), in particular management science (Beck et al. 2022).

The few previous studies on barriers of crowdsourcing in science took into account the perceptions of scholars who organized crowdsourcing in science initiatives (Law et al. 2017; Schlagwein and Daneshgar 2014). However, those barriers may vary among scholars who have little or no experience in this field (Law et al. 2017). Therefore, future researchers are encouraged to conduct research on the barriers to crowdsourcing by scholars, in particular among academics who have little or no experience in this field (Law et al. 2017). The previous findings of other researchers regarding the barriers to use for crowdsourcing in science did not take into account any professional profiles of the surveyed scholars. Literature indicates that while crowdsourcing in science may be useful for every scholar (Eklund, Stamm, and Katja Liebermann 2019), reaching for it may depend on ‘academic age’ meaning the number of years since obtaining a PhD degree (Beck et al. 2022).

Based on the population gap (Miles 2017), our research focuses on identifying barriers to the use of crowdsourcing in science from the perspective of scholars across management who have little or no experience in organizing crowdsourcing in science initiatives. At the same time, we identify the bridges to use crowdsourcing in science. Bridges are understood to be ways to eliminate barriers to the use of crowdsourcing in science (Mälkki and Lindblom-Ylänne 2011). In our research, we focus on the perceptions of scholars across management because it is related to their beliefs, which in turn influences their future intentions and decisions (Beck et al. 2022). With the above in mind, we ask the following research question (RQ):

RQ1. What are the barriers and bridges to the use of crowdsourcing in science among management scholars who have no or little experience of crowdsourcing in science?

Based on these research questions, we adopt an abductive approach (Vila-Henninger et al. 2022), according to which we compare barriers selected while doing literature research and enrich them with categories selected from the qualitative research. To the best of our knowledge, the literature has not provided bridges that can eliminate the barriers to crowdsourcing in science. Therefore, our bridges were identified based on the statements of our research.

Our research makes several contributions to the literature on crowdsourcing in science. Firstly, we respond to the need for a balanced view on what hampers and what fosters the use of crowdsourcing in science (Law et al. 2017). In this research, we base our efforts on the work of Law et al. (2017), and we precise and expand it. In our research, we take into account perceptions of management across scholars’ who have no or little experience in the use of crowdsourcing in science.

Secondly, our research takes into account the professional profiles of the surveyed scholars. Literature indicates that while crowdsourcing in science may be useful for every scholar (Eklund, Stamm, and Katja Liebermann 2019), but ‘age is inversely related to research (…) acceptance of new ideas, with older researchers tending to be less active, more (…) closed-minded’ (Beck et al. 2022, 149). Therefore, we include academic age of the respondents referring to the number of years since obtaining their PhD (Beck et al. 2022). This extends the findings of Law et al. (2017) who presented their findings without the possibility of checking the trends/diversity of research results taking into account age measured by the period since obtaining a PhD.

# Conceptual background

# Crowdsourcing in science

Crowdsourcing in science has become an essential and valuable part of the knowledge-creation landscape (Beck et al. 2022). So far, research in the field of crowdsourcing in science has focused on the functionality of crowdsourcing platforms (Schlagwein and Daneshgar 2014), the typology (Lenart-Gansiniec 2022), the conceptualization (Lenart-Gansiniec et al. 2022), the benefits for scholars, and the general public (Behrend et al. 2011), the characteristics of crowd, ways of motivating them, the quality of the results obtained thanks to their work, the possibility of their control and verification, and the profiling of crowdsourcing projects (Franzoni, Poetz, and Sauermann 2022). Only few studies address the barriers to crowdsourcing in science (Law et al. 2017; Schlagwein and Daneshgar 2014).

# Barriers of using crowdsourcing in science

Few studies show that one of the barriers to the use of crowdsourcing in science is the uselessness of existing crowdsourcing platforms for the specificity of creating scientific knowledge (Schlagwein and Daneshgar 2014). Others point to barriers related to process, data, knowledge, delegation, and quality (Law et al. 2017). With regard to process barriers, research workers are concerned about the mismatch between crowdsourcing in science and research tasks. Another limitation is the non-linear and iterative process of creating scientific knowledge. In this context, Law et al. (2017) indicate that the difficulty of using crowdsourcing in science results from the specificity of conducting scientific research. In the case of data barriers, attention is drawn to the risks associated with sharing raw data and research questions with crowds. Law et al. (2017) pointed out the risk of intellectual property theft, and the fear of unauthorized copying, processing, and using data without the initiator’s knowledge. The risk of excess or deficiency of data provided by the crowd is also important. In turn, barriers related to knowledge refer to the fact that crowd can be both people with specialized knowledge and research skills, but also people who come from outside the scientific community and who do not have sufficient expertise. In the case of barriers to delegation, Law et al. (2017) indicate the discomfort associated with scholars forwarding requests to the crowd to perform a research task. Attention was drawn to difficulties in communicating and interacting with anonymous people, as well as scholars’ concerns about crowd exploitation. Finally, barriers related to quality refers to concerns about the quality of the results of the work of the crowd, the threats resulting from the automation or inattentiveness work of crowd, and falsification of their identity as crowd (Law et al. 2017).

# Methodology

The purpose of this research is to identify barriers and bridges to use crowdsourcing in science from the perspective of scholars across management who have little or no experience in organizing a crowdsourcing in science initiative. Due to the desire to learn about opinions and beliefs and to capture the perception of crowdsourcing in science, qualitative research was conducted (Gioia, Corley, and Hamilton 2013). The qualitative research allows for in-depth insight into a given phenomenon, thanks to which it is possible to explain it and provide recommendations for exploring complex, new or relatively unexplored areas that need clarification and understanding (Lenart-Gansiniec and Chen 2023). Finally, the choice of a qualitative approach was driven by the desire to expand on previous qualitative research (Law et al. 2017). We used unstructured interview to make it possible to recognize the emerging observations of the interviewees, encourage the interviewees to make longer statements, and adjust the order of questions to the interviewer (Lindlof and Taylor 2016).

# Empirical settings and sample

In order to identify barriers and bridges to the use of crowdsourcing in science from the perspective of scholars across management, we decided on purposeful sampling (Palinkas et al. 2016). To maintain maximum heterogeneity (Suri 2011), we invited scholars who met the following criteria: experience in implementing research projects using computer-assisted interview using a website, type of universities, activity in social media, and diversity of gender, age, and their academic positions. In addition, we drew attention to the importance of academic age referring to the number of years since obtaining PhD. We conducted 40 interviews with management scholars who represent different academic ranks (assistant, assistant professor, associate professor, and full professor), which resulted from the desire to be saturated with data (Czakon, Hajdas, and Radomska 2023). Detailed characteristics of the research sample are presented in Table 1.

Regarding the characteristics of the management scholars participating in the interviews, the majority of them were women $( 7 0 \% )$ employed at public $( 8 5 \% )$ universities of economics $( 4 5 \% )$ , located in cities between 250,000 and 1,000,000 inhabitants. The overrepresentation of women in our sample reflects general trends among the structure of scholars (Pelletier et al. 2021).

# Data collection

Prior to the start of the actual research, one researcher developed a preliminary list of questions as part of the triangulation of researchers, and then two researchers verified the correctness and relevance of the developed questions. The actual research was conducted in the period from March to May 2022. The interviews consisted of general questions. Interlocutors were asked to describe their point of view on crowdsourcing in science. They were then asked to reflect on what encourages and stops them from using of crowdsourcing in science. The interviews were conducted with scholars who had little or no experience in organizing a crowdsourcing in science initiative, therefore, when the interviewee indicated that they had never heard of crowdsourcing in science, they were provided with the following explanation of what it was by the interviewer:

Please imagine a situation in which you have a data set or any other research task and you have the opportunity to submit a request for its implementation via a special online platform to some crowd. In response to this invitation, the crowd performs this work for you free of charge or for a fee. The crowd includes both other scholars and people from outside the scientific community.

Due to the restrictions on social interaction and travel caused by the COVID-19 pandemic, the research was conducted online and was conducted using the Microsoft Teams application (Medley-Rath 2019). Interviews lasted an average of 40 min: the longest was 1 h and 45 min, and the shortest lasted 20 min. Their length depended on the openness of the interviewees. All the collected material was transcribed. Transcription was naturalized, which meant writing down statements in a pure form, i.e. without any necessary corrections. In total, the interview transcripts amounted to 427 pages. To maintain anonymity, the interviewees were marked with the letter R (R1, RF2 …).

Table 1. Characteristics of interviewees.   

<html><body><table><tr><td colspan="2">Characteristics</td><td>Frequency</td><td>Percentage</td></tr><tr><td rowspan="2">Sex</td><td>Female</td><td>28</td><td>70%</td></tr><tr><td>Male</td><td>12</td><td>30%</td></tr><tr><td rowspan="4">Academic rank</td><td>Assistant</td><td>10</td><td>25%</td></tr><tr><td>Assistant professor</td><td>10</td><td>25%</td></tr><tr><td>Associate professor</td><td>10</td><td>25%</td></tr><tr><td>Full professor</td><td>10</td><td>25%</td></tr><tr><td rowspan="3">Type of university.</td><td>University of economics</td><td>18</td><td>45%</td></tr><tr><td> University</td><td>17</td><td>42,5%</td></tr><tr><td> University of technology</td><td>5.</td><td>12,5%</td></tr><tr><td rowspan="2">University status</td><td> Public</td><td>34</td><td>85%</td></tr><tr><td>Private</td><td>6</td><td>15%</td></tr><tr><td rowspan="4">University location</td><td>&lt;50,000 inhabitants</td><td>0</td><td>0</td></tr><tr><td>Between 50,000 and 250,000 inhabitants</td><td>3</td><td>7,5%</td></tr><tr><td>Between 250,000 and 1,000,000 inhabitants</td><td>28</td><td>70%</td></tr><tr><td>&gt;1,000,000 inhabitants</td><td>9</td><td>22,5%</td></tr></table></body></html>

$N = 4 0$

# Data analysis

The transcribed data were coded using the abductive approach (Bouncken, Qiu, and García 2021). Firstly, master codes derived from the literature on crowdsourcing in science (deductive coding) were created. Then, the deductive coding phase was followed by the inductive coding phase. The raw data from the unstructured interview were systematically analyzed using a thematic analysis (Braun and Clarke 2006). Following the guidelines for thematic analysis, reading row by row, the raw data were broken down into smaller batches where the master codes were searched. For this purpose, an initial list of codes was developed for further review and correction. Then, the superior codes deduced from the literature and conducted research were grouped according to similarities. Then, those topics were analyzed and compared with the existing literature in the field of crowdsourcing in science. This led to the identification of topics regarding barriers to crowdsourcing in science that were indicated by the participants of the research. For the purpose of reporting the obtained results, quotes dominant in the transcripts were selected. Following the credibility criteria, much care was taken to describe the research procedure in detail, including the description of data collection and analysis (Saldaña 2009). NVivo software (version 1.7) was used to organize the collected empirical material due to its functionality and intuitive operation. For the purposes of reporting the conducted research, we selected the quotes that dominate in the transcript (Davidson 2009).

# Findings

# Barriers of using crowdsourcing in science in the group of management scholars

According to Law et al. (2017), we divided the identified barriers into those related to data, knowledge, delegation, and quality. Our findings did not indicate process-related barriers (Law et al. 2017). We also did not identify barriers related to platforms (Schlagwein and Daneshgar 2014). Additionally, we identified individual and financial barriers. When presenting our findings, we took into account the academic age of the management scholars recommended in the literature, referring to the number of years since obtaining PhD (Beck et al. 2022), where ‘junior scholars’ obtained their PhD up to 7 years ago, and ‘senior scholars’ obtained their PhD more than 7 years ago (Figure 1).

# Individual barriers

Both junior and senior scholars paid attention to individual barriers. In addition, junior scholars raised the issue of trust to crowd: I would not trust outsiders (R2). Senior scholars also point to the issue of distrust: it is different for me to hand over a dataset to someone I know (…) I would have great concerns (…) (R37). Additionally, senior scholars pointed to the issue of less openness to new things: a certain generational distance (…) towards technological solutions (…) younger people are more open to novelties (R30). A methodological preference also plays a major role in understanding the reluctance to use of crowdsourcing in science: I like to encode data myself (…) I know that this is the greatest wealth of knowledge (R26). This fits in with the reluctance to experiment, signaled by one of the senior scholars.

# Data related barriers

Data-related barriers were only mentioned by junior scholars. Most of the junior scholars paid attention to the issue of data sharing and related risks: I am afraid of externalizing personal data (…) I am

![](img/c067e93900750443cec885b75e2e742f4d946cb6a2abaae690b51bca901c01d3.jpg)  
Figure 1. Barriers of using crowdsourcing in science.

afraid that (…) legal problems will arise (R5). Such a situation, according to junior scholars, may increase the threat and the risk of data theft. As one of the interviewees emphasized: (…) someone (…) appropriates this data and then publishes the results of these research himself (R1).

# Knowledge related barriers

Barriers related to knowledge were indicated only by junior scholars. In particular, there were concerns related to the form of insufficient knowledge about the organization of a crowdsourcing in science initiative: it is difficult (…) you must think about it and know (…) how to use it, (…) who (…) we want to examine properly (R20). Additionally, attention was paid to the risk of insufficient crowd knowledge, which can make it difficult to obtain valuable solutions: I am not sure that someone on the Internet has such knowledge (R3).

# Delegation related barriers

Barriers related to delegation were indicated by both junior and senior scholars. Junior scholars in particular pointed out the difficulties in communication with crowd: communication barrier (…) cooperation mainly via the Internet (…) with a person you do not know (R1). The issue of the potential difficulty of understanding the scientific language by crowd was also emphasized: Internet users may not understand scientific language (R18). Moreover, the quoted senior scholars also point to the discomfort associated with the work being done by others: I would not feel comfortable that it was simply not my work (R36).

# Quality related barriers

Both junior and senior scholars indicated quality barriers. In particular, junior scholars also point to their fears that crowd may potentially abandon tasks halfway through: the basic limitation is that (…) those people may not complete their tasks, because they will say that it is too requiring, too difficult (R6). Junior scholars recognize that crowdsourcing in science can intensify and generate additional work in the field of control and verification that will be imposed on scholars: this would include the question of control and whether it is profitable to ask for help, since everything must be verified and controlled later anyway (R2). In turn, junior scholars point to the feeling of wasting time in a situation when they obtain solutions that are inconsistent with their expectations: the problem arises when we start working with someone and that person delivers something (…) and we waste time on completing the task or looking for new people from the crowd (R2). Another junior scholar similarly pointed to the barriers related to the methodological rigor: Internet users do not know the researchers’ tools (…) which is not consistent with principles, research ethics and what methodological rigor should look like (…) (R6). This is also pointed out by senior scholars: the basis is the rigor of the results obtained. This is crucial for me (R34).

# Financial barriers

Both junior and senior scholars drew attention to financial barriers. When discussing barriers, junior scholars pointed to concerns about fees for organizing a crowdsourcing in science initiative on the platform: the challenge (…) are finances (…) you must pay for it out of your own pocket (R20). Importantly, senior scholars pointed out the barrier related to including crowdsourcing in science in the mandatory employee evaluation system: if [crowdsourcing in science] takes form of coercion and is included in the employee evaluation system (…) then it is not encouraging at all (R32).

# Bridges to use of crowdsourcing in science

Based on the statements of junior and senior scholars, we identified the following two bridges that could eliminate barriers to the use of crowdsourcing in science: autonomy of scholars and university support. The proposed bridges are symmetrical to the barriers indicated by the respondents (Figure 2).

One of the bridges suggested by both junior and senior scholars refers to autonomy. This may refer to overcoming individual barriers related to methodological preferences, less openness to new things and trust in crowds. One of the junior scholars emphasizes that: the university and science and scholars are independent and should be autonomous in their decisions (…) whether I do research with this or with that (…) it should be my decision (R4). Senior scholars also drew attention to the issue of researcher autonomy: if it was mandatory or (…) financially rewarded, then (…) I would be afraid (…) that [crowdsourcing in science] would be used in an inappropriate way (R36).

Scholars’ autonomy was also indicated as a bridge to financial barriers. In this sense, regulations or orders regarding the use of crowdsourcing in science may intensify the feeling that it is another requirement that must be met by research workers, as pointed out by one of the junior scholars: this should be some expression of the researchers’ maturity and their internal motivations (…) this is such a natural evolution of a scientist that they want to confront their ideas with an ever-wider group (…) (R13).

The second bridge indicated only by junior scholars refers university support. It is a way to overcome financial barriers, as well as those related to data, knowledge, delegation, and quality. This support may take the form of optional training or workshops, thanks to which scholars will acquire knowledge and skills in the field of organizing crowdsourcing in science initiatives: if I knew how to make Internet users perform my tasks, maybe it would be easier for me to decide (R19). Moreover, according to one of the junior scholars, it may be important to show crowdsourcing in science initiatives that have been successful: it is important to show examples of what was solved this way and how it was done (R26). However, all forms of support should inspire and encourage rather than present crowdsourcing in science as obligatory: because my observation shows that this support is often perceived as some sort of coercion (R20).

![](img/60f8953c29701c0ede8bbadd37d311c3dec3b68e7bf6a8d497b7ef9712054c76.jpg)  
Figure 2. Bridges to barriers of use crowdsourcing in science.

# Discussion and conclusion

In this research, we identified barriers and bridges to the use of crowdsourcing in science from the perspective of scholars across management who have no or little experience in organizing crowdsourcing in science initiatives. In this way, we respond to the challenge for further research on barriers to the use of crowdsourcing in science (Law et al. 2017). Additionally, in the presented empirical material we included academic age, which is also recommended in the literature (Beck et al. 2022).

Firstly, our findings identified individual barriers that have not previously been addressed in the literature. Within those barriers, both junior and senior scholars indicated limited trust in crowds. These results are consistent with concerns about the quality of data obtained by scholars from non-citizen experts (Santos-Fernandez et al. 2023). The interviews also revealed the low openness of scholars to new things and the importance of methodological preferences, which was noticed only by senior scholars. The findings regarding openness to new things are not surprising because the literature shows that senior scholars are more skeptical about involving the public in the implementation of research tasks (Beck et al. 2022) and various alternative ways of creating scientific knowledge. Moreover, in the case of methodological preferences, this is not surprising again because such scholars have more experience in the field of scientific cooperation and can evaluate crowdsourcing in science through the prism of such experiences and negative results of such cooperation.

Secondly, our research revealed data-related barriers. According to Law et al. (2017), we determined that they included the fear of sharing research data with people they do not know. However, only junior scholars pointed to that issue. The obtained results are not surprising, because junior scholars are more cautious in sharing research data and show greater concerns about intellectual property than in the case of senior scholars (Thoegersen and Borlund 2022). In addition, this research allowed to identify another barrier, indicated only by junior scholars: the fear of data theft, which is associated with fears about data sharing (Law et al. 2017). Within data barriers, previous research additionally highlighted scholars’ concerns about excess or insufficient data that crowds may provide. Our findings do not support this. This may be due to the fact that most respondents have no knowledge about crowdsourcing in science.

Thirdly, within the barriers related to knowledge, we identified scholars’ concerns about insufficient crowd knowledge, which is consistent with the findings of Law et al. (2017). Additionally, our research indicates concerns about the perceived insufficient knowledge of scholars about the organization of a crowdsourcing in science initiative. However, only junior scholars indicated barriers related to knowledge. This is an important finding and explains why crowdsourcing can generally be treated as a threat and why it is so underexploited. Additionally, a person’s risk taking, despite perceived ignorance, increases with the person’s experience.

Fourthly, our findings highlight barriers related to delegation. However, there are different indications for junior and senior scholars. More specifically, junior scholars drew attention to the issue of difficulties in communicating with people they do not know and the potential difficulty of understanding the scientific language by crowd, which is consistent with previous research (Law et al. 2017). Lack of communication between scholars and members of the crowd increases the risk of difficulties in accessing the potential of Internet users and abandonment of the task. In addition, there may be a situation when an Internet user wants to obtain information about a problem concerning a current initiative that bothers them. Failure to respond may result in discouragement and abandonment of the task. Our research shows that in the case of senior scholars, the limitation to the use of crowdsourcing in science refers to the discomfort associated with the work being done by others. These findings are consistent with Law et al. (2017). According to previous research, scholars’ fear of crowd exploitation may constitute a barrier to the use of crowdsourcing in science (Law et al. 2017). Our research did not confirm that, which can also be explained by ignorance about the specifics of crowdsourcing in science initiatives.

Fifthly, our research identified quality-related barriers. They were mentioned by both junior and senior scholars. However, junior scholars drew attention to the risk of crowd abandonment of tasks during implementation, additional work in the field of control and verification of crowd work (Law et al. 2017). Our findings reinforce the existing literature (Law et al. 2017) by adding a limitation related to the feeling of wasting time when scholars obtain solutions that do not meet their expectations. Only senior scholars pointed to that. This is not surprising and confirms the existing literature findings. As emphasized by Kyvik and Olsen (2008), senior scholars have less time to use modern research solutions because they are time-consuming. In addition, the use of crowdsourcing in science required making some specific effort, including the coordination of a very large group of people or the verification of the results obtained. Additionally, our research identified an additional factor that was important to both junior and senior scholars: concerns about violating the methodological rigor. Furthermore, our findings do not support the findings of Law et al. (2017), who pointed to scholars’ concerns about the falsification of their identity as a crowd. Those findings are not surprising and may result from the lack of knowledge about crowdsourcing in science (Cuccolo et al. 2021).

Finally, the interviews revealed financial barriers that had not been previously reported in the literature. Those include concerns about the need to pay fees for organizing a crowdsourcing in science initiative. That was especially noticed by junior scholars. The interviews also revealed another factor that was important for senior scholars: the inclusion of crowdsourcing in science in the mandatory employee evaluation system. This is not surprising, because the literature indicates that organizational incentive systems do not determine the intention to use of crowdsourcing in science (Beck et al. 2022). This allows us to conclude that the inclusion of crowdsourcing in science in the system of evaluation of the work of academics may be a limitation to be faced while using for crowdsourcing in science. Too much pressure or official directives may turn out to be a factor that will further increase the reluctance of scientific workers to employ crowdsourcing in science. This approach results from the autonomy of scholars related to the freedom to decide on research topics, their goals, methods, and ways of their implementation without external influences (Niemczyk and Rónay 2022).

We have identified two bridges that can reduce the barriers to the use of crowdsourcing in science. One of them is university support in raising the knowledge of scholars in the field of crowdsourcing in science, which was also raised in previous studies (Schlagwein and Daneshgar 2014). It is very important that a scholars play the most important role in crowdsourcing in science (Law et al. 2017; Schlagwein and Daneshgar 2014), because the success depends on their commitment, motivation, attitude, perception, and belief. Our respondents claimed that signaling support should involve optional training for employees interested in crowdsourcing in science, which was also pointed out by previous studies (Beck et al. 2022).

# Implications for theory of crowdsourcing in science

Our research contributes to the existing literature in crowdsourcing in science (Law et al. 2017) by identifying barriers and bridges that may mitigate the use of crowdsourcing in science from the perspective of scholars across management who have little or no experience in organizing scientific initiatives crowdsourcing.

Firstly, our findings specify barriers and bridges to use of crowdsourcing in science and provide new relevant details and insights (Law et al. 2017; Schlagwein and Daneshgar 2014). Earlier studies were conducted among employees with experience in organizing an initiative involving crowdsourcing in science. Our research took into account the perspective of academics with little or no experience in this field, which is a response to the challenge and an invitation to further scientific exploration of the barriers to the use of crowdsourcing in science (Law et al. 2017). Knowledge obtained in that very context is valuable because it precisely refers to the assessment of a person’s action undertaken before making a decision that may ultimately influence future behavior. Therefore, if crowdsourcing in science is assessed by researchers as something constituting a limitation or difficulty, there is a high probability that negative emotions will arise, which will make the person want to avoid a stressful situation. As a result, crowdsourcing in science might not be considered at all.

Secondly, our findings provide a new perspective on the barriers and bridges to the use of crowdsourcing in science from the perspective of management scholars. Previous research was conducted among scholars representing several ranges of academic disciplines including social sciences, humanities, and science (Law et al. 2017). Each of the mentioned fields of scientific disciplines proposes a different approach to crowdsourcing in science, which is consistent with its dominating paradigm. Moreover, the specificity of crowdsourcing in science depends on the type of scientific field/discipline (Beck et al. 2022). We agree with the recommendations of Beck et al. (2022) that research on crowdsourcing in science should take into account the perspective of management scholars.

Thirdly, our important discovery refers to the generational gap in the use of crowdsourcing in science. Using of crowdsourcing in science may depend on the academic age referring to the number of years since obtaining PhD (Beck et al. 2022). Different barriers were indicated by junior and senior scholars. Only in the case of two barriers, their indications are similar: limited trust in other people (individual barrier) and fear of violating methodological rigor (quality related barrier).

Finally, the conducted research allowed for the expansion and development of knowledge about crowdsourcing in science from the micro perspective, i.e. in the context of the initiator of crowdsourcing in science. Research in this area is recommended to be one of the most important research areas in the upcoming years (Beck et al. 2022). It is the initiator, i.e. the scholar, who plays the most important role (Law et al. 2017; Schlagwein and Daneshgar 2014), because their decisions, attitudes, commitment, motivation, perception, and beliefs influence the potential and usefulness of crowdsourcing in science, which translates into importance of crowdsourcing in science for the creation of scientific knowledge.

# Managerial implications

Our research provides recommendations for managers of higher education institutions that help to eliminate barriers to the use of crowdsourcing in science, and supporting scholars interested in using crowdsourcing in science to create scientific knowledge. Those recommendations relate to the issue of raising the knowledge of research workers in the field of crowdsourcing in science and their autonomy of research workers. Higher education institutions should support, rather than force, scholars to use of crowdsourcing in science. In particular, it is important to show how to organize such an initiative, how to ensure the quality of data, how to evaluate data and ensure methodological rigor and workflows for members of the virtual community. But this support in the form of training cannot be coercive. It should be available only to those scholars who are interested in crowdsourcing in science. Scholars can use the acquired knowledge to organize crowdsourcing in science initiatives that will enable them to carry out research tasks and reduce concerns about shared data, data theft, insufficient crowd knowledge, communicating with the crowd, delegating tasks to the crowd, abandoning tasks by the crowd, additional work in the field of control and verification of crowd work, potential risk of violating methodological rigor, wasting time when tasks are carried out by the crowd contrary to expectations. Additionally, it is important to recognize crowdsourcing in science as optional. Finally, it is also important to show scholars that there are many crowdsourcing platforms that are voluntary based and do not require additional fees from scholars.

# Limitations and directions for future research

The conducted research has its limitations, which at the same time allow to suggest certain directions for further research. Firstly, the research was qualitative in nature. Therefore, it is impossible to discuss any representativeness or possibility of generalizing the results obtained to the entire research population and other contexts. That is why, the research conducted encourages further, additional in-depth research to better understand the factors limiting the use of crowdsourcing in science. Future research should adopt a quantitative research approach and measure the importance of individual-identified barriers to the intention of using of crowdsourcing in science.

In addition, the research focused on the individual perception of crowdsourcing in science by scholars, which may intensify certain limitations that result from the fact that perception itself is a cognitive process that consists of both the selection of information that a given person obtains from the environment, but also giving the information its subjective meaning and sense. Therefore, there may be differences in the factors limiting the use of crowdsourcing in science along with the acquisition of knowledge about it, which results from the fact that a researcher can revise their observations under the influence of new knowledge or opinions of other people. It would be interesting to learn the dynamics of those barriers by means of longitudinal research, which would allow us to learn about their evolution along with the development of knowledge, skills, and competences of the researcher’s experience in organizing a crowdsourcing in science initiative.

Secondly, another limitation may be attributed to the fact that the research was conducted among scholars who had no or little experience with the use of crowdsourcing in science in scientific and research work. On the one hand, this may have an impact on the obtained results, because the interviewees often based their judgments on assumptions. On the other hand, research on crowdsourcing in science among academics who have little, or no such experience is recommended in the literature (Law et al. 2017). With the above in mind, this article addresses those challenges.

Thirdly, our research was conducted among management scholars from one country. Cross-national comparison studies may be interesting, taking into account the specificities of research fields, e.g. STEM vs humanities. Moreover, cultural variations also exist in the adoption of crowdsourcing in science practices, scholars with some cultural contexts. They influence whether scholars are more proactive and enthusiastic for new tools for research than others (Englund, Olofsson, and Price 2018).

Finally, in our research we developed the process of considering the opinions and perceptions of scholars without taking into account the position of various stakeholder groups, e.g. owners of crowdsourcing platforms, decision makers and employers. Those stakeholders can modify the typology in question, taking into account different types of crowdsourcing in science (Lenart-Gansiniec 2022) and the functionality of crowdsourcing platforms. Therefore, it is recommended that future research on barriers to crowdsourcing in science should be of a transdisciplinary nature. This direction of research is recommended because it can be useful in identifying real problems in order to proactively support actions or interventions that improve the way our society functions.

# Acknowledgements

The authors would like to kindly thank Dr Molly Lee and Claire Drake, Editors, and Reviewers of the paper for their valuable comments and help in the review process.

# Disclosure statement

No potential conflict of interest was reported by the author(s).

# Funding

This work was supported by National Science Centre [grant number DEC-2019/35/B/HS4/01446].

# ORCID

Regina Lenart-Gansiniec $\textcircled{1}$ http://orcid.org/0000-0002-9266-9638

# References

Bassi, Harpreet, Christopher J. Lee, Laura Misener, and Andrew M. Johnson. 2020. “Exploring the Characteristics of Crowdsourcing: An Online Observational Study.” Journal of Information Science 46 (3): 291–312. https://doi.org/10. 1177/0165551519828626.   
Beck, Susanne, Carsten Bergenholtz, Marcel Bogers, Tiare-Maria Brasseur, Marie Louise Conradsen, Diletta Di Marco, Andreas P. Distel, et al. 2022. “The Open Innovation in Science Research Field: A Collaborative Conceptualisation Approach.” Industry and Innovation 29 (2): 136–85. https://doi.org/10.1080/13662716.2020.1792274.   
Behrend, Tara S., David J. Sharek, Adam W. Meade, and Eric N. Wiebe. 2011. “The Viability of Crowdsourcing for Survey Research.” Behavior Research Methods 43 (3): 800–13. https://doi.org/10.3758/s13428-011-0081-0.   
Bouncken, Ricarda B., Yixin Qiu, and F. Javier Sendra García. 2021. “Flexible Pattern Matching Approach: Suggestions for Augmenting Theory Evolvement.” Technological Forecasting and Social Change 167:120685. https://doi.org/10.1016/ j.techfore.2021.120685.   
Braun, Virginia, and Victoria Clarke. 2006. “Using Thematic Analysis in Psychology.” Qualitative Research in Psychology 3 (2): 77–101. https://doi.org/10.1191/1478088706qp063oa.   
Brem, A. M., C. L. Tucci, T. Brown, and J. Chen. 2023. “Guest Editorial: The Age of Crowdsourcing and Crowdfunding for Technological Innovation: Where We Are, and Where to Go?” IEEE Transactions on Engineering Management 70 (9): 3015–20. https://doi.org/10.1109/TEM.2023.3270022.   
Bücheler, Thierry, and Jan Henrik Sieg. 2011. “Understanding Science 2.0: Crowdsourcing and Open Innovation in the Scientific Method.” Procedia Computer Science 7: 327–9. https://doi.org/10.1016/j.procs.2011.09.014.   
Cuccolo, Kelly, Megan S. Irgens, Martha S. Zlokovich, Jon Grahe, and John E. Edlund. 2021. “What Crowdsourcing Can Offer to Cross-Cultural Psychological Science.” Cross-Cultural Research 55 (1): 3–28. https://doi.org/10.1177/1069397120950628.   
Czakon, Wojciech, Monika Hajdas, and Joanna Radomska. 2023. “Playing the Wild Cards: Antecedents of Family Firm Resilience.” Journal of Family Business Strategy 14 (1): 100484. https://doi.org/10.1016/j.jfbs.2022.100484.   
Davidson, Christina. 2009. “Transcription: Imperatives for Qualitative Research.” International Journal of Qualitative Methods 8 (2): 35–52. https://doi.org/10.1177/160940690900800206.   
Eklund, Lina, Isabell Stamm, and Wanda Katja Liebermann. 2019. “The Crowd in Crowdsourcing: Crowdsourcing as a Pragmatic Research Method.” First Monday 24 (10), https://doi.org/10.5210/fm.v24i10.9206.   
Englund, Claire, Anders D. Olofsson, and Linda Price. 2018. “The Influence of Sociocultural and Structural Contexts in Academic Change and Development in Higher Education.” Higher Education 76 (6): 1051–69. https://doi.org/10. 1007/s10734-018-0254-1.   
Franzoni, Chiara, Marion Poetz, and Henry Sauermann. 2022. “Crowds, Citizens, and Science: A Multi-Dimensional Framework and Agenda for Future Research.” Industry and Innovation 29 (2): 251–84. https://doi.org/10.1080/ 13662716.2021.1976627.   
Gioia, Dennis A., Kevin G. Corley, and Aimee L. Hamilton. 2013. “Seeking Qualitative Rigor in Inductive Research: Notes on the Gioia Methodology.” Organizational Research Methods 16 (1): 15–31. https://doi.org/10.1177/ 1094428112452151.   
Kyvik, Svein, and Terje Bruen Olsen. 2008. “Does the Aging of Tenured Academic Staff Affect the Research Performance of Universities?” Scientometrics 76 (3): 439–55. https://doi.org/10.1007/s11192-007-1767-z.   
Law, Edith, Krzysztof Z. Gajos, Andrea Wiggins, Mary L. Gray, and Alex Williams. 2017. “Crowdsourcing as a Tool for Research: Implications of Uncertainty.” Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. New York, NY, USA: ACM.   
Lenart-Gansiniec, Regina. 2022. “Towards a Typology Development of Crowdsourcing in Science.” Journal of Information Science, 016555152211180. https://doi.org/10.1177/01655515221118045.   
Lenart-Gansiniec, Regina, and Jin Chen. 2023. “When Will Scientists Say Yes? Antecedents, Consequences and Limitations of Crowdfunding in Research.” Studies in Higher Education 48 (8): 1159–71. https://doi.org/10.1080/ 03075079.2023.2186388.   
Lenart-Gansiniec, Regina, Wojciech Czakon, Łukasz Sułkowski, and Jasna Pocek. 2022. “Understanding Crowdsourcing in Science.” Review of Managerial Science 17 (8): 2797–830. https://doi.org/10.1007/s11846-022-00602-z.   
Mälkki, Kaisu, and Sari Lindblom-Ylänne. 2012. “From Reflection to Action? Barriers and Bridges Between Higher Education Teachers’ Thoughts and Actions.” Studies in Higher Education 37 (1): 33–50. https://doi.org/10.1080/ 03075079.2010.492500.   
Medley-Rath, Stephanie. 2019. “Using Facebook Secret Groups for Qualitative Data Collection.” The Qualitative Report, https://doi.org/10.46743/2160-3715/2019.3963.   
Miles, D. A. 2017. “A Taxonomy of Research Gaps: Identifying and Defining the Seven Research Gaps.” In Doctoral Student Workshop: Finding Research Gaps-Research Methods and Strategies, 1–10. Dallas, TX.   
Moghaddam, Ehsan Noorzad, Alireza Aliahmadi, Mehdi Bagherzadeh, Stefan Markovic, Milena Micevski, and Fatemeh Saghafi. 2023. “Let Me Choose What I Want: The Influence of Incentive Choice Flexibility on the Quality of Crowdsourcing Solutions to Innovation Problems.” Technovation 120: 102679. https://doi.org/10.1016/j. technovation.2022.102679.   
Niemczyk, Ewelina K., and Zoltán Rónay. 2022. “Roles, Requirements and Autonomy of Academic Researchers.” Higher Education Quarterly 77 (2): 327–41. https://doi.org/10.1111/hequ.12403.   
Palinkas, Lawrence A., Sarah M. Horwitz, Carla A. Green, Jennifer P. Wisdom, Naihua Duan, and Kimberly Hoagwood. 2015. “Purposeful Sampling for Qualitative Data Collection and Analysis in Mixed Method Implementation Research.” Administration and Policy in Mental Health 42 (5): 533–44. https://doi.org/10.1007/s10488-013-0528-y.   
Pelletier, Kathe, Malcolm Brown, D. Christopher Brooks, Mark McCormack, Jamie Reeves, Nichole Arbino, Aras Bozkurt, et al. 2021. “2021 EDUCAUSE Horizon Report Teaching and Learning Edition,” 2–50. doi: www.learntechlib.org/p/ 219489/.   
Saldaña, J. 2009. The Coding Manual for Qualitative Researchers. Los Angeles, London, New Delhi, Singapore, Washington DC: Sage Publications Ltd.   
Santos-Fernandez, Edgar, Julie Vercelloni, Aiden Price, Grace Heron, Bryce Christensen, Erin E. Peterson, and Kerrie Mengersen. 2023. “Increasing Trust in New Data Sources: Crowdsourcing Image Classification for Ecology.” Revue Internationale de Statistique [International Statistical Review], https://doi.org/10.1111/insr.12542.   
Schlagwein, Daniel, and Farhad Daneshgar. 2014. “User Requirements of a Crowdsourcing Platform for Researchers: Findings from a Series of Focus Groups.” www.aisel.aisnet.org/pacis2014/195/.   
Suri, Harsh. 2011. “Purposeful Sampling in Qualitative Research Synthesis.” Qualitative Research Journal 11 (2): 63–75. https://doi.org/10.3316/QRJ1102063.   
Taylor, Bryan C., and Thomas R. Lindlof. 2016. “Travelling Methods: Tracing the Globalization of Qualitative Communication Research.” Romanian Journal of Communication and Public Relations 15 (3): 11. https://doi.org/10. 21018/rjcpr.2013.3.192.   
Thoegersen, Jennifer L., and Pia Borlund. 2022. “Researcher Attitudes Toward Data Sharing in Public Data Repositories: A Meta-Evaluation of Studies on Researcher Data Sharing.” Journal of Documentation 78 (7): 1–17. https://doi.org/10. 1108/jd-01-2021-0015.   
Vila-Henninger, Luis, Claire Dupuy, Virginie Van Ingelgom, Mauro Caprioli, Ferdinand Teuber, Damien Pennetreau, Margherita Bussi, and Cal Le Gall. 2022. “Abductive Coding: Theory Building and Qualitative (Re)Analysis.” Sociological Methods & Research 004912412110675. https://doi.org/10.1177/00491241211067508.