# From general critical questions to scheme-relevant critical questions in the instruction on argument evaluation for EFL graduate students: A two-cycle action research☆

Yao Du a,\* , Xinjie Gao b

a Foreign Languages Department, University of Chinese Academy of Sciences, No.19 (A) Yuquan Road, Shijingshan District, Beijing, 100049, PR   
China   
b Yanqing No.1 High School, No.5 Gaota Road, Yanqing District, Beijing, 102199, PR China

# A R T I C L E I N F O

# A B S T R A C T

Handling Editor: Guangwei Hu

Keywords:   
Argument evaluation   
Critical question approach   
Article critique   
Action research

For graduate students to succeed, acquiring skill at evaluating arguments is crucial, but reaching mastery at argument evaluation necessitates perspicacity and a willingness to challenge recognized authorities, published articles, and heretofore accepted “truths.” Teaching university students to become more effective at critique, however, has seldom been the focus of academic study within the context of English as a foreign language (EFL). In this action research, a criticalquestion approach was employed in an English for Academic Purposes (EAP) class designed to instruct graduate-level science majors to critique popular science articles. Instructional focus shifted, as the action research progressed, from general critical questions to scheme-relevant critical questions drawing upon Walton’s argumentation scheme theory. Students’ skill development level was assessed through group oral critiques and individual critique essay writing. Subsequent discourse-based interviews with five students revealed nuances in skill development. Results indicated that students’ oral critique skills improved over time, whereas substantial enhancements in the targeted goal of their critique essay writing did not materialize. This study showcases how EFL university students’ exposure to societal, rhetorical and power dynamics within a specific sociocultural context can impact their performance in critiquing written English text.

# 1. Introduction

Since academic argumentation has a central role in scientific knowledge construction (Leitao,˜ 2000), students in higher education must hone their ability to identify, evaluate and construct arguments to master this complex skill set. Argument evaluation is fundamental to the dialogical force of scientific communication. Well-developed skills in argumentation in scientific fields, as well as in other academic fields, entail being able to determine “whether the functionally distinct components of an argument form an internally consistent and plausible argumentation” (Münchow et al., 2023, p. 708).

Researchers have found notable differences between students and scientists in their ability to evaluate scientific arguments (Hogan & Maglienti, 2001; Koehler, 1993; von der Mühlen et al., 2016ab; von der Mühlen et al., 2016ab). For instance, Koehler (1993) discovered that while both science students and scientists tended to rate evidence that aligned with their prior beliefs as being of higher quality, the impact of prior belief on evidence quality judgment, for scientists, was weaker for specific, analytical judgments as compared to general, evaluative ones. Additionally, von der Mühlen et al. (2016ab) found that students relied heavily on intuition and personal opinion when analyzing the plausibility of scientific arguments. By contrast, science professionals employed refined strategies such as examining logical coherence or the relationship between premise and conclusion.

To help bridge this gap, teaching practitioners have tried educational interventions to foster students’ argument evaluation skills (e. g., Larson et al., 2009; Münchow et al., 2023; von der Mühlen et al., 2018), but these short-term teaching interventions usually used comparative pre-and post-test design or assessed the students’ argument evaluation skills with yes/no judging tasks, which yielded limited insight into student performance or skill development in actual writing assignments. Besides, though some targeted interventions designed for native English speakers did appear to be effective (e.g., Song & Ferretti, 2013; Wissinger & De La Paz, 2016), little is known about whether argument evaluation instruction can enhance Chinese graduate students’ critiquing ability in an EFL learning environment.

In China, reading instruction from primary to tertiary levels primarily focuses on reading comprehension rather than text critique, resulting in minimal instruction on argument evaluation skills. However, increasing evidence highlights the need for Chinese students to learn and practice how to critically evaluate what they read (Du, 2019; Kuzborska & Soden, 2018; Macknish, 2011; Pei et al., 2017). For instance, Kuzborska and Soden (2018) found that lower-scoring Chinese EFL university postgraduates used significantly fewer concessive and contrastive expressions (to construct “opposition relations”) in their argumentative essays as compared to their higher-scoring peers. Du (2019) evaluated an EAP unit’s effectiveness on students’ source-use performance in argumentative essay writing. Students improved in six out of seven source-use operations but had no significant improvement in source-text critique. These findings underscore the importance of developing students’ skills in critical reading and effectively refuting opposing views in a milieu where challenging others’ views is often seen as a threat to social harmony.

The present study was undertaken at a research-oriented university in Beijing, China, where a credit-bearing English for Academic Purposes (EAP) course entitled “Academic Reading and Writing” is mandatory for incoming graduate students. Course goals support strengthening critical reading and source-based writing ability, seeking to offset students’ tendency to accept the arguments made in textbook articles as established truth.

# 1.1. Argument evaluation and critique writing

An argument is defined as “a claim which, according to appropriate procedures of reasonable dialogue, should be relevant to proving or establishing the arguer’s conclusion at issue” (Walton, 2008, p. 1). Argument evaluation refers to “rendering an explicit judgment that an argument is valid or invalid, sound or unsound, good or bad, strong or weak, ethical or unethical” (Schiappa, 1995, p. ix).

To properly evaluate an argument’s strengths and weaknesses, critical reading and analysis skills must be utilized. A student-author learns to examine the adequacy and cogency of factual premises in support of conclusions, so as to assess whether the work under review stands up to scrutiny. Applying critical thinking, the student-writer can shape a comprehensive, balanced judgment of the argument’s overall persuasiveness or lack thereof.

Many students face significant difficulties in evaluating arguments and composing critiques, as highlighted by scholarly research (Larson et al., 2009; Song et al., 2017). When Song et al. (2017) assessed the argument evaluation skills of 1706 middle school students using a critique essay writing task, the results showed that only $1 7 \%$ were able to write an acceptable critique. The study found that the main challenges for a majority of these students were: (a) limited understanding of critiquing as a form of discourse, (b) insufficient ability to recognize false premises or illogical arguments, and (c) difficulty explaining faulty reasoning in source documents.

Students’ inability to evaluate arguments and write critiques can result from limited comprehension of source texts’ argumentation structure (List & Oaxaca, 2024; Neuman & Weizman, 2003), limited subject area knowledge (Wiley, 2005), or a belief bias (Stanovich & West, 2007; West et al., 2008). Thought processes may be hampered if the student is not familiar with inductive versus deductive logic, has not been instructed about common logical fallacies, or is uncertain about the rigorous standards needed for individual facts and evidence to verify the proof. These limitations may encourage faulty reasoning that is overly trusting of authority, or overly reliant on mental shortcuts based on intuition, common wisdom or hearsay.

# 1.2. Previous studies in fostering students’ skills at argument evaluation

A high-quality critique essay, beyond its basic structural integrity, will demonstrate advanced critical thinking. A challenge for an instructor of EFL students lies in seamlessly integrating the instruction of critiquing skills into the English curriculum using step-bystep scaffolding. Recent educational research has delved into examining the effectiveness of interventions aimed at improving students’ critiquing skills (e.g., Larson et al., 2009; Münchow et al., 2023; von der Mühlen et al., 2018). Larson et al. (2009) studied the effectiveness of a $1 5 \mathrm { - m i n }$ tutorial on how to identify high- and low-quality arguments. The tutorial concentrated on an argument’s three key elements (claim, reason, warrant), in an attempt to teach students to distinguish flawed from acceptable arguments and to warn them about common problems in argument evaluation. Although the tutorial attendees (tutorial condition) did become more adept at recognizing unsupported arguments, the research found no significant difference between the tutorial and the baseline conditions in terms of recognizing acceptable or unwarranted argumentation. This result implied the persistence of some students’ inability to distinguish whether a reasoning framework relative to an issue area provided substantive support for argued claims.

In an instructional intervention with German university students, Münchow et al. (2023) examined the effectiveness of two training interventions—argument structure training and argument judgment training—for evaluating typical arguments in scientific discourse. In this randomized crossover trial design, Group A students received argument structure training followed by argument judgment training, whereas Group B underwent the reverse sequence. The argument structure training session focused on conceptual knowledge about argument components (claim, reason, warrant, backing, rebuttal) and linguistic markers to identify them. Argument judgment training provided students with strategies to assess reasons supporting a scientific claim and to identify fallacious logic. While both trainings improved students’ skills in understanding and evaluating arguments, an interference effect was observed between the training interventions. Specifically, students in the active-control group exhibited poorer performance on the argument judgment or structure tests after completing the alternate training module. It suggested that integrating different components of scientific literacy posed a significant challenge for the students.

# 1.3. Adopting Walton’s argumentation scheme theory in argument evaluation instruction

The aforementioned teaching interventions emphasized an argument’s foundational elements: claim, reason and warrant. While overarching principles such as “relevance” and “completeness” (Münchow et al., 2023) to determine the logical plausibility between reasons and claims, and processes like “attending to the predicate of a claim” (Larson et al., 2009) to distinguish flawed from acceptable arguments may be useful in certain contexts, they often oversimplify complex thought processes that connect reasons to claims. Walton’s argumentation scheme theory provides a better choice for understanding argument complexity.

Walton (1996) popularized the notion of argumentation schemes, allowing it to accommodate a wider range of arguments than traditional logical systems did. According to Walton, identifying instances of a fallacy in an argument necessitates comprehension of the underlying argumentation schemes. Argumentation schemes have been defined as “the conventionally acceptable patterns of reasoning that are appealed to in argumentative communication, substantiating the inferential connection between premise(s) and conclusion” (Visser et al., 2021, p. 104). For instance, the argument from an expert opinion scheme involves reasoning that because an expert in a relevant field asserts something, it is likely to be true.

Argumentation schemes are useful and make transparent the discourse patterns for refuting arguments since these schemes can be logically and consciously applied to the text or situations. Associated critical questions in each scheme can serve as triggers to judge argumentative quality. In an argument from consequences scheme, for example, appropriate questions would be: (1) How strong is the likelihood that the cited consequences will occur? (2) What evidence supports the claim that the cited consequences will occur, and is the evidence sufficient to adequately support the strength of the claim? (3) Are there other opposite consequences that should be taken into account (Walton et al., 2008, pp. 332–333)? (A comprehensive list of argumentation schemes and critical questions is given in Walton et al. (2008)). Critical questions can serve as a benchmark for assessing argument plausibility because, if the questions cannot be adequately answered, the claim the scheme has put forward is undermined. Schemes and critical questions may function as heuristics, helping students more effectively evaluate arguments. According to Newell et al. (2011), diverse argumentation schemes can be employed in the classroom to sharpen the analysis of specific types of arguments by presenting the arguments and then raising critical questions regarding each premise connected to those argument types.

The value of using Walton’s framework to instruct students at various educational levels to better evaluate arguments and/or compose complex, domain-general/specific arguments has been supported by recent research. Teaching professionals included in their instruction common argumentation schemes and their accompanying critical questions (e.g., Macagno & Konstantinidou, 2013; Song & Ferretti, 2013; Wissinger & De La Paz, 2016). Song and Ferretti’s (2013) instruction taught 30 college students two common argumentation schemes (argument from consequences, argument from examples) using critical questions associated with those schemes. Students who used the critical questions produced higher-quality work, by including more counterarguments, an alternative standpoint or a strong rebuttal than did students in the scheme-only condition. When the instruction employed critical standards of argumentation, college students’ sensitivity to other perspectives and their ability to critique were enhanced. Similarly, over three weeks, Wissinger and De La Paz (2016) taught middle school students two argument schemes (argument from expert opinion, argument from consequences) and their accompanying critical questions. When compared with students who participated only in traditional discussions, the pupils in the experiment condition, relying on critical questions as a framework for discussion, outperformed them in employing argumentation schemes. It appears that students are more likely to produce arguments based on higher-level thinking when critical questions act as cognitive scaffolding. Answering the critical questions may have aided students in putting aside preliminary conclusions to reach a more thorough, logical comprehension of historical debates.

To make critical questions easier for students to understand, some recent teaching practices (e.g., Nussbaum & Edwards, 2011; Nussbaum et al., 2019) modified Walton et al.’s (2008) version. In a general education seminar for undergraduates, Nussbaum et al. (2019) examined the effect on the students of including a critical question box on Argumentation Vee Diagrams (AVDs) for a written discourse assignment. The students who used AVDs that included a critical question box wrote more integrative refutations as compared to the control group that used AVDs without a critical question box. Critical questions seemed to offer students a frame to climb for evaluating arguments and counterarguments.

Although the aforementioned findings point to the value of using a critical-question approach in argument evaluation training, this approach might not benefit students in an EFL context. Researchers have theorized that students from a Confucian cultural background, where authoritative texts are frequently viewed as infallible, may find argument evaluation and critique essay writing particularly difficult (Du, 2019). Additionally, existing research on the instruction of argument evaluation has typically been of short duration and employed experimental or quasi-experimental research designs. These factors raise concerns about the generalizability of the findings. Confusion or misunderstanding about the nuanced development of argument evaluation skills could lessen the power of instruction in argumentation. Culturally tailored studies that explore methods for teaching argument evaluation in diverse educational contexts, particularly within EFL settings, would be insightful. To the best of the authors’ knowledge, no study to date has investigated the development of students’ critiquing skills in evaluating arguments through the lens of action research.

# 1.4. Research questions

This action research aims to develop Chinese graduate-level science students’ argument evaluation skills by examining the operation of a critical-question approach applied in a university-level EFL academic reading and writing course. It encompasses two cycles (Cycle 1 and Cycle 2), each consisting of four stages: planning, acting, observing and reflecting. Cycle 2 builds upon insights gained from Cycle 1, following an iterative process.

The study is guided by two questions: (1) How do students develop their argument evaluation skills toward textbook articles, as reflected in their oral critiques, when guided by general critical questions in Cycle 1 and scheme-relevant critical questions in Cycle 2? (2) How do students develop their argument evaluation skills in writing critique essays under the guidance of general critical questions in Cycle 1 and scheme-relevant critical questions in Cycle 2?

# 2. Methods

While teaching graduate students in an academic EFL reading and writing course for seven years, we have observed that, though most students comprehend our textbook reading material with respect to recognizing the argumentation, they tend to accept textbook arguments per se without critically examining them.

Inspired by discussions around Walton’s argumentation schemes and critical questions as a partial solution, we conducted action research to observe and measure students’ skill development with a critical-question teaching approach. Action research is defined as “an inquiry conducted into a particular issue of current concern, usually undertaken by those directly involved with the aim of implementing a change in a specific situation” (Hitchcock & Hughes, 1989, p. 7). In the domain of education, action research is characterized as “self-reflective, systematic and critical” (Burns, 2011, p. 6), with the objective of improving student outcomes by addressing problematic issues and implementing targeted interventions. Since classroom-based action research starts from a specific teaching context and involves gathering and analyzing data over time to assess whether change occurred, the following subsections present our study’s teaching context and the methods used to assess teaching and learning outcomes.

# 2.1. Teaching context

At the start of the fall semester in 2022, a survey was conducted among new master’s level students to evaluate their English academic reading and writing proficiency. Students whose reported scores were below certain thresholds for CET4, CET6, TOEFL or IELTS were required to enroll in an EAP reading and writing course. This course, consisting of $1 0 0 { \cdot } \mathrm { m i n }$ sessions once weekly for 16 weeks, was taught by the first author in collaboration with a research assistant, who is the second author. The course textbook compiled by a team of faculty members from the same university was specifically designed for university students with low-tointermediate level English proficiency. In its ten chapters, the initial five focus on improving academic reading and source-based writing ability, and this action research aligns with the primary teaching objectives in those five chapters. While the textbook provides an overview of fundamental aspects of constructing an argument such as claims and supporting evidence, emphasis remains on acquiring declarative knowledge rather than applying it. This highlights the need for instructional approaches that go beyond foundational understanding and actively engage students in utilizing their acquired knowledge in an academic context.

# 2.2. Participants

A total of 29 science majors (21 males, 8 females), all native Mandarin speakers, participated. Prior to this study, all had passed China’s national standardized English CET-4 (College English Test Band 4), with an average score of 481 $\mathrm { ( S D = 4 4 . 7 1 }$ ; CET-4 total score is 710 with a passing grade of 425). All participants received an informative letter about the research. All agreed to have their oral presentation and writing data used anonymously.

# 2.3. Evaluation

Before the two-cycle instruction, students were asked to complete a researcher-designed questionnaire about their perceptions of their English academic reading ability. They were asked to rate on a five-point Likert scale their perceived difficulty in reading academic English and their perceived proficiency as readers of academic English ( $0 =$ most negative; $4 =$ most positive). Specifics about their difficulties with academic reading were also collected via the questionnaire.

As an integral assignment in the course, each student group (5–6 per group) was tasked with delivering a group presentation at the outset of each new module. A group would be required to thoroughly analyze the module’s articles, including comprehension, organization and critical evaluation. Each student’s performance in the oral presentation was graded, with the understanding the score would be part of the student’s final course grade. Given the students’ comparatively weak oral English communication skills, they were allowed to use Presenter Notes in PowerPoints to prepare and write down what they wanted to say in advance. Their notes, meticulously gathered as part of our action research, were a valuable data source.

Critique essay assignments were intentionally used as formal formative assessments to measure students’ argument critique skills for change over time. Students in this course, all science majors, were required to critique the popular science articles in the course textbook. The first article (Flesch-Kincaid Grade Level 10.6, Flesch Reading Ease Score 52.4), discussing the possibility of future computers being conscious, was source material for the first critique essay assignment prior to the two-cycle instruction. The second article (Flesch-Kincaid Grade Level 13.7, Flesch Reading Ease Score 39.1), exploring the public’s differing appreciation of experimental and theoretical physicists, was used for the second critique essay assignment after Cycle 1. The third article (Flesch-Kincaid Grade Level 9.3, Flesch Reading Ease Score 55.8), focusing on the beauty of mathematical equations, was used for the third critique assigned after Cycle 2. These three articles were selected because students reported the highest topic familiarity (at 6.8–7.7 points out of 10, notably higher than the other articles). The chosen three articles are argumentative texts with clear author positions supported by evidence. For an individual score on each of the three articles, rather than a group grade, every student in his or her essay was to provide a reasoned analysis that pinpoints and explains flaws in reasoning or evidence.

To gain deeper insights into students’ skill development, we conducted in-depth, assignment-based interviews with five students (three males, two females). After completion of each critique essay, these five were interviewed to explain their writing process and to self-evaluate their performance. The five students were selected based on their willingness, as indicated in the preliminary survey questionnaire, to participate in interviews. To facilitate smooth communication, the interviews were conducted in Chinese and subsequently translated into English. In addition, both of us wrote reflective journals after each class to document and reflect upon each cycle.

# 2.4. Analysis

To examine the development of students’ critiquing ability over time, an analysis was made of the presenter notes students had used in the PowerPoint slides during the group presentation, as described above. For students’ critiques that targeted the overall argumentation rather than individual points, we adopted as our annotation protocol argument evaluation guidelines (See Appendix 1, upper part.) For critiques that focused on a specific article’s argument, we adopted annotation protocols developed by Song et al. (2017) (See Appendix 1, lower part.)

To score students’ critique essays, we devised a scoring rubric comprising structure (10 points) and substance (15 points). Each student’s writing performance was assessed using a five-level scale, from highest (A) to lowest (E), for each of these two dimensions. For the structure dimension, rubric levels were converted into point scores: Level $\mathsf { A } = 1 0 { - } 9$ points, Level ${ \bf B } = { \bf 8 } - 7$ points, Level ${ \mathrm { C } } = 6 { - } 5$ points, Level $\mathrm { D } = 4 { - } 3$ points, and Level $\mathrm { E } = 2 \mathrm { - } 1$ points. For the substance dimension, the scoring: Level $\mathbf { A } = 1 5 \ – 1 3$ points, Level $\mathbf { B } =$ 12–10 points, Level $\mathrm { C } = 9 { - } 7$ points, Level $\mathrm { D } = 6 { - } 4$ points, and Level $\mathrm { E } = 3 \mathrm { - } 1$ points (See Appendix 2.) Each student’s total score was the sum of the point scores from both dimensions.

We conducted a pilot evaluation on eight student critique essays. To ensure consistent scoring, we held a discussion and established consensus on the evaluation criteria before beginning the formal evaluation. During the formal evaluation, inter-rater reliability was high, with correlation coefficients of $r = 0 . 8 4 4$ for essay structure and $r = 0 . 8 6 6$ for essay substance; both were statistically significant $( p < 0 . 0 0 1 )$ ). In cases where there was a difference of more than 3 points in a dimensional score assigned by each of the two raters, a third rater was requested to evaluate that essay; and then the final score for that essay in the respective dimension became the mean average between the third rater’s score and a score closest to that of the previous two raters. These scores were part of students’ final course grades.

# 3. Action research in two cycles

Following Burns’s (2011) four-stage action research model, both of our project’s two instruction cycles adhered to this order: first, Plan, a prominent problem concerning students’ argument evaluation was identified, i.e., students’ weaknesses at critically analyzing and effectively responding to various claims and positions; next, Act, a progressive series of lesson plans were developed to facilitate students’ argument evaluation skills; then, Observe, students’ ability to evaluate arguments was assessed through oral presentations and critique writing based on textbook articles; and at the last stage, Reflect, a reflection on areas for instructional improvement was carried out. Fig. 1 illustrates the study’s two cycles.

![](img/bcca4cc0b4f6f82583176ca9c151af63ea51a262ad3ff6839da75ed19f535d6b.jpg)  
Fig. 1. The two action cycles.

The pre-instruction questionnaire revealed that a majority $( 7 5 \% )$ ) of students acknowledged finding reading academic English to be a “very difficult” or a “difficult” activity for them. Out of 29 students, 23 regarded themselves as “very weak” or “weak” in academic English reading. In gauging specific areas of difficulty, $5 9 \%$ of those surveyed indicated they found it challenging to “critically engage with source texts”.

We recognized from Group 1 oral presentation prior to this action research that, though students were required to present evaluations of the textbook article, the group’s oral presentation gave only a single critique as follows: “In real life, we tend to compare a computer with human brains. However, the computer can precisely perform calculations and record all kinds of big data that human beings cannot perform.” While this reasoning would lead to critiquing the comparability of the human brain and computer, the student was stymied at applying it to the text when asked to establish how these differences might relate to the author’s argument or how they might weaken that article’s logic. Hence, the “connection” or movement of thought from the reality of the brain versus the computer to the argument in the English text was not made.

Analysis of student essays prior to the action research showed that the class as a whole did not have strong critiquing skills. Among the 29 critique essays from 29 students, only 4 demonstrated an understanding of the basic structure of a critique essay; another 4 followed the basic structure but focused too heavily on the article summary and had limited critique; and 12 omitted the summary and/ or the conclusion. Among the remaining 9 students, 3 responded to the critique essay task by mainly copying information from the article, and 6 approached the critique task as an opportunity to express a personal opinion on the issue. Many students struggled to address the material with impartiality and analytical rigor. In evaluating the strengths and weaknesses of the textbook article about computers, over one-third of the 29 students began their analysis with subjective phrases such as “I agree” or “I disagree.” One or two characteristics might be in play here. Grammatically, it is easier to use a personal pronoun (such as I) as a sentence subject in L2, but this probably also shows that students’ default solution to critique involves affirming or challenging their own beliefs rather than engaging critically and thoughtfully with the text itself. The use of such phrases seemingly indicates that their critiques are heavily influenced by pre-existing opinions, but it may also be that, due to limited exposure, they have a limited understanding of the genre of critique.

Additional insight into Chinese students’ difficulties in composing an L2 critique essay was gained from one-to-one interviews before Cycle 1. Students revealed a cultural reticence to criticize or critique, not just a belief or assumption that their English vocabulary or linguistic ability was too limited. Though told otherwise, some students assumed that articles printed as part of our university textbook were too authoritative to be criticized: “This is an article from the textbook, and I should praise it” (S15), and “I seemed to only focus on its advantages and did not think about its shortcomings” (S1). Whether for cognitive or cultural reasons, many struggled to openly identify such negatives as faulty logic or weak analogies. One participant, S12, who could pinpoint the deficiencies in the logic of the author’s argumentation by, for example, identifying several instances where the author merely presented examples or experiments without establishing a clear argument based on relevant evidence, attempted to justify the author’s work by saying that “maybe it is enough for a popular science article.” Taking into account the challenges observed in students’ oral and written critiques, the first cycle of action research aimed at providing some fundamentals of argumentation and argument evaluation, and at showing students models to familiarize them with critique.

# 3.1. First cycle of action research

The first cycle of action research, three weeks in length (Weeks 2, 3, 4), focused on helping students understand arguments in academic settings, key components in an argument, and the standard format for an argument. Models were provided to show students how to identify and correctly rewrite arguments. Exercises adapted from Morrow and Weston (2019) were given to students to assess their comprehension of argument components. These exercises included using symbols to highlight different argument components, identifying arguments in longer texts, and presenting premises and conclusions in a standard format. The emphasis was on recognizing that an academic argument might not contain signal words and might not explicitly state the premise and conclusion. Model responses and explanatory feedback were provided to assist in refining their skills.

Table 1 General critical questions in argument evaluation (adapted from Swales & Feak, 2012).   

<html><body><table><tr><td>Focus</td><td>General critical questions</td></tr><tr><td>Reliability of sources of</td><td>O Where is the source text published? Can you trust what it says?</td></tr><tr><td>information Author&#x27;s motives or the purposes</td><td>O Is the source text an excerpt/adaptation? Is important information missing? O Why does the author address this topic? Who is the target audience?</td></tr><tr><td>for writing</td><td>O Does the author have a hidden purpose/a vested interest in presenting their arguments?</td></tr><tr><td>Quality of premises and &amp; conclusions</td><td>O What kind of evidence was collcted to support the author&#x27;s argument? Is there any evidence that could or should have been collcted and included but was not? How good is the evidence? How well does the evidence support the author&#x27;s</td></tr><tr><td></td><td>argument? O What conclusions does the author draw from the article? Are the author&#x27;s conclusions valid or plausible based on the</td></tr><tr><td></td><td>evidence? Why or why not?. O Are there any important assumptions underlying the article? How do these influence the conclusions?</td></tr></table></body></html>

To enhance critiquing ability, students were taught general critical questions about the reliability of information sources, author motives and purposes, and argument quality (Table 1). Students were then exposed to group exercises that involved identifying effective/ineffective arguments in the textbook article that discussed the possibility of future computers being conscious.

To familiarize students with discourse forms of oral critique and the critique essay, general guidelines proposed by de Chazal and Moore (2012) and several popular science articles were used. One model article was a current events critique related to the Covid-19 pandemic. In this news article published in China, Dr. Zhang Wenhong (a specialist in epidemiology, and leader of the Shanghai Medical Treatment Expert Group during the pandemic) was quoted about the feasibility of resuming work and production in China after COVID-19’s first wave. Zhang’s responses in an interview, which form an oral critique, are well structured, beginning with background, moving through a summary of public views with commentary on weaknesses in those views, and presenting the speaker’s perspective. To expose students to a contrasting writing style, the students, all science majors, read essays by Newton and Huygens about the nature of light. Newton’s critique is direct and harsh, whereas Huygens’ tone is milder and indirect.

After Cycle 1, Group 2 students’ presentation showed an increased awareness of critiquing. Their first oral critique pointed out weaknesses in the evidence by stating “The entire article does not have any survey data, and uses the words of prominent scientists throughout as proof … which makes the validity difficult to guarantee.” Their second oral critique correctly touched upon a specific argument in the textbook, identifying author bias and some misleading language. The conclusion given in Group 2’s final presentation was that “It can be felt in the text that the author devalues the position of the theorist in the study of physics when making comparisons, and it is misleading to argue that theorists can easily discover new theories and gain great prestige.” This was a sign that these students had found weaknesses in the author’s argument, perhaps after reflecting on the general critical questions they used. Nevertheless, their oral critique would have benefitted from more robust reasoning.

A paired samples t-test was used to compare critique essay scores before and after Cycle 1. The normality of the difference scores was assessed using a Shapiro-Wilk test, confirming normal distribution for the structure of critique $\mathbf { \Delta } W = 0 . 9 7 7$ , $p = 0 . 7 5 1 \mathrm { . }$ ), the substance of critique $W { = } 0 . 9 7 7$ , $\pmb { p = 0 . 7 6 6 } )$ ), and overall critique essay performance $\mathrm { \Delta } W = 0 . 9 5 8$ , $\begin{array} { r } { p = 0 . 5 0 4 ) } \end{array}$ . The paired samples t-test results indicated no significant difference between the two time points in terms of the structure of critique $\left( t \left( 2 8 \right) = - 1 . 9 7 \right.$ , $p > 0 . 0 5 $ ), the substance of critique, $( t \left( 2 8 ) = - 1 . 7 2 , p > 0 . 0 5 \right)$ , and overall critique essay performance ( $t \left( 2 8 \right) = - 1 . 8 6$ , $p > 0 . 0 5 $ ) (Table 2). Despite a lack of statistically significant improvements in students’ critique essay performance, the observed changes were in the expected direction.

Specifically, at the end of Cycle 1, approximately half of the 29 student essays $( \mathtt { n } = 1 4 )$ ) integrated all three structural elements (summary, critique, conclusion). Three students neglected to include a conclusion though a conclusion had been included in their first article critique. A close reading of these three students’ essays showed their argumentation was richer and contained more critique points as compared with their earlier essays. It may be the case that these students found it hard to include different points in the conclusion or felt the conclusion part had relatively less importance as compared with the summary and the critique part.

Surprisingly, the essays of seven students lacked any critique of the source texts despite class instruction to include them. Among these seven, two (S12, S25) had previously written using critique, i.e., in their first essay. The puzzle of why they were hesitant this time in pinpointing the textbook article’s weaknesses could be partly settled by an assumption about where the article was from. “For the first article critique,” said S12 in an interview, “I thought the article was chosen somewhere from the Internet. As the course went on, I realized the articles for critique essay writing were all from the textbook. How dare I critique the textbook article authors? I feel I am not in the right position to do so.”

Regarding substance, students progressed in two aspects in their essays, specifically acknowledging the main argument and critiquing vocabulary. They demonstrated a heightened focus on the main argumentative thread, as opposed to previous writings in which they were sometimes captivated by trivial details. Second, in their Cycle 1 critique essays, there was a significant increase in evaluative words, with the concomitant result that more students were able in English to maintain scholarly distance from the source text and appraise it based on impersonal guidelines.

Though an increasing number of students in their Cycle 1 essays could identify an article’s strengths and/or weaknesses better than they did prior to Cycle 1, two shortcomings emerged. Among students who did attempt to cast a critical eye on the source text, many merely provided a brief, surface-level comment, or a generic or “pat” response without elaboration. To give an example, one student wrote, “The argument is also very convincing, and the overall structure of the article is reasonable.” A second defect shared by many of the student essays was that, even though the essay identified an article’s strengths and weaknesses, it stopped short of explaining how that identified evidence strengthened or weakened the overall argument. One student wrote, “This article gave a lot of examples of the differences between theoretical physicists and experimentalists, such as Gell-Mann and Kendall-Friedman-Taylor trio. Readers can have a deep impression of the fact in this way, but it’s too much.”

Table 2 Comparison of performance before instruction and in the Cycle 1 critique essay.   

<html><body><table><tr><td></td><td colspan="2">Before Instruction</td><td colspan="2">First Cycle</td><td>t (28)</td><td> P-value</td></tr><tr><td></td><td></td><td>SD</td><td></td><td> SD</td><td></td><td></td></tr><tr><td>Structure of critique essay</td><td>5.41</td><td>2.30</td><td>6.47</td><td>2.56</td><td>1.97</td><td>0.059</td></tr><tr><td>Substance of critique essay</td><td>6.98</td><td>3.23</td><td>8.31</td><td>3.94</td><td>1.72</td><td>0.096</td></tr><tr><td>Overall critique essay performance</td><td>12.40</td><td>5.22</td><td>14.68</td><td>6.16</td><td>1.86</td><td>0.073</td></tr></table></body></html>

$^ { * } p < 0 . 0 5$ .

In post-Cycle 1 interviews, students said they had become more familiar with critique essays by reading them. Their adopted strategies included rereading text (S1, S15), using colored pencils to distinguish argument components (S1), making marginal comments (S1), and recalling teacher comments about a previous essay (S23). When students were asked to recall features and strategies used during their composing process, it was apparent that students, in the main, remained resistant to approach critique essays logically and objectively. Most continued to assess the argument from a personal standpoint to compare their opinions against the authors’ or to give a passionate defense rather than logically counter the argument with proper critiquing methods. When a student’s view was in synch with or mirrored that of the author(s), the student felt less inclined to critique; as S15 said, “I feel the author’s claims are reasonable because the examples given resonate with me.” For improved instruction, it seemed it would be beneficial to add more specific criteria for judging arguments.

In addition, some students based their critique on whether or not the article showed the type of writing they were familiar with. S1, who acknowledged his pre-conception that the most reliable knowledge is obtained from experimental findings, was judging a good argument to be one based on experiment findings, whereas the assigned textbook article about the beauty of math argued predominantly with quotations and examples. Some students yearned for a perspective tailored to the text they were critiquing so they could customize the counterarguments rather than use all-purpose ones. As expressed by S23, “I can only write some templates or cliches that can probably be used for all critique essays.”

After considering students’ performance in oral presentations and critique writing as well as the five students’ responses in interviews, we decided to further explore argument evaluation in the second cycle. This involved introducing students to four argumentation schemes from Walton and critical questions attached to each scheme.

# 3.2. Second cycle of action research

To respond to the inadequacy of teaching general principles for argument evaluation, our approach in the second cycle (Weeks 5, 6, 7) shifted toward instructing students about four argumentation schemes and attempting to equip them with a skillset to pose critical questions specific to those argumentation schemes.

Class instruction in Cycle 2 was designed to emphasize four schemes: argument from causality, argument from examples, argument from comparison, and argument from position. These schemes were chosen for their demonstrated effectiveness in crafting persuasive arguments. Each scheme provided a distinct method for reasoning and presenting evidence. Moreover, their selection was driven by their representation of the predominant argument types identified in the selected textbook articles. Table 3, based on Song et al.’s (2017) work, is an overview of categories and critical questions within the four argument types.

The following gives steps used in class to model how argument critique can be generated for a small portion of a text.

Step 1 — Identifying argument type

tudents read the article’s first paragraph which identifies people’s contrasting views toward theorists and experimentalists:

Everyone knows Paul Dirac who conjectured the existence of the positron, but how many know Carl Anderson and his collaborator Seth Neddermeyer who actually found it? People similarly know about Wolfgang Pauli and Enrico Fermi stating the requirement for a ghostly particle called the neutrino in the 30s, but ask popular science enthusiasts if they are aware of the dogged pursuit of the neutrino by Raymond Davis for over 30 years and you will likely see knitted brows. Finally, even today, a schoolchild would likely know Einstein’s prediction of the bending of starlight by the gravitational field of a star, but Arthur

Table 3 Argumentation schemes, categories, and scheme-relevant critical questions.   

<html><body><table><tr><td>Scheme</td><td>Category</td><td>Critical Questions</td></tr><tr><td>Causal</td><td>Causal mechanism</td><td>Is there really a correlation? Is the correlation merely a coincidence (invalid causal relationship)? Are there alternative causal factors?</td></tr><tr><td></td><td>Causal Efficacy</td><td>Is the causal mechanism strong enough to produce the desired effects?</td></tr><tr><td></td><td>Applicability</td><td>Does this causal mechanism apply?</td></tr><tr><td></td><td>Intervening factors</td><td>Are there intervening factors that could undermine the causal mechanism?</td></tr><tr><td>Example</td><td>Accuracy</td><td>Is the proposition presented by the example in fact true?</td></tr><tr><td></td><td> Relevancy</td><td>Is there any reason to think that this case might not really be an example of the general statement we are being asked to believe?</td></tr><tr><td></td><td>Typicality</td><td>Is there any reason to think that this case might not be typical of the kinds of cases that the general statement covers?</td></tr><tr><td></td><td>Special</td><td>Are there any special circumstances that might make it dangerous to generalize from this case to other cases?</td></tr><tr><td></td><td>circumstances</td><td></td></tr><tr><td>Comparison</td><td>Criterion evaluation</td><td>Is this a good criterion?</td></tr><tr><td></td><td>Comparison</td><td>Do we have evidence for a comparison on this criterion?</td></tr><tr><td></td><td>Difference</td><td>Are there any differences between the two incidences that could undermine the fairness of comparison?</td></tr><tr><td></td><td>Additional criterion</td><td>Have we missed any important criterion?.</td></tr><tr><td></td><td>Position</td><td>Is the person in a position to know whether it is true?</td></tr><tr><td>Position</td><td> Source reliability</td><td></td></tr><tr><td></td><td>Assertion</td><td>Is the person a reliable/credible/honest source? Did the person assert that it is true?.</td></tr></table></body></html>

Eddington’s verification of this fact would be little known. I started mulling over this vivid gap between the public’s appreciation of theorists vs experimentalists.

At this point in the text, the teacher informed students that the author asserts the existence of a significant disparity between how the public perceives theorists and how people view experimentalists, and that this claim is supported with three illustrative examples. The author employs arguments from examples in this context.

Step 2 — Asking critical questions

The scheme-relevant critical questions based on the guidelines (See Table 3) are next used in class.

a. Does everyone indeed possess greater knowledge about the one who proposed the existence of the positron compared to the one who discovered it? Do people indeed know more about the person who recognized the need for a ghostly particle known as the neutrino than they do about the explorers who unraveled their nature? Are school children more acquainted with the individual who predicted the phenomena of starlight bending due to the gravitational field of a star than they are with the man who confirmed it through empirical evidence?   
b. Do these examples indeed exemplify the claim that a disparity between public recognition of theorists and experimentalists exists?   
c. Is it possible that the three examples do not represent typical cases to illustrate the clear disparity in public appreciation between theorists and experimentalists?   
d. Are there special circumstances that could negate this generalizing from the three cases to cases involving different views toward theorists and experimentalists? Step 3 — Responding to critical questions

Since questions open the mind to possible refutation, it was assumed that students could more readily respond after considering these critical questions. The rationale was that questions could prompt students to anticipate potential counterarguments, consider opposing perspectives, and refine the original argument. A possible response to the first critical question, given in Step 2 (a) above, could be: It is not necessarily true that everyone possesses greater knowledge about the proposer of the positron’s existence than its discoverer. Ordinary people may be unaware of both theorists and experimentalists and their scientific contributions. To avoid inexact, generic, or overly broad generalizations, one beneficial solution and perhaps “truer” language would be to use qualifying or hedging, such as by replacing a word like “everyone” with “most people.”

After the modeling phase, students were asked to exchange their critique essays with a deskmate. While reading a peer’s essay, each student was required to identify the type of argument presented and then jot down at least one critical question, thereby opening up the opportunity to challenge a fellow student’s logic.

In the concluding phase of Cycle 2 instruction, students were tasked with identifying ineffective arguments found in English language sources such as newspapers, magazines or social network platforms. Sharing and comparing findings led to students’ awareness of the pervasive presence of illogical yet influential arguments.

After the second round of instruction, our action research found that students demonstrated enhanced oral critique skills during presentations. Group 3 identified three distinct argumentation schemes (arguments by analogy, by example, and by authority), and posed six critiques, some of which delved into the overarching argumentation and others that addressed specific arguments. Group 3 students pointed out the article lacked a cohesive perspective on the abstract idea of beauty. They noted that the author offered “too many different interpretations of the word ‘beautiful,’” indicating a lack of uniformity. At the end of the group’s oral critique presentation, one student in that group allocated approximately $3 \mathrm { { m i n } }$ to counterpose an example of the beauty of mathematics, as reflected in the curve of a suspended necklace in a painting by Leonardo da Vinci. The student implied that this example was better than the textbook’s examples.

To compare critique essay scores collected before Cycle 1 and after Cycle 2, a paired samples t-test was employed. A Shapiro-Wilk test used to evaluate the normality of the difference scores confirmed normal distribution for the structure of critique $W { = } 0 . 9 5 8$ , $p =$ 0.301), the substance of critique $W { = } 0 . 9 6 7$ , $\begin{array} { r } { p = 0 . 4 9 3 } \end{array}$ , and overall critique essay performance $W = 0 . 9 5 7$ , $p = 0 . 2 7 4 )$ . The paired sampled $t$ -test revealed a significant enhancement in the structure of critique essays $\mathop { ( t ) } ( 2 8 ) = - 2 . 4 0$ , $p < 0 . 0 5$ ), suggesting notable improvement in students’ organizational skills for critique essays (Table 4). Although there were no statistically significant advancements in the overall critique essay performance, the trends observed were consistent with the expected directions.

Detailed analysis of student essays’ structure revealed that three students who had in the previous cycle omitted one or more parts had incorporated all three in this round, and four who had in the previous cycle included all three components had excluded the conclusion part this time. Though a spiral developmental pattern may be happening for some students, we found that, when considering all three writing instances together, half the students demonstrated an upward trend in essay structure. It is worth noting that four students lacked any elements of critique in the essays they wrote for class until the very end of Cycle 2.

Table 4 Comparison of critique performance before instruction and in the Cycle 2 critique essay.   

<html><body><table><tr><td></td><td colspan="2">Before Instruction</td><td colspan="2">Second Cycle</td><td>t (28)</td><td> p-value</td></tr><tr><td></td><td>m</td><td> SD</td><td>m</td><td>SD</td><td></td><td></td></tr><tr><td>Structure of critique essay</td><td>5.41</td><td>2.30</td><td>6.74</td><td>2.34</td><td>2.40a</td><td>0.023</td></tr><tr><td>Substance of critique essay</td><td>6.98</td><td>3.23</td><td>8.47</td><td>3.41</td><td>1.68</td><td>0.104</td></tr><tr><td>Overall critique essay performance</td><td>12.40</td><td>5.22</td><td>15.21</td><td>5.53</td><td>2.03</td><td>0.052</td></tr></table></body></html>

$p < 0 . 0 5$ .

In the substance dimension, though a statistically significant improvement was not found, the students’ critique essays were somewhat more substantial after Cycle 2. Among the 29 students, five pinpointed two points of critique, and ten addressed three or more aspects of critique in the Cycle 2 essay. This was a notable increase compared to the pre-Cycle 1 essays, where only nine students focused on two or more points, and the post-Cycle 1 essays, where only ten students did so. These results suggested that students had been enabled to adopt diversified perspectives.

Based on our meticulous reading and analysis of student output, two distinct attributes were salient after Cycle 2. One attribute was related to personal view and the other to the essay’s conclusion. Rather than engage in a direct critique of the textbook article, several students opted to articulate a personal perspective on why mathematical equations can seem beautiful. The views expressed were obviously at odds with those of the source authors, or else obliquely provided a thoughtful addition as insight. It seems that the students trusted the readers to discern the implied critique within these views. A second attribute was related to a concluding statement of politeness. Though several students crafted a comprehensive critique within the body of their essays, they “flipped the script” at the end to seem less critical. In fact, an essay’s closing statement might be opposite to the critique. Student S11, for instance, wrapped up his critique essay, with the observation: “Although the argument is weak, the author not only illustrates the beauty of mathematics, but also reminds us that “beauty” can be very unreliable. Overall, I think this article is quite good.” While this concluding compliment demonstrated an effort to maintain politeness and negate the appearance of a quarrel, it diluted the strength of the critique.

Interviews with students conducted after the second cycle divulged challenges they had faced in composing critical essays. During their one-on-one in-person interviews with us, participants S1, S12 and S15 openly stated the shortcomings they saw in the arguments made in the textbook article, but these critical points either had not been addressed or were only cursorily mentioned in written form. S1 commented, “Some parts of the text are not without problems,” and “I feel the examples somewhat redundant,” indicating some criticisms unmentioned in the essay. A sense of uncertainty was expressed by S12, who said, “There might be more instances of the author’s biased perspectives, but I selected one that I am more confident about.” Nonetheless, interviews revealed that enhanced critical thinking skills were having a profound impact on their approach to reading other texts. As S13 explained, “Now when I read articles, I delve deeper into the claims made. I question whether the evidence is reliable and if it connects to the claims. I now consider these issues more thoroughly … I feel that my third critique is an improvement over the previous ones.”

# 4. Discussion

# .1. Argument evaluation skills development in oral and written critiques

Encouragingly, an improvement in students’ critique skills was evident from their oral group presentations, reflected by their heightened critique awareness, increased number of critique points and deeper analysis. Student engagement in the oral discussions held before the group presentations appeared to significantly contribute to this enhancement, with three possible conclusions about the causes. First, engaging in small-group, peer-to-peer oral discussion assists students in internalizing concepts of effective argumentation. Through the process of articulating a position, presenting counterarguments, and engaging in a back-and-forth exchange, students inadvertently rehearsed some skills essential for critiquing. Second, for the average Chinese student a graded assignment requiring logical critique of published articles represents a substantial departure from their previous academic experience, but undertaking the task collaboratively may have mitigated language-oriented, affective or sociocultural anxiety. Within a peer-supportive environment, there was the possibility of peer acceptance of their logical counterargumentation and slightly “more critical” persona. Third, as students engaged in collaborative reasoning, they may have better understood that a logical critique does not necessarily conflict with the values of harmony and respect for authority deeply ingrained in Chinese culture, because logical critique, when carried out in a constructive and respectful manner, can enhance mutual understanding and strengthen group cohesion. This understanding could bolster confidence, empowering students to more effectively critique published articles. Students’ ongoing improvement in oral critique aligns with previous research that has verified the advantage of collaborative reasoning (Dong et al., 2008; Reznitskaya et al., 2009).

Though the students involved in preparing and presenting oral critiques showed improvement in argumentation ability in their subsequent critique essay performance, overall improvement in individual critique essay writing across the student body did not reach the hoped-for level at the end of Cycle 2. By examining students’ written outputs and interview responses, we learned that for students composing an effective critique essay might present a more significant challenge than we expected. Students with limited prior knowledge about a topic may be readily swayed by an author’s argument, as this situation would align with Wiley’s (2005) observation that individuals with less prior knowledge tend to remember more arguments that support their opinion. Conversely, when thoroughly knowledgeable about the topic under discussion, their evaluation of the argument may be influenced by automatically activated prior knowledge and strong personal beliefs. Robust prior knowledge and steadfast personal beliefs can engender what Stanovich and West (2007) term “myside bias,” the predisposition to interpret information in a manner that aligns with one’s perspective. Prior experiments showed that this bias is resilient and largely unaffected by cognitive abilities or individual differences in thinking dispositions (Stanovich & West, 2007). Our study brings attention to the difficulties in instructional efforts to cultivate argument evaluation skills in the presence of strong personal beliefs and prior knowledge.

Critique essay writing necessitates displaying a critical persona that students might be uncomfortable with for various reasons, including cultural and language issues. A previous study indicated that undergraduate Chinese students experienced uneasiness and hesitation when adopting a critical stance toward source authors (Du, 2019). Findings from our study suggest that a reluctance to critique persists among Chinese graduate EFL students despite instruction.

# 4.2. Reflections on the two teaching cycles

Although general critical questions and scheme-relevant critical questions were explicitly taught in the class, the need for more scaffolded tasks that might assist students in crafting higher-quality critiques became apparent. For instance, as a lead-in before drafting critique essays, we could have students write down several general or scheme-relevant critical questions, which would receive feedback from instructors before the essay deadline. Such support may offer critical thinking guidance for students before they write the essays. Furthermore, having students analyze short, successful critiques could bolster their ability by imitating the writing or thinking style.

Students’ familiarity with the topics in articles used in this action research might influence their proficiency in applying argument evaluation skills. We initially surmised that selecting article topics with which students are more familiar might afford them greater latitude for critique. An unforeseen consequence was that excessive familiarity with a topic could trigger entrenched beliefs, posing hurdles to identifying weaknesses in authors’ arguments. Furthermore, while the student body as a whole reported a comparable level of familiarity with the three topics, variations in individual students’ topical knowledge could affect the development of their critical thinking skills. The question of how to optimally select source material for critique is worth exploring for future research.

In the second cycle of our action research, we introduced the four argumentation schemes with the premise that students had epistemologically acknowledged their equal importance and value in the construction of knowledge. However, our observations revealed that, among these schemes, some students struggled to recognize the reliability of arguments based on authority until the end of Cycle 2. This indicated that these students had not fully developed a sophisticated epistemic belief system, one that is well able to weigh the value or reliability of disparate sources of knowledge (Hofer & Pintrich, 1997). Such developmental gaps in epistemic beliefs were noted by other researchers (e.g., Baytelman et al., 2020; Noroozi, 2022) who had examined the relationship between students’ epistemic beliefs and the quality of their argumentation. It was found that students with more sophisticated epistemic beliefs produced higher-quality arguments. Our study underscores the need to further emphasize discussions on diverse approaches to knowledge construction in future educational interventions. By encouraging a broader exploration of what constitutes reliable knowledge, we can help students move away from an over-reliance on authoritative statements as the sole or primary basis for a critique.

Another reflection was about textbook articles’ complexity of language and argument because these could have reduced students’ capacity for argument evaluation. Three of the five interviewees indicated they had invested considerable time in identifying the central argument and understanding supporting details. While all students ultimately achieved a good understanding of the text’s content, the extensive time required to reach the “comprehension stage” may have discouraged them from critically analyzing the article’s shortcomings, and translating their thoughts into written English. Future research may explore the potential of streamlining or simplifying source material to facilitate the critique process for EFL students.

Constrained by the course’s structured curriculum, we were unable to extend this pedagogical inquiry into Cycle 3. Should the research advance to Cycle 3, we will integrate computer-aided instructional tools such as Araucaria, as recommended by Walton (2013, 2016), with the purpose of furthering comprehension of argumentation schemes and critique skills.

# 4.3. Limitations of the action research

The present study acknowledges several limitations. First, comparisons of oral critique performance were necessitated across three different groups of students, as requesting a single group to present three times or having all groups present three times was impractical. Although participants from the three groups share the same first language and level of professionalization as detailed in the methods section, individual traits, motivational levels, prior literacy practices, and group dynamics would have exerted influence on the outcomes observed.

Second, the general critical questions adapted from Swales and Feak (2012) center on the reliability of information sources, authors’ motives and purposes, and the quality of arguments. We did not include questions specifically targeting counterarguments and counterevidence for practical reasons. We assumed the stark absence of counterarguments or counterevidence in the three textbook articles might become a main focus or a cognitive shortcut for critique, inadvertently overshadowing other critical aspects. In retrospect, the exclusion of such questions may limit students’ understanding of the pivotal role counterarguments and counterevidence play in argument critique. Future scholars can develop a more inclusive framework for critical evaluation of arguments.

Third, Cycle 2 in the study was limited to introducing students to only four argument schemes in depth. Recognizing that it would be pedagogically impractical to teach all argumentation schemes and actual arguments do not always conform neatly to specific schemes, an important consideration arises: Would students benefit more from a course concentration on general critical questions with extended practice? To gain more insight into the most effective strategies for enhancing students’ critiquing skills, future investigations could explore comparisons of the effectiveness of teaching general critical questions versus scheme-specific critical questions within the same student population.

Fourth, the scoring rubric in this action research for assessing students’ argument evaluation skills could have been better designed to measure students’ application of the instructional content. Since the rubric’s substance dimension emphasized the persuasiveness of argumentation but instruction emphasized critique, the rubric was insufficiently aligned with what was taught versus what was assessed. This limitation in the rubric may partly explain why students’ individual critique essay writing did not reach the hoped-for level overall. A rubric that could deliver a more precise assessment of each student’s ability to internalize and apply the learned argumentation schemes would draw more data from the critique essay. Considering the close connection between persuasiveness and critique, future studies could also explore the alignment issue in greater depth.

A final limitation of this study was that the relatively small sample size may result in insufficient statistical power to detect significant effects. Consequently, the generalizability of the study’s findings may be constrained. Increasing the sample size would enhance statistical power and possibly deliver additional insight valuable for instructing EFL learners during their university years.

Despite the limitations noted above, this two-cycle action research addresses an urgent need to enhance university students’ critical thinking skills and illustrates how general and scheme-relevant critical questions can act as effective scaffolding for students to refine their ability in argument evaluation. Although argumentation is a complex set of skills that can be challenging to teach (Song et al., 2020), it appears that assigning oral and written critiques is what EAP practitioners will find useful for improving students’ reasoning abilities in the current climate of universal literacy and Internet networking.

# 5. Conclusion

Across various disciplines, the ability to critique is essential for student success at university and as a prelude to career advancement through publishing academic research. Acknowledging the prevalent weaknesses in critiquing skills among Chinese students, this action research addresses the need to cultivate Chinese students’ critiquing skills through a focus on argument evaluation. By employing a critical-question approach, the research demonstrates the successful integration of classroom interventions aimed at nurturing strong argument evaluation skills within specific institutional contexts through meticulously designed classroom activities. EAP practitioners in similar teaching environments might further explore the effectiveness of diverse strategies to incorporate general and/or scheme-relevant critical questions into their instruction, thereby determining which strategies prove most beneficial for enhancing their students’ argument evaluation skills.

# Declaration of interest

None.

# CRediT authorship contribution statement

Yao Du: Writing – review & editing, Writing – original draft, Visualization, Validation, Supervision, Software, Resources, Project administration, Methodology, Investigation, Funding acquisition, Formal analysis, Data curation, Conceptualization. Xinjie Gao: Writing – review & editing, Visualization, Software, Formal analysis, Data curation.

# Acknowledgments

We extend our heartfelt gratitude to the JEAP reviewers and the editorial team for their invaluable and constructive feedback. Our thanks also go to Prof. Xu Hongchen and Ann Marie Ross for their substantial support during the manuscript preparation.

Appendix 1. Annotation protocols for students’ oral critique   

<html><body><table><tr><td>Critique focus</td><td colspan="2">Annotation</td></tr><tr><td>Overall argumentation in the text.</td><td colspan="2">Reliability of sources of information Author&#x27;s motives or the purposes for writing</td></tr><tr><td>Argument schemes</td><td colspan="2">Quality of premises and conclusions</td></tr><tr><td></td><td>Causal</td><td>Causal mechanism</td></tr><tr><td></td><td></td><td>Causal efficacy</td></tr><tr><td></td><td></td><td>Applicability</td></tr><tr><td></td><td></td><td>Intervening factors</td></tr><tr><td></td><td>Example</td><td></td></tr><tr><td></td><td></td><td>Accuracy</td></tr><tr><td></td><td></td><td> Relevancy</td></tr><tr><td></td><td></td><td>Typicality</td></tr><tr><td></td><td>Comparison</td><td>Special circumstances</td></tr><tr><td></td><td></td><td>Criterion evaluation</td></tr><tr><td></td><td></td><td>Comparison</td></tr><tr><td></td><td></td><td>Difference</td></tr><tr><td></td><td></td><td>Additional criterion</td></tr><tr><td></td><td>Position</td><td>Position</td></tr><tr><td></td><td></td><td> Source reliability</td></tr><tr><td></td><td></td><td>Assertion</td></tr></table></body></html>

Appendix 2. Scoring rubric for students’ critique essay writing   

<html><body><table><tr><td>Dimensions</td><td>Level A(10-9 points)</td><td>Level B(8-7 points)</td><td>Level C(6-5 points)</td><td>Level D(4-3 points)</td><td>Level E(2-1 points)</td></tr><tr><td>Structure</td><td>The paper is well-organized and sequenced. It starts with an introduction part in which the author produces a summary of the original text before providing his/ her evaluative comments; the body part discusses the article&#x27;s strengths and/or weaknesses; the conclusion part provides an overall</td><td>The article critique is generally well-organized and flows well.</td><td>The article critique is not well-organized, and ideas could be better sequenced.</td><td>The organization is weak and ideas are not sequenced well.</td><td>The article critique is not properly organized and just contains a piecemeal list of phrases or sentences with no coherence.</td></tr><tr><td>Dimensions Substance</td><td>evaluation of the article. Level A(15-13 points) The article critique displays an excellent persuasive</td><td>Level B(12-10 points) The article critique displays a reasonably good</td><td>Level C(9-7 points) The article critique displays a recognizable argument but is limited in effectiveness, i.e., it states a</td><td>Level D(6-4 points) The article critique</td><td>Level E(3-1 points) The article critique</td></tr><tr><td></td><td>argument, i.e., it states a clear point of view and gives good and sufficient reasons to support it. The reasonings are clearly explained and well- elaborated by using convincing information/ examples. The critique may present reasonable opposing view (s) and also refute the opposing view (s) appropriately, though these are not required.</td><td>argument, i.e., it states a reasonably clear point of view and gives generally plausible reasons to support it. The reasons are explained and elaborated to some extent, though not enough. There may be one or two inconsistencies or pieces of irrelevant information.</td><td>point of view and gives one or two good reasons to support it. The reasons are not explained or not supported in a fully coherent way. The reasons may be of limited plausibility and some inconsistencies exist.</td><td>displays a minimally acceptable argument paper, with major gaps in reasoning, i.e., it states some sort of a point of view, but it is vague or general. No reasons are provided for the point of view, or the reasons given are unrelated to or inconsistent with the point of view.</td><td>displays no argument, i.e., the content of the paper is not relevant to the task.</td></tr></table></body></html>

# References

Baytelman, A., Iordanou, K., & Constantinou, C. P. (2020). Epistemic beliefs and prior knowledge as predictors of the construction of different types of arguments on socioscientific issues. Journal of Research in Science Teaching, 57(8), 1199–1227. https://doi.org/10.1002/tea.21627   
Burns, A. (2011). Doing action research in English language teaching: A guide for practitioners. Beijing: Foreign Language Teaching and Research Press.   
de Chazal, E., & Moore, J. (2012). Oxford EAP: A course in English for academic purposes (advanced/CI). Oxford: Oxford University Press.   
Dong, T., Anderson, R. C., Kim, I.-H., & Li, Y. (2008). Collaborative reasoning in China and korea. Reading Research Quarterly, 43(4), 400–424. https://doi.org/ 10.1598/rrq.43.4.5   
Du, Y. (2019). Effect of an EAP unit on EFL students’ effective and appropriate source-use skills. Journal of English for Academic Purposes, 40, 53–73.   
Hitchcock, G., & Hughes, D. (1989). Research and the teacher: A qualitative introduction to school-based research. London: Routledge.   
Hofer, B. K., & Pintrich, P. R. (1997). The development of epistemological theories: Beliefs about knowledge and knowing and their relation to learning. Review of Educational Research, 67(1), 88–140.   
Hogan, K., & Maglienti, M. (2001). Comparing the epistemological underpinnings of students’ and scientists’ reasoning about conclusions. Journal of Research in Science Teaching, 38(6), 663–687. https://doi.org/10.1002/tea.1025   
Koehler, J. J. (1993). The influence of prior beliefs on scientific judgement of evidence quality. Organizational Behavior and Human Decision Processes, 56, 28–55.   
Kuzborska, I., & Soden, B. (2018). The construction of opposition relations in high-, middle-, and low-rated postgraduate ESL Chinese students’ essays. Journal of English for Academic Purposes, 34, 68–85. https://doi.org/10.1016/j.jeap.2018.03.013   
Larson, A. A., Britt, M. A., & Kurby, C. A. (2009). Improving students’ evaluation of informal arguments. The Journal of Experimental Education, 77(4), 339–365. https://doi.org/10.3200/jexe.77.4.339-366   
Leitao, ˜ S. (2000). The potential of argument in knowledge building. Human Development, 43, 332–360.   
List, A., & Oaxaca, G. S. C. (2024). Comprehension and critique: An examination of students’ evaluations of information in texts. Reading and Writing: An Interdisciplinary Journal, 37(3), 641–671. https://doi.org/10.1007/s11145-023-10417-3   
Macagno, F., & Konstantinidou, A. (2013). What students’ arguments can tell us: Using argumentation schemes in science education. Argumentation, 27(3), 225–243. https://doi.org/10.1007/s10503-012-9284-5   
Macknish, C. J. (2011). Understanding critical reading in an ESL class in Singapore. TESOL Journal, 2(4), 444–472. https://doi.org/10.5054/tj.2011.269747   
Morrow, D. R., & Weston, A. (2019). A workbook for arguments: A complete course in critical thinking. Indianapolis: Hackett Publishing Company, Inc.   
Münchow, H., Tiffin-Richards, S. P. P., Fleischmann, L., Pieschl, S., & Richter, T. (2023). Promoting students’ argument comprehension and evaluation skills: Implementation of two training interventions in higher education. Zeitschrift Fur Erziehungswissenschaft, 26(3), 703–725. https://doi.org/10.1007/s11618-023- 01147-x   
Neuman, Y., & Weizman, E. (2003). The role of text representation in students’ ability to identify fallacious arguments. Quarterly Journal of Experimental Psychology, 56 (5), 849–864. https://doi.org/10.1080/02724980244000666   
Newell, G. E., Beach, R., Smith, J., & VanDerHeide, J. (2011). Teaching and learning argumentative reading and writing: A review of research. Reading Research Quarterly, 46(3), 273–304. https://doi.org/10.1598/rrq.46.3.4   
Noroozi, O. (2022). The role of students’ epistemic beliefs for their argumentation performance in higher education. Innovations in Education & Teaching International, 60(4), 501–512. https://doi.org/10.1080/14703297.2022.2092188   
Nussbaum, E. M., Dove, I. J., Slife, N., Kardash, C. M., Turgut, R., & Vallett, D. (2019). Using critical questions to evaluate written and oral arguments in an undergraduate general education seminar: A quasi-experimental study. Reading and Writing: An Interdisciplinary Journal, 32(6), 1531–1552. https://doi.org/ 10.1007/s11145-018-9848-3   
Nussbaum, E. M., & Edwards, O. V. (2011). Critical questions and argument stratagems: A framework for enhancing and analyzing students’ reasoning practices. The Journal of the Learning Sciences, 20(3), 443–488. https://doi.org/10.1080/10508406.2011.564567   
Pei, Z., Zheng, C., Zhang, M., & Liu, F. (2017). Critical thinking and argumentative writing: Inspecting the association among EFL learners in China. English Language Teaching, 10(10), 31–42. https://doi.org/10.5539/elt.v10n10p31   
Reznitskaya, A., Kuo, L., Clark, A., Miller, B., Jadallah, M., Anderson, R. C., & Nguyen-Jahiel, K. (2009). Collaborative reasoning: A dialogic approach to group discussions. Cambridge Journal of Education, 39(1), 29–48. https://doi.org/10.1080/03057640802701952   
Schiappa, E. (1995). Introduction. In E. Schiappa (Ed.), Warranting assent: Case studies in argument evaluation. Albany: State University of New York Press (pp. ixxxviii).   
Song, Y., Chao, S.-F., & Attali, Y. (2020). Exploring the effect of a scaffolding design on students’ argument critique skills. Informal Logic, 40(4), 605–628. https://doi. org/10.22329/il.v40i4.6034   
Song, Y., Deane, P., & Fowles, M. (2017). Examining students’ ability to critique arguments and exploring assessment and instructional implications. (Research Report No. RR17-16). Princeton, NJ: Educational Testing Service.   
Song, Y., & Ferretti, R. P. (2013). Teaching critical questions about argumentation through the revising process: Effects of strategy instruction on college students’ argumentative essays. Reading and Writing: An Interdisciplinary Journal, 26(1), 67–90. https://doi.org/10.1007/s11145-012-9381-8   
Stanovich, K. E., & West, R. F. (2007). Natural myside bias is independent of cognitive ability. Thinking & Reasoning, 13(3), 225–247. https://doi.org/10.1080/ 13546780600780796   
Swales, J. M., & Feak, C. B. (2012). Academic writing for graduate students: Essential tasks and skills (3rd ed.). Ann Arbor: University of Michigan Press.   
Visser, J., Lawrence, J., Reed, C., Wagemans, J., & Walton, D. (2021). Annotating argument schemes. Argumentation, 35(1), 101–139. https://doi.org/10.1007/ s10503-020-09519-x   
von der Mühlen, S., Richter, T., Schmid, S., & Berthold, K. (2018). How to improve argumentation comprehension in university students: Experimental test of a training approach. Instructional Science, 47(2), 215–237. https://doi.org/10.1007/s11251-018-9471-3   
von der Mühlen, S., Richter, T., Schmid, S., Schmidt, E. M., & Berthold, K. (2016a). Judging the plausibility of arguments in scientific texts: A student-scientist comparison. Thinking & Reasoning, 22(2), 221–249. https://doi.org/10.1080/13546783.2015.1127289   
von der Mühlen, S., Richter, T., Schmid, S., Schmidt, E. M., & Berthold, K. (2016b). The use of source-related strategies in evaluating multiple psychology texts: A student-scientist comparison. Reading and Writing: An Interdisciplinary Journal, 29(8), 1677–1698. https://doi.org/10.1007/s11145-015-9601-0   
Walton, D. (1996). Argument schemes for presumptive reasoning. Mahwah, NJ: Erlbaum.   
Walton, D. (2008). Informal logic (2nd ed.). Cambridge: Cambridge University Press.   
Walton, D. (2013). Methods of argumentation. New York: Cambridge University Press.   
Walton, D. (2016). Argument evaluation and evidence. Switzerland: Springer International Publishing.   
Walton, D., Reed, C., & Macagno, F. (2008). Argumentation schemes. New York: Cambridge University Press.   
West, R. F., Toplak, M. E., & Stanovich, K. E. (2008). Heuristics and biases as measures of critical thinking: Associations with cognitive ability and thinking dispositions. Journal of Educational Psychology, 100(4), 930–941. https://doi.org/10.1037/a0012842   
Wiley, J. (2005). A fair and balanced look at the news: What affects memory for controversial arguments? Journal of Memory and Language, 53(1), 95–109. https://doi. org/10.1016/j.jml.2005.02.001   
Wissinger, D. R., & De La Paz, S. (2016). Effects of critical discussions on middle school students’ written historical arguments. Journal of Educational Psychology, 108 (1), 43–59. https://doi.org/10.1037/edu0000043