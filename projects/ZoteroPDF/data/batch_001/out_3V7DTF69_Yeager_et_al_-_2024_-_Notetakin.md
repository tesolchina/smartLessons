# Notetaking as validity evidence: A mixed-methods investigation of question preview in EAP listening assessment

Rebecca Yeagera\*, GoMee Parkb, Ray J.T. Liaoc

a University of Iowa, USA b University of Texas Rio Grande Vally, USA c National Taiwan Ocean University, Taiwan

# ARTICLEINFO

# ABSTRACT

Handling Editor: Hilary Nesi

# Keywords:

Listening   
Test format   
Testwise strategies   
Washback   
Cognitive validity

Recent scholarship has questioned the cognitive validity of listening tests with preview, in which test-takers can see test questions before listening. This study mined student notes for evidence of cognitive processes in listening tests with and without preview, using a mixed-methods design that explored the effect of test format on notetaking behaviors. Qualitative analysis indicated that students who previewed items were more likely to systematically omit information, highlight previewed keywords, and engage in shallower structural representation. Conversely, KruskalWallis tests revealed that students who listened without preview took more notes, especially of main ideas and details, and had better coverage of the lecture. However, correlation and hierarchical linear regression analyses found these notetaking achievements did not predict higher scores in the no-preview condition, while in the preview condition, only note quantity and focus on minor ideas predicted scores. Both strands of data suggest that students' cognitive processes were shaped by the format of the exam they experienced. These findings may bear on validity arguments for listening assessment and inform the way that language instructors prepare their students for academic listening.

# 1. Introduction

In university classrooms, students engage in many tasks which involve istening. Foremost among these is the academic lecture, in which students typically listen, take notes, ask clarification questions, and review those notes for study later (Lynch, 2011 Siegel. 2020). Listening instruction in English for Academic Purposes (EAP) contexts might be expected to focus on these key skill. However, when listening skill are ssessed in the context, asesments are often stripped f notetaking, questioning, and review components in an attempt to get at a "pure' istening construct (O'Grady, 2021). Comprehension is often measured through multiple-choice questions (McQs) answered during or immediately afer the lecture (Rukthong, 2021). However, the inauthenticity of this task type raises questions about the validity of claims made on the basis of such asessments, and further about the consequences of preparing students to expect test formats they will not encounter again outside the EAP classroom.

This study is concerned with the impact of test format on cognitive validity, in which the mental process engaged by the test match the mental processes engaged by authentic listening tasks (Weir, 2005). Threats t cognitive validity arise from the introduction of "testwise strategies" enabled by features of the test which diffe from the target language use environment (Cohen, 2007). Weir's (2005) socio-cognitive validity framework has been applied to istening assessment by Taylor and Geranpayeh (2013), in a model which combines test-taker characteristics and evidence of cognitive, context, scoring, consequential, and criterion-related validit to support claims about tes-taker istening ability. We have selected Taylor and Geranpayeh's (2013) model to undergird our study because it recognizes the importance of social context in interpreting scores. In an academic context, learners may have acess to reading materils, ofice hours, and ther dicussion opportunities before taking tets. In a language tetig contex, such reources are rarely available, which may reasonably be expected to impact the cognitive processes that test-takers employ.

This study explores the impact of one common listening task type, question preview, on cognitive validity. In question preview, multiple-choice questions are presented to test-takers before they listen in full fllpreview) or in part (stem-preview or optionpreview present only the question stem or only the multiple-choice options and reveal the rest of the item after listening). We are interested in this task type beause t ca be found in AP asesments and textbooks around the world, bt there is ittl gidance from the literature concerning its validity in EAP contexts.

The existing research on preview mostly focuse on its mpact on dffclty and affect In terms of difficulty, full-preiew and stem preview appear to perform similarly (limura, 2010; Koyama et al., 2016; Li et a., 2017; O'Grady, 2021; Yanagawa & Gree, 2008) but full-preview tends to be easier than option-preview (Koyama et al., 2016; Sadeghi & Zeinali, 2015; Yanagawa & Green, 208) or no-preview (limura, 2010; Koyama et al., 2016; OGrady, 2021). In other words, the advantage of preview primarily lies in access to the question stems, with access to the options providing only marginal benefit I terms ofaffect, students generally expresspreferences for preview (limura, 2010; Li et al., 2017). However, the impact of this tas type on cognitive processing is unclear. Therefore, our study explores the cognitive validity of listening tes with and without preview through mixed-methods analysis of student notes and test scores.

# 2. Literature review

# 2.1. Preview and cognitive validity

Theoretically, preview could impact cognitive validity in one of two ways. First, it is possible that preview could simulate the processes activated in an academic context, allowing students to predict and plan by serving as a replacement for other classroom resources which ad the listener in preparing to learn. There is some evidence that preview can outperform other prelistening activities such as vocabulary activities (Chang & Read, 2006) and prereadings (Alavi & Janbaz, 2014). However, both vocabulary activities (Berne, 1995; Madani & Kheiradeh, 2022) and prereadings (Chang & Read, 2006) sometimes do as well or better than preview. In general, then, it would aear that preview can function as a listening resource, but other activities may accomplish this same goal. The other posbilit is that preview could alter the processes employed by learners during the istening task to the extent that thir cognitive processes do not match those of the listening construct. This effct would be undesirabl, as it would make it difficult to extrapolate from a student's performance on a istening test to their future peformance in a universty clasroom t couldalso mislead students and language instructors to prioritize test preparation strategies which wil not be transferable to a university context.

Several studies have investigated strategy use in tests with preview through surveys or stimulated recall. Many have uncovered troubling patterns, such as using preview t sectively attend only to points in th ctures that will e assese (Field, 2011), guesing or eliminating options (Cheng, 2004; Field, 2012), and aural scanning for keywords without comprehending the structure of the text (Field, 2011, 2012). Badger and Yan (2012) in fact discovered that these types of testwise trategies were used equall by L1 and L2 listeners when completing a istening test with preview, sugesting that tes format may have more impact than test-taker charateristics on strategy use. On the other hand, Innami and Koizumi (2022) compared metacognitive survey responses with performance on while-listening-performance (WLP) tests with preview and post-listening-performance (PLP) tests without preview, and found that only scores on the WLP test were related to planning and evaluation strategie. They interpret these results as evidence that students may have used the questions in the wLP format to help them plan for the listening task.

Cognitive processes during listening tests have also been explored via eye-tracking and Functional Near-Infrared Spectroscopy (fNIRS; Aryadoust et al., 2022; Zhai & Aryadoust, 2022). These studies indicate that test-takers exhibit differences in eye gaze behavior, fixations, and neural activity when taking a WLP test with preview and a PLP test without preview. Although some of these results may be explained by the difference in response timing, many of the behavioral patterns during WLP tests would not be possible without access to the questions during the lecture. These results provide evidence that preview may enable and reward listening strategies which are not possble in EAP contexts. Aryadoust et al. (2022), for example, observed that \*the gaze behavioral patterns exhibited during the WLP tests suggested that the test-takers adopted keyword matching and 'shallow listening," and further that "test-takers isplayed lower activity levels acros brain regions supporting comprehension during the WLP tests relative to the PLP tests' (p. 56). Together, these studies indicate reason for concern that some cognitive processes enabled by preview may not be transferable to academic listening tasks.

# 2.2. Notetaking as validity evidence

One data source for observing the impact of preview on cognitive processes has been underexplored: notetaking. Notetaking is notoriously difficult to analyze because it is known to vary widely across learners and contexts. Variables impacting note quantity include lecture topic and speed (Siegel, 2022a), accessto visuals (Cubilo & Winke, 2013), and task type (Cubilo, 2011; Oakhill Davies, 1991). Confounding these factors, students may deploy a range of eficiency strategies which make later interpretation of notes difficult, including abbreviations, symbols, and translanguaging (Zhou et al., 202). onsidering these complications, it i perhaps not surprising that notetaking has been pushed to the side in the search for evidence of cognitive processing on listening exams.

However, this oversight is unfortunate. Notetaking is a valuable source of data about student comprehension and can be used as an assesment tol in its own right (Nakayama et al., 2017; Song, 2011). And although the features of notetaking listed above can make it difficult to analyze quantitatively, it yields itself readily to qualitative analysis (Cushing, 1991). More importantly, evidence of notetaking behavior during a istening test should be systematically collected as part of test validation. Test formats which reward empirically-supported notetaking choices should be favored over test formats which reward testwise notetaking strategies. This evi. dence should be evaluated as part of an ongoing attempt to ensure positive washback.

In order to identif the nottking behaviors that tests should eek to elicit, it i firt nessary to survey the liteatre on academ notetaking in both L1 and L2 contexts. In L1 contexts, the benefits of notetaking have been theorized to fall into two categories: encoding (the process function) and external storage (the product function; Kim, 2018). Encoding refers to the advantages that arise when students are forced to selectively attend to the key points of alecture, paraphrase, and visually represent its structure. The external storage function of notes allows learners to review key points lter. Meta-analyses have confirmed moderate effects for encoding and strong effects for external storage (Kobayashi, 2005, 2006). In L1 academic contexts, notetaking appears to aid comprehension particularly where the test is delayed (Chen et al, 2017; Kim, 2018), the task is productive (Kobayashi, 2005; Oakhill & Davies, 1991), or the content is unfamiliar (Brobst, 1996), with main ideas predctig succes better than total notations (Northern et al., 2023). Notation of details appears to matter comparativelylittle on immediate tests, but becomes important on cumulative exams (Kiewra et al., 1987).

Even in the 21st century, failure to learn notetaking skillscontinues to impact student success. Although many digital resources exist today that offr improved means of external storage, the proces function of notetaking appears to its besteffect when students must selectively atend to main ideas and represent tructure visually on paper i real time. Multiple meta-analyses have confirmed a significant advantage for handwritten notes over typed ones in classoom context (Allen et al., 2020; Lau, 2022). Although this distinction largely disappears in laboratory studies where distractions are eliminated from devices, longhand notetaking is stil su. perior for longer lctures and when class grades are the measurement variable (Voyer et al., 2022). Guided notes or partial outlines provided by the instructor, do improve performance short-term, but risk creating a dependence on instructional resources which may not always be available (Chen et al., 2017; Konrad et al., 2009). While it i becoming more commonplace for instructors to post thir slides online, students who rely solely on instructor slides for review miss out on the encoding function of notetaking (Kim, 2018); similarly, Wong and Lim (2023) found that students who snapped photos of lecture slides significantly underperformed students who took handwritten notes. Another resource that i becoming popular is lecture capture technology, whic allows lectres to be recorded and shared online. This technology is popular among students, particularly international students (Cortinhas, 2017), but the time-consuming nature of rewatching lectures has led some students to complain about opportunity cost compared to other study methods (Fletcher, 2023). Scholars recommend that lecture capture be used judiciously to supplement notetaking rather than as a replacement for it (Nordmann et al., 2022). Overall, whil educational technology has changed the resources that are available to students in university contexts, none of these resources obviate the need to build independent notetaking skills.

These findings from L1 contexts are generally replicated in L2 contexts as well. The preponderance of evidence suggests that L2 students perform better on tests when allowed totake notes (Carrll, 2007; Hayati & Jalilifar, 2009; Jin & Webb, 2023; Kim, 2023), especially ater instruction in notetaking trategies (Siegel, 2020; Yang & McAllister, 2023). As in L1 contexts, measures of content (Dunkel, 1988) and structure (Chaudron et al., 1994; Cushing, 1991) seem to be more meaningful than measures of note quantity. Further, the benefits of notetaking are more pronounced for productive tass (Cubilo & Winke, 2013; Liu & Hu, 2012; Song, 2011), and where review is allowed (Carrll, 007; Hayati & Jalilifar, 2009). As in 1 contexts, L2 students perform beter (Lee, 2020) and report greater self-efficacy (Siegel, 2022b) when notes are handwritten rather than typed.

Surprisingly, however, some L2 studies found that notetaking had no impact on score (Clark et al., 2014; Sadeghi & Zeinali, 2015), and in one study students scored lower ater being forced to take notes (Hale & Courtney, 1994). These findings may be due to the use of different comprehension tasks. The L2 studies above with positive associations for notetaking used productive tasks or MCQs without preview. In one study, notetaking was associated with summary scores but not with MCQ scores (Liu & Hu, 2012). Among the three L2 studies we identified which explicitly used preview tasks, two found no relationship between notetaking and score (Clark et al., 2014; Sadeghi & Zeinali, 2015), and one found no efect when notetaking was allowed and negatie efects when i was forced (Hale & Courtney, 1994). These studies offer preliminary evidence that notetaking may not be rewarded on preview tasks.

From this brief review, we can observe a few general principles for notetaking in academic contexts: notes that represent the structure of key ideas on paper may lead to higher scores, especially on tasks which are productive and allow for review. Unfortunately, L2 notetakers almost universally underperform L1 notetakers in these contexts, especially when it comes to capturing main ideas (Asaly-Zetowi & Lipka, 2019; Clerehan, 1995; Olsen & Huckin, 1990), organizing notes to replicate the macrostructure of the text (Faraco et al, 2002; Olsen & Huckin, 1990), and self-fficacy (Deselle & Shane, 2019; Dunkel & Davy, 1989). L2 dificulties in ac. ademic notetaking likely involve a complex interplay of factors, including lingustic challenges and cultural expectations, but may be exacerbated by a lack of practice on the types of listening tasks that are most common in the university. These studies underscore the lack of preparation that 2 students have for the demands of notetaking in universty contexts, and motivate a closer lok at the impact of test tasks on the way that learners conceive of and prepare for EAP listening.

# 2.3. Theoretical framework

This study builds on Field's (2013) model f istening comprehension, which is referenced in other studies on cognitive validity in listening asessment (Holznecht et al., 2017; Rukthong, 2021), and was specifically developed through examination of the diferences between istening processes in tasks with and without preview (Field, 2012). In his model, the final stage in listening comprehension includes four discourse-construction proesses selectingdetrmining which ids are worthy of attention), intgratig (relating points to each other), monitoring (deciding whether incoming information makes sense against what has been heard before), and structur$e$ building (mapping the hierarchical relationship of ideas). We focus specificall on these processes because they are sometimes critically under-represented in tests that purport to measure academic lstening ability (Field, 2011; Holznecht et al., 2017).

# 2.4. Research questions

Our study eeks to explore the impact f tem-preview on student notes in an attempt to establish the cognitie validity of this task type. We have chosen to focus on stem-preview out of all the preview types because it is the type that has the most theoretical justification: by alowing acces to the question stems but not the response options stm-preview may plausibly becompared to the use of guided notes or astudy guide, both resources seen in university contexts (limura, 2010; OGrady, 2021; Yanagawa & Green, 2008)

Accordingly, this study investigates student notes for evidence of cognitive processes in listening tests with and without stempreview. This research agenda is addressed through three nested lines of inquiry:

1. What evidence of discourse-construction processes is discernible in student notes with and without preview?   
2. Which ideas (main, major, minor, and detail) are selected and recorded most frequently instudent notes with and without preview?   
3. What is the relationship between ideas in notes and test scores with and without preview?

# 3. Methods

Our study adopted a convergent mixed-methods design (Creswell & Plano Clark, 2018), depicted in Fig. 1. Through qualitative analysis of text and images (Saldana, 2015), we sought to establish an overarching view of student notetaking choices with and without acces to preview. Qualitative analys enabled us to observe the interubjectivity of student nottaking trategie, focusing onthe four discourse-construction processes identified by Field (2013). These observations were then supported by quantitative analysis of the first of those processes,selecting. Finally, we investigated the relationship between selection choices and tet scores. Both strands of analysis converge, presenting an overall depiction of test-taking processes across conditions.

# 3.1. Research context

This data was collcted at a large public universit in 2019 while exploring a possile revision to a local EAP placement exam. At the time of data collction, the listning portion of the exam included two 10-min lectures followed by eight multiple-choice questions each. Historically, the placement test had contained a brief list of vocabulary words over K8 in Lex Tutor Compleat Vocab Profile (Cobb, 2020), which provided some spelling support, but did not include any definitions or pronunciation information. We wanted to investigate the impact of removing the vocabulary list and adding stem-preview to determine which format would elicit the most ecologically valid test behaviors. An earlier study relying on the same dataset focused on item diffculty, item type, and item discrimination (Yeager & Meyer, 2022). Instruments and analysis from that study are available on the Open Science Framework (OSF): https://osf.io/7x5yd/?view_only $=$ 13b96a9619214f49ae4320fb8d23a305.

![](img/d60619185763936cf202570fb0b5e9304c65883d5249c85149f602749ac2f36e.jpg)  
Fig. 1. Research design.

# 3.2. Instruments

Two 10-min lectures with eight MCQs each were developed following specifications for the placement exam. Both lectures were semi-scripted (Wagner & Wagner, 2016), included naturalistc oracy features including repair, redundancy, and hesitation phenomena (Taylor & Geranpayeh, 2013), and were edited in Audacity for sound quality and length (Audacity Team, 2019) Four MCQs were global items targeting main ideas and inference, and four were local items targeting detals and vocabulary. One additional item targeting a trivial detail was designed to explore the impact of preview on item type, a key focus of the first study, and was excluded from the current analysis. For the no-preview condition, words over K8 in the lecture were identified and listed at the top of ablank page for taking note (Cobb, 2020); students could not se the items until aftr listening. For the preview condition, stdents were able to read the question stems before istening, but could not see the options or answer the question until afe the lecture was completed. All lectures and items underwent two rounds of piloting and revision. Following Koyama et al. (2016), we calculated reliability and dependability estimates separately for each combination of lecture and condition. All materials and reports are available on OSF.

# 3.3. Participants

Notetaking samples and test scores $\displaystyle ( \mathtt { n } = 9 4 )$ were collected from consenting undergraduate students in eight intact listening classes. Students placed in these classes had a TOEFL score between 80 and 99 or an IELTS score between 6.5 and 7.5. Follwing local Institutional Review Board protocol for intact classoom research, we did not collct identifying information about students. However, registrar data from 2019 indicates that the majority of international students enrolled in Fall 2019 were from China $( 5 0 . 8 \% )$ , and that female-identifying students $( 5 2 . 9 ~ \% )$ outnumbered male-identifying students $( 4 7 . 1 ~ \% )$

# 3.4. Data collection

During the second week of the semester, Lecture A was administered to four classes with stem-preview, and to four classes without one week later, Lecture B was administred with preview condition counterbalanced. Al students were given two pages on which they were encouraged but not forced to take notes; the no-preview group had two blank shets, whil the preview group received one blank sheet and one preview sheet.

# 3.5. Qualitative analysis

To address RQ1 and RQ2, we undertook a qualitative analysis of student notes using ATLAS.ti (Mac Version 22.0.6.0). Provisional codes (Saldana, 2016) were developed based on a literature review of notetaking studies. Two researchers independently coded $1 0 0 \%$ of the data, and resulting codes were revised In second-cycle focused coding, codes were grouped into categories which aligned with the four discourse-construction processes described in Field (2013). Fig. 2 represents the outcome of four total rounds of coding and discussion completed by two researchers, in which codes and categories were combined, condensed, and finalized.

![](img/3a2d3182b9ddf24df133fff740526cfde21f30d76e91ce9c0c7c4e6848588659.jpg)  
Fig. 2. Finalized qualitative codes.

# 3.6. Quantitative analysis

To address RQ2 and RQ3, it was first necessary to analyze both lectures for propositional structure. We adopted procedures from Kiewra et al. (1987) and Song (2011), identifying each proposition as either a main idea, major idea, minor idea, or detail. Two researchers analyzed both lectures independently; absolute agreement for Lecture A was 0.95 and for Lecture B 0.98. Disagreements were resolved through discussion.

Next, student notes were transcribed and analyzed for evidence of these ideas at each level. To asst with accuracy, words that were unique to each proposition were identified and highlighted in student notes. Two researchers then independently rated $1 0 ~ \%$ of the data; inter-rater agreement was at $1 0 0 \%$ and subsequently, one researcher rated the remainder of the data.

Following Nakayama et al. (2017), each notetaking sample was further scored in two ways. First, we wanted to control for dif ferences in number of ideas across lectures. To accomplish this, we divided the number of ideas at each level i student notes by the umber of ides at that level in the lectre. This provided a measure of lecture coverage. Secondy, we wanted to control for differences in student writing fluency. We accomplished this by dividing the number of ideas students recorded at each level by the number of ideas they captured overall. This provided a measure of which level students focused on in their notes. We labelled these measures Coverage and Focus, respectively.

Quantitative analysis was conducted in Pss (IBM Corp, 2022). We ran nonparametric Kruskal-Wallis tests to investigate whether preview affected notestudents took at each level. Before conducting the analyse, we evaluated certain assumptions using the criteria outlined by Thondike and Thorndike-Christ (2009). These included checking: (1) the normality of data distribution, (2) the presence of multicollinearity, (3) the homoscedasticity of the data, (3) the normal distribution of reiduals, and (4) the identification of any significant outliers. Upon assessing the assumptions, the reults of the Shapiro-Wilk test for the normality of data distribution indicated that certain variables (e.g., Main, Minor, and Detail Totals, and Minor Coverage) did not fllow a normal distribution. Consequently, we opted to use Kruskal-Wallis tests instead of multivariat analysis of variance (MAnovA). In the Kruskal-Walls tests, the independent variables (treatment variables) were preview and no-preview test conditions, while the dependent variables (outcome var iables; summarized in Table 1) were Total Notations (TN) and Ideas Total (IT) Main, Major, Minor, and Detail Totals (T1, T2, T3, T4); Main, Major, Minor, and Detail Coverage (C1, C2, C3, C4); and Main, Major, Minor, and Detail Focus (F1, F2, F3, F4).

To understand how note-taking levels relate to listening performance in both preview and no-preview test conditions, we used correlation and herarchical linear regressin (HLR) analyses. While examining th assumptions for these analyses, we found no isues with multicollnearity, indicating that the predictor variables were not highly correlated. Notably, the tolerance scores for allthe predictor variables exceeded the recommended threshold of 0.1, ranging from 0.75 to 0.99. The scatterplots of standardized predicted values against standardized residuals indicated that our data exhibited homoscedasticity. Additionally, the residuals were normal distributed, as confirmed by the histogram and normal probabilit plots. We also checked for outliers using Cook's distance and found none. When conducting the HLR analyses, we used the outcome variables obtained from the Kruskal-Walli tests mentioned earlier as the predictor variables. The criterion variables in these HLR analyses were the listening test scores of the students.

# 4. Results

# 4.1. Qualitative results

Qualitative analysis revealed several salient patterns in student notes across the four dimensions of discourse-construction iden. tified by Field (2013). Within the category of selecting, two distinguishable pattrns emerged. First, students in the preview condition tended to omit mor ections f the leture from ther notes if no questions clearly addressed those sections , eture olution 2) These omissions weresystematic and predictable, while in the no-preview condition, omissions were inconsistent and unpredictable. Second, iferences emerged in student emphasis of notes (indicated visuall by underlining, circling, and staring words). Students in both conditions engaged in self-emphasis of words they had wrtten, though this was more common if notes were taken freestyle in the no-preview condition or on the blank page in the preview condition). Marked differences appeared, however, in terms of item-emphasis over half of the students in the preview condition underlined or circled keywords in the test booklet, indicating that they were relying on those words to help them make connections in the lecture. This behavior was uncommon in the no-preview condition for the frst lecture administration; however, in the second lecture administration, there was a notable uptick in students who attempted to employ item-emphasis in the no-preview condition by underlining or circling words in the heading, instructions, or vocabulary list $3 6 ~ \%$ Lecture A; $5 6 \%$ Lecture B). Fig. 3 displays representative examples of emphasis across conditions.

Table 1 Abbreviations, labels, and definitions for notetaking variables.   

<html><body><table><tr><td>Notetaking Variable Abbreviations</td><td>Notetaking Variable Labels</td><td>Notetaking Variable Definitions</td></tr><tr><td>TN</td><td>Total Notations</td><td>Total number of notations including words, abbreviations, and symbols</td></tr><tr><td>IT</td><td>Ideas Total</td><td>Total number of ideas referenced in student notes across all four levels; the sum of T1, T2, T3, and T4</td></tr><tr><td>T1</td><td>Main Idea Total</td><td>Total number of main ideas referenced in student notes</td></tr><tr><td>T2</td><td> Major Idea Total</td><td>Total number of major ideas referenced in student notes</td></tr><tr><td>T3</td><td>Minor Idea Total</td><td>Total number of minor ideas referenced in student notes</td></tr><tr><td>T4</td><td>Detail Idea Total</td><td>Total number of detail ideas referenced in student notes</td></tr><tr><td>C1</td><td> Main Idea Coverage</td><td>T1 divided by the number of main ideas in the lecture</td></tr><tr><td>C2</td><td> Major Idea Coverage</td><td>T2 divided by the number of major ideas in the lecture</td></tr><tr><td>C3</td><td> Minor Idea Coverage</td><td>T3 divided by the number of minor ideas in the lecture</td></tr><tr><td>C4</td><td>Detail Idea Focus</td><td>T4 divided by the number of detail ideas in the lecture</td></tr><tr><td>F1</td><td>Main Idea Focus</td><td>T1 divided by IT</td></tr><tr><td>F2</td><td>Major Idea Focus</td><td>T2 divided by IT</td></tr><tr><td>F3</td><td>Minor Idea Focus</td><td>T3 divided by IT</td></tr><tr><td>F4</td><td>Detail Idea Focus</td><td>T4 divided by IT</td></tr></table></body></html>

Integrating strategies were the most frequently represented in student notes across both conditions. There were only two codes in this category, but each was heavily used, with numbering being the most common,followed by arrows. Both of these strategies were frequently used in both conditions, but especially in the no-preview condition or in freestyle preview notes.

Student use of monitoring strategies appeared to be fairly constant across conditions. Four students used translanguaging in each condition likely the same four, judging from handwriting). Editing strategies, such as cros-outs and insertions, were frequent in both conditions, with slightly higher prevalence in the preview condition. Vocabulary strategies (such as guessing or phonetic spelling of unknown words) appeared to be consistent across conditions.

The greatest number of codes were clustered in the structure-building category, in which three salient pattrns emerged. First unstructured codes (random/messy or linear) were over-exemplified in the preview condition. Second, framing references (in. troductions and conclusions) were nearly absent in the preview condition. inall, subordination strategies (indentation, word clouds, brackets, section dividers, and cohesive devices) were used extensively in the no-preview condition; indentation, for instance, sometimes reached up to five levels. Conversely, indentation in notes taken under the question stems in the preview condition was extremely rare (only i examples) and never exceeded two levels. Notably, in Lecture B administration, there was a marked increase in the number of students in the preview condition who chose to take notes freestyle $2 3 \%$ Lecture A; $4 3 \%$ Lecture B). Multiple levels of indentation were sometimes observed in freestyle preview notes. Fig. 4 llustrates indentation use across both conditions.

# 4.2. Quantitative results

Table 2 details descriptive statistics of students' test scores and notes at each level across conditions.

Kruskal-Wallis tests revealed that test condition had significant effects on students' notes at each level, as indicated in Table 3. In particular, test condition had statistically significant effects at the $\mathtt { p } < 0 . 0 5$ level on IT $( \chi 2 ( 1 ) = 5 . 9 3 )$ , T1 $( \chi 2 ( 1 ) = 6 . 1 0 )$ 1, and C4 (x2(1) $= 5 . 8 9 \AA$ , and at the $\mathbf { p } < 0 . 0 1$ level on TN $( \chi 2 ( 1 ) = 9 . 2 3 )$ , T4 $( \chi 2 ( 1 ) = 7 . 0 8 )$ , C1 $( \chi 2 ( 1 ) = 8 . 2 4 )$ , C2 $( \chi 2 ( 1 ) = 6 . 8 6 )$ , and C3 $( \chi 2 ( 1 ) = 6 . 7 5 ) $ Specificall, the results of Kruskal-Wallis tts showed that the mean ranks for Total Notations, Ideas Total, Main Total, Detail otal, and Coverage measures at al four levels were significantly higher in the no-preview condition. In other words, students without preview tended to take more notes overall especially main ideas and detals, and had better coverage of ideas from the lecture cross all levels.

![](img/141416d8c7d9d50ce27d13165efb39d4b4a9701bcd294fd124d87044ab1c0e60.jpg)  
Fig. 3. Representative examples of item-emphasis and self-emphasis in preview and no-preview conditions.

![](img/119eefba11a9ada73f36ec3630e57062161fbb017b99dd048941be34f8d8a2c2.jpg)  
Fig. 4. Representative examples of subordination strategies in preview and no-preview conditions.

Table 2 Means and standard deviations of scores and notes in preview and no-preview conditions.   

<html><body><table><tr><td rowspan="3">Notetaking Variables</td><td colspan="4">Condition</td></tr><tr><td colspan="2">Preview (n = 55)</td><td colspan="2">No-Preview (n = 40)</td></tr><tr><td></td><td>SD</td><td></td><td>SD</td></tr><tr><td>Test Scores</td><td>5.43</td><td>1.79</td><td>4.90</td><td>2.09</td></tr><tr><td>TN</td><td>105.96</td><td>47.40</td><td>140.58</td><td>55.21</td></tr><tr><td>IT</td><td>29.91</td><td>12.55</td><td>36.88</td><td>14.60</td></tr><tr><td>T1</td><td>1.69</td><td>1.25</td><td>2.40</td><td>1.39</td></tr><tr><td>T2</td><td>10.16</td><td>4.57</td><td>11.80</td><td>4.39</td></tr><tr><td>T3</td><td>9.69</td><td>4.92</td><td>11.47</td><td>6.12</td></tr><tr><td>T4</td><td>8.40</td><td>5.07</td><td>11.25</td><td>5.25</td></tr><tr><td>C1</td><td>.37</td><td>.29</td><td>.54</td><td>.30</td></tr><tr><td>C2</td><td>.45</td><td>.19</td><td>.55</td><td>.17</td></tr><tr><td>C3</td><td>.41</td><td>.20</td><td>.52</td><td>.19</td></tr><tr><td>C4</td><td>.21</td><td>.11</td><td>.27</td><td>.13</td></tr><tr><td>F1</td><td>.05</td><td>.04</td><td>.06</td><td>.03</td></tr><tr><td>F2</td><td>.34</td><td>.09</td><td>.33</td><td>.08</td></tr><tr><td>F3</td><td>.32</td><td>.08</td><td>.30</td><td>.07</td></tr><tr><td>F4</td><td>.28</td><td>.12</td><td>.30</td><td>.09</td></tr></table></body></html>

To explore the relationship between levels of notes and listening scores, we adopted corrlation and HLR regression for both conditions. Corrlation analyses displayed in Table 4 show that in the preview condition, Total Notations, Minor Total, Minor Coverage, and Minor Focus in student's notes were statistically and positively correlated with test scores. In other words, in the preview condition, students scored higher if they took more notes overall, and especially if they focused on minor idea.

Finally, for both testcondtions all independent variables were entered into a stewise regression. The regresion analyses showed that Total Notations and Minor Idea Focus in the preview condition were the only significant predictors of score. The resulting models in Table 6 showed that these two variables explained $3 3 \%$ $( r = 0 . 5 8$ $R ^ { 2 } = 0 . 3 3 )$ of the variance in test scores of the preview condition, with sole contributions from Minor Idea Focus (Model 1, $\Delta R ^ { 2 } = 0 . 2 3 )$ and Total Notations (Model 2, $\Delta R ^ { 2 } = 0 . 1 0$

# 5. Discussion

This study explored the impact of stem-preview on cognitive validity in EAP listening assessment by examining student notes and test score in two conditions. In resonse to Q1, qualitive analysis of student notes reveled evidence of more dicourse construction processes in the no-preview condition. Similar integating and monitoring strategies were observed in both conditions, but distinctions emerged in selecting and structure-building. Students who previewed questions were much more likely to omit information if a question was not directly targeting it even f that information was structurally important to the lecture. They were aso more likely to highlight keywords in the stems than they were to highlight keywords in their notes. In terms of structure-building, students who previewed questions were more likely to adopt a random or linear notetaking style, and to omit introductory and concluding materil. Students who did not have accessto preview were more likely to incorporate subordination strategies such as indentation, word clouds, and brackets, with up to five levels of indentation observed in some no-preview samples.

Table 3 Kruskal-Wallis test of statistical significance (degree of freedom ${ \bf \Phi } = { \bf 1 } { \bf \Phi } .$   

<html><body><table><tr><td>Notetaking Variables</td><td colspan="2"> Mean rank</td><td>Kruskal-Wallis chi-square</td></tr><tr><td></td><td>Preview (n = 55)</td><td>No-Preview (n = 40)</td><td></td></tr><tr><td>TN</td><td>40.67</td><td>58.08</td><td>x2(1) = 9.23, p = 0.00**</td></tr><tr><td>IT</td><td>42.13</td><td>56.08</td><td>x2(1) = 5.93, p = 0.02*</td></tr><tr><td>T1</td><td>42.17</td><td>56.01</td><td>x2(1) = 6.10, p = 0.01*</td></tr><tr><td>T2</td><td>43.55</td><td>54.13</td><td>x2(1) = 3.43, p = 0.06</td></tr><tr><td>T3</td><td>44.47</td><td>52.85</td><td>x2(1) = 2.15, p = 0.14.</td></tr><tr><td>T4</td><td>41.60</td><td>56.80</td><td>x2(1) = 7.08, p = 0.00**</td></tr><tr><td>C1</td><td>41.13</td><td>57.45</td><td>x2(1) = 8.24, p = 0.00**</td></tr><tr><td>C2</td><td>41.69</td><td>56.68</td><td>x2(1) = 6.86, p = 0.01*</td></tr><tr><td>C3</td><td>41.75</td><td>56.60</td><td>x(1) = 6.74, p = 0.00**</td></tr><tr><td>C4</td><td>42.15</td><td>56.04</td><td>x2(1) = 5.89, p = 0.02*</td></tr><tr><td>F1</td><td>44.75</td><td>52.48</td><td>x2(1) = 1.83, p = 0.18</td></tr><tr><td>F2</td><td>49.81</td><td>45.51</td><td>x2(1) = 0.53, p = 0.45</td></tr><tr><td>F3</td><td>50.72</td><td>44.26</td><td>x2(1) = 1.27, p = 0.26</td></tr><tr><td>F4</td><td>45.57</td><td>51.34</td><td>x2(1) = 1.01, p = 0.31</td></tr></table></body></html>

$^ { * * } p < 0 . 0 1$ \*p < 0.05.

Table 4 Correlations between notes and test scores: Preview.   

<html><body><table><tr><td></td><td>TN</td><td>IT</td><td>T1</td><td>T2</td><td>T3</td><td>T4</td><td>C1</td><td>C2</td><td>C3</td><td>C4</td><td>F1</td><td>F2</td><td>F3</td><td>F4</td></tr><tr><td>Score</td><td>.30*</td><td>.23</td><td>.09</td><td>.17</td><td>.39**</td><td>.02</td><td>.11</td><td>.12</td><td>.34**</td><td>.11</td><td>.02</td><td>.20</td><td>.48**</td><td>.20</td></tr></table></body></html>

\*\*p < 0.01. \*p < 0.05.

Table 5 Correlations between levels of notes and test scores: No-Preview.   

<html><body><table><tr><td></td><td>TN</td><td>IT</td><td>T1</td><td>T2</td><td>T3</td><td>T4</td><td>C1</td><td>C2</td><td>C3</td><td>C4</td><td>F1</td><td>F2</td><td>F3</td><td>F4</td></tr><tr><td>Score</td><td>.01</td><td>.22</td><td>.11</td><td>.28</td><td>.28</td><td>.03</td><td>.07</td><td>.25</td><td>.30</td><td>.09</td><td>.01</td><td>.03</td><td>.25</td><td>.14</td></tr></table></body></html>

$^ { * } p < 0 . 0 5$

Table 6 Summary of stepwise regression model: Preview.   

<html><body><table><tr><td>Entry</td><td>Predictors</td><td>r</td><td>Total R2</td><td>R2 change</td><td>B</td><td>SE B</td><td></td></tr><tr><td>1</td><td>Minor Idea Focus (F3)</td><td>.48</td><td>.23</td><td>.23</td><td>9.67</td><td>2.43</td><td>.48**</td></tr><tr><td>2</td><td>Total Notations (TN)</td><td>.58</td><td>.33</td><td>.10</td><td>.01</td><td>.00</td><td>.32**</td></tr></table></body></html>

$^ { * * } p < 0 . 0 1$

In response to RQ2, quantitative analysis confirms our qualitative findings on the impact of preview on the proces of selecting; in other words, preview condition impacted the type and amount of ideas that students tended to select and record in their notes. We found that students without preview were significantly more likely to take more notes,capture more ideas overall (with more main ideas and details in particular), and have better coverage of ideas at all four levels.

However, in answer to RQ3, none of these advantages predicted scores in the no-preview condition. In the preview condition, students scored higher if they tok more notes overal, and specifically if they focused on minor ideas. This finding isespecially noteworthy considering that the items in the test were designed to focus on global and local ideas equall (with four questions about each). Regardless, students in the preview condition who focused on minor ideas tended to perform better on the test.

In addition to these findings to our research questions, our qualitative analysis revealed additional unexpected evidence of a washback effect acrosthe two test administrations. Students who experienced Lecture A with preview were more ikely to employ item-emphasis when they took Lecture  without preview a week ater: without aces to the uestion stems, some students reverted to circling keywords in the vocabulary lit. In other words, when students were led to expect that they could preiew questions, they were more likely to adopt a keyword approach to listening even on later tests without preview. The reverse was true for students who experienced Lecture A without preview; they were more likely to take notes for Lecture B freestyle onthe blank page that was provided rather than under the stems, even though they had acces to the questions. Students who opted to take notes freestyle were much more likely to apply no-preview-style strategies, such as self-emphasis, framing, and subordination. In other words, when students were given the opportunity to practice taking notes frestyle, they were more likely to continue taking notes freestyle and to ignore the question stems, even when they had aes to them later. This sugests that strategy use on listening tests may be susceptible to washback from test format, underlining the importance of giving students the opportunity topractice the formats that they wil experience in the real world.

Overall, these findings corroborate concerns that preview promotes passive listening strategies. Regarding selection of material to include in notes, Field (2011) oserves that in MQ tests with preview, \*much of the necessary decision-making is taken care of by item writers. They, not the listener, determine which points of information are relevant and which are not; and they reduce the information in the recording to a string of discrte points, regardless of how each contributes to the line ofargument (p. 110). In contrast, when students are forced to create a mental representation of the text on their own, they must make these choices independently. Selecting and structure-building strategies can also facilitat crucial aspects of the writing process, which could explain why productie tasks consistently reward notetaking, even when MCQ tasks do not (Liu & Hu, 2012). Rukthong (2021) observed that students took disorganized or linear notes when readying for an MCQ tak, but used indentation and arrows in their notes in preparation for a summary task. Our study contributes evidence that tasks without preview elicit more discourse-construction proceses, whil student with preview employed more passive strategies.

Specifically, thesereults corroborate reports that preview faclitates the use of testwise strateies such as keyword matching, aural scanning, and guesing (Badger & Yan, 2012; Field, 2011, 2012). This evidence may also partially explain findings that WLP tests with preview elicit eye gaze behaviors and neural activation patterns consistent with shallow processing (Aryadoustet l., 2022; Zhai & Aryadoust, 2022). In other words, the differences observed in these studies could be potentiall attributable to preview instead of response timing, in that the behaviors observed would not have been possible without access to the questions whil listening.

Beyond influencing which trategies students use, task format appears to reward those strategies differentily. Our results suggest that preview actually rewarded the use of shallower processing strategies, whilestudents in the no-preview condition who took more organized notes and focused on main ideas did not se gains i scores. These results may contextualize the findings from Innami and Koizumi (2022), who observed a relationship between self-reports of planing and evaluation strategies and WLP/preview test scores, but not PLP/no-preview scores. In our study, tes-takers used question stems to predict keywords and make selections about what to include in their notes, which can be interpreted as evidence of planning strategies, and these strategies were rewarded with higher scores. However, as planning strategies may differ by task, the planning strategies that are elicited on preview tests may not transfer outside the test context, raising concerns about score interpretability of tests with preview.

# 6. Implications

A number of implications can be drawn from this study for language pedagogy, test development, and asessment research. First language instructors in EAP contexts are often expected to prepare students for standardized listening asessments that employ question preview. In this context they may feelpresureo riti tet preparation over uthentic listening tas. This tensioncan e addressed through honest discussion with students about the limitations of listening tests with preview, and by varying classroom assesment types to include practice for standardized exams along with integrated listning tasks, while simultaneously providing instruction in notetaking strategies. Such instruction can be very effective (Siegel, 2020; Yang & McAllistr, 2023), especially when measured by productive tasks (Cubilo & Winke, 2013; Song, 2011), and need not be time-consuming, as even brief instruction on notetaking can be helpful (Smith et al., 2022). In some cases, instructors may wish to provide notetaking scafolding through the use of guided notes, which can range in pecificity from basic headings to cloze tasks (Chen et al., 2017; Cushing, 1991; Song, 2011). Konrad et al. (2009) recommends a "systematic fading" of guided notes, with greater specificity provided at the beginning of the semester which is gradually withdrawn until students are able to take organized notes on their own (p. 440). If lecture capture is provided, instructors should guide students in using it selectively to ill i gaps in their notes at home, rather than rewatching the entire lecture (Nordmann et al., 2022). Instructors can also find i valuable to collect notes periodicall in order to provide students with feedback on things like omissions and subordination strategies.

Second, n terms of est development, MCQ presentation formats should not be assumed to be interchangeable. Access to preview may impact the strategies that are available to test-takers, resulting in potential threats to cognitive validity and the interpretation of test scores. In particular, EAP test developers should ensure that listening tasks facilitate and reward listening behaviors which will transfer to academic listening contexts. Tasks which are observed to foster and reward testwise strategies should be questioned.

Finally, the time is ripe for a Copernican revolution in L2 notetaking research. Some have questioned the value of notetaking for L2 learners after finding only weak associations between notetaking and L2 standardized test scores (e.g., Clark et al., 2014). However, our findings suggest that this interpretation should be reversed: rather than questioning of the value of notetaking, we ought to tun our critical gaze around and question the appropriacy of istening tasks which do not facilitate good note. Notetaking has proven to be indicative of successon L2 integrated tass (Field, 2012; Liu & Hu, 2012; Rukthong, 2021; Rukthong & Brunfaut, 2020), and critical to successin university contexts (Asaly-Zetowi & Lipka, 2019; Clerehan, 1995; Olsen & Huckin, 1990). Further, notes provide a visible recod of a listrs cognie rc rn est ing rttion the rs and t o th otf listg (Faraco et al., 2002). As such, notes can serve as a form of cognitive validity evidence in listening assessment, supplementing other measures used for this purpose, including self-report, eye-tracking, or fNIRs. While self-report measures can reliably indicate test-takers self-knowledge and self-regulation, they may not be not as reliable in indicating behavior (Craig et al., 2020). onverely, if used in isolation, eye-tracking and fNIRs data may reveal behavioral patterns that are difficult to interpret for example, Holznecht (2019) observes that behaviors such as focusing and zoning out may appear indistinguishable in eye-tracking data unles supplemented with stimulated recall. Alongside these measures, notes can provide a more interpretable record of test-taker behavior, and thus should supplement these data sources in validit research. Whil notetaking datais les readily quantifiable, it yields itself readil to qualitative analysis. We should not shy away from analyzing notetaking because of it complexity, but mine those complexities for validity evidence.

# 7. Limitations, future research, and conclusions

The nature of our research context limits our confidence in making inferences about the efect of preview in other contexts. First because the listening syllabus emphasized the importance of notetaking, we would expect that, regardless of preview condition, students were more motivated to take notes than might be expected in some ther contexts. In adition, our sample was limited by class size, which limits the conclusions we can draw from our quantitatie analysis. Although our data met test assumptions for correlation and regression analyses, the quantitatie strand of our study ought to be interpreted as explorative and supportive of our qualitative indings, which onstute the main pilla of this study. Finll, the anonymous nature of our data collection prevents us from making claims about the interaction between performance and individual characteristics.

Two other features of our study limit the generalizability of our results, but provided an expected opportunity to observe a washback effct across conditions. irst, we allowed students in the preview condition a choice about whether to take notes under the stems or on the blank page, a choice that is pedagogicall recommended but is not offered on most standardized tests with preview. This decision probably minimized the differences we might otherwise expect to find between the two groups; however, i allowed us to observe that students who listened without preview first were more likely to ignore the preview questions and take notes freestyle later. Second, the vocabulary lis in the no-preview condition was an artifact of the dein of the local placment test, but i no offered in most standardized tests without preview. Because no pronunciation information or definitions were provided, this ist barely qualifies aocabulary resource, excet as n aid to spelling it was removed from laer ersions of the placment test. Hower, this feature permitted us to observe another unexpected washback efect, in that students who had previe first tried to use the vocabulary list as a preview substitute when they took the second lecture without preview later.

Despite these limitations, our findings motivate further exploration of preview and notetaking in EAP contexts. We hope that future studies will examine the impact of preview on notetaking in other contexts, varying the type of preview (preview types which include options might conceivably have a greater impact on notetaking), the asessment task (students might employ different notetaking strategies when expecting a MCQ test or a summary) and the time of testing (notetaking may impact immediate and delayed post-tes differently). Incorporating multilesources of evidence, such as follow-up interviews or eye-tracking, could triangulate evidence about notetaking strategy use across multile task formats, while collcting notetaking data from different demographics, including EFL, young learners, and L1 collge students, could provide more information about how test-taker characteristics interact with task characterisics. Beyond this, we hope to see investigation of even more inovative istening test formats which include opportunities for integrating source, discussion, and review. Recent scholarship has established the importance of asessing writing in EAP contexts through authentic tasks which facilitate positive washback. We hope the time has come to put listening assessment under similar inspection (Lynch, 2011).

The present study found evidence of major omissions, shallower structural representation, and minor idea focus in student notes when stems were previewed before alistening test. In the absence of preview, notes were more comprehensive, represented more levels of structure, and focused more on main deas. However, these behaviors were not rewarded when the task was scored. Given the importance of notetaking for student sucess in academic contexts, i is vital to ensure that listening asssment elicts and rewards cognitively valid notetaking behaviors. We hope to see more investigation of notetaking and a commitment to the development of test formats which better prepare students for success in academic contexts.

# Funding

This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.

# Disclosure statement

No potential conflict f interest was reported by the author(s). No generative Al tols were used at any stage of this research project.

# CRediT authorship contribution statement

Rebecca Yeager: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Super vision, Validation, Visualization, Writing - original draft, Writing - review & editing. GoMee Park: Conceptualization, Formal analysis, Investigation, Methodology, Software, Writing - review & editing, Validation, Visualization. Ray J.T. Liao: Conceptualization, Formal analysis, Ivestigation, Methodology, Software, Validation, Visualization, Writing - original draft, Writing - review & editing.

# Acknowledgements

The authors wish to thank the students and instructors at the Universt of Iowa who participated in data collection for this study. We are grateful to Zach Meyer and Ryan Lidster for their helpful contributions at the conceptual stage of this study, and to Stacy Sabraw, Melssa Meisterheim, and Jieun Kim for their comments on an earlier version of this manuscript. Any remaining erors are our own. Finall, we acknowledge that our research was conducted on land stolen from from indigenous peoples, including the Chippewa, Potawatomi, Lakota, and many other sovereign nations, and our debt to them remains unpaid.

# References

Alai ., Jaz, . 2014).m ristn p t  ty or e  l, 453) 253267. /o. org/10.1177/0033688214546963   
Al  e e     the   tt  i   uo. Southern Communication Journal, 85(3), 143-154. https://doi.org/10.1080/1041794X.2020.1764613   
Arydst       t f  i  s assessments? Language Testing, 39(1), 56-89. https://doi-org.proxy.lib.uiowa.edu/10.1177/02655322211026876.   
Asal 1    n , i 10. https://doi.org/10.3389/fpsyg.2019.00870   
ATLAS.ti scientific software development GmbH.(2022) [ATLAS.ti 22 Windows] https://atlasti.com.   
Audacity Team. (2019). Audacity.   
dger,  012     h  t    h 76. / www.ielts.org/-/media/research-reports/ielts_rr_volume09_report2.ashx.   
Bee (195).   n -in ii at sd  in eh, 7 162. /.0.230/ 345428   
Bost College, Columbia University. 35. ETS. https:/files.eric.ed.gov/fulltext/EJ1111620.pdf.   
Chang   f i  i  35-7ry wiley.com/doi/pdf/10.2307/40264527.   
Chad  ky   .199 s i  k  is perspectives (pp. 75-92). Cambridge University Press.   
e t , 36(4), 719-732. https://doi.org/10.1007/s12144-016-9459-6   
Cheg 0      th  ii. 27(4), 544-553. https://doi.org/10.1111/j.1944-9720.2004.tb02421.x Center for Advanced Study of Language. https://www.govtilr.org/Publications/Notetaking.pdf. 00003-A   
Cobb, T. (2020). Compleat web VP. https://www.lextutor.ca/vp/comp/.   
Chen,A2007) g f a f r n t-at  J. och i .)  i  (11) University of Ottawa Press.   
Corts1 e    . http://ideas.repec.org/s/exe/wpaper.html.   
ra  . Metacognition and Learning, 15, 155-213. https://doi.org/10.1007/s11409-020-09222-y   
Creswell, J., & Plano Clark, V. (2018). Designing and conducting mixed methods research (3rd ed.). Sage. University. https://doi.org/10.25335/M5TF1C taking. Language Assessment Quarterly, 10(4), 371-397. https://doi.org/10.1080/15434303.2013.824972 Unpublished manuscript.   
Desee,   h . (019 tive ish ae d ish a    nts oe a i r of Phamay health systems course. Research in Socidl and Administrative Phamacy, 15(9), 11541159. https://doi.0rg/10.1016/j.sapharm.2018.09.023   
Dukel, P. (198  c o1 a sts   ad t reti t t pe ry, 2(, 2981. /o./10.230/ 3586936   
Dukel, ., , . (189. The ristc of  g he Amri t o f meri ntiod g h ae practice f nottaking. Englis for Spcific Purpses, 8(1), 33-50. tps://www.sciencedirec.com/sciece/article/pi/0889490689900057.   
Fao ,  00        .) writing, volume 11: New directions for research in L2 writing (pp. 145-167). Kluwer Academic Publishers.   
Field, J. 201). It the md f t  ister.  f Esh for Ac P, 102), 102-112. hp/./10.1016/ja.01.04.02   
Fi . /a research-reports/ielts_rr_volume09_report1.ashx.   
Field, J. (2013). Cognitive valdity. In L. Taylor, & A. Geranpayeh (Eds.), Examinng lisening (p. 77-151). Cambridge University Press.   
Flecher,  3    i l. /e./atch? v=gQSay5H3874&list=PL0Bm7zZ1IYkRzv7-k0pDaj6BvuM7mlKq7&index=36&t=3s.   
Hale . e,  (199  k n istn mpehn he t o is   a  in 1) 2947 https://doi.org/10.1177/026553229401100104   
Hyti,  0h    t    / files.eric.ed.gov/fulltext/EJ1082250.pdf.   
Hzht 0 te 969/ 2019holzknechtphd.pdf#:\~text=Candidates%20displayed%20more%20higher%2D%20order,and%20were%20markedly%20less%20anxious. arags/looking-listening-using-eye-tracking.   
IBM Corp. (2022). IBM SPSS statistics for windows. IBM Corp. Version 29.0. Technology, 47, 17-36. https://doi.org/10.24539/let.47.0_17   
In    i  i ai     l of Listening, 36(2), 100-117. https://doi.org/10.1080/10904018.2021.1955683   
Jin,   . 2023). he etive of  kn th r  int: me-nlysi.  d sond  qtin (. 1-23. Advanced online publication. https://doi.org/10.1017/s0272263123000529.   
Kiwa     198.ie ac  g  o   g t   hen Journal of Instructional Psychology, 14(3), 110-117.   
m        my term. Computers & Education, 123, 13-25. https:/doi.org/10.1016/j.compedu.2018.04.004   
m .  5)..g 10.31219/osf.io/r83by   
Kobayashi, 005).  the f  k mlc . 30262. . org/10.1016/j.cedpsych.2004.10.001   
Kobayashi, 06).mid e f n-kiw g and the t tgh nti  mytic i. tion Psychology, 26, 459-477. https://doi.org/10.1080/01443410500342070   
nad,    g  209. c f   d 3),421-44//stor. stable/42900031.   
yaa,  06f    is    01) 148-165. http:/lib.dr.iastate.edu/engl pubs/73.   
La, T.  f n   .it Louisville. https://doi.org/10.18297/etd/3982   
Le, . 20.  .  .   2/./16/.200.ie publication.   
Li C,  , , ,  i,  h  017 e l rit  tin r   te ig n f 2 learners at the secondary level. International Joundl of Listning, 31(2), 98-112. htps:/doi.org/10.1080/10904018.2015.1058165   
Li  2   s 35(4), 506-518. https://doi.org/10.1515/cjal-2012-0036   
Lych, . 011 ist in the1s  g  af h.  f Eh for c e, 10 798. /oi.rg. 10.1016/j.jeap.2011.03.001   
Madni  h  22h  f ps ie n   ist i o  f Lisg 6, 53-67. https://doi.org/10.1080/10904018.2018.1523679   
ayma1    e  tion ia blended g vironmet Inational Joal f tionl Thogy in Higher Ectio, 146) htp://di.g/10.1186/41239-017-0048-z   
a   Pn , n L e  22  P n r st ad instructors. Scholarship of Teaching and Learning in Psychology, 8(3), 174-193. htps://doi.org/10.1037/stl0000190   
hrn, r   03t d  t  i   nt note-taking. Journal of Applied Research in Memory and Cognition, 12(1), 94-104. https:/doi.org/10.1037/mac0000032 179-189. https:/doi.org/10.1111/j.2044-8295.1991.tb02392.x   
rd, . 21)lch i   o  is  .ane online publication. https://doi.org/10.1177/1362168820985367   
9  n .  or    -.. edu/bitstream/handle/2027.42/28773/0000605.pdf;sequence=1.   
.  i .1016 j.system.2020.102439   
Rkt    i   3(1, 31-53. https://doi.org/10.1177/0265532219871470   
Sdghi,  i,15.f my  r   i .    g 2) 81-101. https://doi.org/10.22054/ILT.2015.7227   
Saldana, J. (2015). Thinking qualitatively: Methods of mind. Sage.   
Saldana, J. (2016). The coding manual for qualitative researchers. Sage.   
Sigel, . f k tin   is   qi-m s.  of s fr d Purposes, 46, 1-10. https:/doi.org/10.1016/j.jeap.2020.100868   
Siegel, . (202) Fors fting naking oce. n Inetio joo istg. ine ublicatio. p/i.org/10.1080/ 10904018.2022.2059484.   
Siegel . (2).   paor teri 2 is sts v a a.  an in , ../.g.1016/. caeo.2022.100120. Advance online publication.   
mith , J  , .  - i    tht n is r Academic Purposes program. English for Specific Purposes, 66, 80-93. https:/doi.org/10.1016/j.esp.2022.01.001   
Sg  011 qlt d ad is t  et 1), 67./.1/6532211415379   
Taylor, L., & Geranpayeh, A. (Eds.). (2013). Examining listening. Cambridge University Press. lorndike, I Thorndike-Chris (2009). Measurem t and evaluation in psychology and education (8th ed.). Pearson.   
Voer   . 2f      r  a.ti Psychology, 68. https:/doi.org/10.1016/j.cedpsych.2021.102025. Advance online publication.   
ager,  W, . 016. ri  r ke s u  istn  ota  ha, J,    aust J. Fox (Eds.), Trends in language asessment practice and research (pp. 438-463). Cambridge Scholars Publishing.   
Weir, C. (2005). Language testing and validation: An evidence-based approach. Palgrave-Macmillan.   
Wone, .,  im . (2023). ake oe, nt ph in-wdrin mde the mct of t-takig straes on vidrod te g pfomance. Journal of Experimental Psychology: Applied, 29(1), 124-135. https://doi.org/10.1037/xap0000375 test. System, 36(1), 107-122. https://doi.org/10.1016/j.system.2007.12.003   
ng  ite0.  t w ief ak isee SL classrooms. Behavioral Sciences, 13, 395. https:/doi.org/10.3390/bs13050395   
Yegr, e ns r  i f  t i t e, n discrimination. The International Journal of Listening, 36(3), 299-324. https://doi.org/10.1080/10904018.2022.2029705.   
ha https://doi.org/10.3389/fpsyg.2022.930075   
hu  e f  sc i ie International review of applied linguistic in Language teaching. Advance online publication. htps:/oi.org/10.1515/iral-2022-0127

fse n     ti k Journal of Listening.

e  t f International Journal of Science Education.