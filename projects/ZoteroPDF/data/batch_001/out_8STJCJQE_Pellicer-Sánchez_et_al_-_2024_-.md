# How does lexical coverage affect the processing of L2 texts?

Ana Pellicer-Sánchez1,\*, , Stuart Webb2, , and Andi Wang3,

1 IOE, Faculty of Education and Society, University College London, 20 Bedford Way, London WC1H 0AL, United   
Kingdom   
2 Faculty of Education, Western University, 1137 Western Road, London, Ontario N6G 1G7, Canada   
3 School of English and International Studies, Beijing Foreign Studies University, No. 2 Xisanhuan North Road,   
Haidian District, Beijing, 100089, China

\* Corresponding author. IOE, Faculty of Education and Society, University College London, 20 Bedford Way, London WC1H 0AL, United Kingdom. E-mail: a.pellicer-sanchez@ucl.ac.uk

Lexical coverage, i.e. the extent to which words in a text are known, is considered an important predictor of reading comprehension, with studies suggesting $9 8 \%$ lexical coverage leads to adequate comprehension. However, no studies to date have examined how the various lexical coverage percentages suggested in the literature are refected by the cognitive effort involved in processing text and the attention that is devoted to the unknown vocabulary. This study used eye-tracking to examine how lexical coverage affects the processing of text (global measures) and unknown vocabulary (word-level measures), as well as the relationship between processing time on unknown vocabulary and learning. Advanced L2 learners of English read a text in one of four lexical coverage conditions $90 \%$ , $9 5 \%$ , $9 8 \%$ , $100 \%$ ) while their eye movements were recorded. Knowledge of unknown pseudowords in the texts was assessed via an immediate, meaning recall post-test. Results showed that only one of the three global measures examined showed a processing advantage for the $9 8 \%$ condition, refected by longer saccades and less effortful reading than the $90 \%$ and $9 5 \%$ conditions. Crucially, lexical coverage did not have a signifcant impact on the amount of attention spent on unknown vocabulary. Processing times were found to signifcantly predict vocabulary gains.

# Introduction

There has been a great deal of interest in lexical coverage as a factor that may affect second language (L2) learning. Lexical coverage refers to the extent to which the words encountered in spoken or written text are known. For example, a 1,000-word text with $9 5 \%$ lexical coverage would consist of 950 known words and 50 unknown words, while the same text at $9 8 \%$ lexical coverage would include only 20 unknown words. The rationale behind investigating lexical coverage as a variable in studies of SLA is that as the number of words that are known in a text increases, the more likely learners will be able to understand that text and potentially learn any unknown words that are encountered (Webb 2021). Research tends to show that as the percentage of known words increases from $90 \%$ to $100 \%$ so does comprehension (Schmitt et al. 2011; Van Zeeland and Schmitt 2013). Studies have aimed to determine a threshold from which L2 learners might be able to understand spoken and written text. Results have varied according to the type of input and required level of comprehension. Laufer’s (1989) study indicated that L2 learners may have adequate reading comprehension of a general academic text at $9 5 \%$ coverage. However, more recent studies tend to indicate that knowing $9 8 \%$ of the words in a text provides a much better chance that readers will be able to understand L2 written text (Hu and Nation 2000). In contrast, Van Zeeland and Schmitt (2013) found that L2 learners achieved a similar level of listening comprehension at $90 \%$ and $9 5 \%$ coverage but suggest that $9 5 \%$ coverage is a more appropriate indicator of adequate listening comprehension because learners more consistently understood spoken text at that level.

One of the pedagogical implications of studies of lexical coverage has been that when L2 learners have the lexical coverage necessary to understand a text then they may have greater success at incidentally learning any unknown words that are encountered in that text (e.g. Nation 2006; Webb and Rodgers 2009). Nation (2006) reports that $9 8 \%$ coverage is most appropriate for incidental vocabulary learning. This is supported in part by studies of incidental vocabulary through extensive reading (e.g. Horst et al. 1998; Webb and Chang 2015), as well as Sweller’s (1988) Cognitive Load Theory (CLT), which suggests that there is a limited number of resources that can be allocated to working memory and that attention is often split between different sources of information. CLT suggests a split-attention effect when learners have to divide their attention between different sources of information that need to be integrated for comprehension (Schnotz and Kürschner 2007). Thus, in texts with lower levels of lexical coverage, attention may be divided among the larger number of lexical items that are unknown. Split attention might make it more diffcult to attend to each unknown word by reducing the amount of time that could be spent on each one. In contrast, in texts with higher levels of lexical coverage, greater resources might be allocated to attending to the few words that are not clearly understood.

Because research indicates that the greater the amount of attention that is focused on unknown words during reading, the more likely they are to be learned (Godfroid et al. 2018; Mohamed 2018; Montero Perez et al. 2015; Pellicer-Sánchez 2016), we should also expect there to be greater vocabulary learning through reading texts with higher levels of lexical coverage. Surprisingly, only two studies have investigated the effects of lexical coverage on vocabulary learning and they have reported mixed fndings. Liu and Nation (1985) found that both native and non-native teachers of English had greater success at inferring unknown words in a text with $9 6 \%$ lexical coverage than in one at $90 \%$ coverage. However, Webb and Pellicer-Sánchez (in preparation) found no signifcant differences in the number of words learnt at various levels of lexical coverage $90 \%$ , $9 5 \%$ , and $9 8 \%$ ). Crucially, no previous studies have examined the effect that different percentages of lexical coverage have on text processing and the relationship that potential processing differences might have with learning gains. Using eye-tracking should help to reveal the degree to which attention is divided among the varying numbers of unknown words in texts at different levels of coverage. However, this type of empirical evidence is yet to be provided.

The present study addressed these gaps by looking at how lexical coverage affected the online processing of L2 texts. L2 advanced speakers of English were asked to read one of four versions of a text with different levels of lexical coverage $90 \%$ , $9 5 \%$ , $9 8 \%$ , or $100 \%$ ) while their eye movements were recorded. The main aim of the study was to determine the extent to which processing of the text was affected by changes in lexical coverage. Two main aspects of text processing were examined: (1) how lexical coverage affected the processing of the whole text (global measures); (2) as well as how it affected attention to unknown words (word-level measures). The relationship between eye-movement measures and vocabulary learning was also explored. Investigating the effects of lexical coverage on L2 reading behavior should reveal how cognitive effort is affected by reading texts with different levels of coverage. Moreover, the results of the present study should also help to explain the degree to which L2 learners are able to attend to unknown words when reading texts that are more and less comprehensible.

# Lexical coverage and comprehension

Laufer’s (1989) study was the frst to compare reading comprehension at differing amounts of lexical coverage. She found that most L2 learners had reasonable comprehension of an academic text (operationalized as a score of $5 5 \%$ or higher on a comprehension test) when they knew $9 5 \%$ of the words in that text. However, follow-up studies by Hu and Nation (2000) and Schmitt et al. (2011) indicated that readers may require $9 8 \%$ lexical coverage in order to have adequate reading comprehension. Adequate comprehension was operationalized as reaching a similar level of comprehension to that of L2 learners who read the text with $1 0 0 \%$ lexical coverage in Hu and Nation (2000) (scores of $8 5 \%$ or higher on a multiple-choice test and $5 6 \%$ or higher on a written recall test) and $6 8 \%$ on a comprehension test in Schmitt et al. (2011). Most recently, Webb et al. (under review) also showed that the $8 5 \%$ threshold for adequate comprehension was achieved by more than half of the participants $( 6 1 \% )$ in the $9 8 \%$ condition. Researchers have suggested that $9 5 \%$ coverage might be needed for minimal comprehension, while $9 8 \%$ might be necessary for optimal comprehension (Laufer 2020; Laufer and Ravenhorst-Kalovski 2010). Research has also revealed that $9 0 \% - 9 5 \%$ lexical coverage may support adequate listening comprehension (participants had a mean score of $7 3 . 5 \%$ at $90 \%$ lexical coverage and a mean score of $7 6 \%$ at $9 5 \%$ coverage: Van Zeeland and Schmitt 2013) and that $90 \%$ coverage may be suffcient for viewers to understand L2 television programs (participants had mean viewing comprehension scores of $6 8 \%$ at $9 0 \%$ lexical coverage: Durbahn et al. 2020).

Together these studies indicate that the more words we know in a text, the better the understanding of that text is likely to be. However, it is important to note that even when all of the words in a text are known, comprehension may not be perfect because there are many factors that affect comprehension such as background knowledge (Stahl and Jacobson 1986), the amount of redundant information (Kameenui et al. 1982), and individual differences (Mezynski 1983). Laufer and Sim (1985) did fnd however, that lexical coverage had the greatest impact of several factors on comprehension (including knowledge of subject matter, discourse markers, and syntactic structure). Thus, lexical coverage thresholds should be considered an indicator of comprehension but not a guarantee that text will be understood (Webb 2021). Researchers often use these lexical coverage fgures to provide evidence that the selected reading materials are likely to be understood. However, these lexical coverage thresholds are based on comprehension scores and the effect of lexical coverage on reading behavior remains to be examined. For example, we might expect the speed of processing text to increase along with lexical coverage, with texts of lower lexical coverage requiring more cognitive effort. It also stands to reason that learners would spend more time attending to unfamiliar language features with increased coverage because their attention would be less divided with fewer unknown words. However, to our knowledge, the effect of lexical coverage on reading behavior is yet to be examined.

# Lexical coverage and vocabulary learning

Although there has been much discussion of vocabulary learning at different levels of lexical coverage (e.g. Nation 2006; Webb 2010; Webb and Rodgers 2009), there is very little research that has looked at the relationship between vocabulary learning and coverage. Liu and Nation (1985) compared native and non-native English language teachers’ ability to infer pseudowords that had replaced actual words in a text at two levels of lexical coverage: $90 \%$ and $9 6 \%$ . They found that the participants’ ability to successfully infer the words was higher for the participants who read the text with greater coverage. In a follow-up study, Laufer (2020) explored inferencing ability in different lexical coverage conditions $90 \%$ , $9 5 \%$ , and $9 8 \%$ ), using Hu and Nation’s (2000) texts as the reading materials, and the same approach to manipulate lexical coverage as Liu and Nation (1985), i.e. replacing real words in the text with pseudowords. Results showed that inferencing ability was lower at $90 \%$ than at $9 5 \%$ or $9 8 \%$ . She argued that lexical coverage percentage can increase through successful inferencing. Combining the amount of vocabulary that participants knew well before reading the text (i.e. sight vocabulary) and the successfully inferred words, the lexical coverage was relatively high in all conditions $( 9 0 \% \Rightarrow 9 5 . 2 \%$ ; $9 5 \% \Rightarrow 9 9 \%$ ; $9 8 \% \Rightarrow 9 9 . 6 4 \%$ . She argued that after successfully inferring words, similar lexical coverage levels across conditions explained the lack of differences in comprehension. While these studies shed light into the effect of lexical coverage on readers’ ability to infer words from context and the role that lexical inferencing might play in comprehension, they did not directly assess how many words were indeed remembered from texts with different lexical coverages. To address this gap, Webb and Pellicer-Sánchez (in preparation) compared L2 learners’ incidental vocabulary learning through reading a text with different levels of lexical coverage $90 \%$ , $9 5 \%$ , $9 8 \%$ ), as well as their ability to successfully infer unknown words in the texts. The authors also used Hu and Nation’s (2000) texts as the reading materials and results revealed that participants were only able to recall the meanings of $2 . 2 6 \%$ of the words on average. Although success in inferring words improved at higher levels of lexical coverage, none of the differences across conditions were signifcant. It is thus still unclear whether (or how) changes in lexical coverage percentages affect vocabulary learning. Moreover, it is not clear how lexical coverage affects the processing of words and whether any resulting changes in reading behavior make vocabulary learning more likely to occur. Increased attention to unknown words is likely to be a contributing factor, however, there are many factors that affect incidental vocabulary learning (see Laufer (1997) and Peters (2020) for comprehensive reviews of this topic) and so examining changes in reading behavior in relation to changes in lexical coverage may shed light on why some words are learned incidentally while others are not.

# Eye movements during L2 reading

Eye-tracking has allowed researchers to gain a thorough understanding of how readers process a text and the different characteristics that impact cognitive effort during reading. Three types of eye movements are primarily recorded in eye-tracking studies: saccades, i.e. rapid movements of the eyes (typically eight characters in length); fxations, i.e. when the eyes stop (225 ms on average in silent reading); and regressions, i.e. movements back in a text when reading. During fxations, readers engage in two main processes: word recognition and integration of words into a larger sentence or context (Clifton et al. 2007).

Many factors have been shown to affect word recognition and eye-movement patterns during reading, including frequency, familiarity, age of acquisition, predictability, plausibility, lexical ambiguity, and cognate status (Conklin et al. 2018) (see Rayner 2009 for a review). Particularly relevant for the present study is the effect of familiarity. Previous studies have shown that less familiar words elicit more and longer fxations (e.g. Juhasz and Rayner 2003; Williams and Morris 2004). When words are completely unfamiliar, as it is the case of unknown words, readers tend to spend more time processing those words than matched controls at the frst encounter in a text. This effect has been reported for both L1 (e.g. Chaffn et al. 2001; William and Morris 2004) and L2 readers (e.g. Elgort et al. 2018; Godfroid et al. 2013; Godfroid et al. 2018; Pellicer-Sánchez 2016; Pellicer-Sánchez et al. 2021). The processing of unknown words becomes faster with subsequent exposures, as a result of increased familiarity (e.g. Elgort et al. 2018; Mohamed 2018; Pellicer-Sánchez 2016; Pellicer-Sánchez et al. 2021). Similar eye-movement patterns have also been reported for multiword items (Pellicer-Sánchez et al. 2022). Although results are not conclusive, various studies have reported a positive relationship between amount of attention to unknown target items in reading and learning gains, with longer fxation durations being predictive of higher scores in post-reading vocabulary tests (e.g. Godfroid et al. 2013; Mohamed 2018; Pellicer-Sánchez 2016).

Eye-tracking studies in reading have also looked at the integration of words into larger sentences and contexts. Studies on syntactic processing and text comprehension typically look at larger areas of interest, examining global measures of text processing (e.g. average fxation duration, saccade length, regression rate). Word integration in sentence contexts is affected by many different factors, impacting the fuency and speed with which words are read. Processing diffculties may arise because of implausibility, complexity or syntactic misanalysis, as well as task demands and goals of the reader (Conklin et al. 2018). Processing diffcult texts is refected by longer fxations, more regressions, shorter saccades, and lower skipping probability (Castelhano and Rayner 2008). This increased text diffculty and consequent increased cognitive effort is caused by various features of the text, including lexical and syntactic features (Cop et al. 2015; Nahatame 2021). Thus, we might expect increased cognitive effort of readers of a text that includes more unknown words and that this will be refected by longer average fxation durations, increased regression rates, decreased skipping rates, and shorter saccades.

While empirical evidence shows that lexical properties affect eye movements in word recognition and integration and that unknown vocabulary in a text leads to increased attention, no empirical evidence has been provided yet to show how (and if) differences in the lexical coverage percentages usually examined for successful reading comprehension affect the cognitive effort involved in reading such texts. We might expect texts with more unknown words to be read more slowly with a great number of fxations and longer fxation durations than texts with fewer unknown words. In addition, CLT would suggest that attention may be split to a larger degree among the different unknown words in a text with lower coverage than a text with higher coverage, because in the former there are a greater number of unknown words. Higher lexical coverage might allow learners to allocate increased attentional resources to the unknown words. However, empirical evidence to support these claims is yet to be provided.

# The study

Despite the clear effects that lexical properties, such as familiarity, have on both word recognition and text processing, very little is known about how changes in lexical coverage affect the processing of texts, including both the whole text and attention to individual words. In addition, while reading times have been reported to be predictors of learning gains, it is still unknown whether lexical coverage would modulate this relationship. In order to address these gaps, the present study sought to answer the following questions:

1. To what extent does lexical coverage affect the cognitive effort involved in text reading (as refected by global eye tracking measures)?   
2. To what extent does lexical coverage affect the processing of unknown words (as refected by word-level eye tracking measures)?   
3. Does processing time on unknown words predict participants’ ability to recall their meanings? And does lexical coverage modulate this relationship?

In order to address these questions, L2 readers of English were asked to read a text in one of four lexical coverage conditions ( $90 \%$ , $9 5 \%$ , $9 8 \%$ , $100 \%$ ) while their eye movements were recorded. After reading, their learning of the unknown words in the text was assessed by a meaning recall test. Both global measures of text processing (RQ1) and word-level measures (RQ2) were examined. The relationship between reading times on unknown vocabulary and vocabulary gains was also explored (RQ3). Given the existing evidence for the effects of lexical properties on text processing (Cop et al. 2015; Nahatame 2021), it was hypothesized that increased lexical coverage would be refected by shorter fxation durations, fewer fxations, and longer saccades (RQ1). Similarly, according to the predictions of the CLT, lexical coverage was hypothesized to have an effect on the processing of unknown vocabulary, with lower levels of lexical coverage leading to shorter and fewer fxations (RQ2). Finally, in line with previous research, a positive relationship between processing time and vocabulary test scores was also expected (RQ3).

# Methodology

The data reported here are part of a larger study exploring the effect of lexical coverage on a range of outcome measures. The impact of lexical coverage on comprehension, vocabulary

learning, inferencing and perceived diffculty is reported in Webb et al. (under review) and in Webb and Pellicer-Sánchez (in preparation).

# Participants

Ninety-four L2 advanced learners of English initially participated in the study. Data from seven participants were removed from the analyses because of problems with eye-movement recordings and data quality, resulting in a total sample of eighty-seven participants (male $= 6$ , female $= 8 1$ ; Age: $M = 2 5 . 8$ , $\mathrm { S D } = 4 . 7$ ). Participants were all postgraduate students at a UK university from a variety of L1 backgrounds (Chinese $= 7 3$ ; Turkish $= 2$ ; Spanish $= 3$ ; Japanese $= 3$ ; Indonesian $= 2$ ; Italian $= 2$ ; Korean $= 1$ ; Russian $= 1$ ). They had a mean vocabulary size of 6,748 $\mathrm { \Delta } \left( \mathrm { S D } = 8 7 9 \right)$ ), as measured by the V_Yes-No v1.01 vocabulary size test (Meara and Miralpeix 2015). Participants’ self-rated knowledge of English on a 10-point Likert scale was: speaking 6.93 $\mathrm { \Delta } ^ { \prime } \mathrm { S D } = 1 . 3 5 ) $ ; listening 7.67 $\mathrm { \prime } _ { \mathrm { S D } } = 1 . 2 3 ) $ ; reading 7.74 $\mathrm { { S D } } = 1 . 1 7 ,$ ); and writing 6.66 ( $\mathrm { S D } = 1 . 3 3 $ ). A oneway ANOVA showed that there were no signifcant differences in the average vocabulary size of participants across the four lexical coverage conditions $( F ( 3 , 8 3 ) = 0 . 5 6 0$ , P .6.643), or in their selfrated knowledge of English (speaking: $F ( 3 , 8 3 ) = 0 . 5 0 3$ , P .6.681; listening: $F ( 3 , 8 3 ) = 0 . 4 4 7$ , $P = . 7 2 0$ ; reading: $F ( 3 , 8 3 ) = 1 . 8 3$ , $\mathrm { P } = . 1 4 8$ ; writing: $F ( 3 , 8 3 ) = 1 . 8 5$ , P .1.144) across the four lexical coverage conditions (see Appendix S1 for descriptive statistics).

# Reading materials

The story from Hu and Nation (2000) (“The escaped man”) was selected as the reading stimuli for the current study (621 words). Four versions of the story corresponding to the four lexical coverage conditions $90 \%$ , $9 5 \%$ , $9 8 \%$ , and $100 \%$ ) were designed by replacing low frequency words in the text with pseudowords. The words to be replaced were chosen by Hu and Nation (2000) based on their frequency. No other criteria were used to determine the spread of the words over the text. The $90 \%$ , $9 5 \%$ , and $1 0 0 \%$ versions were originally used in Hu and Nation (2000) and the $9 8 \%$ version was used in Laufer’s (2020) study. Ten percent of the $9 0 \%$ version consisted of pseudowords (forty-seven types; sixty-two tokens). In the $9 5 \%$ version, $5 \%$ of the words were replaced by pseudowords (twenty-four types; thirty-one tokens), while only $2 \%$ of the $9 8 \%$ version consisted of pseudowords (twelve types; thirteen tokens). The $1 0 0 \%$ version only contained real, highfrequency words. The pseudowords embedded in the versions were from Hu and Nation (2000). All the remaining words in the text were from the 3,000 most frequent words in Nation’s (2012) BNC-COCA lists, as measured by Lextutor (Cobb, n.a.) (see Appendix S2 for the $90 \%$ text and Appendix S3 for a complete list of pseudowords).

# Vocabulary test

Participants’ knowledge of the pseudowords was assessed via a decontextualized, meaning recall test. The pseudowords were presented one by one and participants were asked to recall their meaning, providing a synonym, L2 explanation or L1 translation. Given the advanced level of profciency of participants, most of them provided their responses in the L2. A few Chinese participants provided some L1 translations, and the third researcher, a native speaker of Chinese, translated these into English. One mark was given for each correct answer. Responses were considered correct when they clearly represented knowledge of the meanings of the items. A second rater scored $1 0 \%$ of the responses and the inter-rater reliability was perfect $\overset { \prime } { K } = 1 . 0$ , P .0.000) (Landis and Koch 1977).

# Procedure

Data were collected individually in an eye-tracking laboratory. After receiving information about the study, participants provided their written consent. They were then randomly assigned to either one of the three experimental conditions $90 \%$ , $n = 2 1$ ; $9 5 \%$ , $n = 2 1$ ; $9 8 \%$ , $n = 2 5$ ), or the control condition $100 \%$ , $n = 2 0$ ) and were asked to read the corresponding text in the eye-tracking computer. They were told that the purpose of the study was to examine reading comprehension and were asked to read the text as naturally as possible for comprehension. They were not informed of the post-reading vocabulary test. The text was presented over eight trials with an average of 75 words per trial (Min $= 7 0$ , $\mathrm { M a x } = 8 4 $ ). Participants’ eye movements were recorded with EyeLink 1000 Plus (SR Research), in desk-mounted mode with monocular recording (right eye). Head movements were minimized with a head and chin rest. The stimulus was presented on a 19-inch monitor with a $1 , 9 2 0 \times 1 , 0 8 0$ screen resolution. The experiment started with a short practice session to ensure that participants were familiar with the procedure. A nine-point calibration was performed before the practice session and another one before the start of the experimental text. Participants had to press the space bar to move from one page to the next. There was no time limit to read each page, but they could not go back to re-read previous pages. After the reading, they completed the meaning recall test, followed by a fourteen-item comprehension test from Hu and Nation (2000) (results of the comprehension scores reported in Webb et al. (under review), the vocabulary size test, and a language background questionnaire. The treatment lasted an average of $3 0 \mathrm { m i n }$ for the $1 0 0 \%$ group, 45 min for the $9 8 \%$ group, 60 min for the $9 5 \%$ group, and around 70 min for the $90 \%$ group, due to the different lengths of the meaning recall test.

# Analyses

To answer RQ1, each page/trial of the reading session was set as an interest area (IA). To answer RQ2 and RQ3, IAs were the target pseudowords in the three experimental conditions and their corresponding real words in the control condition. Fixations shorter than $8 0 \mathrm { m s }$ were merged if they were within $1 ^ { \circ }$ of visual angle and those that were still shorter than 80 ms were deleted from the dataset ( $3 . 1 9 \%$ of the data).

Statistical analyses were conducted with R (v 3.6.3; R Core Team 2020). We ftted linear, Poisson, and logistic mixed-effects models according to the type of dependent variables with lmer or glmer function in the lme4 package (Bates et al. 2015). We winsorized outliers beyond mean $3 \pm \mathrm { S D }$ by the corresponding cutoff value. Tukey post-hoc tests were run using the multcomp package (v 1.4.17; Hothorn et al. 2008) for pairwise comparisons. The best models were constructed using forward selection and reported based on likelihood ratio tests and on Akaike information criterion scores. Random intercepts (i.e. participant and page for RQ1; participant and item for RQ2 and RQ3) were always added. Participant-level variable (i.e. vocabulary size) and item-level variables (i.e. word frequency of occurrence, word class, and word length) were entered into the models when available to control for their potential effects but were only kept in the model if they signifcantly improved model ft. All the continuous variables were log transformed before conducting the analyses. A two-tailed alpha level of 0.05 was used for all statistical tests. We used Cohen’s $d$ to calculate the effect size for linear regressions. D values of 0.40, 0.70, and 1.00 were considered small, medium, and large effect sizes, respectively (Plonsky and Oswald 2014). For logistic regression, we used odds ratio (OR) to measure the effect size (Field et al. 2012). ORs greater than 3 or less than 0.33 were considered strong (Haddock et al. 1998), equal to a probability of $7 5 \%$ .

In response to RQ1, we investigated the effects of lexical coverage (i.e. condition) on participants’ global text processing using four eye-tracking measures, with each trial as IA: average fxation duration (the average duration of all fxations per page), total reading time (the sum of all fxations per page), fxation count (the total number of fxations per page), and average saccade amplitude (the average size of all saccades per page, measured in degrees of visual angle). These measures were chosen as they have been commonly reported as global eye-tracking measures in studies of text processing: average fxation duration (Cop et al. 2015; Nahatame 2021; Rayner et al. 2006); fxation count (Cop et al. 2015; Rayner et al. 2006); total reading time (Cop et al. 2015; Rayner et al. 2006); and average saccade amplitude (Nahatame 2021; Rayner et al. 2006).

To answer RQ2, we examined the effects of lexical coverage (i.e. condition) on participants’ processing of the pseudowords. The design of the different versions of the text meant that there was a different number of pseudowords across conditions, and crucially, that their lexical properties also differed. Potential factors such as word length, frequency of occurrence and word class were included in the statistical analyses but there were other differences (e.g. amount of contextual support) that were not controlled for in the original text and that could affect processing patterns. We therefore decided to focus on the pseudowords that were common across conditions. If the difference in the number of pseudowords across conditions led to processing differences, this would have been captured in the analyses performed in response to RQ1, which included all words and pseudowords in the texts. In order to explore whether lexical coverage had an effect on attention to pseudowords, analyzing only the common words across conditions avoided the potential effects of other confounding variables. There were eleven items that appeared across the three experimental conditions (see Appendix S3). As most of the pseudowords appeared only once in the texts, we only included the frst appearance of the common pseudoword which appeared twice (i.e. stragar). One of the common items was deleted (i.e. sorance $=$ fright) because it was from the same word family as another common item (i.e. sorant $=$ frightened) which appeared frst in the story. This resulted in ten common pseudowords included in the analysis for RQ2 (see Appendix S4). Seven eye-tracking measures were examined. Early measures were used to examine learners’ efforts in recognizing word forms (Conklin et al. 2018), and included frst fxation duration (the duration of a reader’s frst fxation made on an IA), frst-pass reading time (the sum of all fxations before exiting), and skip rate (whether the IA was fxated upon). We used late measures including total reading time (the sum of all fxations), fxation count (the total number of fxations), and second-pass reading time (the sum of fxations on an IA when the eyes visit the IA a second time) to investigate participants’ more controlled cognitive processes (Conklin et al. 2018). To analyze participants’ potential processing diffculties, we also included regression in count (the number of times the IA was entered from an IA on its right) in the analysis. The combination of multiple measures is believed to more completely capture learners’ cognitive processes (Godfroid 2020). Linear, Poisson, and logistic mixed-effect models were constructed for continuous (i.e. average fxation duration, total reading time, saccade amplitude, frst fxation duration, frst-pass reading time, and second-pass reading time), count (i.e. fxation count, regression in count), and binary (i.e. skip rate) dependent variables accordingly.

To answer RQ3, we investigated the potential predictive role of participants’ eye movements to the pseudowords (as measured by total reading time, frst-pass reading time, and second-pass reading time) on vocabulary learning gains (as measured by scores in the meaning recall test) for the three experimental conditions (i.e. $90 \%$ , $9 5 \%$ , and $9 8 \%$ lexical coverage). Thirty-one pseudowords were selected for the analysis $( \mathsf { a d j } ) = 6$ ; $\mathtt { V } = 8$ ; $\mathrm { n } = 1 4$ ; adv $= 3$ ) based on the following criteria: their meanings did not appear anywhere else in the texts; they were not part of the same word family; they were not part of compounds; they appeared the same number of times across the experimental conditions. Most of the items appeared once in the texts. For the fve pseudowords that appeared more than once in the text, we added up the eye-tracking data for each of them for each participant and included frequency of occurrence as an item-level variable in the analysis (see Appendix S5 for details).

# Results

# RQ1—The effects of lexical coverage on global text processing

Table 1 summarizes the descriptive statistics of the global eye-movement measures in the fou lexical coverage conditions.

To answer RQ1, four sets of mixed-effects models were ftted separately for each eye-tracking measure. Pearson correlation of the four eye-movement measures revealed that apart from fxation count and saccade amplitude, all measures signifcantly correlated with each other (see Appendix S6). Although the $9 8 \%$ condition seemed to have a relatively shorter total reading time and fewer fxations than other conditions, no signifcant group differences were revealed across conditions in terms of average fxation duration, $\textsf { X } ^ { 2 } \left( 3 \right) = 1 . 8 6$ , $P = . 6 0$ , $R ^ { 2 } = 0 . 0 2$ , total reading time, $\mathsf { X } ^ { 2 } \left( 3 \right) = 5 . 9 6$ , $\mathrm { P } = 0 . 1 1$ , $R ^ { 2 } = 0 . 0 4$ , and fxation count, $\mathsf { X } ^ { 2 } \left( 3 \right) = 4 . 5 3$ , $\mathrm { P } = 0 . 2 1$ , $R ^ { 2 } = 0 . 0 4$ . However, saccade amplitude revealed signifcant group differences (see Appendix S7 for model summary). Post-hoc comparisons in Table 2 show that the $9 8 \%$ condition had signifcantly larger saccade amplitude than the $90 \%$ and $9 5 \%$ conditions, with large effect sizes.

<html><body><table><tr><td></td><td colspan="2">90% (n = 21)</td><td colspan="2">95% (n = 21)</td><td colspan="2">98% (n = 25)</td><td colspan="2">Control 100% (n = 20)</td></tr><tr><td></td><td>M (SD)</td><td>95% CI</td><td>M (SD)</td><td>95% CI</td><td>M (SD)</td><td>95% CI</td><td>M (SD)</td><td>95% CI</td></tr><tr><td>Average fixation duration (ms)</td><td>232 (20)</td><td>[229, 235]</td><td>227 (31)</td><td>[222, 231]</td><td>226 (17)</td><td>[224, 228]</td><td>223 (22)</td><td>[220, 227]</td></tr><tr><td>Total reading time (ms)</td><td>27,155 (9,461)</td><td>[25,713, 28,596] 25,514 (9304)</td><td></td><td>[24,097, 26,931] 22,402 (7,162)</td><td></td><td>[21,403, 23,400] 24,605 (7401) [23,449, 25,760]</td><td></td><td></td></tr><tr><td>Fixation count.</td><td>118 (42)</td><td>[111, 124]</td><td>113 (40)</td><td>[107, 119]</td><td>99 (32)</td><td>[95, 104]</td><td>111 (35)</td><td>[105, 116]</td></tr><tr><td>Average saccade amplitude</td><td>3.96 (1.59)</td><td>[3.72, 4.20]</td><td>4.08 (1.41)</td><td>[3.87, 4.30]</td><td>5.61 (1.44)</td><td>[5.41, 5.81]</td><td>4.70 (1.26)</td><td>I coverage affect t [4.50, 4.90]</td></tr></table></body></html>

Table 2. Results of post-hoc comparisons for saccade amplitude.   

<html><body><table><tr><td></td><td>b</td><td>SE</td><td>95% CI</td><td>z</td><td>p</td><td>d</td></tr><tr><td>Group</td><td colspan="6">Total reading time</td></tr><tr><td>95%-90%</td><td>0.04</td><td>0.08</td><td>[-0.12, 0.20]</td><td>0.47</td><td>.97</td><td>0.13</td></tr><tr><td>98% &gt; 90%</td><td>0.31</td><td>0.08</td><td>[0.15, 0.47]</td><td>4.01</td><td>&lt;.001</td><td>1.13</td></tr><tr><td>Control-90%</td><td>0.16</td><td>0.08</td><td>[0.003, 0.32]</td><td>1.99</td><td>.19</td><td>0.59</td></tr><tr><td>98% &gt; 95%</td><td>0.27</td><td>0.08</td><td>[0.11, 0.43]</td><td>3.53</td><td>.003</td><td>1.08</td></tr><tr><td>Control-95%</td><td>0.12</td><td>0.08</td><td>[-0.04, 0.28]</td><td>1.53</td><td>.42</td><td>0.50</td></tr><tr><td>Control-98%</td><td>0.15</td><td>0.08</td><td>[-0.31, 0.01]</td><td>1.89</td><td>.23</td><td>0.62</td></tr></table></body></html>

# RQ2—The effect of lexical coverage on the processing of pseudowords

Table 3 summarizes the descriptive statistics for the processing of the ten common pseudowords in the four lexical coverage conditions.

To answer RQ2, seven sets of mixed-effects models were ftted separately for each eye-tracking measure. Pearson correlation of the seven eye-movement measures revealed that most of the measures had a signifcant positive correlation with each other (see Appendix S8), while skip rate had a signifcant negative correlation with the other six measures. First fxation duration did not signifcantly relate to fxation count, regression in count, and second-pass reading time, whereas frst-pass reading signifcantly but negatively correlated with regression in count and second-pass reading time.

Results of the seven mixed-effects models revealed signifcant group differences (see Appendix S9 for model summaries). Results of post-hoc comparisons with early measures (Table 4) showed that the $90 \%$ and $9 8 \%$ lexical coverage conditions spent signifcantly longer frst-pass reading time on the pseudowords than the $1 0 0 \%$ condition with small effect sizes, but no signifcant difference was reported between the $9 5 \%$ and $100 \%$ conditions. The $90 \%$ condition had signifcantly longer frst fxation durations than the control condition, with a small effect size. No signifcant group differences were revealed in terms of the skip rate across conditions (see Table 5).

Regarding late eye-tracking measures, results of the post hoc tests showed that the three experimental conditions spent signifcantly more time on the pseudowords than the control condition did on the corresponding real words. However, no signifcant differences were revealed between the experimental conditions. As shown in Table 4, the effect sizes were small to medium between the $9 0 \%$ and $1 0 0 \%$ conditions in total reading time and second-pass reading time, whereas they were medium to large between the $1 0 0 \%$ condition and both the $9 5 \%$ and $9 8 \%$ conditions. Moreover, as shown in Table 5, the $90 \%$ , $9 5 \%$ and $9 8 \%$ conditions also had signifcantly higher regression in count and fxation count on the pseudowords than the $1 0 0 \%$ condition did on the corresponding real words. The odds of regression in count were 2.70, 3.78, and 3.63 times higher than in the $1 0 0 \%$ condition, and the odds of fxation count was 1.77, 1.95, and 1.92 times higher than in the $1 0 0 \%$ condition, respectively.

Word length had a negative effect on regression in count. One letter increase in pseudowords decreased the probability of regression in count by $3 0 \%$ . Adding word length for the other measures did not improve the model ft so it was excluded from the models (likely due to the small number of items and small range of word length).

<html><body><table><tr><td></td><td colspan="2">90% (n = 21)</td><td colspan="2">95% (n = 21)</td><td colspan="2">98% (n = 25)</td><td colspan="2">Control 100% (n = 20)</td></tr><tr><td></td><td>M (SD)</td><td>95% CI</td><td>M (SD)</td><td>95% CI</td><td>M (SD)</td><td>95% CI</td><td>M (SD)</td><td>95% CJ</td></tr><tr><td>Total reading time (ms)</td><td>867 (596)</td><td>[786, 948]</td><td>942 (676)</td><td>[851, 1034]</td><td>953 (603)</td><td>[878, 1028]</td><td>416 (248)</td><td>[382, 4</td></tr><tr><td>First-fixation duration (ms)</td><td>274 (111)</td><td>[259, 289]</td><td>253 (108)</td><td>[238, 268]</td><td>273 (122)</td><td>[258, 288]</td><td>231 (97)</td><td>[218, 2</td></tr><tr><td>First-pass reading time (ms)</td><td>456 (270)</td><td>[420, 493]</td><td>387 (266)</td><td>[351, 423]</td><td>478 (348)</td><td>[434, 521]</td><td>310 (153)</td><td>[288, 3</td></tr><tr><td>Second-pass reading time (ms)</td><td>243 (295)</td><td>[203, 283]</td><td>287 (307)</td><td>[245, 328]</td><td>282 (331)</td><td>[241, 323]</td><td>84 (141)</td><td>[64, 10</td></tr><tr><td>Fixation count</td><td>3.40 (2.31)</td><td>[3.09, 3.71]</td><td>3.75 (2.55)</td><td>[3.40, 4.10]</td><td>3.60 (2.32)</td><td>[3.31, 3.88]</td><td>1.86 (1.15)</td><td>[1.70, </td></tr><tr><td>Regression in count</td><td>0.36 (0.59)</td><td>[0.28, 0.44]</td><td>0.51 (0.72)</td><td>[0.41, 0.61]</td><td>0.49 (0.70)</td><td>[0.40, 0.58]</td><td>0.14 (0.34)</td><td>[0.09, </td></tr><tr><td>Skip rate</td><td>0.01 (0.10)</td><td>[0.00, 0.02]</td><td>0.01 (0.10)</td><td>[0.00, 0.02]</td><td>0.03 (0.17)</td><td>[0.01, 0.05]</td><td>0.05 (0.22)</td><td>[0.02, </td></tr></table></body></html>

Table 4 Results of post-hoc comparisons for total reading time, frst-pass reading time, frst fxation duration, and second-pass reading time.   

<html><body><table><tr><td></td><td>b</td><td>SE</td><td>95% CI</td><td>z</td><td>P</td><td>d</td></tr><tr><td>Group</td><td colspan="6">Total reading time</td></tr><tr><td>95%-90%</td><td>0.06</td><td>0.17</td><td>[-0.27, 0.39]</td><td>0.35</td><td>.99</td><td>0.12</td></tr><tr><td>98%-90%</td><td>0.01</td><td>0.16</td><td>[-0.30, 0.32]</td><td>0.07</td><td>1.00</td><td>0.14</td></tr><tr><td>Control &lt; 90%</td><td>0.84</td><td>0.17</td><td>[-1.17, -0.51]</td><td>-4.84</td><td>&lt;.001</td><td>0.98</td></tr><tr><td>98%-95%</td><td>0.05</td><td>0.16</td><td>[-0.26, 0.36]</td><td>0.30</td><td>.99</td><td>0.02</td></tr><tr><td>Control &lt; 95%</td><td>0.90</td><td>0.17</td><td>[-1.23, -0.57]</td><td>-5.19</td><td>&lt;.001</td><td>1.02</td></tr><tr><td>Control &lt; 98%</td><td>0.85</td><td>0.17</td><td>[-1.18, 0.52]</td><td>5.11</td><td>&lt;.001</td><td>1.12</td></tr><tr><td></td><td colspan="6">First-pass reading time</td></tr><tr><td>95%-90%</td><td>0.21</td><td>0.13</td><td>[-0.46, 0.04]</td><td>1.59</td><td>.38</td><td>0.26</td></tr><tr><td>98%-90%</td><td>0.13</td><td>0.13</td><td>[-0.38, 0.12]</td><td>-1.01</td><td>.75</td><td>0.07</td></tr><tr><td>Control &lt; 90%</td><td>0.51</td><td>0.13</td><td>[-0.76, -0.26]</td><td>-3.84</td><td>&lt;.001</td><td>0.66</td></tr><tr><td>98%-95%</td><td>0.08</td><td>0.13</td><td>[-0.17, 0.33]</td><td>0.65</td><td>.92</td><td>0.29</td></tr><tr><td>Control-95%</td><td>0.30</td><td>0.13</td><td>[-0.55, 0.05]</td><td>2.26</td><td>.11</td><td>0.35</td></tr><tr><td>Control &lt; 98%</td><td>0.38</td><td>0.13</td><td>[-0.63, -0.13]</td><td>2.98</td><td>.02</td><td>0.60</td></tr><tr><td colspan="6">First fixation duration</td><td></td></tr><tr><td>95%-90%</td><td>-0.10</td><td>0.10</td><td>[-0.30, 0.10]</td><td>-0.98</td><td>.76</td><td>0.19</td></tr><tr><td>98%-90%</td><td>0.12</td><td>0.10</td><td>[-0.32, 0.08]</td><td>-1.21</td><td>.62</td><td>0.01</td></tr><tr><td>Control &lt; 90%</td><td>0.33</td><td>0.10</td><td>[-0.53, 0.13]</td><td>3.19</td><td>.01</td><td>0.41</td></tr><tr><td>98%-95%</td><td>0.02</td><td>0.10</td><td>[-0.22, 0.18]</td><td>0.0</td><td>1.00</td><td>0.17</td></tr><tr><td>Control-95%</td><td>0.23</td><td>0.10</td><td>[-0.43, -0.03]</td><td>2.22</td><td>.12</td><td>0.21</td></tr><tr><td>Control-98%</td><td>0.21</td><td>0.10</td><td>[-0.41, -0.01]</td><td>2.10</td><td>.15</td><td>0.38</td></tr><tr><td colspan="6">Second-pass reading time</td><td></td></tr><tr><td>95%-90%</td><td>0.58</td><td>0.41</td><td>[-0.22, 1.38]</td><td>1.43</td><td>.48</td><td>0.15</td></tr><tr><td>98%-90%</td><td>0.20</td><td>0.39</td><td>[-0.56, 0.96]</td><td>0.50</td><td>.96</td><td>0.13</td></tr><tr><td>Control &lt; 90%</td><td>1.59</td><td>0.41</td><td>[-2.39, 0.79]</td><td>3.83</td><td>&lt;.001</td><td>0.68</td></tr><tr><td>98%-95%</td><td>0.39</td><td>0.39</td><td>[-1.15, 0.37]</td><td>0.99</td><td>.76</td><td>0.01</td></tr><tr><td>Control &lt; 95%</td><td>2.17</td><td>0.41</td><td>[-2.97, -1.37]</td><td>5.24</td><td>&lt;.001</td><td>0.84</td></tr><tr><td>Control &lt; 98%</td><td>1.78</td><td>0.40</td><td>[-2.56, -0.99]</td><td>-4.48</td><td>&lt;.001</td><td>0.75</td></tr></table></body></html>

# RQ3—Relationship between pseudoword processing and vocabulary gains

To answer RQ3, we constructed three sets of logistic mixed-effect models for each eye-tracking measure (i.e. total reading time, frst-pass reading time, and second-pass reading time).

Table 5. Results of post-hoc comparisons for regression in count and skip rate.   

<html><body><table><tr><td></td><td>b</td><td>95% CI</td><td>SE</td><td>z</td><td>OR</td><td>OR 95% CI</td><td>P</td></tr><tr><td>Group</td><td colspan="7">Regression in count</td></tr><tr><td>95%-90%</td><td>0.33</td><td>[-0.06, 0.72]</td><td>0.20</td><td>1.72</td><td>1.39</td><td>[0.94, 2.06]</td><td>.31</td></tr><tr><td>98%-90%</td><td>0.30</td><td>[-0.07, 0.67]</td><td>0.19</td><td>1.56</td><td>1.35</td><td>[0.93, 1.96]</td><td>.40</td></tr><tr><td>Control &lt; 90%</td><td>-0.99</td><td>[-1.48, -0.50]</td><td>0.25</td><td>-3.90</td><td>0.37</td><td>[0.23, 0.61]</td><td>&lt;.001</td></tr><tr><td>98%-95%</td><td>0.04</td><td>[-0.39, 0.31]</td><td>0.18</td><td>0.23</td><td>0.96</td><td>[0.68, 1.37]</td><td>.99</td></tr><tr><td>Control &lt; 95%</td><td>-1.33</td><td>[-1.82, -0.84]</td><td>0.25</td><td>-5.37</td><td>0.26</td><td>[0.16, 0.43]</td><td>&lt;.001</td></tr><tr><td>Control &lt; 98%</td><td>1.29</td><td>[-1.76, -0.82]</td><td>0.24</td><td>5.31</td><td>0.28</td><td>[0.17, 0.44]</td><td>&lt;.001</td></tr><tr><td></td><td colspan="7">Fixation count</td></tr><tr><td>95%-90%</td><td>0.09</td><td>[-0.13, 0.31]</td><td>0.11</td><td>0.82</td><td>1.09</td><td>[0.88, 1.36]</td><td>.31</td></tr><tr><td>98%-90%</td><td>0.08</td><td>[-0.14, 0.30]</td><td>0.11</td><td>0.73</td><td>1.08</td><td>[0.87, 1.34]</td><td>.40</td></tr><tr><td>Control &lt; 90%</td><td>-0.57</td><td>[-0.81, 0.33]</td><td>0.12</td><td>-4.86</td><td>0.57</td><td>[0.45, 0.72]</td><td>&lt;.001</td></tr><tr><td>98%-95%</td><td>0.01</td><td>[-0.23, 0.21]</td><td>0.11</td><td>0.13</td><td>0.99</td><td>[0.80, 1.23]</td><td>.99</td></tr><tr><td>Control &lt; 95%</td><td>-0.67</td><td>[-0.91, -0.43]</td><td>0.12</td><td>-5.65</td><td>0.51</td><td>[0.40, 0.65]</td><td>&lt;.001</td></tr><tr><td>Control &lt; 98%</td><td>0.65</td><td>[-0.87, -0.43]</td><td>0.11</td><td>-5.75</td><td>0.52</td><td>[0.42, 0.65]</td><td>&lt;.001</td></tr><tr><td colspan="8">Skip rate</td></tr><tr><td>95%-90%</td><td>0.00</td><td>[-2.10, 2.10]</td><td>1.07</td><td>0.00</td><td>1.00</td><td>[0.12, 8.14]</td><td>1.00</td></tr><tr><td>98%-90%</td><td>1.02</td><td>[-0.72, 2.76]</td><td>0.89</td><td>1.14</td><td>2.77</td><td>[0.48, 15.87]</td><td>.66</td></tr><tr><td>Control-90%</td><td>1.77</td><td>[0.06, 3.48]</td><td>0.87</td><td>2.03</td><td>5.87</td><td>[1.07, 32.30]</td><td>.17</td></tr><tr><td>98%-95%</td><td>1.02</td><td>[-0.72, 2.76]</td><td>0.89</td><td>1.14</td><td>2.77</td><td>[0.48, 15.87]</td><td>.66</td></tr><tr><td>Control-95%</td><td>1.77</td><td>[0.06, 3.48]</td><td>0.87</td><td>2.03</td><td>5.87</td><td>[1.07, 32.30]</td><td>.17</td></tr><tr><td>Control-98%</td><td>0.74</td><td>[-0.53, 2.01]</td><td>0.65</td><td>1.15</td><td>2.10</td><td>[0.59, 7.49]</td><td>.65</td></tr></table></body></html>

As reported in Webb and Pellicer-Sánchez (in preparation), overall vocabulary uptake was low, with participants recalling the meanings of $2 . 2 6 \%$ of the words on average. Tables 6 and 7 show that total reading time and second-pass reading time signifcantly predicted meaning recall scores across conditions, suggesting that a one-second increase in total reading time and second-pass reading time on one pseudoword increased the odds of a correct answer by 2.36 and 8.26 times despite the group difference. The difference in odds ratio indicates that total reading time was a worse predictor of meaning recall scores, possibly due to the small number of items recalled and the fact that the majority of pseudowords $( 9 8 \% )$ were fxated on, as indicated by total reading time. In contrast, second-pass reading time might have refected attempts to integrate information inferred from context, being a better predictor of recall scores. The addition of condition as a fxed effect did not signifcantly improve the model ft for either total reading time, $\mathsf { X } ^ { 2 } \left( 2 \right) = 3 . 4 0$ , $\mathrm { P } = . 1 8$ , $R ^ { 2 } = 0 . 7 7$ , or second-pass reading time, $\mathsf { X } ^ { 2 } \left( 2 \right) = 3 . 0 9$ , $\mathrm { P } = . 2 1$ , $R ^ { 2 } = 0 . 9 8$ , suggesting similar predictive roles across conditions. Pseudowords with higher frequency of occurrence were signifcantly more likely to be answered correctly on meaning recall with 3.88 and 4.67 times higher odds of a correct answer with a one-second increase in total reading time and second-pass reading time, respectively. First-pass reading time failed to predict meaning recall gains, $\textsf { X } ^ { 2 } \left( 1 \right) = 0 . 0 6$ , $\mathrm { P } = . 8 1$ , $\mathrm { R } ^ { 2 } < 0 . 0 0 1$ .

Table 6. Relationship between total reading time on pseudowords and meaning recall.   

<html><body><table><tr><td>Fixed effects</td><td>b</td><td>SE</td><td>z</td><td>P</td><td>OR</td><td>95% CI</td></tr><tr><td>Intercept</td><td>8.12</td><td>1.02</td><td>-7.94</td><td>&lt;.001</td><td>&lt;0.001</td><td>[0.00, 0.002]</td></tr><tr><td>Total reading</td><td>0.86</td><td>0.21</td><td>4.16</td><td>&lt;.001</td><td>2.36</td><td>[1.57, 3.53]</td></tr><tr><td>FoOa</td><td>1.36</td><td>0.42</td><td>3.22</td><td>.001</td><td>3.88</td><td>[1.70, 8.87]</td></tr><tr><td>Random effects</td><td></td><td>Variance</td><td>SD</td><td></td><td></td><td></td></tr><tr><td>by participant</td><td>Intercept</td><td>1.37</td><td>1.17</td><td></td><td></td><td></td></tr><tr><td>by item</td><td>Intercept</td><td>1.02</td><td>1.01</td><td></td><td></td><td></td></tr><tr><td colspan="7">Best model: Scoreb ~ Total reading time + FoOa + (1|Participant) + (1|Item)</td></tr><tr><td colspan="7">Hosmer and Lemeshow&#x27;s R2 = 0.84.</td></tr></table></body></html>

aFrequency of occurrence. bMeaning recall score.

Table 7. Relationship between second-pass reading time on pseudowords and meaning recall.   

<html><body><table><tr><td>Fixed effects</td><td>b</td><td>SE</td><td>z</td><td>P</td><td>OR</td><td>95% CI</td></tr><tr><td>Intercept</td><td>8.29</td><td>1.09</td><td>7.60</td><td>&lt;.001</td><td>&lt;0.001</td><td>[0.00, 0.002]</td></tr><tr><td>Second-pass reading</td><td>2.11</td><td>0.41</td><td>5.15</td><td>&lt;.001</td><td>8.26</td><td>[3.70, 18.46]</td></tr><tr><td>FoO</td><td>1.54</td><td>0.44</td><td>3.46</td><td>&lt;.001</td><td>4.67</td><td>[1.95, 11.16]</td></tr><tr><td>Random effects</td><td></td><td>Variance</td><td>SD</td><td></td><td></td><td></td></tr><tr><td>by participant</td><td>Intercept</td><td>1.23</td><td>1.11</td><td></td><td></td><td></td></tr><tr><td>by item</td><td>Intercept</td><td>1.26</td><td>1.12</td><td></td><td></td><td></td></tr><tr><td colspan="7">Best model: Scoreb ~ second-pass reading time + FoOa + (1|Participant) + (1|Item).</td></tr><tr><td>Hosmer and Lemeshow&#x27;s R2 = 0.93.</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

aFrequency of occurrence. bMeaning recall score.

# Discussion

This study examined the effect of lexical coverage (i.e. $90 \%$ , $9 5 \%$ , $9 8 \%$ , $100 \%$ ) on text processing, looking at L2 readers’ eye movements to the whole text (global measures) (RQ1) as well as attention to the unknown vocabulary (word-level measures) (RQ2). The relationship between attention to unknown vocabulary and meaning recall scores was also examined (RQ3).

In response to the frst research question, we hypothesized that increased lexical coverage would be refected by shorter fxation durations, fewer fxations, and longer saccades. This hypothesis was only partially supported by the results. Only one of the four measures examined, i.e. saccade amplitude, showed an effect of lexical coverage. The $9 8 \%$ condition exhibited longer saccades than the $90 \%$ and $9 5 \%$ conditions, indicating easier processing and reduced text diffculty. This result provides further support for the effects of familiarity and lexical properties on text processing (Cop et al. 2015; Nahatame 2021), and that processing texts that are lexically more diffcult is refected by shorter saccades (Castelhano and Rayner 2008). This less effortful processing of the $9 8 \%$ condition might be related to the advantage of the $9 8 \%$ coverage for adequate comprehension reported in previous studies (e.g. Hu and Nation 2000; Schmitt et al. 2011). This result might also explain the fndings reported in Webb et al. (under review) study, indicating that, while there were no signifcant differences across lexical coverage conditions in comprehension scores, the $9 8 \%$ version was the condition where most learners reached the required level of adequate comprehension. It is important to note though, that this processing advantage of the $9 8 \%$ condition over the $90 \%$ and $9 5 \%$ conditions was only observed in saccade amplitude and no other differences in the remaining three eye-movement measures were reported. This suggests that it might not be a robust effect and thus, results need to be interpreted with caution. This result might also suggest that saccade amplitude might be a more sensitive measure to discriminate reading behavior of texts at different levels of diffculty.

The second research question sought to examine the effect of lexical coverage on the processing of unknown vocabulary. Analysis of the processing of the common unknown pseudowords showed signifcant differences between the three experimental conditions ( $90 \%$ , $9 5 \%$ , and $9 8 \%$ ) and the control $100 \%$ condition. Unknown pseudowords in the $9 0 \%$ , $9 5 \%$ , and $9 8 \%$ received more and longer fxations than the real words in the control condition, as refected by the late measures and frst-pass reading time. First fxation durations on unknown pseudowords in the $9 0 \%$ condition were also longer than those in the control condition. This provides further evidence for the facilitative role of familiarity in word recognition and is in line with results of previous eye-tracking studies on incidental vocabulary learning from reading, indicating that unfamiliar or unknown words receive more attention than matched controls (e.g. Elgort et al. 2018; Mohamed 2018; Godfroid et al. 2013, 2018; Pellicer-Sánchez 2016; Pellicer-Sánchez et al. 2021, 2022).

Regarding processing differences across conditions, in line with the predictions of CLT and the split-attention effect (Sweller 1988), we hypothesized a signifcant effect of lexical coverage, with lower levels of lexical coverage leading to shorter and fewer fxations on unknown pseudowords. It was expected that readers in the lower lexical coverage conditions would need to divide their attention among the larger number of unknown words, which would lead to less time available to process each of the unknown lexical items. Notably, this hypothesis was not supported by the data. The analyses did not reveal any signifcant differences in the processing of unknown pseudowords across the three experimental conditions ( $90 \%$ , $9 5 \%$ , and $9 8 \%$ ). This fnding supports the lack of differences in vocabulary learning across lexical coverage conditions reported in Webb and Pellicer-Sánchez (in preparation). The presence of a higher number of unknown pseudowords did not seem to impact the amount of attention devoted to process them. L2 readers did not seem to spend more time processing the unknown words when there were fewer of them, as it was initially hypothesized. Learners might have still engaged in different underlying cognitive processes, but these were not refected by amount of attention, at least as measured by the eye movement measures examined in the present study. For example, the same amount of attention could have refected effort to guess the meaning of the pseudoword from context and encode it to memory in some cases, or processing diffculties in others. This is in line with recent claims for the need to isolate these potentially different subprocesses in eye-movement data more clearly (Godfroid et al. 2018), because the examination of eye movements on their own might not be enough to disambiguate these distinctive effects (Pellicer-Sánchez 2020).

Another possible explanation for the lack of processing differences for the pseudowords across experimental conditions $90 \%$ , $9 5 \%$ , $9 8 \%$ ) is the advanced level of profciency of the L2 readers in the present study, which might have overrun the potential effect of lexical coverage on online processing. It would be important in future research to examine lower percentages of lexical coverage to fnd the threshold at which processing differences start to emerge. Moreover, future studies should also explore the effect of these lexical coverage conditions on online processing with learners of lower profciency who might lack the ability to make use of other processing strategies to compensate for the higher number of unknown items in lower lexical coverage conditions. Differences in the ability to deal with unknown vocabulary in the text might then be refected by processing differences. It is also important to note that, even within a same profciency group, there might still be a certain degree of individual variation, as suggested by the SDs in the present study. Future studies should also explore causes of individual variation in learners’ processing of novel vocabulary in various lexical coverage conditions.

Finally, in response to the third research question, we hypothesized that processing times on unknown pseudowords would be positively related to vocabulary test scores. Results showed that two of the measures explored, i.e. total reading time and second-pass reading time, signifcantly predicted meaning recall scores, in line with previous research fndings reporting a similar relationship (Godfroid et al. 2013; Mohamed 2018; Pellicer-Sánchez 2016; Pellicer-Sánche et al. 2021). This suggests that longer reading times refected successful inferencing of the words from context that was later refected in higher test scores. Importantly, the predictive role of these measures did not vary across lexical coverage conditions. This might suggest that attention has a larger effect than lexical coverage in incidental vocabulary learning, at least when coverage ranges from $90 \%$ to $9 8 \%$ . However, results should be treated with caution, given the small number of words that were in fact learned. It is important to note that the conditions created in the study did not lead to much learning and this could affect the results for the relationship between attention and vocabulary gains.

There are important limitations in the present study that should be noted. First of all, as argued earlier, the study was conducted with a homogeneous sample of advanced L2 readers, which might explain the lack of major processing differences reported. Lower-profciency learners might fnd it harder to distribute attention over the unknown lexical items. Future studies should be conducted with lower profciency readers. Secondly, in the present study, the text and items from Hu and Nation (2000) were used to allow for comparability with previous research. However, the original items differed in various intra-lexical and contextual factors (e.g. position in sentences, contextual support). While we controlled for this via a strict item deletion procedure, future research should use other more tightly controlled materials, allowing for the inclusion of a larger number of items in the analysis. The use of the original, unmodifed materials also meant that, when dividing the text into different trials for the eye-tracking experiment, the lexical coverage of each trial was not always the same as the coverage for that particular condition, as there were variations in the number of pseudowords in each trial. Future studies should attempt to control for lexical density in each trial. Similarly, there were sentences in the experimental text that contained more than one pseudoword. It will be interesting to examine how sentence density impacts comprehension and processing. The use of the original materials also involved a between-subjects design. Future research could also implement within-subjects design, having participants read texts in all lexical coverage conditions.

It is important to note too that the advanced participants in the present study were very likely familiar with the meanings of the words that the pseudowords replaced. Thus, the vocabulary learning situation represented in the present study is one in which participants were learning new forms for meanings they already knew. This is a common learning scenario but not representative of all types of vocabulary learning (e.g. learning new forms for new meanings). In addition, while participants were asked to read as naturally as possible for comprehension, the presence of a large number of unknown items in the text (particularly in the lower coverage conditions) might have alerted them to the vocabulary focus of the study. Finally, the present study focused on the examination of lexical coverage and thus the number of unknown pseudowords was the only variation across lexical coverage conditions. However, text processing and attention allocation might be infuenced by other variables that were not explored here, such as topic familiarity and individual differences. While we controlled for differences in vocabulary knowledge and English profciency, there might have been other differences between the groups in other skills such as reading comprehension, reading speed, or word recognition that might have had an infuence on results. Future research should explore the effect of other learner-related and text-related features that may interact with lexical coverage.

# Conclusion

The present study suggests that lexical coverage does not seem to have a major effect on the cognitive effort involved in processing the text and novel vocabulary. Only one of the three global measures of text processing showed a facilitative role of lexical coverage, as refected by longer saccades in the $9 8 \%$ condition. Crucially, results showed a lack of effect of lexical coverage on the amount of attention spent on unknown vocabulary. The larger number of pseudowords in the lower coverage conditions did not lead to split attention among the unknown vocabulary, suggesting that the advanced learners in the present study might have employed other processing strategies that allowed them to deal with the unknown vocabulary that were not necessarily refected by increased attention. The present study shows that small increments in lexical coverage percentages might not necessarily make texts easier to read, pointing to the need to explore the role of lexical coverage in relation to other potentially moderating factors. Finally, despite the small vocabulary gains, the results provide further evidence for the predictive role of processing times on vocabulary gains, attesting to the signifcant role of attention in L2 vocabulary learning.

# Supplementary data

Supplementary data is available at Applied Linguistics online.

# Notes on contributors

Ana Pellicer-Sánchez is an Associate Professor of Applied Linguistics and TESOL at University College London. Her research centers around the teaching and learning of vocabulary in a second/foreign language, with a particular focus on the use of eye-tracking to explore the cognitive processes involved in vocabulary learning. She is coauthor of Eye-tracking: A guide for Applied Linguistics Research (CUP) and coeditor of Understanding Formulaic Language (Routledge).

Stuart Webb is a Professor of Applied Linguistics at the University of Western Ontario. He currently teaches on the Masters in TESOL program and supervises students at the MA, PhD, and post-doctorate levels. His articles have been published in journals such as Applied Linguistics and Language Learning. His latest books are The Routledge Handbook of Vocabulary Studies, and How Vocabulary is Learned (with Paul Nation).

Andi Wang is a lecturer at the School of English and International Studies, Beijing Foreign Studies University. Her current research focuses on learning vocabulary in a second/foreign language through multimedia input. She is interested in the combination of various research methods (e.g. eye-tracking, verbal reports, offine tests) to explore learners’ vocabulary learning processes and outcomes. Her research has been published in journals such as Studies in Second Language Acquisition and Language Learning.

# Funding

None declared.

# References

Bates, D. et al. (2015) ‘Fitting Linear Mixed-effects Models Using lme4’, Journal of Statistical Software 67: 1–48. https://doi.org/10.18637/jss.v067.i01   
Castelhano, M. S., and Rayner, K. (2008) ‘Eye Movements During Reading, Visual Search, and Scene Perception: An Overview’, in K. Rayner, D. Shen, X. Bai, and G. Yan (eds.) Cognitive and Cultural Infuences on Eye Movements, pp. 3–33. Tianjin People’s Publishing House.   
Chaffn, R., Morris, R. K. and Seely, R. E. (2001) ‘Learning New Word Meanings from Context: A Study of Eye Movements’, Journal of Experimental Psychology Learning Memory and Cognition 27: 225–35. https://doi.org/10.1037/0278-7393.27.1.225   
Clifton, C., Jr., Staub, A. and Rayner, K. (2007) ‘Eye Movements in Reading Words and Sentences, in R. P. G. van Gompel, M. H. Fischer, W. S. Murray, and R. L. Hill (eds.) Eye Movements: A Window on Mind and Brain. Elsevier, pp. 341–71. https://doi.org/10.1016/B978-008044980-7/50017-3   
Cop, U., Drieghe, D. and Duyck, W. (2015) ‘Eye Movement Patterns in Natural Reading: A Comparison of Monolingual and Bilingual Reading of a Novel,’ PLoS One 10: e0134008. https://doi.org/10.1371/ journal.pone.0134008   
Durbahn, M., Rodgers, M. and Peters, E. (2020) ‘The Relationship Between Vocabulary and Viewing Comprehension’, System 88: 102166. https://doi.org/10.1016/j.system.2019.102166   
Elgort, I., et al. (2018) ‘Contextual Word Learning During Reading in a Second Language: An Eye Movement Study’, Studies in Second Language Acquisition 40: 341–66. https://doi.org/10.1017/ S0272263117000109   
Field, A., Miles, J. and Field, Z. (2012) Discovering Statistics using R. Sage.   
Godfroid, A. (2020) Eye Tracking in Second Language Acquisition and Bilingualism: A Research Synthesis and Methodological Guide. New York: Routledge.   
Godfroid, A. et al. (2018) ‘Vocabulary Learning in a Natural Reading Context: An Eye-tracking Study’, Bilingualism: Language and Cognition 21: 563–84. https://doi.org/10.1017/S1366728917000219   
Godfroid, A., Boers, F., and Housen, A. (2013) ‘An Eye for Words: Gauging the Role of Attention in Incidental L2 Vocabulary Acquisition by Means of Eye Tracking’, Studies in Second Language Acquisition 35: 483–517. https://doi.org/10.1017/S0272263113000119   
Haddock, C. K., Rindskopf, D., and Shadish.,W. R. (1998) ‘Using Odds Ratios as Effect Sizes for Metaanalysis of Dichotomous Data: A Primer on Methods and Issues,’ Psychological Methods 3: 339–53. https://doi.org/10.1037/1082-989X.3.3.339   
Horst, M., Cobb, T., and Meara, P. (1998) ‘Beyond a Clockwork Orange: Acquiring Second Language Vocabulary through Reading’, Reading in a Foreign Language 11: 207–23.   
Hothorn, T., Bretz, F., and Westfall, P. (2008) ‘Simultaneous Inference in General Parametric Models’, Biometrical Journal. Biometrische Zeitschrift 50: 346–63. https://doi.org/10.1002/bimj.200810425   
Hu, H., and Nation, P. (2000) ‘Unknown Vocabulary Density and Reading Comprehension’, Reading in a Foreign Language 13: 403–31.   
Juhasz, B. J., and Rayner, K. (2003) ‘Investigating the Effects of a Set of Intercorrelated Variables on Eye Fixation Durations in Reading,’ Journal of Experimental Psychology: Learning, Memory and Cognition 29: 1312–8. https://doi.org/10.1037/0278-7393.29.6.1312   
Kameenui, E. J., Carnine, D. W., and Freschi, R. (1982) ‘Effects of Text Construction and Instructional Procedures for Teaching Word Meanings on Comprehension and Recall’, Reading Research Quarterly 17: 367–88. https://doi.org/10.2307/747525   
Landis, J. R., and Koch, G. G. (1977) ‘The Measurement of Observer Agreement for Categorical Data’, Biometrics 33: 159–74. https://doi.org/10.2307/2529310   
Laufer, B. (2020) ‘Lexical Coverages, Inferencing Unknown Words and Reading Comprehension: How Are They Related?’, TESOL Quarterly 54: 1076–85. https://doi.org/10.1002/tesq.3004   
Laufer, B. (1989) ‘What Percentage of Text-Lexis is Essential for Comprehension’ in C. Lauren and M. Nordman (eds.) Special Language: From Humans Thinking to Thinking Machines pp. 316–23. Clevedon, PA: Multilingual Maters.   
Laufer, B. (1997) ‘What’s in a Word that Makes it Hard or Easy? Intralexical Factors Affecting Vocabulary Acquisition’, in N. Schmitt and M. McCarthy (eds.): Vocabulary: Description, Acquisition, and Pedagogy, pp. 140–55. Cambridge: Cambridge University Press.   
Laufer, B., and Ravenhorst-Kalovski, G. (2010) ‘Lexical Threshold Revisited: Lexical Text Coverage, Learners’ Vocabulary Size and Reading Comprehension’, Reading in a Foreign Language 22: 15–30. http://nfrc.hawaii.edu/rf/   
Laufer, B., and Sim, D. D. (1985) ‘Measuring and Explaining the Reading Threshold Needed for English for Academic Purposes Texts’, Foreign Language Annals 18: 405–11. https://doi. org/10.1111/j.1944-9720.1985.tb00973.x   
Liu, N., and Nation, I. S. P. (1985) ‘Factors Affecting Guessing Vocabulary in Context’, RELC Journal 16: 33–42. https://doi.org/10.1177/003368828501600103   
Meara, P., and Miralpeix, I. (2015) ‘V_YesNo (Version 1.01),’ http://www.lognostics.co.uk/tools/V_ YesNo/V_YesNo.htm, accessed 1 September 2021. Training on Reading Comprehension’, Review of Educational Research 53: 253–79. https://doi. org/10.3102/00346543053002253   
Mohamed, A. A. (2018) ‘Exposure Frequency in L2 Reading: An Eye-movement Perspective of Incidental Vocabulary Learning’, Studies in Second Language Acquisition 40: 269–93. https://doi.org/10.1017/ S0272263117000092   
Montero Perez, M., Peters, E., and Desmet, P. (2015) ‘Enhancing Vocabulary Learning through Captioned Video: An Eye‐tracking Study’, The Modern Language Journal 99: 308–28. https://doi.org/10.1111/ modl.12215   
Nahatame, S. (2021) ‘Text readability and Processing Effort in Second Language Reading: A Computational and Eye-Tracking Investigation’, Language Learning 71: 1004–43. https://doi. org/10.1111/lang.12455   
Nation, I. S. P. (2006) ‘How Large a Vocabulary is Needed for Reading and Listening?’, Canadian Modern Language Review 63: 59–82.   
Nation, P. (2012) The BNC/COCA word family lists, available at www.victoria.ac.nz/lals/about/staff/ paul-nation   
Pellicer-Sánchez, A. (2016) ‘Incidental Vocabulary Acquisition From and While Reading: An Eyetracking Study’, Studies in Second Language Acquisition 38: 97–130. https://doi.org/10.1017/ S0272263115000224   
Pellicer-Sánchez, A. (2020) ‘Expanding English Vocabulary Knowledge through Reading: Insights from Eye-tracking Studies’, RELC Journal 51: 134–46. https://doi.org/10.1177/0033688220906904   
Pellicer‐Sánchez, A., Conklin, K., and Vilkaitė‐Lozdienė, L. (2021) ‘The Effect of Pre‐reading Instruction on Vocabulary Learning: An Investigation of L1 and L2 Readers’ Eye Movements’, Language Learning 71: 162–203. https://doi.org/10.1111/lang.12430   
Pellicer-Sánchez, A., Siyanova-Chanturia, A., and Parente, F. (2022) ‘The Effect of Frequency of Exposure on the Processing and Learning of Collocations: A Comparison of First and Second Language Readers’ Eye Movements’, Applied Psycholinguistics 43: 727–56. https://doi.org/10.1017/ S014271642200011X   
Peters, E. (2020) ‘Factors Affecting the Learning of Single-word Items’, in S. Webb (ed.) The Routledge Handbook of Vocabulary Studies, pp. 125–42. New York: Routledge. https://doi. org/10.4324/9780429291586   
Plonsky, L., and Oswald, F. L. (2014) ‘How Big is ‘Big?’ Interpreting Effect Sizes in L2 Research’, Language Learning 64: 878–912. https://doi.org/10.1111/lang.12079   
R Core Team. (2020) R: A language and environment for statistical computing (Version 3.6.3) [Computer software]. Vienna, Austria: R Foundation for Statistical Computing, available at https://www.R-project.org/   
Rayner, K. (2009) ‘Eye Movements and Attention in Reading, Scene Perception, and Visual Search’, Quarterly Journal of Experimental Psychology (2006) 62: 1457–506. https://doi. org/10.1080/17470210902816461   
Rayner, K. et al. (2006) ‘Eye Movements as Refections of Comprehension Processes in Reading’, Scientifc Studies of Reading 10: 241–55. https://doi.org/10.1207/s1532799xssr1003_3   
Schmitt, N., Jiang, X., and Grabe, W. (2011) ‘The Percentage of Words Known in a Text and Reading Comprehension’, The Modern Language Journal 95: 26–43. https://doi. org/10.1111/j.1540-4781.2011.01146.x   
Schnotz, W. and Kürschner, C. (2007) ‘A Reconsideration of Cognitive Load Theory’, Educational Psychology Review 19: 469–508. https://doi.org/10.1007/s10648-007-9053-4   
Stahl, S. A. and Jacobson, M. G. (1986) ‘Vocabulary Diffculty, Prior Knowledge, and Text Comprehension’, Journal of Reading Behavior 18: 309–23.   
Sweller, J. (1988) ‘Cognitive Load During Problem Solving: Effects on Learning’, Cognitive Science 12: 257–85. https://doi.org/10.1207/s15516709cog1202_4   
van Zeeland, H., and Schmitt, N. (2013) ‘Lexical Coverage in L1 and L2 Listening Comprehension: The Same or Different from Reading Comprehension?’, Applied Linguistics 34: 457–79. https://doi. org/10.1093/applin/ams074   
Webb, S. (2010) ‘A corpus Driven Study of the Potential for Vocabulary Learning through Watching Movies’, International Journal of Corpus Linguistics 15: 497–519. https://doi.org/10.1075/ ijcl.15.4.03web   
Webb, S. (2021) ‘Research Investigating Lexical Coverage and Lexical Profling: What WE Know, What We Don’t Know, and What Needs to be Examined’, Reading in a Foreign Language 33: 287–302. http://hdl.handle.net/10125/67407   
Webb, S., and Pellicer-Sánchez, A. (in preparation). Lexical Coverage and Vocabulary learning.   
Webb, S., Pellicer-Sánchez, A., and Wang, A. (under review). Lexical Coverage and Reading Comprehension.   
Webb, S., and Rodgers, M. P. H. (2009) ‘The Vocabulary Demands of Television Programs’, Language Learning 59: 335–66. https://doi.org/10.1111/j.1467-9922.2009.00509.x   
Webb, S. A., and Chang, A. C. -S. (2015) ‘Second Language Vocabulary Learning through Extensive Reading: How does Frequency and Distribution of Occurrence Affect Learning?’, Language Teaching Research 19: 667–86. https://doi.org/10.1177/1362168814559800   
Williams, R., and Morris, R. (2004) ‘Eye Movements, Word Familiarity, and Vocabulary Acquisition’, European Journal of Cognitive Psychology 16: 312–39. https://doi.org/10.1080/09541440340000196