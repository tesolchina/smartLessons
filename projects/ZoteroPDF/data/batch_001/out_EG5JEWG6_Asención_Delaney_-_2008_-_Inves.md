# Investigating the reading-to-write construct

Yuly Asencio´n Delaney\*

Modern Languages Department, Northern Arizona University, Babbit Academic Annex, Office 202, Flagstaff, AZ 86011-6004, USA

# Abstract

This study explored the extent to which the reading-to-write construct is the sum of one’s reading and writing abilities or an independent construct. The investigation included an analysis of (a) test tasks, (b) the relationship of test task scores and scores on reading and writing measures, and (c) the effects of proficiency level and educational level on reading-to-write performance. A sample of 139 participants performed two reading-to-write tasksda summary and a response essaydbased on the same source text. Results indicate that the test tasks were different dimensions of the reading-to-write ability, and that the reading-to-write ability seems to be a unique construct weakly associated with reading for comprehension and disassociated from writing an essay without background reading support. In addition, it was found that language proficiency and educational level had a modest effect on the performance of the tasks.

$^ { © }$ 2008 Elsevier Ltd. All rights reserved.

eywords: L2 reading-to-write; L2 writing; L2 summary writing; L2 reading; EAP

# 1. Investigating the reading-to-write construct

One of the primary tenets of communicative language learning has been the importance of integrative language tasks. This view has been reinforced by English for Specific Purposes (ESP) surveys showing that reading-to-write tasks are common in university settings (Bridgeman & Carson, 1983; Hale et al., 1996; Rosenfeld, Leung, & Oltman, 2001). Nevertheless, relatively little research has been reported about L2 student performance on reading-to-write tasks in relation to the individual and external factors that affect performance.

The research context of this study is a placement testing situation in a TESOL teacher training program in a Venezuelan university. In an effort to replicate the tasks that students encounter in their classes, the placement test included two reading-to-write tasksda summary and a response essay. While investigating the validity of the inferences made about learners’ reading-to-write ability, questions arose about the nature of the construct in terms of the factors that affect student performance.

# 1.1. What is reading-to-write?

The term ‘‘reading-to-write’’ can be examined from two perspectives: pedagogical and theoretical. The pedagogical perspective refers to instructional tasks that combine reading and writing for various educational purposes (e.g., summary writing as a learning tool, Fitzgerald & Shanahan, 2000). The theoretical perspective is more closely asso ciated with the underlying abilities that learners display when performing these tasks. The reading-to-write construct can be examined from a reading, writing, or constructivist approach depending on the importance given to the literacy skills involved.

From the reading perspective, reading-to-write involves either reading to learn (Carver, 1997; Enright et al., 2000; Kintsch, 1998) or reading to integrate information (Grabe & Stoller, 2002). When reading to learn or to integrate, reader/writers construct elaborate models of the text structure and situation, enabling them to select information from the source text, evaluate it, and use it for writing purposes (Kintsch, 1998).

From the writing perspective, Hayes’ (1996) cognitive-affective model of writing offers a reasonable explanation of the role of reading in writing. Reading comprehension, which is a central feature of this model, serves three different purposes: to access topic knowledge, to understand the tasks, and to revise or evaluate the written outcome.

From the constructivist perspective, reading comprehension and composing are seen as processes of building meaning (Kucer, 1985; Nelson & Calfee, 1998). In other words, the reader/writer establishes new meanings from the reading that he/she later articulates on paper. The meaning construction occurs by means of three key textual operations: organizing, selecting, and connecting (Spivey, 1990, 1997).

Perhaps reading-to-write should be conceptualized as a reciprocal interaction between literacy skills, in which the basic processes and strategies used for reading and writing are modified by an individual’s goals and abilities, and also by external factors. Reading-to-write certainly involves the interplay of reading and writing processes, as advocated by constructivists, but merely having reading and writing abilities is not sufficient to perform reading-to-write tasks successfully (Grabe, 2001).

# 1.2. Factors affecting reading-to-write performance

The successful performance of reading-to-write tasks is influenced by numerous variables (Belcher & Hirvela, 2001; Carson & Leki, 1993; Flower et al., 1990; Grabe, 2003; Hirvela, 2004; Ruiz-Funes, 1999; Spivey, 1997). Among these variables, task demands seem to be critical. Reading-to-write tasksde.g., summariesdcan be placed on a continuum of difficulty determined for example by the nature of the topic, the number and nature of sources used, and the writing conventions required.

Few studies have addressed the issue of task demands by comparing performance in different tasks. Durst (1987) found that writing an analytic essay required more critical thinking than writing a summary. When summarizing, the students in his study processed the information superficially and monitored their writing processes less, whereas in analytic writing students reported more mental operations that involved making plans, inferences, generalizations, and assessments of the quality of the content and language in their essays. However, when comparing the writing products, both kinds of writing turned out to be similar.

Asencio´n (2004) found similar results in the cognitive processing of second language learners during the readingto-write tasks of writing a summary and a response essay. In her study, examinees monitored their processing more often while writing their response essay than while summarizing. They also planned the task, form, and content of the response essay more than when they summarize. Comparisons have been made of the processes reported, but more evidence on the relationship between the tasks is needed.

Previous research exploring some of the individual variables that affect reading-to-write performancedliteracy expertise, language proficiency, and educational leveldsuggests that reading or writing ability influences the way reader/writers organize and connect information from the source text with their own texts. For instance, reading ability can affect the amount of notes taken and the elaboration of the content of the notes (Kennedy, 1985), as well as the organizing/transforming operations made during task completion (Risemberg, 1996). Writing expertise similarly affects the integration of information at different levels (e.g., verbatim, propositional) and the use of problem-solving behaviors (Cumming, Rebuffot, & Ledwell, 1989).

Other research has yielded inconclusive evidence about the relationship between general reading, general writing, and reading-to-write performance. Correlations vary from .69 in Trites and McGroarty’s (2005) study comparing Nelson-Denny reading scores with a measure of reading-to-integrate, to .80 in Enright, Bridgeman, and Cline (2002), in which scores for the new TOEFL tasks (reading and writing) were correlated with reading scores in the paper-and-pencil TOEFL. Watanabe (2001) concluded that although both the reading and writing measures accounted for $40 \%$ of the variance in the two reading-to-write tasks, the reading scores on their own were not a significant predictor of reading-to-write performance.

Most studies have shown a strong relationship between reading-to-write and writing. Watanabe (2001) noted that the reading-to-write tasks in his study were a more reliable measure of writing ability than of reading ability; the writing scores were a significant predictor of reading-to-write performance. In another study, Lewkowicz (1994) found no significant difference in scores for an essay written with or without reading support; it seems that both writing tasks involved the same skills.

Only one study found contradictory evidence for the strong relationship between reading-to-write and writing. Messer (1997) reported a low relationship between summary performance and writing ability; he concluded that the skill measured for the ESL Integrated Summary Profile and the ability measured in the writing test had very little in common.

Language proficiency can of course affect reading-to-write performance in various ways. Specifically, less proficient L2 reader/writers can have difficulties with English vocabulary, grammar, syntax, and discourse-level reading and writing skills, which can affect their reading and writing processes (Connor & Krammer, 1995). According to Campbell (1990), non-native speakers’ lack of linguistic sophistication can also make their style and tone be perceived as less academic because they do not integrate information smoothly. Taking into account such linguistic problems, it seems reasonable that to write academic summaries, ESL students should have at least a high intermediate level of proficiency (Kirkland & Saunders, 1990).

The role of L2 proficiency seems better explained as an additive to other variables that affect reading-to-write performance than as a single causative factor. For instance, less proficient L2 learners have been found to do more copying and less combining of ideas (Johns & Mayes, 1990). In other studies, L2 proficiency interacted with individual aspects such as L1 literacy skills or expertise to determine the amount of direct copying from the source text (Corbeil, 2000), the overall quality of the writing, and the processes that reader/writers pay attention to during task completion (Cumming, 1989).

Educational level may also affect reading-to-write performance because students with higher educational levels are thought to be more familiar with these kinds of academic tasksdthe topics discussed in their discipline, their teachers’ expectations about performance, and their discourse community demands in general. However, research has not provided conclusive findings about this relationship. Some studies show no relationship, as in Mathison’s (1996) study where the quality of critiques written by students with different educational levels (i.e., freshman, sophomores, juniors, and seniors) did not differ significantly. Other studies however have shown a relationship between the two variables.

In an analysis of linguistic features of academic writing across educational levels (e.g., 200-, 400-, graduate-level, and professional texts), Conrad (1996) found that students developed most of the features of professional writing as they went from introductory to graduate courses. Trites and McGroarty (2005) included the educational variable operationalized as graduate and undergraduate participants, producing a significant effect on the essay scores of students who used information from two source articles (i.e., reading-to-integrate measure). The fact that neither the definitions of educational level in the studies nor the level of analysis of the texts produced is similar precludes definite conclusions.

The inconclusive evidence about the factors affecting reading-to-write performance highlights the need for more research. This study therefore investigated the reading-to-write construct by analyzing data from a reading-to-write test taken by learners of English, and by addressing the following research questions: (1) How does the reading-towrite ability of participants manifest itself across two different tasks? (2) What is the relationship between participants’ reading-to-write ability and literacy skills? (3) What is the effect of language proficiency and educational level on participants’ reading-to-write performance?

# 2. Methods

# 2.1. Participants

The 139 participants of this study consisted of 50 native speakers of English (graduate and undergraduate) and 89 learners of English (62 ESL and 27 EFL learners). Both native speakers and ESL learners were recruited in two American universities. The ESL learners were classified into two English proficiency groups, intermediate ( $\mathrm { T O E F L } = 4 5 0 { - } 5 4 3 )$ and advanced $( \mathrm { T O E F L } = > 5 5 0 )$ . The EFL learners consisted of student teachers in a TESOL program in Venezuela, whose proficiency measure was not available. However, two previous studies on reading strategies with the same population of learners reported TOEFL mean scores ranging between 477 and 536 (Cardozo, 2000; Quintero, 2003). For comparison purposes, this group was considered as a different proficiency group.

The age of the participants, 41 males and 98 females, ranged between 18 and 51 (mean $= 2 6$ ). Most of them (about $7 5 \%$ ) had a major associated with the social sciences or the humanities, with $41 \%$ majoring in TESOL or Applied Linguistics. They also had different native languages: English $( 3 6 \% )$ , Spanish $( 2 3 \% )$ , Chinese $( 9 . 4 \% )$ , Dutch $( 3 . 6 \% )$ , and French $( 3 . 6 \% )$ .

# 2.2. Materials

# 2.2.1. Reading-to-write measure

The reading-to-write test involved two tasks: writing a summary and a response essay. For the summary task, examinees were asked to read a source text and summarize the main ideas expressed by the author. The second task consisted of writing an essay in response to a question about the main idea of the textdCan men and women be differentiated by the way they use language?

The source reading was chosen from a pool of six argumentative texts from current magazines and college textbooks related to social issues, business, and science that were piloted with college students. The six argumentative texts were first piloted with a sample of L2 learners for perceived difficulty and topic familiarity. The selected 1,663-word text was from a college textbook on linguistic anthropology dealing with the issue of the relationship between gender and speech.

It should be noted that using one text limited the generalization of the findings to the specific characteristics of the gender and speech text. Also, using the same text for both tasks could introduce an element of practice in reading-towrite performance with this text. Thus, the researcher counter-balanced the groups of participants in their completion of the different tasks.

The scoring procedure used two analytical scales, one per task, with three scoring categories derived from other scoring methods employed to assess academic writing (Hamp-Lyons, 1991a, 1991b) and from studies in which reading-to-write tasks were used for testing or research purposes (Allison, Berry, & Lewkowicz, 1995; Flower et al., 1990; Newell, Garriga, & Peterson, 2001; UIUC EPT Video-Reading Based Academic Test, 1999). The scoring categories were organization and development of ideas and arguments, use of information from the reading, and language.

Scoring was performed by pairs of raters from a pool of four, one male and three female teachers, aged between 26 and 43. Two were native speakers of English and two were non-native $\mathrm { L } 1 = \mathrm { S }$ panish). The NNS raters were teaching in a TESOL program in a Venezuelan university and the other two were Ph.D. students of Applied Linguistics in an American university. The average inter-rater reliability coefficients for both tasks were high (summary .88, response essay .89).

# 2.2.2. Reading measure

Information about the participants’ basic reading level was collected using form H of the Nelson-Denny Reading Test (ND) (Brown, Fishco, & Hanna, 1993). The ND test consisted of a vocabulary section with 80 multiple-choice items and a comprehension section with 38 multiple-choice questions. The comprehension section included a subsection to determine the reading rate in one minute. The reading test proved to be reliable with the sample of participants in the study (KR-20 estimate of .96).

# 2.2.3. Writing measure

The basic writing ability of the subjects was assessed with an analytical rubric developed by Jacobs, Zinkgraf, Wormuth, Hartfiel, and Hughey (1981). The task was adapted from the one Khaldieh (2000) used in his study on the learning and writing strategies of learners of Arabic. Two raters, native speakers and experienced teachers of English, assessed the participants’ writing pieces using the ESL Composition Profile by Jacobs et al. (1981), which yields a composition profile with five weighted aspects: content (30 points), organization (20 points), vocabulary (20 points), language use (25 points), and mechanics (5 points). The inter-rater reliability for the scoring of this measure was .91.

# 2.3. Procedure

Participants completed the measures of the study in two working sessions scheduled for groups consisting of up to 20 people each time. Test conditions as well as instructions for the test sessions were the same for all participants. In the first session, students completed the reading and the writing measures plus a short measure of their ability to solve arithmetic problems (not reported here). The reading and the arithmetic measures were timed and performed first, while the writing test was completed without any time limit. In the second session, conducted within a week, participants completed the reading-to-write measures and filled out a demographic questionnaire. Both reading-to-write measures were completed in a counter-balanced fashion and students could take as much time as needed. Both testing sessions lasted between an hour and thirty minutes and two hours fifteen minutes.

# 3. Results

A correlation analysis was conducted to examine the relationship between the tasks as part of the reading-to-write ability. Scores on the summary were correlated with scores on the response essay, yielding a coefficient of ${ \bf r } = 0 . 3 8$ , $\mathsf { p } < . 0 5$ . This positive yet weak relationship between the scores indicates that the variance in the two measures overlapped by only $1 4 . 4 \%$ . If we take into account the ranges of correlation coefficients suggested by Hatch and Lazaraton (1991) for two tests measuring the same ability (i.e., high .80s or .90s), or measuring different aspects of the same ability (i.e., .30e.50), it is clear that the performance on the summary and the response essay tasks could be considered two different dimensions of the reading-to-write ability. This finding is further explored in section 3.2 when performance in both tasks is examined taking into account participants’ language proficiency and educational level.

# 3.1. Relationship between reading-to-write and L2 literacy skills

The theory of the reading-to-write construct led to an expectation of strong relationships among reading and writing measures. The L2 reading and writing measures in this study were a reading test (the Nelson-Denny Reading Test), and a writing test scored with Jacobs et al.’s (1981) ESL Writing Profile. The descriptive statistics for the reading scores are displayed in Table 1.

The native speakers outperformed the non-native speakers; all the non-native speakers performed below the mean for the total sample. Confidence intervals $( 9 5 \% )$ showed that some groups had different reading performances; the reading performance of the ESL-I group was significantly lower than the rest of the participants.

In the ESL Composition Profile, scores ranged from 67 to 92 (see Table 2). The native speakers of English had the highest scores $( \mathrm { m e a n } = 8 2 . 0 4 $ ), while the lowest mean scores belonged to the ESL-I (74.89) and EFL (75.61) participants. Similarities in writing performance were observed in the overlapping $9 5 \%$ confidence intervals between the ESL-I group and the EFL group. Also, the NS performance overlapped with the ESLA group, indicating no true difference in the writing performances of these groups.

To determine the degree of relationship between the L2 literacy measures and the summary and response essay tasks, a Pearson product-moment coefficient was calculated (see Table 3). There were low positive correlation coefficients between the writing measure and the test tasks (summary and writing, $\mathbf { r } = . 2 0$ ; response essay and writing, $\mathbf { r } = . 1 2 )$ ; they were not significant. It seemed that the writing ability as measured by the ESL Composition Profile was different from the writing performed in the reading-to-write tasks of the study. The test tasks were found to be significantly related to the reading measure, though weakly (summary and reading, $\mathbf { r } = . 2 8$ ; response essay and read ing, $\mathbf { r } = . 3 8$ ).

Table 1 Descriptive statistics for the reading measure (Nelson-Denny Reading Test)   

<html><body><table><tr><td>Participants</td><td>n</td><td>Min</td><td>Max</td><td>Mean</td><td> SD</td><td>95% CI-lower</td><td>95% CI-upper</td></tr><tr><td>Total sample</td><td>139</td><td>154</td><td>257</td><td>212.17</td><td>29.44</td><td>207.24</td><td>217.11</td></tr><tr><td>NS</td><td>50</td><td>219</td><td>257</td><td>244.00</td><td>9.60</td><td>241.38</td><td>246.82</td></tr><tr><td>ESL-A</td><td>53</td><td>170</td><td>236</td><td>201.23</td><td>16.72</td><td>196.62</td><td>205.83</td></tr><tr><td>ESL-I</td><td>9</td><td>154</td><td>176</td><td>167.22</td><td>8.17</td><td>160.94</td><td>173.50</td></tr><tr><td>EFL</td><td>27</td><td>160</td><td>231</td><td>189.52</td><td>20.28</td><td>181.50</td><td>197.54</td></tr></table></body></html>

${ \mathrm { N S } } =$ Native speaker, ESL-A $=$ ESL Advanced, ESL-I $=$ ESL Intermediate, EFL $=$ English as Foreign Language. Maximum possible score is 258.

Table 2 Descriptive statistics for the writing measure   

<html><body><table><tr><td>Participants</td><td>N</td><td>Min</td><td>Max</td><td>Mean</td><td> SD</td><td>95% CI-lower</td><td>95% CI- upper</td></tr><tr><td>Total sample</td><td>139</td><td>67</td><td>92</td><td>79.55</td><td>5.52</td><td>78.63</td><td>80.48</td></tr><tr><td>NS</td><td>50</td><td>71</td><td>92</td><td>82.04</td><td>5.30</td><td>80.53</td><td>83.55</td></tr><tr><td>ESL-A</td><td>53</td><td>70</td><td>90</td><td>80.01</td><td>5.12</td><td>78.60</td><td>81.42</td></tr><tr><td>ESL-I</td><td>9</td><td>70</td><td>77</td><td>74.89</td><td>2.20</td><td>73.79</td><td>76.58</td></tr><tr><td>EFL</td><td>27</td><td>67</td><td>85</td><td>75.61</td><td>4.37</td><td>73.88</td><td>77.34</td></tr></table></body></html>

${ \mathrm { N S } } =$ Native speaker, ESL-A $=$ ESL Advanced, ESL-I $=$ ESL Intermediate, EFL $=$ English as Foreign Language. Maximum possible score is 100.

# 3.2. Effect of individual factors on reading-to-write performance

Performance in both tasks (see Table 4) may at first glance appear to be similar, as seen in their means (summary 17.22 vs. response essay 17.14); the slight observed difference in means was not statistically significant $( \mathrm { t } = 0 . 2 4$ , $\mathrm { d f } = 9$ , $\mathsf { p } > . 0 5 )$ . However, task performance proved to be a more complex issue when taking into account individual factors such as language proficiency and educational level.

The importance of language proficiency is evidenced by the fact that native speakers (NS) had the highest mean score for all groups in both the summary (18.34, ${ \bf s d } = 2 . 7 4$ ) and the response essay (18.56, $\mathrm { s d } = 2 . 8 8$ ; see Table 5). The EFL learners scored lowest in the summary task (mean 15.85, ${ \bf s d } = 4 . 1 0 $ ), and the ESL-I learners scored lowest in the response essay task (mean 12.94, $\mathrm { s d } = 3 . 3 5$ ). Lack of sophisticated linguistic resources seemed to interact differently with task demands since the lower proficiency groups performed significantly different only in the response essay.

Previous research has shown that differences between the graduate and undergraduate levels of education are likely for both test tasks. However, only the NS and the ESL-A proficiency groups had samples of both educational levels; the ESL-I and EFL participants were all undergraduate students in either an American or a Venezuelan university. Table 6 shows that for the summary task, the graduates outperformed the undergraduates (mean of 17.99 vs. 16.74), but the slightly overlapping confidence intervals for the means indicate that this difference may not be significant. The graduate students also outperformed the undergraduates on the response essay (mean of 18.74 vs. 16.15), being the difference observed statistically significant. Academic experience with the response essay and the critical thinking that it can entail might have been one of the factors differentiating group performance.

Table 3 Pearson coefficients between L2 literacy measures and test tasks   

<html><body><table><tr><td></td><td> Reading</td><td>Writing</td></tr><tr><td>Summary</td><td>.28a</td><td>.20</td></tr><tr><td>Response Essay</td><td>.38a</td><td>.12</td></tr></table></body></html>

a After the Bonferroni adjustment for multiple comparisons, $p < . 0 1$ .

Table 4 Descriptive statistics for the summary and response essay   

<html><body><table><tr><td></td><td>Min</td><td>Max</td><td>Mean</td><td> SD</td></tr><tr><td>Summary</td><td>7.50</td><td>24.50</td><td>17.22</td><td>3.47</td></tr><tr><td>Response Essay</td><td>7.00</td><td>25.00</td><td>17.14</td><td>3.51</td></tr></table></body></html>

Maximum possible score in both tasks is 25.

Table 5 Descriptive statistics for language proficiency across test tasks   

<html><body><table><tr><td>Language Proficiency</td><td>N</td><td>Min</td><td>Max</td><td>Mean</td><td>SD</td><td>95% CI-lower</td><td>95% CI-upper</td></tr><tr><td>Summary</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NS</td><td>50</td><td>13.00</td><td>24.00</td><td>18.34</td><td>2.74</td><td>17.56</td><td>19.12</td></tr><tr><td>ESL-A</td><td>53</td><td>8.50</td><td>25.00</td><td>17.03</td><td>3.49</td><td>16.20</td><td>18.16</td></tr><tr><td>ESL-I</td><td>9</td><td>9.50</td><td>22.00</td><td>16.17</td><td>3.54</td><td>13.44</td><td>18.89</td></tr><tr><td>EFL</td><td>27</td><td>7.50</td><td>24.00</td><td>15.85</td><td>4.10</td><td>14.23</td><td>17.47</td></tr><tr><td>Response essay</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NS</td><td>50</td><td>13.00</td><td>25.00</td><td>18.56</td><td>2.88</td><td>17.74</td><td>19.38</td></tr><tr><td>ESL-A</td><td>53</td><td>9.00</td><td>24.00</td><td>17.18</td><td>3.54</td><td>16.20</td><td>18.16</td></tr><tr><td>ESL-I</td><td>9</td><td>7.00</td><td>17.00</td><td>12.94</td><td>3.35</td><td>10.37</td><td>15.52</td></tr><tr><td>EFL</td><td>27</td><td>9.50</td><td>20.00</td><td>15.81</td><td>3.09</td><td>14.59</td><td>17.04</td></tr></table></body></html>

${ \mathrm { N S } } =$ Native Speaker, ESL-A $=$ ESL Advanced, ESL-I $=$ ESL Intermediate, EFL $=$ English as Foreign Language. Maximum possible score is 25.

The effects of language proficiency and educational level were investigated using a multivariate analysis of variance (MANOVA). The more conservative Pillais criterion was used to evaluate multivariate significance with unequal group sizes, as recommended by Tabachnick and Fidell (2001). Table 7 displays the results of the MANOVA analysis.

As indicated by the significant Pillais values (see Table 7), the combined summary and response essay scores were significantly affected by both proficiency level and educational level, but not by their interaction. The results reflect a modest association between proficiency level and the combined task scores, and between educational level and the combined task scores. Therefore, the proficiency level and the educational level can explain only $14 \%$ of the variance in the two dependent variables.

A univariate analysis revealed a significant L2 proficiency effect on the response essay scores, $\mathrm { ~ F ~ } ( 3 , 1 3 5 ) = 2 . 4 1$ , $\mathsf { p } < . 0 5$ , but no significant impact on summary scores. The post-hoc Games-Howell test showed that, on average, ESLI participants scored lower than NS and ESLA examinees. EFL students scored on average lower than NS students.

A similar picture was observed for the effect of the educational level, which significantly affected performance in the response essay task but not in the summary task. The effects of both the proficiency level and the educational level were modest on the response essay scores. These findings support the assumption that the summary and the response essay tasks are two different dimensions of the reading-to-write construct. Performance in reading-to-write tasks is affected by individual factors in different ways. It seems that more cognitively demanding reading-to-write tasks like the response essay would require more linguistic resources and academic experience with the task to enable reader-writers express and structure content that satisfies the task requirements.

Table 6 Descriptive statistics for the test tasks by educational level   

<html><body><table><tr><td>Participants</td><td>N</td><td>Min</td><td>Max</td><td>Mean</td><td>SD</td><td>95% CI-lower</td><td>95% CI-upper</td></tr><tr><td colspan="8">Summary</td></tr><tr><td> Undergraduate</td><td>86</td><td>7.50</td><td>25.00</td><td>16.74</td><td>3.61</td><td>15.96</td><td>17.51</td></tr><tr><td>Graduate</td><td>53</td><td>10.00</td><td>24.00</td><td>17.99</td><td>3.11</td><td>17.13</td><td>18.85</td></tr><tr><td colspan="8">Response essay</td></tr><tr><td>Undergraduate</td><td>86</td><td>7.00</td><td>24.00</td><td>16.15</td><td>3.32</td><td>15.44</td><td>16.86</td></tr><tr><td>Graduate</td><td> 53</td><td>11.00</td><td>25.00</td><td>18.74</td><td>3.24</td><td>17.84</td><td>19.63</td></tr></table></body></html>

Table 7 MANOVA with two factors effect on summary and response essay task   

<html><body><table><tr><td>Source</td><td>Pillais</td><td>Hypoth. df</td><td>Error df</td><td>Multivariate F</td><td>p</td></tr><tr><td>Proficiency level</td><td>.15</td><td>6</td><td>266</td><td>3.48</td><td>p&lt;.05</td></tr><tr><td>Educational level</td><td>.07</td><td>2</td><td>132</td><td>4.64</td><td>p&lt;.05</td></tr><tr><td>Proficiency  Education</td><td>.04</td><td>2</td><td>132</td><td>2.86</td><td>p&gt;.05</td></tr></table></body></html>

# 4. Summary and discussion

The reading-to-write construct was measured via two tasks: a summary and a response essay. These tasks were found to represent different dimensions of the reading-to-write construct, which accords well with previous studies by Asencio´n (2004) and Durst (1987). In their studies, analytic writing or response essays were found to engage learners in more critical thinking than summaries. The quantitative evidence in this study did not compare the cognitive operations reported by learners in both tasks, but it did show that the response essay represented a more challenging task for lower-proficiency groups and undergraduates with advanced linguistic resources.

The complexity of the response essay in terms of information processing demands imposed on participants interacted more strongly with learners’ factors to better differentiate performance among the groups. The summary writing requires identifying important information in a source text and organizing and condensing information into propositions reflecting its main ideas. This reading-to-write task involves description and public writing, a type of writing believed by experts (Hamp-Lyons & Mathias, 1994) to be easier than the argumentative/public writing represented by the response essay task.

The response essay, on the other hand, seemed to involve more critical thinking in constructing the task representation. Low language proficiency or lack of extensive practice with the task seemed to affect the decisions made by participants when completing the task. These decisions, as Flower et al. (1990) explained, may include what major source of information to use (e.g., the text, the text and reader/writer’s comments, what the reader/writer already knew), the text format and features (e.g., notes plus summary, summary plus opinion, persuasive essay), the organizing plan for writing (e.g., to summarize, to respond to the topic, to review and comment, to interpret for reader/writer’s own purpose), and other participants’ goals when performing the task (e.g., present what I learned, fulfill a page requirement, influence the reader). It might be the case, for example, that an intermediate learner who lacked enough linguistic resources to fulfill the requirements of the task might use parts of the source text that he/she could understand and write comments about those parts as the main source of information for the response essay. This decision could provide a summary plus opinion format with a review and comment-organizing plan for the essay.

The fact that there is a myriad of reading-to-write tasks representing different degrees of cognitive complexity and perceived difficulty points to the notion that the reading-to-write construct cannot be conceived as a unitary ability, but rather as a dynamic ability that interacts with task demands and individual factors. The ability to write a summary does not necessarily indicate an ability to perform other tasks that combine reading and writing like the response essay.

The constructive perspective on reading-to-write emphasizes the interaction of both reading and writing to promote meaning construction; therefore, the reading-to-write ability should be highly related to reading and writing. Findings in this study revealed, however, that reading-to-write scores were weakly related to reading ability. The ability to read a text and then use that information for writing appears to differ from the ability to read for basic comprehension. This low correlation is in line with previous research in which reading score was not a significant predictor of reading-to-write performance (Watanabe, 2001). Being a good reader contributes to being a good reader/ writer, but it is not a sufficient condition to be one. In addition, correlation coefficients showed that there was no relation between the test scores and the writing scores. In this study, the ability to write using information from a source text differed from the ability to write an essay without background reading, as also reported by Messer (1997) in his study with summaries. Therefore, it seems that the pedagogy of reading-to-write should not only emphasize the development of reading for comprehension and writing an essay without background reading support, but it should also foster the awareness that reading-to-write involves an interaction of these skills affected by internal and external factors.

Language proficiency and educational level both had a modest effect on reading-to-write performance, specifically on the response essay task. NS and ESLA learners scored higher than the ESL-I and EFL learners, and graduate reader/writers scored higher than undergraduates. This finding supports the position that the tasks tapped different dimensions of reading-to-write ability because the two test tasks were affected differently by language proficiency and educational level. The modest effect found for language proficiency on reading-to-write performance is in line with previous findings in which language proficiency was considered an additive factor to other variables such as writing expertise to describe the quality of L2 writing (Cumming, 1989). Similarly, the significant effect of educational level on reading-to-write performance is in agreement with the difference in performance between graduates and undergraduates documented by Trites and McGroarty (2005), and also supports the claim by Conrad (1996) that students with an advanced level of education should have extensive practice in the writing required in their disciplines. Language proficiency and educational level had only minimal effects on the reading-to-write tasks in this study, but they did seem to affect reading-to-write performance more strongly when tasks are more cognitively demanding.

# 5. Implications

The theoretical framework for describing reading-to-write in this study was based on the constructivist view of literacy tasks in which the information from a source text is used by the writer to produce a text with his/her own communicative goal (Spivey, 1990, 1997). In this sense, reading-to-write involves the interaction of the meaning construction processes used when reading and writing, as shaped by individual and context factors.

In this study, the summary task and the response essay task reflected the reading-to-write ability differently; this suggests that the ability to read in order to write manifests itself differently across various college literacy tasks. Reading-to-write then should be conceived as a literacy ability that is complex and dynamic because of the multiple factors derived from the task context and the learners’ representations of that context. Therefore, teachers adopting an integrated perspective on reading and writing should provide extensive practice on the specific tasks that students will encounter in their academic careers. Each task could be analyzed in terms of the resources available to complete it and the demands of the discipline, course, or professor for which it was assigned. Similarly, inferences about the reading-to-write ability should not be based on a single task; testing procedures should involve a representative sample of reading-to-write tasks.

In this study, reading-to-write performance was measured using a summary and a response essay based on one source. Future research should include different test tasks such as critiques and discourse synthesis in reading-to-write assessments, as well as comparing performance using different numbers or kinds of source texts. The use of tasks consisting of different degrees of cognitive complexity could also provide more information on the effect of individual factors on the performance of more demanding tasks.

The reading-to-write ability involves some reading for comprehension, but it implies more than this kind of reading because reader/writers read with a writing goal in selecting information from the source that can help them write their texts.

Source text information can include content that stimulates ideas or structures used in the source that could be applied to the reader/writer’s own arguments (Hirvela, 2004).

The reading-to-write ability also differed from the ability to write texts without using sources, in that content and form are affected by the kind of information selected from the source text, by the organization of the source text being imposed on the text being created, and by how well the reader/writer made connections with such information or with their own previous knowledge. The reading-to-write construct would seem to call for an integrated pedagogy of reading and writing. Teachers should not assume that teaching reading and writing separately will prepare students to face the challenges of reading-to-write tasks. In addition, assessing learners’ readiness to perform academic tasks in a second language should involve the performance of reading-to-write tasks; inferences derived about the reading-to-write construct cannot be based solely on independent reading and writing measures.

As these findings do not conform with most of the evidence from previous research on the relationships between reading-to-write performance, reading, and writing, there is a need for more studies that explore these literacy relationships by correlating performance in these specific test tasks and other reading-to-write tasks with other measures of reading comprehension and writing, such as the reading and writing sections of TOEFL and GRE examinations. These multiple comparisons would enable researchers to further evaluate the reading-to-write construct in terms of its connections with reading and writing measures.

# References

Allison, D., Berry, V., & Lewkowicz, J. (1995). Reading-writing connections in E.A.P. classes: a content analysis of written summaries produced under three mediating conditions. RELC Journal, 26, 25e43.

Asencio´n, Y. (2004). Validation of reading-to-write assessment tasks performed by second language learners. Unpublished doctoral dissertation, Northern Arizona University.   
Belcher, D., & Hirvela, A. (2001). Linking literacies. Perspectives on $L 2$ reading-writing connections. Ann Arbor: University of Michigan Press.   
Bridgeman, B., & Carson, S. (1983). Survey of academic writing tasks required of graduate and undergraduate foreign students. [TOEFL Rep. No. 15]. Princeton, NJ: Educational Testing Service.   
Brown, J., Fishco, V., & Hanna, G. (1993). Nelson-Denny reading test. Manual for scoring and interpretation. Forms G & H. Chicago: Riverside.   
Campbell, C. (1990). Writing with other’s words: using background reading text in academic compositions. In B. Kroll (Ed.), Second language writing: Research insights for the classroom (pp. 211e230). Cambridge: Cambridge University Press.   
Cardozo, R. (2000). Estrategias de lectura utilizadas por futuros docentes de ingle´s como lengua extranjera para la comprensio´n de textos expositivos en este idioma. Unpublished master’s thesis, Universidad Pedago´gica Experimental Libertador, Caracas, Venezuela.   
Carson, J., & Leki, I. (1993). Reading in the composition classroom. Second language perspectives. Boston: Heinle and Heinle.   
Carver, R. (1997). Reading for one second, one minute, or one year from the perspective of the rauding theory. Scientific Studies on Reading, 1, 3e43.   
Connor, U., & Krammer, M. (1995). Writing from sources. Case studies of graduate students in business management. In D. Belcher, & G. Braine (Eds.), Academic writing in a second language: Essays on research and pedagogy (pp. 155e182). Norwood, NJ: Ablex.   
Conrad, S. (1996). Academic discourse in two disciplines: professional writing and student development in Biology and History. Unpublished doctoral dissertation, Northern Arizona University.   
Corbeil, G. (2000). Exploring the effects of first- and second-language proficiency on summarizing in French as a second language. Canadian Journal of Applied Linguistics, 3, 35e62.   
Cumming, A. (1989). Writing expertise and second-language proficiency. Language Learning, 39, 81e141.   
Cumming, A., Rebuffot, J., & Ledwell, M. (1989). Reading and summarizing challenging texts in first and second languages. Reading and Writing: An Interdisciplinary Journal, 2, 201e219.   
Durst, R. (1987). Cognitive and linguistic demands of analytic writing. Research in the Teaching of English, 21, 347e376.   
Enright, M., Grabe, W., Koda, K., Mosenthal, P., Mulcahy-Ernt, P., & Schedl, M. (2000). TOEFL 2000 reading framework: A working paper. [TOEFL Monograph MS-17]. Princeton, NJ: Educational Testing Service.   
Enright, M., Bridgeman, B., & Cline, F. (April 2002). Prototyping a test design for a new TOEFL. In D. Eignor (Chair), Research in support of the development of new TOEFL. Symposium conducted at the meeting of the National Council on Measurement in Education. New Orleans. Available from http://www.ets.org/research/conferences/aera2002.html. Accessed 29.04.04.   
Fitzgerald, J., & Shanahan, T. (2000). Reading and writing relations and their development. Educational Psychologist, 35, 39e50.   
Flower, F., Stein, V., Ackerman, J., Kantz, M., McCormick, K., & Peck, W. (Eds.). (1990). Reading-to-write: Exploring a cognitive and social process. New York: Oxford University Press.   
Grabe, W. (2001). Reading-writing relations: theoretical perspectives and instructional practices. In D. Belcher, & A. Hirvela (Eds.), Linking literacies. Perspectives on L2 reading-writing connections (pp. 15e47). Ann Arbor: University of Michigan Press.   
Grabe, W. (2003). Reading and writing relations: second language perspectives on research and practice. In B. Kroll (Ed.), Exploring the dynamics of second language writing (pp. 242e262). Cambridge: Cambridge University Press.   
Grabe, W., & Stoller, F. (2002). Teaching and researching reading. London: Longman.   
Hale, G., Taylor, C., Bridgeman, B., Carson, J., Kroll, B., & Kantor, R. (1996). A study of writing tasks assigned in academic degree programs. [TOEFL Rep. No. 54]. Princeton, NJ: Educational Testing Service.   
Hamp-Lyons, L. (1991a). Reconstructing ‘‘academic writing proficiency’’. In L. Hamp-Lyons (Ed.), Assessing second language writing in academic contexts (pp. 127e153). Norwood, NJ: Ablex.   
Hamp-Lyons, L. (1991b). Scoring procedures for ESL contexts. In L. Hamp-Lyons (Ed.), Assessing second language writing in academic contexts (pp. 241e276). Norwood, NJ: Ablex.   
Hamp-Lyons, L., & Mathias, S. P. (1994). Examining expert judgment of task difficulty on essay tests. Journal of Second Language Writing, 3, 49e68.   
Hatch, E., & Lazaraton, A. (1991). The research manual. Design and statistics for Applied Linguistics. Boston: Heinle & Heinle.   
Hayes, J. (1996). A new framework for understanding cognition and affect in writing. In C. M. Levy, & S. Ransdell (Eds.), The science of writing (pp. 1e27). Mahwah, NJ: Lawrence Erlbaum.   
Hirvela, A. (2004). Connecting reading & writing in second language writing instruction. Ann Arbor: University of Michigan Press.   
Jacobs, H., Zinkgraf, A., Wormuth, D., Hartfiel, V., & Hughey, J. (1981). Testing ESL composition. Rowley, MA: Newbury House.   
Johns, A., & Mayes, P. (1990). An analysis of summary protocols of university ESL students. Applied Linguistics, 11, 253e271.   
Kennedy, M. (1985). The composing process of college students writing from sources. Written Communication, 2, 434e456.   
Khaldieh, S. (2000). Learning strategies and writing processes of proficient vs. less-proficient learners of Arabic. Foreign Language Annals, 33, 522e534.   
Kintsch, W. (1998). Comprehension: A paradigm for cognition. New York: Cambridge.   
Kirkland, M., & Saunders, M. (1990). Maximizing student performance in summary writing: managing cognitive load. TESOL Quarterly, 25, 105e121.   
Kucer, S. (1985). The making of meaning: reading and writing as parallel processes. Written Communication, 2, 317e336.   
Lewkowicz, J. (1994). Writing from sources: does source material help or hinder students’ performance? Paper presented at the Annual International Language in Education Conference, Hong Kong. [ERIC Document Reproduction Service No. ED386050].   
Mathison, M. A. (1996). Writing the critique, a text about a text. Written Communication, 13, 314e354.   
Messer, S. D. (1997). Evaluating ESL written summaries: an investigation of the ESL Integrated Summary Profile (ISP) as a measure of the summary writing ability of ESL students. Unpublished doctoral dissertation, Florida State University.   
Nelson, N., & Calfee, R. (1998). The reading-writing connection viewed historically. In N. Nelson, & R. Calfee (Eds.), The readingewriting connection. Ninety-seven yearbook of the National Society for the Study of Education (pp. 1e52). Chicago: National Society for the Study of Education.   
Newell, G., Garriga, M., & Peterson, S. (2001). Learning to assume the role of author: a study of reading-to-write one’s own ideas in an undergraduate ESL composition course. In D. Belcher, & A. Hirvela (Eds.), Linking literacies: Perspectives on L2 reading-writing connections (pp. 164e185). Ann Arbor: University of Michigan Press.   
Quintero, M. (2003). Comparacio´n de las estrategias de lectura utilizadas en textos en ingle´s presentados en pantalla y presentados en papel. Un published master’s thesis, Universidad Pedago´gica Experimental Libertador, Caracas, Venezuela.   
Risemberg, R. (1996). Reading-to-write: self-regulated learning strategies when writing essays from sources. Reading Research and Instruction, 35, 365e383.   
Rosenfeld, M., Leung, S., & Oltman, P. K. (2001). Identifying the reading, writing, speaking, and listening tasks important for academic success at the undergraduate and graduate levels. [TOEFL Monograph Series MS-21]. Princeton, NJ: Educational Testing Service.   
Ruiz-Funes, M. (1999). The process of reading-to-write used by a skilled Spanish-as-a-foreign-language student: a case study. Foreign Language Annals, 32, 45e62.   
Spivey, N. (1990). Transforming texts: constructive processes in reading and writing. Written Communication, 7, 256e287.   
Spivey, N. (1997). The constructivist metaphor: Reading, writing and the making of meaning. San Diego: Academic Press.   
Tabachnick, B., & Fidell, L. (2001). Using multivariate statistics (4th ed.). Boston: Allyn and Bacon.   
Trites, L., & McGroarty, M. (2005). Reading to learn and reading to integrate: new tasks for reading comprehension tests? Language Testing, 22, 174e210.   
Uiuc Ept Video-Reading Based Academic Test. (1999). http://ux6.cso.uiuc.edu/%7Efgd/rvressay.htm. Accessed 13.05.00.   
Watanabe, Y. (2001). Read-to-write tasks for the assessment of second language academic writing skills: investigating text features and rater reactions. Unpublished doctoral dissertation, University of Hawaii.

Yuly Asencio´n Delaney is an assistant professor of Spanish at Northern Arizona University. Her research focuses on reading-to-write in a second language and language assessment. Her research has been presented in such conferences as the American Association for Applied Linguistics and she has published in Hispania and the Canadian Modern Language Review.