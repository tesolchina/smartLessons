# The use of interactive input in EAP listening assessment

John Read\*

School of Linguistics and Applied Language Studies, Victoria University of Wellington, PO Box 600, Wellington, New Zealand

# Abstract

The assessment of listening comprehension for academic purposes is an area which has not received much attention from researchers. This article focuses on the form of the input for EAP listening tests. While there is a great deal of interest currently in the use of visual media for listening assessment, it is likely that tests with purely auditory input will continue to have a significant role. The article reports on the development of a test in two audiotaped versions: a scripted monologue and an unscripted discussion of the same topic by three speakers. The test was administered to two matched groups of learners taking an intensive pre-sessional EAP course. In contrast to the results of an earlier study by Shohamy and Inbar [Langugage Testing 8 (1991) 23], it was found that the monologue version was significantly less difficult than the discussion. Various possible reasons for the difference in findings are presented and the article concludes with a consideration of what can be learned from the research for the design of listening test tasks with interactive input.

$©$ 2002 Elsevier Science Ltd. All rights reserved.

# 1. Introduction

Listening is an important skill for learners of English in an academic study context, since so much of what they need to understand and learn is communicated through the oral medium. Although this skill often functions as an integral part of two-way oral interaction, students routinely find themselves in what Buck (2001: 98) calls ‘‘non-collaborative’’ listening situations, in which they listen to a lecture, seminar presentation or tutorial discussion without having the opportunity to participate orally themselves. Thus, non-collaborative listening tasks are an established component of EAP proficiency tests and are likely to remain so. In the design of such tests, a common practice is to take the university lecture as the prototypical target language use situation (in terms of Bachman & Palmer’s (1996), framework for test design). This means that the stimulus material has the form of a talk or minilecture on a general academic topic and the learners are required to demonstrate their comprehension of the content by responding to a set of test items.

However, if we set out to design such a test from scratch, we are confronted with numerous questions:

Should the test include other target language use situations besides the lecture, such as the seminar discussion or individual consultation with a lecturer?   
Should the stimulus material be taken from a genuine lecture, and if not, should it be scripted or not?   
How should differences among the learners in background knowledge of the topic(s) be addressed?   
Should the test input be delivered live, on audiotape, or through visual media like videotape and multimedia packages?   
Should the stimulus material be heard only once, or should it be repeated? Should the test-takers be allowed—or expected—to take notes while they are listening?   
To what extent should the response task be a ‘‘pure’’ measure of listening comprehension, involving only a minimum amount of reading or writing?

Undoubtedly the questions are answered in various ways in tests used by EAP programmes around the world. In addition, Buck’s (2001) book Assessing Listening gives a comprehensive overview of the issues and a wealth of practical advice on test design, drawing on relevant research where it is available. However, the published research studies on listening assessment are limited in number, and those involving EAP tests are even fewer. This means there is plenty of scope for further research on what Bachman and Palmer (1996) call the ‘‘task characteristics’’ of listening comprehension tests.

Some researchers have looked at choice of topic, and more specifically the role of background knowledge of the topic on listening test performance, since its effect on reading comprehension is well documented. Chiang and Dunkel (1992) compared their Taiwanese students’ understanding of a talk in English on a culturally familiar topic (Confucianism) with an unfamiliar one (the Amish people in the USA) and found a significant effect, although it manifested itself only in test questions which could be answered without reference to the passage. A more clear-cut result was obtained by Schmidt-Rinehart (1994), who found that her American students of Spanish understood a topic already covered in their textbook much better than one which they had not studied in Spanish before. In an EAP setting in the USA, Jensen and Hansen (1995) produced inconsistent results indicating that students who had previously studied the topic of a lecture could comprehend it better in some cases, particularly if the topic was more technical in nature. Thus, the evidence for a significant background knowledge factor is mixed. As Jensen and Hansen point out, one of the difficulties in obtaining more conclusive results is to find an adequate operational measure of how much the test-takers know about the listening test topic.

Research has also focused on the type of test items used and how they relate to the input text. Shohamy and Inbar (1991) found that ‘‘local’’ questions on factual details and the meanings of lexical items were easier to answer than ‘‘global’’ questions on main ideas and inferences, while ‘‘trivial’’ questions on incidental numbers, dates and names yielded inconsistent responses. Two TOEFL research studies have analysed how various features of the short dialogues (Nissan, DeVincenzi, & Tang, 1995) and the minitalks (Freedle & Kostin, 1999) used as stimuli in the TOEFL listening section influence the difficulty of the multiple-choice items based on them. Freedle and Kostin concluded that, while characteristics of the test items themselves have some effect, it is text and text/item overlap variables which are the most significant predictors of item difficulty.

Another aspect of listening test design that has received some attention is the issue of when to present the test items. Both Groot (1975) and Sherman (1997) found that the test scores were essentially the same whether the questions were given after the learners had listened to the stimulus material rather than before. However, Sherman’s experiment also showed that significantly higher scores were achieved with a ‘‘sandwich’’ format, whereby the test-takers listened to the input text once, were given the questions and then heard the text a second time. It was also the format which created the least anxiety and was strongly preferred by the test-takers over both the ‘‘questions-before’’ and ‘‘questions-after’’ conditions.

One area which is now being addressed is the form of the input. The standard method of presenting the stimulus material for a listening test, especially large-scale and high-stakes ones, is by means of a pre-recorded audiotape. However, given the routine use of videos and other visual material in the contemporary language classroom, there has been increasing interest in providing various forms of visual input in listening assessment. Already the listening section of the computer-based TOEFL test incorporates photos to accompany each of the dialogues and mini-talks (Ginther, 2001). In addition, several writers, notably Gruba (1997), have advocated the use of visual media in language assessment and there has been research to investigate the use of video input for listening comprehension tests (Coniam, 2001; Gruba, 1993; Progosh, 1996).

The results of the research have been rather variable. Progosh (1996) elicited very positive attitudes to video-based listening tests among the Japanese learners he surveyed. In her TOEFL study, Ginther (2001) found that test-takers generally preferred listening input with accompanying visuals, and the pictures had some positive effect on comprehension when they provided content information for mini-talks or assisted the test-takers to identify the various speakers in an academic discussion. On the other hand, Gruba’s (1993) results showed that a video version of a simulated lecture had no measurable effect on students’ test performance, as compared to a purely audio version. In a similar kind of comparative study involving English teachers in Hong Kong, Coniam (2001) found that if anything the group listening to an audio version of an educational discussion understood it better than those who took the video version. In addition, over $80 \%$ of the video group considered that the video had not helped their comprehension at all, over a third of them (almost) never looked at the screen during the test, and the majority expressed a preference for audio as a listening test medium. Such reports are not uncommon in more informal accounts of trials with visual input of listening tests in various parts of the world.

Clearly, further investigation and conceptual development is required to determine how best to employ visual material for listening assessment, in keeping with the construct definition of listening comprehension that is appropriate for particular testing programmes. In the meantime, for practical as much as for more principled reasons, listening tests with only auditory input will continue to have a prominent role for the foreseeable future. This means that there is still scope for research on various characteristics of listening test input based on audiotaped texts.

One substantial contribution to such research was the study by Shohamy and Inbar (1991), who focused on the degree to which listening test stimuli have the features of natural speech, as distinct from formal written language. They developed three versions of a listening test in English for Israeli high school students, each with a different text type: a scripted monologue simulating a news broadcast, a lecturette in which the speaker interacted with an addressee, and a consultative dialogue between an expert and an addressee. The versions were designed to represent distinct points along a literate to oral continuum. According to the results, the monologue was significantly more difficult to understand than the two more ‘‘oral’’ text types. Shohamy and Inbar pointed out that the three texts were distinguished by numerous discoursal and pragmatic features, such as the density of the propositions, the amount of repetition and redundancy, and the degree of grammatical complexity. Although the authors saw the three types of input as being ranged along a continuum, the essential difference appears to be the interaction between speaker and audience in the lecturette and dialogue, as compared to the one-way communication of the news broadcast. Shohamy and Inbar’s conclusion was that a variety of text types should be used in a listening comprehension test, depending on the purpose of the test and the learners’ listening needs.

The present study can be seen as exploring the implications of Shohamy and Inbar’s findings in an EAP context. It was undertaken as a joint research project between the applied linguistics programmes of the University of Melbourne, Australia and Victoria University of Wellington, New Zealand. Both institutions had existing academic listening tests as part of their institutional EAP proficiency test batteries. In Melbourne the whole test was used primarily to assess the English proficiency of international students on arrival at the university. The listening test was based on an audiotaped talk, with short-answer questions. On the other hand, in Wellington the proficiency test was administered towards the end of a 3-month intensive EAP course as a basis for reporting on the students’ proficiency to the university admissions office, the sponsoring agency (where applicable) and the students themselves. In this case, the listening test incorporated two scripted talks that were presented live to all the students assembled in a large lecture theatre.

At both institutions there was a desire to explore alternative formats for listening assessment. Initially the intention was to develop a new form of the Melbourne test using videotaped input and compare it with an audio version based on the same content. However, two main considerations caused us to change tack. One was the force of Gruba’s (1997) argument against the meaningfulness of comparisons between tests using different input media. His position is that each medium needs to be evaluated separately, in its own terms. The other was a more practical concern about the cost of good-quality video production, especially since parallel forms of the video-based test would be required on an ongoing basis if it were to be incorporated into the operational test battery. Instead, the decision was made to compare two forms of audiotaped input: a scripted monologue and a unscripted discussion of the same content by three speakers.

Thus the aims of the research were first to investigate the feasibility of developing a suitable audiotaped discussion and then to compare it as input material for an EAP listening test with a conventional scripted monologue on the same topic.

# 2. The study

# 2.1. Development of the test material

The initial step in designing the test was to identify a topic that was appropriate for academic study purposes and would lend itself to discussion from more than one point of view. The topic chosen was ethical issues in medical research. To provide the basis for the test material, we used two case studies involving ethically dubious practices which had received prominent media attention in our two countries. The first one was the subject of a series of investigative articles in a Melbourne newspaper, which reported how in the late 1940s and early 1950s medical researchers developing vaccines for childhood diseases had conducted trials in babies’ homes and orphanages. The trials had been largely unsuccessful in preventing infection and the issue was whether the researchers were justified in carrying out tests of unproven medicine with young children separated from their families. The second case was one that had created considerable controversy in New Zealand in the mid-1980s when it was revealed that a professor of gynaecology at an Auckland hospital had sought to challenge the then-conventional wisdom that a condition called carcinoma in situ (CIS) was a precursor of cervical cancer, by withholding from numerous patients the normal treatment for CIS (namely, a hysterectomy) in order to see whether cancer would develop. The women were not informed either that they had CIS or that they were participants in an experiment.

From a present-day perspective, the two cases raised similar issues: the ethics of experimentation on patients using unproven or controversial treatments; the need to protect the rights and interests of vulnerable members of society; and the necessity of obtaining from patients or their guardians informed consent to the treatment proposed by the doctors.

A script was written for the monologue version of the test in four sections. The first part presented the case of the Melbourne vaccine trials, followed by a section which discussed the ethical issues arising from it. The third and fourth parts did the same for the cervical cancer case in Auckland. The preparation of the script enabled us to clarify what factual information needed to be presented and the ethical issues that were involved, as well as to identify the unavoidable technical terms which were required for the discussion of the topic. The script was written in the relatively formal style of a broadcast talk on public radio, without inserting features of informal speech, such as hesitations, false starts or fillers such as ‘‘you know’’, ‘‘you see’’ or ‘‘Now, ..’’. In this respect, it was similar to the stimulus material for the existing Melbourne listening test.

Next the discussion version of the test was developed. An initial attempt to make a recording involved a free-wheeling discussion of the two medical cases by three speakers. This proved to be unworkable because the conversation lacked a clear discourse structure, assumed too much shared knowledge and did not lend itself to the writing of suitable test questions. Thus, a second, more carefully planned version was prepared. It was designed to match the four-part structure of the monologue, while at the same time simulating a university tutorial rather than a scripted lecture. The three speakers had read the source articles on the two cases, as well as the monologue script, and they planned in advance how the discussion would proceed, but it was not scripted at all. In order to make it easier for listeners to distinguish the speakers, each took a distinct role. One, a middle-aged woman, took the tutor’s role, introducing each section of the discussion, allocating turns, clarifying certain points and summing up as necessary. The other two younger speakers—a female and a male—acted as students, who each summarised the facts of one of the cases and then debated the ethical issues. Apart from their gender, the students were distinguishable by the fact that the female took a more critical stance towards the medical researchers, whereas the male tended to put the case for the doctors and to defend their actions in relation to the prevailing ethical standards at the time the research was conducted.

In addition to the spoken input, a written text of about 500 words was prepared. Its purpose was to orient the test-takers to the subject matter before they listened to the spoken test material. The text gave general information about the topic, with some background on each of the two cases but without revealing any of the specific information needed to respond to the test items. Some key terms were highlighted in bold font and defined in the text e.g. ethics, vaccine, trials, uterus, cervix/cervical cancer and hysterectomy. Time constraints on the administration of the test meant that the test-takers were allowed just $3 \ \mathrm { m i n }$ to read the text, although it was also available to refer to while they were taking the test itself.

Both versions of the input material were recorded on audiotape, with standard instructions and pauses included. There was a pause of 2 min before each of the four sections of the test to allow time to read the test items based on that section. The test-takers needed to write answers to the test items while they were listening to the input, although there was also a 1-min pause at the end of each section (and $2 \mathrm { m i n }$ after the last section) for them to complete and check their responses. While it can be argued that listening to input and writing answers in this concurrent fashion is cognitively quite demanding, it simulated the target language use situation in that university students commonly need to take notes as they listen to lectures or seminar discussions. The total running time for the monologue tape was $3 2 \mathrm { { m i n } }$ and for the discussion tape it was $3 7 ~ \mathrm { { m i n } }$ . Thus, the interactive nature of the discussion meant that it took several minutes longer to cover the test content, as compared to the scripted talk.

A common set of test items was developed for the two versions of the test. They were originally drafted on the basis of the monologue script, for the practical reason that the discussion version had not been transcribed at that point. They were subsequently checked against the transcript, to ensure that all of the information needed to respond to the items was included in the discussion and that the order of the items followed the sequence of information in both versions. The items were of the shortanswer type, requiring responses ranging from a single word or number to a sentence. In its final form, there were 36 items in the test. They were presented to the test-takers in a booklet in which they wrote their responses. The booklet also contained the introductory written text.

In addition to the test material, a questionnaire was written to be administered to the participants after they had completed the test. Part 1 of the instrument contained six items to obtain background information from the learners. This included their first language, length of residence in the country, plans after the course, and selfrating of their listening skills. Part 2 elicited the learners’ opinions about the test by means of eight three- or four-option closed-response items. The items focussed on the comprehensibility of the speech, the suitability of the topic, the presentation of the test questions and the overall difficulty of the test. At the end of the questionnaire, the students were invited to write their own comments about the test. As it turned out, very few of them did so and thus this final question yielded no usable data.

# 2.2. Participants

The original plan was to conduct the study in both Melbourne and Wellington. However, there were only a small number of learners available to be tested in Melbourne during the period when the data-gathering needed to be carried out. Thus, most of the participants in the study were adult non-native speakers of English taking a 3-month intensive English course at Victoria University of Wellington. They came from a range of national and language backgrounds, as is typical for courses of this kind in English-speaking countries. Their predominant geographical origin was East and Southeast Asia, accounting for three-quarters of the total. According to the questionnaire responses, the largest language groups were speakers of Chinese (31), Japanese (11), Korean (9), Vietnamese (9) and Thai (5). Other regions of the world represented were Europe (10), South Asia (5), East Africa (2) and the South Pacific (2). The learners were a mix of relatively recent immigrants and international students who had entered the country on a student visa. Twentyseven of them had been in the country only since the beginning of the course and a further 34 had been resident for up to 1 year. The others had lived in the country for varying periods, but only four had been there for more than 5 years.

Most of the learners were studying English for academic or occupational purposes. About two-thirds of them reported that, after the end of the course, they would undertake studies at the undergraduate or postgraduate level. Of the others, 10 planned to return to their own country, nine intended to work in New Zealand and seven were going to take another English course.

The New Zealand participants were in six classes, constituted partly on the basis of proficiency level, as determined by a placement test at the beginning of the course, but also according to whether they planned to study at the undergraduate or postgraduate level. They were in the same class for most of the $^ { 2 3 \mathrm { ~ h ~ } }$ of instruction per week, following an integrated-skills course based on a series of weekly study themes. Each week all the classes attended a talk by a guest lecturer and they also had two sessions in an audio-visual classroom equipped with a language laboratory system. The present study was conducted in the eighth and ninth weeks of the course.

An additional eight students were individually recruited in Melbourne and were tested following the same procedure as in Wellington.

# 2.3. Procedure

The testing took place in Wellington during two of each class’s weekly sessions in the audio-visual classroom. In the first week of the study, the students took one version of the existing Melbourne listening test on the topic of dreams. This functioned as a pre-test, to provide a common measure of the participants’ listening proficiency and to give data necessary for the formation of matched pairs. The students were paired primarily on the basis of their pre-test scores but—especially when an odd number of them obtained a particular score—class membership, gender and national origin were also taken into account. One member of each pair was randomly assigned to Group A and the other went to Group B.

The following week, the six classes were scheduled in twos for their sessions in the two adjoining audio-visual classrooms. At each session, students from both classes who had been assigned to Group A went to one classroom and those assigned to Group B went to the other. It had been randomly determined that Group A would hear Version 1 (monologue) and Group B Version 2 (discussion). Nine students who took the pre-test did not attend the experimental test session in Week 2. Conversely, there were 11 students who missed the pre-test but took the experimental one. The latter participants were paired with another absentee from their own class as much as possible and randomly assigned to Group A or B. In one class, all the students took both tests; otherwise the absentees were fairly evenly spread across the five remaining classes. In total, then, 96 participants took the experimental test: 47 in Group A and 49 in Group B.

First, the test-takers received the answer booklet containing the 500-word written background text as well as the listening test items. They were given three minutes to read the text and then the test itself began. As in the pre-test, the test tape was played from the language laboratory console and the participants listened through headsets at their individual booths. When they had completed the test, the answer booklets were collected and the questionnaire was handed out. The participants filled in the questionnaire right away and returned it to the test supervisor when they had finished.

# 3. Results and discussion

# 3.1. The whole test

The test proved to be quite difficult overall, with a mean score for all 96 test-takers of just 16.0 out of 36 and a standard deviation of 8.0. Given that some subjective judgement was involved in scoring the short-answer questions, the reliability of the whole test was quite satisfactory, at 0.89 (based on a Rasch calculation of the reliability of the person ability estimates).

In Part B of the post-test questionnaire, there were some items that applied to the test as a whole and the two groups of test-takers had very similar response patterns to them. Thus, it is useful to review these items before proceeding to consider the comparison of the two versions of the test. The combined responses from the two groups are presented in Table 1.

With regard to the topic of medical ethics, two-thirds of the test-takers reported it was very familiar to them, which was a little surprising, and it is unlikely that more than a few of them had prior knowledge of the two particular cases which were discussed. Their level of interest in the topic was somewhat lower than its familiarity, but still most of them rated it as at least ‘‘interesting’’.

The other three questionnaire items in Table 1 were concerned with the way that the test questions were presented in the test booklet. The majority of the students indicated that they did not have a problem understanding how they were to respond to the questions. On the other hand, between a quarter and a third of them considered the layout hard to follow, the test questions difficult to understand and/or the reading time insufficient. A review of the individual responses shows that these were not the same students in each case; only five of them gave this exact response pattern. Nevertheless, the fact that a substantial number expressed these concerns— including students who obtained good scores in the test—means that these aspects should be further investigated before any operational administration of the test.

Table 1 Reactions to the test by the test-takers as a whole   

<html><body><table><tr><td colspan="5">The topic (medical ethics) was.</td></tr><tr><td>Very familiar</td><td>Familiar</td><td>Not familiar</td><td> No response</td><td>Total</td></tr><tr><td>67 Very interesting 30</td><td>22 Interesting</td><td>2 Not interesting</td><td>5 No response</td><td>96 Total</td></tr><tr><td></td><td>57</td><td>7</td><td>2</td><td>96</td></tr><tr><td colspan="5">The layout of the test booklet was.</td></tr><tr><td>Very clear</td><td>Clear</td><td>Hard to follow.</td><td>Total</td><td></td></tr><tr><td>13</td><td>56</td><td>27</td><td>96</td><td></td></tr><tr><td colspan="5">Understanding the test questions was.</td></tr><tr><td>Easy</td><td>Quite easy</td><td>Difficult</td><td>Total</td><td></td></tr><tr><td>18</td><td>48</td><td>30</td><td>96</td><td></td></tr><tr><td colspan="5">The time allowed for reading the test questions was</td></tr><tr><td>Too short</td><td> Just right</td><td>Too long</td><td>Total</td><td></td></tr><tr><td>26</td><td>67</td><td>3</td><td>96</td><td></td></tr></table></body></html>

# 3.2. The comparison of the versions

The key question in this study was how the two groups performed according to which version of the test they took. The relevant statistics are set out in Table 2. When we compare the mean scores, we find that Group A clearly had the higher mean (18.0 vs. 14.16) and the $t$ -test confirmed that it was a significant difference at the 0.05 level. Thus, contrary to our expectation based on the Shohamy and Inbar (1991) study, the monologue version of the test turned out to be less difficult than the discussion one.

There are a number of possible reasons for the higher mean score achieved by the group who took the monologue version of the test. One is simply a practice effect. Group A had taken a very similar test the previous week, one which also involved listening to a scripted talk. On the other hand, Group B were presented with a new form of input, which they may not have previously encountered in a listening test. To control for this potential effect, the pre-test should ideally have been in a different format from either of the tests used in the experiment.

The second advantage that the monologue group may have had concerns the relationship between the two forms of stimulus material and the test items. As previously explained, the items were originally written on the basis of the monologue script, then later checked against the transcript and modified where necessary to ensure that all of the information needed to answer the test items was presented in both texts. Despite this precaution, the items are still likely to have had a closer relationship to the monologue script than to the discussion. For instance, a subsequent review of the test material shows that for at least one pair of items the information required was given in the reverse order in the discussion, whereas it followed the same sequence as the items in the monologue. In addition, a couple of the students who took the discussion version commented to the test administrator afterwards that they had understood what was said reasonably well but had found it difficult to compose their responses to the questions requiring full-sentence answers.

Table 2 Statistical analysis of the two versions of the experimental test   

<html><body><table><tr><td></td><td>Group A (monologue)</td><td>Group B (discussion)</td></tr><tr><td>No. of test-takers</td><td>47</td><td>49</td></tr><tr><td>No. of items</td><td>36</td><td>36</td></tr><tr><td>Mean</td><td>18.0</td><td>14.16</td></tr><tr><td>Standard deviation</td><td>8.32</td><td>7.40</td></tr><tr><td>Reliabilitya</td><td>0.90</td><td>0.88</td></tr><tr><td>T-test</td><td colspan="2">t=2.38 (P&lt;0.05; df=94)</td></tr></table></body></html>

a The Rasch analogue of KR-20, an internal consistency measure.

This suggested the idea that a scripted monologue may present ideas in a form that makes it easier to supply answers than a discussion does.

The third source of evidence for the differences in group performance is the posttest questionnaire, particularly in items where the two groups gave different patterns of response. The results are presented in Table 3. The first three items in this category, which related to perceptions of the speech the test-takers heard on the tape, all produced statistically significant differences. With regard to the speed of the speech, half of the group who listened to the discussion rated the speakers as ‘‘too fast’’, whereas the monologue group mostly found the speech was at a good speed for them. This result is understandable, given that the discussion group had to contend with three speakers, who often had a succession of short turns. However, the testtakers’ perceptions of the voices are more difficult to interpret. The group who heard the monologue version rated the clarity of the speaker’s voice and accent lower than the discussion group did. This is somewhat surprising, since the monologue group had only one speaker to listen to, whereas the discussion group heard three. In addition, one of the speakers in the discussion was actually the presenter of the monologue.

Table 3 Responses of the two groups of test-takers to the different versions of the test   

<html><body><table><tr><td>The speaker(s) spoke</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Too fast</td><td>At a good speed Too slowly.</td><td></td><td>Total</td></tr><tr><td>Group A (monologue) 12</td><td></td><td>35</td><td>0</td><td>47</td></tr><tr><td>Group B (discussion) 24</td><td></td><td></td><td>1</td><td>49</td></tr><tr><td></td><td></td><td></td><td></td><td>x2=7.13 (p&lt;0.01, df=1)</td></tr><tr><td colspan="5">The voice(s) on the tape were.</td></tr><tr><td></td><td>Very clear</td><td>Quite clear</td><td>Not very clear</td><td> No response</td></tr><tr><td>Group A</td><td>0</td><td>17</td><td>29</td><td>1</td></tr><tr><td>Group B</td><td>2</td><td>26</td><td>21</td><td>0</td></tr><tr><td></td><td></td><td></td><td></td><td>x2=3.94 (p&lt;0.05, df=1)</td></tr><tr><td colspan="5">The accent of the speaker(s) was.</td></tr><tr><td></td><td>Very clear</td><td>Reasonably clear Hard to understand Total</td><td></td><td></td></tr><tr><td>Group A</td><td>0</td><td>21</td><td>26</td><td>47</td></tr><tr><td>Group B</td><td>3</td><td>30</td><td>16</td><td>49</td></tr><tr><td></td><td></td><td></td><td></td><td>x2=4.45 (p&lt;0.05, df=1)</td></tr><tr><td colspan="5">The time allowed for answering the questions was</td></tr><tr><td></td><td>Too short</td><td> Just right</td><td>Too long</td><td>Total</td></tr><tr><td>Group A (monologue) 32</td><td></td><td>15</td><td>0</td><td>47</td></tr><tr><td>Group B (discussion)25</td><td></td><td>24</td><td>0</td><td>49</td></tr><tr><td></td><td></td><td></td><td></td><td>x2=2.23 (n.s., df=1)</td></tr><tr><td colspan="5">In general, the test was</td></tr><tr><td></td><td></td><td>Very difficult Quite difficult.</td><td>Quite easy</td><td>Very easy</td><td>Total</td></tr><tr><td>Group A</td><td>8</td><td>34</td><td>5</td><td>0</td><td>47</td></tr><tr><td>Group B</td><td>18</td><td>26</td><td>5</td><td>0</td><td>49</td></tr><tr><td></td><td></td><td></td><td></td><td>x2=4.87 (n.s., df=2)</td></tr></table></body></html>

The next item elicited the test-takers’ perceptions of the amount of time allowed for answering the questions. There was no significant difference in the group responses in this case. If anything, it was those who listened to the monologue who tended to consider that there was not enough time. Thus, this item does not provide any evidence that Group B were at a disadvantage through having to respond to the test questions by listening to the discussion rather than the scripted talk.

Finally, the last questionnaire item referred to the general level of difficulty of the test. The test-takers as a whole responded on the ‘‘difficult’’ side of the scale and those in Group B had a stronger tendency to rate it as being very difficult, but overall the response patterns were not significantly different. Both groups perceived it to be a challenging test.

In summary, then, the questionnaire responses have not produced consistent evidence that might help to explain the significantly higher scores obtained by the group who received the monologue version of the test.

Another approach to explaining the differences between the two kinds of input focuses on the discrepancy between our results and those of Shohamy and Inbar (1991). It is likely that our input texts were not strictly comparable with those used in the earlier study. If we take the monologue texts first, the Shohamy and Inbar ‘‘news broadcast’’ was deliberately designed to have the characteristics of a written text, with ‘‘literate and explicit vocabulary, complex sentences and . . .no redundancies, repetitions or pauses’’ (1991: 28). By contrast, our monologue script was written to be somewhat more ‘‘oral’’ in nature, with a restricted amount of nominalization and syntactic complexity, some repetition of key vocabulary and numerous discourse markers to give explicit signals of the structure of the text. It may in fact have had more in common with the Shohamy and Inbar lecturette, apart from the fact that there was no interaction with an addressee.

In the case of the comparison between our discussion text and the consultative dialogue of the earlier study, there were undoubtedly many shared features. However, one crucial difference involves the way that the material was developed. Although this is not explicitly stated by Shohamy and Inbar (1991), their consultative dialogue was scripted in advance, as were their other two texts, and then the features of each text were carefully analysed in order to ensure that they defined a distinct genre along the literate-oral continuum (Inbar-Lourie, 1987). This was quite different from the process by which our discussion text evolved, as described above. Although our speakers prepared beforehand and the actual discussion was guided so that it would shadow the structure of the monologue, none of the speech was scripted. Consequently, our discussion was probably a more genuine sample of the kind of ‘‘spontaneous’’ and ‘‘colloquial’’ speech that Shohamy and Inbar set out to simulate in their consultative dialogue, to the extent that ours was very demanding for an audience of non-native speakers to listen to.

Thus, in various respects our input texts had different characteristics from those used in the previous study. It is on this basis, then, that we can interpret the inconsistency between our result—that the discussion was more difficult for our learners to comprehend—and Shohamy and Inbar’s finding that oral texts incorporating dialogue were easier to understand. One implication of the discrepancy is that the notion of a single literate-oral continuum is an inadequate concept to capture the complex ways in which oral texts may vary, as for instance Biber (1988) demonstrated through his multidimensional analysis of the features of spoken and written texts. It appears that the analysis undertaken by Shohamy and Inbar was able to distinguish their three forms of input as text, but it did not fully account for the rhetorical complexity of these kinds of discourse from a listener’s perspective, especially when more than one speaker was involved and the speech was unscripted.

# 4. Conclusion

The comparison of the two forms of listening test input is a somewhat artificial exercise. Ultimately the point is not simply to demonstrate that one is superior to the other, or alternatively that the two are interchangeable. Each needs to be considered for inclusion in a test on its own terms. The conclusion that Shohamy and Inbar (1991: 37) drew from their study was that listening tests should consist of more than one form of input in order to reflect the range of genres inherent in underlying theories of listening comprehension. In an EAP context, this means that we should question whether a scripted simulation of a lecture is an adequate way of operationalising the construct of academic listening ability. The value of a comparative study then is, first of all, to demonstrate that students perform differently according to the nature of the input and secondly, to bring out the distinctive features of each genre.

From this perspective, the fact that our two texts had the reverse order of difficulty from that found by Shohamy and Inbar (1991) is not a matter of great concern. As previously discussed, there are some possible reasons for the different result related to the way that we conducted our study, but the key factor is likely to have been the complexity of the variables involved in any comparison of distinctive types of spoken text. While Shohamy and Inbar highlighted features of an interactive text which could make it easier to understand, there are numerous other characteristics which can create greater difficulty for listeners, particularly when it is an unscripted discussion on a topic of which they may have limited background knowledge. Thus, the issue for listening test design is how to adjust the level of difficulty of an interactive text to make it suitable for the proficiency level of the test-takers, while at the same time maintaining its authenticity as a sample of academic speech.

One of the problems in achieving this balance is the lack of research on speaking in academic contexts that might serve as a reference point for the validation of test tasks. There is some work on university lectures and the problems non-nativespeaking students experience in them (see, e.g. Flowerdew, 1994), but less on other academic listening situations. A few discourse analyses of seminar discussions have now appeared (Basturkmen, 2002; Tapper, 1996). In addition, there are least two corpora, the TOEFL Spoken and Written Academic Corpus (Biber et al., 2002) and the Michigan Corpus of Academic Spoken English (MICASE) (Simpson et al.,

2000), which include recordings of speech in various campus settings. The MICASE corpus in particular is freely available on the Web and, within the limitations of its relatively small size, will allow researchers and test developers to obtain evidence of the key features of different kinds of spoken interaction in academic contexts.

However, as some authors have noted recently (see Douglas & Nissan, 2001; Buck, 2001: 154–160), it is difficult to find corpus texts or other authentic spoken material which can be used directly as input for a listening test. Therefore, creating stimulus material specifically for the test tends to be the most feasible option, and our experience with the discussion text may be useful as a model for other test development projects. If the text is not to be scripted, a certain amount of preparation is required to help ensure that the discussion will be accessible to the listeners and will have the kind of structure and content that facilitate the writing of test items, while still capturing the qualities of natural speech. As we found, the first attempt may need to be discarded. It is interesting to note in this regard that, for the Hong Kong project reported by Coniam (2001: 6), no fewer than five discussions were recorded before the moderation committee decided on the most suitable one for their listening test. With an audio recording one potential problem for the listeners is to distinguish the participants in a discussion. Even when (as in our case) the test items do not require the test-takers to attribute information or opinions to particular speakers, it is helpful to be able to identify the individual voices. We found that gender, role, age and point of view could be used effectively to distinguish the three speakers in our discussion.

This study has addressed some of the questions about listening test design which I listed at the outset, but by no means all of them. On the issue which was the main focus of our research—the form of the input—I have shown that there are numerous variables which can affect the difficulty of an interactive text in audio form. While work on various forms of visual input will undoubtedly continue, there is also room for further work on audiotaped alternatives to the scripted talk in order that tests may represent more fully the construct of academic listening ability.

# Acknowledgements

My collaborators in this project were Kathryn Hill and Elisabeth Grove at the Language Testing Research Centre of the University of Melbourne, and I express my great appreciation for all their contributions to the design of the study, the development of the test material and the analysis of the data. I also wish to thank Alastair Ker and Susan Smith for their help in planning and implementing the project in Wellington. The work was funded by a grant from the Collaborative Research Program at the University of Melbourne, together with matching support from the School of Linguistics and Applied Language Studies, Victoria University of Wellington.

# References

Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice. Oxford: Oxford University Press.   
Basturkmen, H. (2002). Negotiating meaning in seminar-type discussion and EAP. English for Specific Purposes, 21, 233–242.   
Biber, D. (1988). Variation across speech and writing. Cambridge: Cambridge University Press.   
Biber, D., Conrad, S., Reppen, R., Byrd, P., & Helt, M. (2002). Speaking and writing in the university: a multidimensional comparison. TESOL Quarterly, 36, 9–48.   
Buck, G. (2001). Assessing listening. Cambridge: Cambridge University Press.   
Chiang, C. S., & Dunkel, P. (1992). The effect of speech modification, prior knowledge, and listening proficiency on EFL lecture listening. TESOL Quarterly, 26, 345–374.   
Coniam, D. (2001). The use of audio or video comprehension as an assessment instrument in the certification of English language teachers: a case study. System, 29, 1–14.   
Douglas, D., & Nissan, S. (2001). Developing listening prototypes using a corpus of spoken academic English. Paper presented at the Language Testing Research Colloquium, St. Louis, MO.   
Flowerdew, J. (Ed.). (1994). Academic listening: research perspectives. Cambridge: Cambridge University Press.   
Freedle, R., & Kostin, I. (1999). Does the text matter in a multiple-choice test of comprehension? The case for the construct validity of TOEFL’s minitalks. Language Testing, 16, 2–32.   
Ginther, A. (2001). Effects of the presence and absence of visuals on performance on TOEFL CBT listeningcomprehensive stimuli. TOEFL Research Report, 66. Princeton, NJ: Educational Testing Service.   
Groot, P. J. M. (1975). Testing communicative competence in listening comprehension. In R. L. Jones, & B. Spolsky (Eds.), Testing language proficiency. Arlington, VA: Center for Applied Linguistics.   
Gruba, P. (1993). A comparison study of audio and video in language testing. JALT Journal, 16, 85–88.   
Gruba, P. (1997). The role of video media in listening assessment. System, 25, 335–345.   
Inbar-Lourie, O. (1987). The effect of text and question type on achievement in listening comprehension tests in English as a Foreign Language. Unpublished Master’s thesis, Tel-Aviv University.   
Jensen, C., & Hansen, C. (1995). The effect of prior knowledge on EAP listening test performance. Language Testing, 12, 99–119.   
Nissan, S., DeVincenzi, F., & Tang, K. L. (1996). An analysis of factors affecting the difficulty of dialogue items in TOEFL listening comprehension. TOEFL Research Report, 51. Princeton, NJ: Educational Testing Service.   
Progosh, D. (1996). Using video for listening assessment: opinions of test-takers. TESL Canada Journal, 14, 34–44.   
Schmidt-Rinehart, B. C. (1994). The effects of topic familiarity on second language listening comprehension. Modern Language Journal, 78, 179–189.   
Sherman, J. (1997). The effect of question preview in listening comprehension tests. Language Testing, 14, 185–213.   
Shohamy, E., & Inbar, O. (1991). Validation of listening comprehension tests: the effect of text and question type. Language Testing, 8, 23–40.   
Simpson, R. C., Briggs, S. L., Ovens, J., & Swales, J. M. (2000). The Michigan corpus of spoken academic English. Ann Arbor, MI: The Regents of the University of Michigan [Available online at www.lsa.umich.edu/eli/micase/micase.htm].   
Tapper, J. (1996). Exchange patterns in the oral discourse of international students in college classrooms. Discourse Processes, 22, 25–55.