# Make Words Click! Learning English Vocabulary with clickers: users’ perceptions

Anne-Marie Sénécal, Walcir Cardoso & Vanessa Mezzaluna

To cite this article: Anne-Marie Sénécal, Walcir Cardoso & Vanessa Mezzaluna (2024) Make Words Click! Learning English Vocabulary with clickers: users’ perceptions, Computer Assisted Language Learning, 37:4, 766-788, DOI: 10.1080/09588221.2022.2055082

To link to this article: https://doi.org/10.1080/09588221.2022.2055082

# Make Words Click! Learning English Vocabulary with clickers: users’ perceptions

Anne-Marie Sénécal $\textcircled{1}$ , Walcir Cardoso $\textcircled{1}$ and Vanessa Mezzaluna

Department of Education, Centre for the Study of Learning and Performance (CSLP), Concordia University, Montreal, Canada

# ABSTRACT

Clickers are hand-held devices that wirelessly transmit student input to a computer: students answer multiple-choice questions using their clickers and the answer distribution is displayed on a screen. Previous studies suggest that the pedagogical use of these devices may contribute to learning and that they are positively perceived by students in general and second language education. Despite these optimistic outcomes, clicker studies remain scarce in L2 education and in K-12 contexts.

This study investigated 61 adolescent students’ and their teacher’s perceptions of using clickers to learn vocabulary in an English as a Second Language context. Two intact groups of students were assigned to a treatment group (Clicker Group, $n = 3 1$ ; Non-Clicker Group, $n = 3 0$ ). Their perceptions were examined via surveys and interviews, guided by four measures: Learning, Self-assessment, Engagement, and Interactivity. The results suggest that students in the Clicker Group had significantly more positive perceptions than those in the Non-Clicker Group for most measures. This corroborates previous findings regarding students’ perceptions of clickers. Interviews were conducted to assess the teacher’s perceptions. In contrast to the students, the teacher’s perception was predominantly neutral to negative, contradicting existing literature.

# KEYWORDS

Learner response systems; clickers; L2 pedagogy; users’ perceptions

The integration of clickers or Learner Response Systems (LRSs) in education is not a novelty (Abrahamson, 2006; Judson & Sawada, 2002; Kay & LeSage, 2009), as the first LRS system was introduced in 1966 at Stanford University (Abrahamson, 2006; Judson & Sawada, 2002). However, because LRSs were expensive, inaccessible, and unreliable, improved prototypes were developed and tested at various universities over the next decades (Abrahamson, 2006). These upgraded versions became commercially available in the 1990s, leading to a rapid expansion of the use of LRSs in educational institutions at the beginning of the twenty-first century (Abrahamson, 2006; Judson & Sawada, 2002). The rapid gain in popularity of LRSs may be explained by developments in educational theories, particularly those that promote peer instruction or active learning (Mazur, 1997), interaction (Hake, 1998), and the delivery of feedback that is immediate (assumed to increase learners’ performance in comparison with delayed feedback – Cardoso, 2013). It could also be hypothesized that the use of a similar tool in popular television shows like "Who Wants to be A Millionaire" played a role in the widespread use of LRSs (Caldwell, 2007; Cardoso, 2013).

The popularity of LRSs is no longer limited to institutions of higher learning, as they have been adopted in numerous primary and secondary institutions. In 2005, for example, it was estimated that over 3,000 American primary and secondary schools had integrated this technology into their classrooms (Abrahamson, 2006) and that over 10,000 courses have adopted LRSs worldwide according to the latest statistical report (Chien et  al., 2016).

Given LRSs’ wide integration in primary and secondary institutions (Abrahamson, 2006), it is important to examine the extent to which the affordances of the technology in a K-12 educational context can benefit these students. However, the literature related to the pedagogical benefits of LRSs in the K-12 context remains scarce, especially in the field of instructed second language acquisition (ISLA). Since the aforementioned affordances associated with the pedagogical approach characterized by the use of clickers (e.g., interaction, peer instruction) align with interactionist theories of ISLA (e.g., Long, 1996), clicker-based instruction seems to have pedagogical potential in the second language (L2) classroom. Accordingly, the scope of the Make Words Click! project, a two-part study, is to examine the implementation of an LRS to learn vocabulary in an L2 classroom in a K-12 educational context. While Part 1 examined the impact of LRSs on learning gains involving vocabulary acquisition (see Sénécal et  al., 2018), the current study (Part 2) investigates the users’ perceptions of the pedagogical benefits and suitability of LRSs, including both students and teachers.

# Background

# Learner response systems: introduction and method of operation

Clickers, a type of LRS, are small handheld response keypads (see Figure 1B) that allow for an entire class to quickly and anonymously respond

# A

#

# Which of the following is a good definition for the word dew?

Which of the following is a good definition for the word dew?

![](img/814c67f16b287371c42fa9687e94d93f5bc231b3abfd9feb97e963516c9876b6.jpg)  
Figure 1. Clickers: method of operation.

to multiple-choice questions (MCQs) displayed on a projection screen at specific moments during a lesson (Figure 1A). Prior to the lesson, MCQs are created by the instructor using clicker software (e.g., TurningPoint by Turning Technologies), which are often integrated into presentation software (e.g., Microsoft PowerPoint). After allowing time for the students to read and think about the question and their answer, the instructor opens the polling period, allowing the students to anonymously submit their responses to the question using their own clickers. The students record their individual responses by pressing the button on the clicker that they feel corresponds to the answer (Figure 1B), which is instantaneously and wirelessly sent to a receiver on the instructor’s computer (Figure 1C). The answers can then be instantly displayed in a visual format such as a histogram or column chart (Figure 1D). The answer distribution graphic informs the students of the distribution within the classroom as well as the instructor of the level of understanding of the target question.

The distribution of results may influence how the instructor will proceed (Bruff, 2009; Cardoso, 2013; Kay & LeSage, 2009). For instance, if most students choose the correct answer (item “a” in Figure 1), the teacher can decide to proceed to other teaching targets. If, on the other hand, the students are divided in their answers (Figure 1D), the instructor might suggest a convince-your-neighbour discussion (thus promoting peer instruction; Mazur, 1997), after which a re-vote could take place to re-assess student understanding.

# Student perception

Although researchers agree that the interactions afforded by clickers may be beneficial for learning (Bruff, 2009; Cardoso, 2011), it remains unclear how users (students and teachers) perceive their pedagogical use, particularly in a K-12 context. Examining users’ perceptions is an important component of curriculum design, as it helps teachers and school administrators select the best tools to address students’ and teachers’ interests and needs (Nation & Macalister, 2010).

Research investigating students’ perceptions of their pedagogical experience using LRSs indicate overwhelmingly positive perceptions (e.g., Caldwell, 2007; Draper & Brown, 2004; Judson & Sawada, 2002; Kaleta & Joosten, 2007). During an exploratory study that took place over a period of two academic years, Draper and Brown (2004) surveyed over 3,000 participants involved in classes that ranged in size from 20-300 students, dispersed among various departments at the University of Glasgow. The benefits of the pedagogical use of LRSs were elicited from the students, which the researchers grouped into a list of nine items. For example, students felt the LRS increased their enjoyment and peer-topeer interaction, gave them a certain degree of anonymity, as well as provided them with opportunities to share opinions, to compare level of understanding with other students, to self-assess understanding, and to self-identify problem areas. Similar findings were observed in another large-scale study by Kaleta and Joosten (2007) who found positive student perceptions as well.

More recent literature has also revealed overwhelmingly positive student perception (Hunsu et  al., 2016). In fact, Oigara and Keengwe (2013) study of 24 undergraduate students enrolled in a Physical Geography class at an American university assessed student perception in terms of various affordances of LRSs (i.e., increased participation and engagement, assessment capabilities) and noted positive attitudes toward all of them. These results were corroborated by those of Blasco-Arcas et  al. (2013) and Wang (2015) who also examined engagement, student learning, and interactivity. Using Kahoot! as the LRS, Wang (2015) compared long-term versus short-term use of the application in a university context to examine the possibility of a wear-off effect in extended usage of LRSs. The study found no considerable wear-off effects, indicating that the long-term use of LRSs may be pedagogically beneficial (Wang, 2015).

Although research involving the perceptions of L2 students remains limited, more studies have begun to emerge in this field. For instance, Cardoso (2011) investigated students’ perceptions of the use of LRSs among 30 adult participants in an English as a Foreign Language classroom in Brazil. The author categorized the recurrent affordances from the literature into six common denominators: motivation/interest, involvement/participation, self-assessment, comparison of performance, interaction, and contribution to learning. Survey results indicated positive perceptions in all areas, except for the category of “increased interaction” (possibly due to the already interactive methodology adopted by the school where the experiment occurred). Similar positive outcomes in perception were also observed in Serafini (2013) and McDonough and Foote (2015), who investigated the attitudes of students in terms of potential for learning, assessment, and engagement (see also Marlow, 2010; Song et  al., 2017 and Hung, 2017 for similar results).

# Operationalization of user perception

A review of the literature on users’ perceptions of LRSs reveals four recurring measures that assess perception based on the educational theories that support the potential for clickers to increase or enhance students’ ability:

To learn – Learning (e.g., Caldwell, 2007). For this study, Learning refers to the perception of the number of vocabulary words learned (see forthcoming methods) and what participants attribute their learning to.   
To (self-)assess their performance – Self-assessment (e.g., Hunsu et  al., 2016). This measure fits in tightly with feedback, and it offers individuals an indication as to whether an answer is accurate or inaccurate when the correct answer is provided (Hattie & Timperley, 2007). This allows students to internalize the correct answer instead of only recognizing that a mistake was made (Lantz, 2010). The concept refers to evaluating one’s own knowledge, becoming aware of new vocabulary, and comparing understanding and knowledge amongst peers.   
To engage – Engagement (e.g., Hunsu et  al., 2016). The term refers to student qualities pertaining to commitment, involvement, or participation (Fredricks et  al., 2004). In this study, Engagement represents student motivation, participation, and enjoyment. Finally, to interact – Interactivity (e.g., Cutrim Schmid, 2008). This measure refers to the interactions that take place between the students, teacher, and technology. It indirectly relates to the level of comfort participants feel to share their answers with other students or the teacher (Cutrim Schmid, 2008).

In a meta-analysis examining the pedagogical effects of LRSs, Hunsu et  al. (2016) further subsumed the measures utilized in clicker research into two broad categories: (1) “cognitive” measures (e.g., skills that the brain uses to learn, evaluate, and reason), and (2) “non-cognitive” measures (e.g., abilities associated with “soft skills” such as motivation and interaction). Based on their insights, we adopted the following conceptual classification for the four measures adopted in the current study: cognitive measures (Learning and Self-assessment) and non-cognitive measures (Engagement and Interactivity). For details on the rationale behind these measures and their subcomponents, see Cardoso, 2011. The prominent literature in L2 and general education contexts that has measured students’ perceptions using these measures is summarized in Table 1. As shown, most perception studies focus on an adult population (post-secondary students) and include at least three of the four measures targeted by the current study.

Table 1. S tudent perception of LRS s: an overview of recent studies.   

<html><body><table><tr><td></td><td></td><td></td><td></td><td colspan="2">Cognitive Measures</td><td colspan="2">Non-cognitive Measures</td></tr><tr><td>Author</td><td>Age</td><td> Sample</td><td>Subject</td><td>&quot;learn&quot;</td><td>&quot;assess&quot;</td><td>&quot;engage&quot;</td><td>&quot;interact&quot;</td></tr><tr><td>Blasco-Arcas et al. (2013)</td><td>Adults</td><td>198</td><td>Non L2</td><td>+</td><td>N/A</td><td>+</td><td>+</td></tr><tr><td>Cardoso (2011)</td><td>Adults</td><td>30</td><td>L2</td><td>+</td><td>+</td><td>+</td><td>=</td></tr><tr><td>Cutrim Schmid (2007)</td><td>Adults</td><td>30</td><td>L2</td><td>N/A</td><td>+</td><td>+</td><td>+</td></tr><tr><td>Cutrim Schmid (2008)</td><td>Adults</td><td>30</td><td>L2</td><td>N/A</td><td>+</td><td>+</td><td>+</td></tr><tr><td>Draper and Brown (2004)</td><td>Adults</td><td>4000+</td><td>Non L2</td><td>+</td><td>+</td><td>+</td><td>+</td></tr><tr><td>Hung (2017)</td><td>Adults</td><td>44</td><td>L2</td><td>N/A</td><td>N/A</td><td>+</td><td>+</td></tr><tr><td>Kaleta and Joosten (2007)</td><td>Adults</td><td>2684</td><td>Non L2</td><td>+</td><td>+</td><td>+</td><td>+</td></tr><tr><td>MacGeorge et al. (2008)</td><td>Adults</td><td>390</td><td>Non L2</td><td>+</td><td>+</td><td>+</td><td>N/A</td></tr><tr><td>Marlow (2010)</td><td>Adults</td><td>90</td><td>L2</td><td>+</td><td>N/A</td><td>+</td><td>+</td></tr><tr><td>McDonough and Foote (2015)</td><td>Adults</td><td>44</td><td>L2</td><td>+</td><td>+</td><td>+</td><td>N/A</td></tr><tr><td>Oigara and Keengwe (2013)</td><td>Adults</td><td>24</td><td>Non L2</td><td>+</td><td>+</td><td>+</td><td>+</td></tr><tr><td>Serafini (2013)</td><td>Adults</td><td>37</td><td>L2</td><td>+</td><td>+</td><td>+</td><td>N/A</td></tr><tr><td>Song et al. (2017)</td><td>Adults</td><td>23</td><td>L2</td><td>+</td><td>N/A</td><td>+</td><td>+</td></tr><tr><td>Wang (2015)</td><td>Adults</td><td>126</td><td>Non L2</td><td>+</td><td>N/A</td><td>+</td><td>+</td></tr></table></body></html>

Note. The column headers correspond to the four measures adopted: “learn” $=$ Learning, $\prime \mathtt { a s s e s s } ^ { \prime \prime } =$ Self-assessment, “engage” $=$ Engagement, and “interact” $=$ Interactivity. $^ +$ indicates the adoption of that measure in the respective study.

# Teacher perception

The overwhelmingly positive perceptions of students of the pedagogical experience with LRSs are aligned with that of teachers in the literature (e.g., Chevalier, 2011; Fuller & Dawson, 2017). However, teacher perception studies tend to focus on the pedagogical benefits (e.g., attendance taking, formative and summative evaluations) and challenges (e.g., student dishonesty and technical difficulties) associated with the tool’s implementation (e.g., Milholland, 2015).

Research investigating teacher perceptions of LRSs is more abundant in the general education context (specifically K-12) compared to the L2 context (K-12 and beyond). For instance, Chevalier’s (2011) study of 11 science teachers in a high school who used LRSs suggested that teachers held positive views of LRS; particularity, the technology helped them cover more subject matter in a shorter period and facilitated the administration of formative assessments (for similar results, see Fuller & Dawson, 2017). In an L2 setting, one of the few studies to examine ESL teachers’ perception of clickers was conducted by Agbatogun (2014), using the Technology Acceptance Model as a framework. Findings revealed positive perceptions in terms of usefulness and ease of use, which consequently affected the participants’ intention to implement the technology in their classrooms.

A limitation of research on teachers’ perception is that, in many studies, teachers were required to have voluntarily implemented LRSs in their classroom for at least one semester to be eligible to participate in the study (e.g., Chevalier, 2011; Milholland, 2015). It may be anticipated that if a teacher decides to implement a technology in their classroom, they will have positive perceptions of that tool and of its pedagogical effectiveness. Another limitation is that studies that focus on teachers do not target the comparable measures to those investigating students’ perceptions. The current study addresses these two limitations by examining a teacher who had not used LRSs prior to the experiment. In addition, it utilizes the same set of measures adopted for examining students’ perceptions, mutatis mutandis.

# The current study

The goal of this study is to examine learners’ and their teacher’s perceptions of clickers as pedagogical tools based on a set of cognitive (Learning, Self-assessment) and non-cognitive measures (Engagement, interactivity). As such it aims to contribute to the existing literature by providing additional evidence of users’ perceptions of the pedagogical use of LRSs. It also aims to address some of the existing gaps in research: it examines clicker users from an understudied population (i.e., L2 context; K-12 students and their teacher), it provides a comparison between two experimental groups (one group answering MCQs with clickers and the other with hand-raising, both assisted by PowerPoint presentations), and it employs the same set of cognitive and non-cognitive measures to assess students’ and teacher’s perceptions. As such, this research was guided by the following two research questions:

1. What are the users’ (students’ and their teacher’s) perceptions of their pedagogical experience with clickers in a high school L2 English classroom, in terms of cognitive (Learning, Self-assessment) and non-cognitive measures (Engagement, Interactivity)?   
2. How do students’ perceptions of the cognitive and non-cognitive measures of clickers compare between the two experimental groups?

Regarding the first question, based on the overwhelmingly positive perceptions of both students and teachers in the literature, we hypothesize that these users will perceive their technology-enhanced pedagogical experience positively. Concerning the second research question, we hypothesize that the students’ perceptions of both the cognitive and non-cognitive measures will be more positive among the students using clickers.

# Method

# Participants: students and teacher

Student participants were 61 high-school students (Grade 8; 13-14 years of age) enrolled in an ESL program at a private francophone high school in Montréal, Québec. According to the school’s placement process, the students were in the Core ESL program, indicating they were high beginners. Using convenience sampling, two intact classes taught by the same teacher were recruited to participate in the study.

The teacher participant $\left( n = 1 \right)$ was a woman over 40-years old with 15 years of experience teaching ESL. Although the teacher had access to and knowledge of using technology in her classroom, (e.g., she had used PowerPoint in some of her classes), she used clickers for the first time during this study.

The project received ethics clearance from Concordia University’s office of research (Certificate $\# 3 0 0 0 9 0 2 0 )$ . Further, all participants provided written consent to participate. Considering that all student participants were minors, parents also submitted consent for their children to participate in the study.

# Materials: vocabulary instruction treatment

The study was built around the novel James and the Giant Peach by Roald Dahl, which is both age- and level-appropriate for the target Grade 8 students. This novel was chosen because it was part of the booklist taught during the year, which allowed for the treatments to be integrated into the curriculum. The novel was also selected for the presence of rich vocabulary. The target vocabulary words selected to assess vocabulary retention were 30 low-frequency words to limit prior learning or exposure to the target learning items (see Sénécal et  al., 2018, for the rationale).

Students took part in vocabulary instruction sessions guided by the MCQs embedded in PowerPoint presentations created by the researchers in which the target words were highlighted (via italics and bolding), using the actual contexts in which they occurred in the novel (see Figure 1 for an example). For the design of the MCQs, standard procedures to assess vocabulary knowledge were adopted (e.g., Thornbury, 2002), including translation, definition (see Figure 1 for an example), and synonyms.

The two intact groups were randomly assigned to a treatment: Clicker Group (CG; $n = 3 1$ ) or Non-Clicker Group (NCG; $n = 3 0 \mathrm { \mathrm { ~ } }$ ). In the CG, the MCQs were designed in a clicker software (i.e., TurningPoint by Turning Technologies) and subsequently embedded in PowerPoint presentations, and students anonymously responded to the MCQs via their clickers. In the NCG, the participants received the same treatment without the clickers. Instead of clickers, this group used hand-raising or their voices to answer the same MCQs embedded in PowerPoint Presentations.

# Instruments

# Perception survey (students)

The participants completed a survey of 13 Likert-scale items with responses on a scale ranging from 1 (strongly disagree) to 5 (strongly agree). The survey, adapted from Cardoso (2011), was constructed to assess student perception of the four cognitive and non-cognitive measures, as described earlier. It was administered in French, the participants’ first language. Due to attrition, only 29 students completed the survey in each group. For the participants in the CG, the survey questions focused on the pedagogical benefits of MCQs answered by clickers. For the NCG, the survey questions addressed the pedagogical benefits of MCQs in conjunction with hand-raising and other techniques.

# Perception interview (students and teacher)

To substantiate the quantitative data from the survey results, volunteer-students participated in interviews (CG: $n = 8$ ; NCG: $n = 8$ ). The interviews were individual, semi-structured, and lasted approximately 15 minutes each. The interviews, conducted in the participants’ native language (French), consisted of open-ended “Please explain” questions that focused on the four measures as well as questions related to identifying positive and negative aspects of the implementation of MCQs with (CG) or without the clickers (NCG). For instance, to probe the participants’ view on the Learning measure, they ranked their perception of the following statement: The instructional approach [CG: MCQs with PowerPoint and clickers; NCG: MCQs with PowerPoint and hand-raising] contributed to my learning of words. Additionally, the participants in the NCG were introduced to clickers to gather their perceptions of how they would compare the pedagogical approaches.

The teacher’s interview consisted of similar questions but amended to accommodate the interviewee’s status. For example, for the measure of Learning, the teacher was asked the following question: Do you believe the clickers had a direct impact on how much they learned the words being taught: why or why not? The interview lasted twenty minutes and was carried out in English (the teacher’s L1). The teacher’s perceptions were only measured via qualitative instruments – comparisons were not possible because there was only one teacher.

# Research design

Figure 2 illustrates the research design of the Make Words Click! Project, which was divided into two parts: (1) performance (shown in grey), reported in Sénécal et  al. (2018) and (2) perception (shown in blue), which is the focus of the current study.

The first part (see Sénécal et  al., 2018) focused on student performance and followed a between-groups pretest-posttest-delayed posttest experimental research design to measure the participants’ acquisition of the target vocabulary items between the two experimental groups after taking part in vocabulary instruction sessions. After initial exposure to the target words while reading the novel, the vocabulary instruction sessions included five in-class sessions. Each session covered six target words and lasted approximately 30 minutes (i.e., five minutes per word). The treatments lasted two months (weeks 2-8). The participants’ learning gains were assessed on three occasions (pretest: week 1; immediate posttest: week 8; delayed posttest: week 12), where they were asked to demonstrate their knowledge of the target words through various means such as drawing, translating, and defining the word. The results of Study

![](img/e18d1ed4bda201b23b4a7c67da7995128a31e584febb44037b2571206060ba6d.jpg)  
Figure 2. R esearch design.

1 indicated that there were comparable learning gains in both groups; however, the Clicker Group yielded higher standard deviations on the two posttests, suggesting that some participants benefited more than others in that group, possibly due to individual differences (Sénécal & Cardoso, 2021).

The second part, the current study, focuses on the users’ (students and teacher) perceptions of their use of clickers via a mixed-methods research design. Following the immediate posttest, students’ perceptions were gathered quantitatively via surveys (week 8). To substantiate the quantitative data, interviews were carried with students as well as with the teacher after the delayed posttest (week 12) to provide qualitative input.

# Data analysis

The quantitative data obtained from the survey consisted of the responses to 13 multiple-choice items, which were converted into numeric values. All statements were positively worded (i.e., no reverse coding). The numeric values were then entered into SPSS (v.25.0) and were analyzed via descriptive statistics (mean, median, standard deviation). A Mann-Whitney U test was run to compare student perception between the CG and the NCG. This test was selected as it is a non-parametric test, appropriate for data that follow a non-normal distribution (Field, 2018). Pearson $r$ coefficient was calculated to measure effect size (Cohen, 1988).

The survey was assessed for reliability. Field (2018) suggests that reliability is acceptable at Cronbach’s $\mathrm { ~ a ~ } = \mathrm { ~ . 7 0 ~ }$ . Internal reliability was high for the scale of cognitive measures, Cronbach’s $\mathrm { ~ a ~ } = \mathrm { ~ . 7 5 ~ }$ . Within the cognitive measures, Learning had high reliability, Cronbach’s ${ \mathfrak { a } } =$ .77. However, Self-assessment, the other subscale of the cognitive measures, had low reliability, Cronbach’s $\mathrm { ~ a ~ } = \mathrm { ~ . 4 6 ~ }$ . Internal reliability was high for the scale of non-cognitive measures, Cronbach’s $\texttt { a } = \ldots 7 2$ . Within the non-cognitive measures, Engagement had relatively low reliability, Cronbach’s $\mathrm { ~ a ~ } = \mathrm { ~ . 6 9 ~ }$ . Interactivity, the other subscale to the non-cognitive measures, also had relatively low reliability, Cronbach’s $\mathrm { ~ a ~ } = \mathrm { ~ . 5 9 ~ }$ .

It is suggested that the more items are on the scale, the more Cronbach’s α will increase (Field, 2018). Considering the largest number of items on a scale was four, high internal reliability was not probable. A lower cut-off point (Cronbach’s $\mathfrak { a } = . 6 0 \mathrm { \AA }$ ) is generally accepted in the existing literature assessing internal reliability for tests related to user perception of the pedagogical benefits of clickers (e.g., Agbatogun, 2014; MacGeorge et  al., 2008; Song et  al., 2017).

Because the scales referring to Self-assessment $( \mathsf { a } = . 4 6 )$ and Engagement $\mathbf { \check { a } } = . 6 9 )$ remained with a relatively low reliability, we decided to consult a panel $\displaystyle { \big ( n = 9 \big ) }$ selected based on their background in ISLA and pedagogy to critically analyze internal reliability for the four scales. The panel categorized the thirteen survey items in the four measures (Learning, Self-assessment, Engagement, Interactivity): they were asked to match survey items such as "contribution to learning of words" and "interactivity with teacher" with one of the four measures. Internal reliability was established since the placement of the items in the subscales corresponded to the initial distribution. For example, for Engagement, $7 4 \%$ of the panel assigned the three related survey items to that category, as originally conceptualized by the researchers.

The qualitative data obtained from the interviews carried out with students (from the two groups: CG and NCG) and the teacher consisted of the audio-recordings of responses to open-ended questions. These recordings were transcribed by the researchers, who later translated key passages into English for analyses and dissemination. These key passages were then compiled to extract elements related to the target measures.

To facilitate the analysis, the transcriptions were organized in an Excel spreadsheet, which was organized with a column for each question. Answers were placed underneath their respective questions. For example, one column for the question in which students discussed how the pedagogical approach had an impact on their interaction with other students included all answers related to that question.

The interview transcriptions were analyzed using a deductive thematic approach (Creswell, 2014). The initial step of the qualitative analysis consisted of identifying response patterns targeting the four measures. When reading the interview answers, themes (e.g., interaction, participation, enjoyment, motivation) were included in brackets after the piece of information corresponding to that theme. For example, one participant from the CG said “Yes, [we interacted much more] than if it had been a paper-based activity”. This response was categorized under the theme of interaction.

Then, the responses related to the measures were coded by the researchers based on Saldaña’s (2009) magnitude scale (positive, negative, neutral, mixed) to assess student perception of each measure. As such, the example above was categorized under the theme of “interaction” and was coded as a “positive” comment, showing the interactional benefit of clickers.

Two researchers independently categorized the measures and coded them based on the magnitude scale, after which they verified the entire set of data to ensure consistency in the analysis. In case of discrepancies, which affected less than $5 \%$ of the interview transcripts, they worked to reach consensus. If no consensus was achieved, the problematic passage was discarded from the analysis.

# Results

# Quantitative results

# Students’ cognitive measures

Table 2 shows the assessment of the students’ perception of the pedagogical benefits of clickers in terms of the two cognitive measures: Learning and Self-assessment. These results (in bold) indicate that students in the CG perceived their pedagogical experience with clickers more positively than those in the NCG, as will be clarified below.

The students in the CG ( ${ \bf \nabla } \cdot { \bf n } = 2 9 ,$ ) had positive perceptions of the pedagogical benefits of clickers for the overall category of cognitive measures $\mathbf { \Phi } ^ { \prime } \mathbf { M } = 3 . 8 3$ , $\mathrm { S D } = 0 . 6 3 )$ ), since the means for the survey items related to Learning $( \mathrm { M } = 3 . 7 8 $ , $\mathrm { S D } = 0 . 8 4 \rangle$ and Self-assessment $\mathbf { \zeta } ^ { \prime } \mathbf { M } = 3 . 9 1$ , $\mathrm { S D } = 0 . 5 9 )$ ) were above the level of neutrality (established at 2.5 on the scale of 1 to 5). Similarly, the students in the NCG $( n = 2 9 )$ also had positive perceptions of their learning experience without clickers for the overall category of cognitive measures $\mathbf { \check { M } } = 3 . 4 2$ , $\mathrm { S D } ~ = ~ 0 . 6 6 )$ : the survey items related to the measures of Learning $\mathbf { M } = 3 . 2 7$ , $\mathrm { S D } = 0 . 8 4 \mathrm { , }$ ) and Self-assessment $\mathbf { M } = 3 . 6 3$ , $\mathrm { S D } ~ = ~ 0 . 6 9 )$ ) were above the level of neutrality.

The Mann-Whitney U test suggests that perceptions of the pedagogical benefits of clickers for the overall category of cognitive measures in the CG $\textprime M d n = 4 . 0 0$ ) were significantly more positive than those in the NCG $( M d n = 3 . 0 0 )$ ), $U = 2 5 3 . 0 0$ , $z = - 2 . 6 2$ , $p < . 0 1$ , $r = . 3 0$ . The Pearson $r$ coefficient represents a medium effect size. Within this category, perceptions for Learning in the CG $( M d n = 4 . 0 0 )$ ) were significantly more positive than in the NCG $( M d n = 3 . 0 0 )$ ), $U { = } 2 7 2 . 0 0$ , $z = - 2 . 3 2$ , $p \ < \ . 0 5$ , $r = . 2 9$ . The Pearson $r$ coefficient represents a small to medium effect size. However, perceptions for Self-assessment in the CG $M d n = 4 . 0 0$ ) did not differ significantly from those in the NCG $\textprime M d n = 4 . 0 0$ ), $U { = } 3 3 5 . 5 0$ , $z = - 1 . 3 5$ , $p = . 1 8 0$ , $r = . 2 1$ .

Table 2. Comparing student perception of the cognitive measures between the CG and the NCG.   

<html><body><table><tr><td colspan="3">CG n=29</td><td colspan="3">NCG n=29</td><td></td></tr><tr><td>Survey items</td><td>Mean/5</td><td>SD</td><td>Mean/5</td><td>SD</td><td>p</td><td>r</td></tr><tr><td>Cognitive measures (a = .75)</td><td>3.83</td><td>.63</td><td>3.42</td><td>.66</td><td>.008**</td><td>.30</td></tr><tr><td>Learning (a = .77)</td><td>3.78</td><td>.84</td><td>3.27</td><td>.84</td><td>.020*</td><td>.29</td></tr><tr><td>Contribution to learning of words</td><td>3.79</td><td>1.18</td><td>3.07</td><td>.96</td><td>.003**</td><td>.32</td></tr><tr><td>Perceived learning benefits</td><td>3.72</td><td>1.00</td><td>3.28</td><td>1.28</td><td>.198</td><td>.19</td></tr><tr><td>Cross-curricular learning benefits</td><td>4.17</td><td>1.26</td><td>3.48</td><td>1.06</td><td>.010*</td><td>.28</td></tr><tr><td>Added learning benefit of tool</td><td>3.41</td><td>.91</td><td>3.24</td><td>1.19</td><td>.734</td><td>.08</td></tr><tr><td>Self-assessment (a = .46)</td><td>3.91</td><td>.59</td><td>3.63</td><td>.69</td><td>.180</td><td>.21</td></tr><tr><td>Peer comparison</td><td>3.93</td><td>.88</td><td>3.76</td><td>1.02</td><td>.624</td><td>.09</td></tr><tr><td>Self-assessment of previous</td><td>3.90</td><td>.86</td><td>3.72</td><td>1.03</td><td>.629</td><td>.09</td></tr><tr><td>knowledge Self-assessment</td><td>3.90</td><td>.86</td><td>3.41</td><td>.95</td><td>.058</td><td>.26</td></tr></table></body></html>

Note. $\yen 123,45$ . $\ast \ast _ { \mathsf { p } } < . 0 1$ .

Table 3. Comparing student perception of the non-cognitive measures between the CG and the NCG.   

<html><body><table><tr><td colspan="3">CG n=29</td><td colspan="4">NCG n=29</td></tr><tr><td>Survey items</td><td>Mean/5</td><td>SD</td><td> Mean/5</td><td>SD</td><td>P</td><td>r</td></tr><tr><td>Non-cognitive measures (a = .72)</td><td>3.6</td><td>.72</td><td>3.4.</td><td>.62</td><td>.299</td><td>.15</td></tr><tr><td>Engagement (a = .69)</td><td>4.15</td><td>.65</td><td>3.70</td><td>.63</td><td>.006**</td><td>.33</td></tr><tr><td>Motivation</td><td>4.07</td><td>.80</td><td>3.38</td><td>.86</td><td>.001**</td><td>.38</td></tr><tr><td> Participation</td><td>3.86</td><td>.79</td><td>3.52</td><td>.87</td><td>.126</td><td>.20</td></tr><tr><td>Satisfaction</td><td>4.52</td><td>.87</td><td>4.21</td><td>.77</td><td>.061</td><td>.19</td></tr><tr><td>Interactivity (a = .59)</td><td>3.14</td><td>.88</td><td>3.14</td><td>.72</td><td>.928</td><td>.00</td></tr><tr><td>Interactivity with peers</td><td>3.10</td><td>1.08</td><td>3.83</td><td>.97.</td><td>.012*</td><td>-.34</td></tr><tr><td>Interactivity with teacher</td><td>2.62</td><td>.98</td><td>2.59</td><td>1.09</td><td>.977</td><td>.01</td></tr><tr><td>Willingness to share answers</td><td>3.69</td><td>1.23</td><td>3.00</td><td>.93</td><td>.012*</td><td>.30</td></tr></table></body></html>

Note. $\yen 123,45$ . $\ast \ast _ { \mathsf { p } } < . 0 1$ .

# Students’ non-cognitive measures

Table 3 shows the results of the students’ perception of the pedagogical use of clickers in terms of non-cognitive measures: Engagement and Interactivity. These findings indicate no significant differences between the two groups in this category, as will be clarified below. However, the measure of Engagement was perceived significantly more positively in the CG.

The students in the CG ${ \bf \nabla } \cdot { \bf n } = 2 9 ,$ ) had positive perceptions for the overall category of non-cognitive measures $( \mathrm { M } = 3 . 6 0$ , $\mathrm { S D } = 0 . 7 2 )$ ) since the means for the survey items related to the Engagement $( \mathrm { M } = 4 . 1 5$ , SD $= \ 0 . 6 5 )$ and Interactivity $( \mathrm { M } = 3 . 1 4 $ , $\mathrm { S D } = 0 . 8 8 )$ were above the level of neutrality. Similarly, the students in the NCG $( n = 2 9 )$ also had positive perceptions of their learning experience without clickers for the overall category of non-cognitive measures $\mathbf { \chi } ^ { \prime } \mathbf { M } = 3 . 4 0$ , $\mathrm { S D } ~ = ~ 0 . 6 2 )$ : the results for Engagement $\mathrm { ( M } = 3 . 7 0$ , $\mathrm { S D } = 0 . 6 3 )$ and Interactivity $( \mathrm { M } = 3 . 1 4 , ~ \mathrm { S D } ~ =$ 0.72) were above the level of neutrality.

The Mann-Whitney U test suggests that the perceptions of the pedagogical benefits of clickers for the overall category of non-cognitive measures in the CG $\textprime M d n = 4 . 0 0$ ) did not differ significantly from those in the NCG $( M d n = 3 . 0 0 )$ ), $U { = } 3 5 3 . 5 0$ , $z = - 1 . 0 5$ , $ { p } = \ . 2 9 9$ , $r ~ = ~ . 1 5$ . However, perceptions for Engagement in the CG $( M d n = 4 . 0 0 )$ were significantly more positive than in the NCG $M d n = 4 . 0 0$ ), $U = 2 4 6 . 5 0$ , $z = - 2 . 7 4$ , $p < . 0 1$ , $r = . 3 3$ . The Pearson $r$ coefficient represents a medium effect size. Perceptions for Self-assessment in the CG $( M d n = 3 . 0 0 $ ) did not differ significantly from those in the NCG $\textprime M d n = 3 . 0 0$ ), $U { = } 4 1 4 . 5 0$ , $z = - 0 . 1 0$ , $ { p ^ { \mathrm { ~ = ~ } } }  { 9 2 8 }$ , $r = . 0 0$ .

# Qualitative results: students and teacher

The qualitative analysis substantiates the results from the quantitative analysis and highlights aspects that were less discernible from the survey results, such as the reasons behind the lack of significance in self-assessment and Interactivity.

# Cognitive measures

Learning. In terms of Learning, the students in the CG reported they believed clickers helped them learn more during instruction than they would have benefited from hand-raising. Seven (out of $/ 8 \AA$ ) participants from the CG indicated that the clickers allowed them to learn more $\left( n = 5 \right)$ or at least the same amount $\left( n = 2 \right)$ of words. Contrastively, six (/8) participants from the NCG indicated that they perceived vocabulary learning without clickers was the same $\left( n = 5 \right)$ or less $( n = 1 )$ as if they had used the clickers. A CG participant stated that "clickers probably helped me more than if I had raised my hand, because it would have been more boring", a statement that was supported by a participant in the opposing NCG, who stated that students "would have learned a few more words with clickers".

The teacher’s perception diverged from those of the students as she did not identify a perceived increase in Learning related to the answering of MCQs via clickers. The following excerpt epitomizes the teacher’s perspective: "I don’t feel that one group learned over the other group […] I didn’t see an immediate impact".

Self-assessment. Regarding Self-assessment, as reported earlier, the quantitative analysis of the surveys did not indicate a significant difference between the two groups. This was substantiated in the interviews, as participants in both groups reported positive perceptions of the Self-assessment affordances offered in the two learning settings. One NCG participant explained that "it’s the same thing if you raise [your hand] or if you click on a button. It’s either you know it, or you don’t, and it’s not because you click on a number that you know it more". However, a CG participant indicated a preference for the answer distribution graphic provided with clickers, since “you can see the percentage, so you can see if many people think like you or if people have different opinions”.

This preference for the multi-modal feedback afforded by clickers is also reflected in the teacher’s comments: $^ { \mathfrak { c } } \mathrm { I }$ like the fact that the students could see the results [in the CG]. I thought that the students were a lot more interested in seeing the results, whereas with the [Non-clicker] class, when we discussed the answers, there was no visual".

# Non-cognitive measures

Engagement. In terms of Engagement, although there was a significant difference in the survey analysis, the interviews showed unanimous positive perceptions in both the CG and the NCG. One CG participant enjoyed the game-like environment offered by the clickers: "I think it’s more motivating to have something in your hand. It felt like a videogame because you could see how many people answered this or that". Similarly, this measure was perceived positively in the NCG: "the students were more open, and I also found that [MCQs answered via hand-raising and PowerPoint Presentations] were a different way of teaching. It’s a change, so it’s more than if the teacher would say ‘learn the words’ with a list of words".

The teacher’s perception of the students’ engagement radically contrasted with that of the students. Contrary to the highly positive survey results observed for Engagement in the CG, the teacher claimed that "they [CG] just liked the fact that there was something in their hands. It wasn’t a very exciting game for them. Some of them were sitting there and they just looked kind of bored at times".

Interactivity. For Interactivity, no difference between the two groups was observed in the quantitative survey analysis, a phenomenon that was clearly reflected in the interviews. For instance, one CG participant reported that they “had to talk to each other to find the right answer. .. [and the teacher] would give [them] time to talk to [their] teammates”. Similar statements were also observed among member of the NCG: “the teacher told us that two students had to talk about their answer, and we talked to our teammates”. Interestingly, students from both groups noticed a decrease in the amount of verbal interaction with the teacher: "I would say there was less interaction with the teacher because we would see the question on the board and the teacher would just tell us to talk to each other" (CG participant) and "I interacted more with the other students, but not with the teacher" (NCG participant).

Once again, the overall perception of the teacher contrasted with that of the students when she revealed that "the NCG would have interacted more than normal because they had to consult their neighbours", whereas in the CG, "if their percentage was above the target percentage [on the answer distribution graphic], they didn’t".

# Discussion

# Student perception

Considering a set of cognitive (Learning, Self-assessment) and non-cognitive measures (Engagement, Interactivity), the first research question inquired about students’ and their teacher’s perceptions of the pedagogical use of clickers in a secondary L2 English classroom. Focusing on students, the results suggest that participants in the CG had positive perceptions across all measures in the following rank (from most positive): Engagement $\mathbf { \chi } ^ { \prime } \mathbf { M } = 4 . 1 5$ , $\mathrm { S D } = . 6 5 ,$ ), Self-assessment $\mathbf { \zeta } ^ { \prime } \mathbf { M } = 3 . 9 1$ , $\mathrm { S D } =$ .59), Learning $\mathrm { \Delta } ^ { \prime } \mathrm { M } = 3 . 7 8$ , $\mathrm { S D } ~ = ~ . 8 4 )$ , and Interactivity $\mathbf { \chi } ^ { \prime } \mathbf { M } = 3 . 1 4$ , $\mathrm { S D ~ = ~ }$ .88). The fact that students rated the survey items above the level of neutrality for the four measures demonstrates that students perceive the use of clickers positively, thus confirming our hypothesis.

These results corroborate previous findings which show that students have positive perceptions of their pedagogical experience with LRSs (e.g., Cardoso, 2011; Cutrim Schmid, 2007; Serafini, 2013; Song et  al., 2017). Positive student perceptions were anticipated, since the pedagogical approach associated with the use of clickers offers various affordances such as a game-like approach to teaching (Bruff, 2009), anonymity (Cutrim Schmid, 2007), a collaborative learning environment (Mazur, 1997), and the immediate multi-modal feedback via a graphic display of the answer distribution and the feedback of the teacher and/or peers (Caldwell, 2007; Hunsu et  al., 2016).

The second research question asked if the students’ perceptions of the cognitive and non-cognitive measures of the instructional approach were more positive in the CG than those found in a classroom that uses presentation software without clickers. In terms of the cognitive measures (i.e., Learning and Self-assessment), the quantitative results indicated that the student’s perceptions in the CG were significantly more positive than those in the NCG, once again confirming our hypothesis. These results corroborate existing literature that report positive perceptions in terms of these two measures (e.g., Cardoso, 2011; Serafini, 2013). When isolating the two measures within the non-cognitive measures, while the CG participants viewed Learning significantly higher than the NCG members, the same did not hold for Self-assessment. The lack of significance in Self-assessment was unexpected considering the positive perceptions for that measure in the literature (e.g., Serafini, 2013; McDonough & Foote, 2015).

The lack of significance for Self-assessment may be explained by the relatively similar teaching methodologies used in the two experimental groups. For instance, the PowerPoint presentations used in both the CG and the NCG allowed for instant feedback on the MCQs (e.g., it showed the correct answer) and, consequently, involved a certain degree of Self-assessment. As such, the design of the study was conducive to peer comparison due to the voting component included in both groups: while students in the CG voted with clickers and saw the results on the answer distribution graphic, the students in the NCG voted via hand-raising and could also visually compare their response to that of other students.

In terms of the non-cognitive measures (Engagement and Interactivity), the survey results did not confirm our hypothesis, as the students’ perceptions in the CG were not significantly more positive than those in the NCG for the overall category. These results were not expected, as the existing literature reports positive perceptions for both Engagement and Interactivity (i.e., Cutrim Schmid, 2008; Song et  al., 2017). However, when the two subcomponents were analysed in isolation, significant results were found for Engagement, favouring the CG. These findings only partially corroborate those reported in Hung (2017), who found that both Engagement and Interactivity were perceived significantly more positively by the students who made use of clickers. Note, however, that comparisons with Hung (2017) are not optimal: their study involved the use of an online-based LRS, Kahoot! (not clickers), and the course was taught through a flipped classroom approach.

Focusing exclusively on Interactivity, it is possible that the age and lower level of cognitive maturity of the high school participants in this study might have misled them to believe that perceived reduced interactions with the teacher (as observed in this study) meant less interaction. Interestingly, Chapelle (2004) and Cutrim Schmid (2008) argue that human-machine interactions (such as those found with clickers) can be characterized as computer-mediated interactions with the teacher. The lack of significance in Interactivity for the CG may be also explained by the fact that the highly interactive nature of the convince-your-neighbour activities, which were used in both groups. As evidenced in the teacher interview, students in the NCG interacted with each other after every question, whereas the CG only interacted when the percentages on the answer distribution graphic did not indicate that an overwhelming majority of students had the correct answer. This suggests that the students in the CG did not interact with their peers as often as those in the NCG, perhaps explaining why the mean score for the interactivity-with-peers survey item was lower in the CG than in the NCG.

# Teacher perception

The first research question regarding the users’ perceptions of the pedagogical benefits of clickers revealed that the teacher had predominantly neutral to negative perceptions, contrary to our hypothesis based on previous clicker research in general and L2 education (e.g., Agbatogun, 2014; Chevalier, 2011; Fuller & Dawson, 2017).

It could be argued that since the teacher was a first-time LRS user and had only implemented the technology for the purpose of the study, she lacked familiarity and comfort with LRSs and possibly motivation to use them effectively. This finding, however, cannot be validated by the existing literature suggesting positive perceptions even in studies with teachers who were first-time users of LRSs (e.g., Agbatogun, 2014; Fuller & Dawson, 2017). Another unexpected finding that emerged was the contrast between her perceptions of the adopted technology and those of her students in the CG. For example, while a participant in the CG felt that clickers and the game-like environment were motivating, the teacher felt that the students were bored and lacked excitement. It is possible that the teacher-participant was an outlier in terms of their perceptions of the technology and consequently not representative of the teacher population. This leads to the question: To what extent do teacher perceptions affect the students’ performance and perceptions of clickers? Only future studies will elucidate this conundrum.

# Conclusion

The purpose of this study (part of the Make Words Click! project) was to examine the potential of the implementation of LRSs in a K-12 educational context, focussing on users’ perceptions of the technology. Overall, although our results indicate that the students using LRSs have overwhelmingly positive perceptions of the pedagogical use of clickers, the teacher showed predominantly neutral (and sometimes negative) attitudes toward the technology. Focusing on learners’ attitudes towards LRSs as pedagogical tools, we conclude that the findings reported here provide evidence that the technology has great potential for use in the L2 classroom. More specifically, teachers should capitalize on students’ positive perceptions towards the pedagogical use of clickers to design lessons that target vocabulary acquisition via this technology. Other LRS technology that emulate the affordances of the device-based clickers could alternatively be adopted in different educational settings. For example, in a context in which learners can use mobile devices in the classroom, web-based clickers (e.g., Kahoot, Socrative) could be used for practical reasons.

Despite these promising results, there are limitations that should be considered for future research. The fact that all data from surveys and interviews were self-reported (and not observed by the researchers or obtained via other measures) challenges the validity and reliability of the data. Observation of the participants’ reactions and interactions while taking part in the vocabulary instruction activities in both the CG and the NCG would have allowed the researchers to further assess the participants’ perceptions, particularly in terms of non-cognitive measures such as Engagement and Interactivity. Another limitation is that perception was only assessed once: both the teacher and the students were only surveyed and/or interviewed at the end of the treatment.

Without gathering user perceptions over a longer period, it is impossible to track any kind of evolution in the users’ perceptions or to detect the potential effects of a novelty bias (Clark, 1983) throughout the experiment. Ultimately, the fact that teacher perceptions were gathered from only one teacher does not allow us to generalize teacher perceptions beyond this study. However, the rationale behind recruiting one teacher for both groups was to ensure standardization in the implementation of the treatment between the two groups.

Further research should examine teacher perceptions of clickers. The unexpected neutral to negative perceptions of the teacher motivate the need to better understand the role of instructors in an LRS-based setting. As previously mentioned, the teacher was doubtful about the benefits of using the clickers in the classroom and did not have particularly positive attitudes towards their use. In future studies, researchers could examine whether the teacher can have an impact on their students’ perceptions and overall pedagogical performance. Research should also examine if training sessions for teachers on how to use and integrate LRSs in lessons would be beneficial. Finally, research should examine different approaches to the implementation of LRSs, including the timing of questions, and the effects of different types of MCQs and discussion prompts. While LRSs have been shown to contribute to learning and to evoke positive perceptions from students and their instructors, research moving forward should now attempt to determine the most effective approach for their pedagogical implementation, particularly in L2 education.

# Disclosure statement

No potential conflict of interest was reported by the authors.

# Notes on contributors

Anne-Marie Sénécal is a graduate student of Applied Linguistics at Concordia University. Her research interests include the pedagogical use of clickers, educational games, and speech technologies (TTS, ASR) in L2 education. She also teaches ESL at a College in Montréal (Canada).

Walcir Cardoso is a Professor of Applied Linguistics at Concordia University. He conducts research on the L2 acquisition of phonology, morphosyntax and vocabulary, and the effects of computer technology (e.g., clickers, text-to-speech synthesizers, automatic speech recognition,intelligent personal assistants) on L2 learning.

Vanessa Mezzaluna is a graduate student in Educational Studies at Concordia University.   
She is also a College English as a Second Language Teacher in Montréal.

# ORCID

Anne-Marie Sénécal $\textcircled{1}$ http://orcid.org/0000-0001-8725-0413 Walcir Cardoso $\textcircled{1}$ http://orcid.org/0000-0001-6376-185X

# References

Abrahamson, L. (2006). A brief history of networked classrooms: Effects, cases, pedagogy, and implications. In D. Banks (Ed.), Audience response systems in higher education (pp. 1–25). Information Science Publishing.   
Agbatogun, A. (2014). Developing learners’ second language communicative competence through active learning: Clickers or communicative approach? Educational Technology & Society, 17(2), 257–269.   
Blasco-Arcas, L., Buil, I., Hernandez-Ortega, B., & Sese, F. (2013). Using clickers in class. The role of interactivity, active collaborative learning and engagement in learning performance. Computers & Education, 62, 102–110. https://doi.org/10.1016/j. compedu.2012.10.019   
Bruff, D. (2009). Teaching with classroom response systems: Creating active learning environments. Wiley.   
Caldwell, J. (2007). Clickers in the large classroom: Current research and best-practice tips. CBE Life Sci Educ, 6(1), 9–20. https://doi.org/10.1187/cbe.06-12-0205   
Cardoso, W. (2011). Learning a foreign language with a learner response system: The student’s perspective. Computer Assisted Language Learning, 24, 393–417. https://doi. org/10.1080/09588221.2011.567354   
Cardoso, W. (2013). Learner response systems in second language teaching. In C. Chapelle (Ed.), The encyclopedia of applied linguistics (pp. 1–7). Blackwell.   
Chapelle, C. (2004). Technology and second language learning: Expanding methods and agendas. System, 32(4), 593–601. https://doi.org/10.1016/j.system.2004.09.014   
Chevalier, J. (2011). Teachers’ perception of handheld response systems as a tool for formative assessment in high school classrooms (Publication No. 3481408) [Doctoral dissertation]. Walden University.   
Chien, Y., Chang, Y., & Chang, C. (2016). Do we click in the right way? A meta-analytic review of clicker-integrated instruction. Educational Research Review, 17, 1–18. https:// doi.org/10.1016/j.edurev.2015.10.003   
Clark, R. (1983). Reconsidering research on learning from media. Review of Educational Research, 53, 445–459. https://doi.org/10.3102/00346543053004445   
Cohen, J. (1988). Statistical power analysis for the behavioral sciences. Erlbaum Associates.   
Creswell, J. (2014). Research design: Qualitative, quantitative, and mixed method approaches. Sage Publications.   
Cutrim Schmid, E. (2007). Enhancing performance knowledge and self-esteem in classroom language learning: The potential of the ACTIVote system component of interactive whiteboard technology. System, 35, 119–133. https://doi.org/10.1016/j. system.2007.01.001   
Cutrim Schmid, E. (2008). Using a voting system in conjunction with interactive whiteboard technology to enhance learning in the English language classroom. Computers & Education, 50(1), 338–356. https://doi.org/10.1016/j.compedu.2006.07.001   
Draper, S., & Brown, M. (2004). Increasing interactivity in lectures using an electronic voting system. Journal of Computer Assisted Learning, 20(2), 81–94. https://doi. org/10.1111/j.1365-2729.2004.00074.x   
Field, A. (2018). Discovering statistics using IBM SPSS statistics. Sage.   
Fredricks, J., Blumenfeld, P., & Paris, A. (2004). School engagement: Potential of the concept, state of the evidence. Review of Educational Research, 74(1), 59–109. https:// doi.org/10.3102/00346543074001059   
Fuller, J., & Dawson, K. (2017). Student response systems for formative assessment: Literature-based strategies and findings from a middle school implementation. Contemporary Educational Technology, 8(4), 370–389. https://doi.org/10.30935/cedtech/6206

Hake, R. (1998). Interactive-engagement versus traditional methods: A six-thousand-student survey of mechanics text data for introductory physics courses. American Journal of Physics, 66(1), 64–74. https://doi.org/10.1119/1.18809

Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81–112. [Database] https://doi.org/10.3102/003465430298487   
Hung, H. (2017). Clickers in the flipped classroom: Bring your own device (BYOD) to promote student learning. Interactive Learning Environments, (25)(8), 983–995. https://doi.org/10.1080/10494820.2016.1240090   
Hunsu, N., Adesope, O., & Bayly, D. (2016). A meta-analysis of the effects of audience response systems (clicker-based technologies) on cognition and affect. Computers & Education, 94, 102–119. https://doi.org/10.1016/j.compedu.2015.11.013   
Judson, E., & Sawada, D. (2002). Learning from past and present: Electronic response systems in college lecture halls. Journal of Computers in Mathematics & Science Teaching, 21, 167–181.   
Kaleta, R., & Joosten, T. (2007). Student response systems: A University of Wisconsin study of clickers (Research Bulletin, 6). EDUCAUSE Center for Applied Research.   
Kay, R., & LeSage, A. (2009). Examining the benefits and challenges of using audience response systems: A review of the literature. Computer & Education, 53(3), 819–827. https://doi.org/10.1016/j.compedu.2009.05.001   
Lantz, M. (2010). The use of ‘Clickers’ in the classroom: Teaching innovation or merely an amusing novelty? Computers in Human Behavior, 26, 556–561. https://doi. org/10.1016/j.chb.2010.02.014   
Long, M. (1996). The role of the linguistic environment in second language acquisition. In W. Ritchie & T. Bhatia (Eds.), Handbook of second language acquisition (pp. 413–468). Academic Press.   
MacGeorge, E., Homan, S., Dunning, J., Elmore, D., Bodie, G., Evans, E., & Geddes, B. (2008). The influence of learning characteristics on evaluation of audience response technology. Journal of Computing in Higher Education, 19, 125–145. https://doi. org/10.1007/BF03033425   
Marlow, D. (2010). Engaging syntax: Using a personal response system to encourage grammatical thought. American Speech, 85(2), 225–237. https://doi.org/10.1215/ 00031283-2010-012   
Mazur, E. (1997). Peer instruction: A user’s manual. Prentice Hall.   
McDonough, K., & Foote, J. (2015). The impact of individual and shared clicker use on students’ collaborative learning. Computers & Education, 86, 236–249. https://doi. org/10.1016/j.compedu.2015.08.009   
Milholland, E. (2015). A multiple case study of instructors utilizing classroom response systems (CRS) to achieve pedagogical goals. [Doctoral dissertation]. Colorado State University.   
Nation, I., & Macalister, J. (2010). Language curriculum design. Routledge.   
Oigara, J., & Keengwe, J. (2013). Students’ perceptions of clickers as an instructional tool to promote active learning. Education and Information Technologies, 18(1), 15–28. https://doi.org/10.1007/s10639-011-9173-9   
Saldaña, J. (2009). The coding manual for qualitative researchers. Sage.   
Sénécal, A.-M., & Cardoso, W. (2021). A world of differences: The role of individual differences in L2 vocabulary learning with clickers. In C. Brudermann, M. Grosbois, C. Sarré, N. Zoghlami, L. Bradley, & S. Thouësny (Eds.), Call & professionalization –  short papers from EUROCALL 2021. Research-publishing.net.   
Sénécal, A.-M., Mezzaluna, V., & Cardoso, W. (2018). Make words click! Leaning English vocabulary with clickers. In P. Taalas, J. Jalkanen, L. Bradley, & S. Thouësny (Eds.), Future-proof CALL: Language learning as exploration and encounters – Short papers from EUROCALL 2018 (pp. 290–295). Research-publishing.net.   
Serafini, E. (2013). Learner perceptions of clickers as a source of feedback in the classroom. In K. McDonough, & A. Mackey (Eds.), Second language interaction in diverse educational contexts (pp. 209–224). John Benjamins.   
Song, D., Oh, E., & Glazewski, K. (2017). Student-generated questioning activity in second language courses using a customized personal response system: A case study. Educational Technology Research and Development, 65(6), 1425–1449. https://doi. org/10.1007/s11423-017-9520-7   
Thornbury, S. (2002). How to teach vocabulary. Longman.   
Wang, A. (2015). The wear out effect of a game-based student response system. Computers & Education, 82, 217–227. https://doi.org/10.1016/j.compedu.2014.11.004