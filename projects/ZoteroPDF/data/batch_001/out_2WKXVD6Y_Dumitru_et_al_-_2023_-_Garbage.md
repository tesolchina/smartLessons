# Garbage in, garbage out: An analysis of HTML text extractors and their impact on NLP performance

Vlad Dumitru   
Computer Science Department   
University Politehnica of Bucharest   
Bucharest, Romania   
vlad.dumitru2212@upb.ro   
Denis Iorga   
R&D Department   
Research Technology   
Bucharest, Romania   
Interdisciplinary School of Doctoral Studies   
University of Bucharest   
Bucharest, Romania   
denis.iorga@drd.unibuc.ro   
Stefan Ruseti   
Computer Science Department   
University Politehnica of Bucharest   
Bucharest, Romania   
stefan.ruseti@upb.ro   
Mihai Dascalu   
Computer Science Department   
University Politehnica of Bucharest   
Bucharest, Romania   
R&D Department   
Research Technology   
Bucharest, Romania   
mihai.dascalu@upb.ro

Abstract—Technological advancement has significantly facilitated the research and development of Artificial Intelligence, with particular emphasis on Natural Language Processing (NLP). High-quality data is crucial to achieving success in this area. This aspect becomes particularly important considering the recent widespread adoption of large language models trained on a considerable amount of text from the Internet. This research expands on the issue of data quality in NLP by examining the impact of automated text extraction techniques from HTML on the performance of specific NLP tasks. For this purpose, an empirical evaluation was conducted to assess the efficacy of various automated techniques for HTML text extraction using 300 news articles written in English, Romanian, and French. The evaluation was conducted by comparing the results of the most popular automated text extraction technologies (i.e., “boiler”, “justext”, “newspaper”, “readability”, and “trafilatura”) against the results of human-validated texts. Both extracted texts, automated and human-validated, were subjected to three NLP tasks: named entity recognition, sentiment analysis, and text summarization. Our analysis of the NLP results indicates that text from Romanian online news articles should be extracted with “newspaper”, whereas “trafilatura” should be used for English and French articles, regardless of the NLP task. Overall, our study provides a comprehensive understanding of the performance of the selected technologies for extracting the text of online news articles by language and NLP task.

Keywords—Web Crawling, Web Scraping, Natural Language Processing, Data analysis

# I. INTRODUCTION

Artificial Intelligence (AI), with emphasis on Machine Learning (ML) and Deep Learning (DL) techniques, represents a key player in the technological advancement of recent years. Researchers are continuously racing to develop more accurate

AI/ML systems that can actively be part of people’s lives by easing their work. These systems are already present in our daily endeavors ranging from recommender systems to conversational agents. However, achieving the best performance in a practical environment is impossible without proper training data fed to the model, no matter how complex an ML model’s architecture is or how solid its underlying theoretical framing is. As such, data is a cornerstone in developing an accurate ML model. With this in mind, the current study focuses on analyzing how data acquisition impacts the results of such a model.

Many studies use article news to train models for different tasks, such as identifying fake news [26], classifying news per topic [6], and analyzing sentiments expressed in the news [2]. As a first step, all these papers imply getting news articles from the Internet. For this purpose, libraries are designed to fetch and parse the content of HTML pages to extract the text of the news.

Our research objective is to engage the most known news scrapers (i.e., “boiler”, “justext”, “newspaper”, “readability”, and “trafilatura”) in extracting relevant content from HTML pages and evaluate how the accuracy of these extractions plays a role in different subsequent NLP tasks, such as identifying entities in texts, classifying news by sentiment scores, and obtaining summarizations. Our evaluation was accomplished using statistical tests and metrics computed between automated extraction texts and human-validated texts.

This study also introduces a novel web browser plug-in to extract HTML pages from the Internet. Usually, HTML pages of websites are extracted using automation tools that simulate a web browser, such as Selenium1 or Requests2. But the problem with these kinds of tools is that some pages block automated access or make the user play a captcha game to get access. It is quite a challenge to overcome these situations in an automated fashion. Moreover, some of these libraries sometimes fail to get the HTML content after it is fully loaded because they cannot execute JavaScript code to solve the dynamic content of pages. To avoid these scenarios and achieve our research objective, we implemented a Google Chrome extension that captures the complete HTML content of every visited page.

# II. RELATED WORK

Generally, studies use public datasets obtained through crawling or own crawlers and scrapers to gather training data for ML models. Such studies often focus on analyzing the models’ performances and may omit to check the data quality before performing the analyses. For example, a fake news dataset obtained by scraping and published on the Kaggle platform [16] has missing or short non-relevant texts for some news; moreover, there is no description of how articles were curated. However, the dataset was used by Huang et al. [14] in their study to make feature analysis and to train an ML model that predicts fake news. Knowing what crawler and scraper to use for what language or the influence of the accuracy of the extracted texts from HTML pages on NLP tasks, which are topics covered by our study, would ease the process, enabling researchers to focus on developing models and inspecting results rather than gathering data.

Khder [18] published a survey that sketches a more detailed description of text extractors from HTML pages (or web scrapers), what they are used for, and how they work. In his study, the web scraping process comprises three stages: fetching, extraction, and transformation. Fetching refers to retrieving an HTML page using the HTTP protocol, and the extraction phase means getting the important data from HTML. At the same time, the transformation stage is related to converting the relevant data from the previous step to a structured format for storage or presentation. Moreover, the difference between a crawler and a scraper is clearly stated. While the crawler fetches data, the scraper extracts and stores important information. Khder shows that web scraping is used to construct corpora in various computer science fields such as Artificial Intelligence, Big Data, Business Intelligence, or Data Science. The biggest advantages of automated extraction over manual processing are speed and cost, meaning that more data can be gathered using crawlers and scrapers at lower costs. The same paper affirms that Python is a good choice for scraping because it is easy to learn and, being so popular, has a sizable support community. Numerous libraries in Python support users to crawl and scrape data.

Uzun et al. [29] compare three different Python web data extraction libraries. The first option is using regular expressions, which can cause problems when the number of inner tags is ambiguous. As an alternative solution, DOM-based libraries such as BeautifulSoup3 or $1 \mathrm { x m } ^ { 4 }$ can be used. DOM stands for Document Object Model and acts as an Application Programming Interface (API) for XML and HTML documents. In a DOM interface, the HTML is transformed into a tree structure, with every element being a node of this structure. The three methods are mainly compared from a speed point of view. Regular expressions are the fastest, but the downside is that the accuracy of extractions is not that good. BeautifulSoup and lxml are more accurate than regex, with lxml being faster than BeautifulSoup.

Another problem that could be encountered when scraping is that not all web pages statically contain all the information. The response could look completely different from what is shown when the page is inspected using the browser’s developer tools. This is called a dynamic website that uses JavaScript code to instruct the browser how to create the desired HTML; thus, receiving the page in Python script will not execute the JavaScript code and, therefore, does not create the same page as viewed in the browser. Han et al. [12] used Selenium to simulate human browsing behavior and overcome this issue. This tool uses a WebDriver to open a browserspecific window and run actions specified in the code. It can get HTML elements by XPath, class name, id, and CSS selector, get the text content of an element, click on identified buttons, scroll down the page, or navigate to another page. As such, every action done manually by a user can be done automatically with Selenium. The downside of this approach is that such a crawler cannot be used generically because it targets specific web pages. Another crawler that folds into the new structure must be used on different web pages.

In his study, Barbaresi [3] proposed a Python library for scraping entitled Trafilatura. This research also reinforces the idea that having a corpus that meets scientific expectations regarding text quality is crucial. Such data collection can be achieved only using a performant tool that discards unnecessary information while keeping the desired content of texts. In contrast to the previous study’s approach, Trafilatura is designed to work generically on many different web documents. It focuses on metadata, main text, and comments and can run online or offline, getting URLs, HTML files, or parsed HTML trees as input. The software does a good job with scraping and has great support for crawling. It uses a queue to process URLs and access them using time restrictions based on domain names, such as not to disturb those domains with too many requests in a short interval of time. The extraction algorithm is based on a multitude of rule-based filters and heuristics but also provides some fallbacks. The first step is content delimitation which is obtained using XPath expressions that target common HTML elements and attributes, firstly excluding undesired parts of the HTML code and then getting the nodes that contain the wanted content. The selected HTML nodes are next checked for relevance and processed to get the content. In case of a possibly faulty extraction, two other libraries are run as a fallback: readability-lxml [11] and jusText [9]. Their approach is different, applying heuristics based on text-to-markup ratio, line length, and position of elements in the HTML tree. A baseline extraction is run in the worst scenario where none of the algorithms work.

The above-mentioned study also compares its performances with other Python libraries focusing on text extraction. Amongst them, there are boilerpy3 [8], newspaper [10], readability-lxml, and jusText. Together with Trafilatura, these technologies are part of our study as well. On the selected benchmark, Trafilatura had the best results.

The article of Rao and Sachdev [25] on news classification is a classic example of ML studies that need crawlers and scrapers before starting their main research. Their study focused on classifying news articles based on location, especially cities. They train several models using different algorithms and analyze the results. When training a model, it is important to eliminate boilerplate elements from texts. Advertisements, menus, and navigation bars would only make models less accurate when classifying texts. For their survey, they developed a tool to get news articles from websites and extract the main texts. Using an existing tool would have eased their work and let them focus only on the study’s main purpose.

# III. METHOD

The first step was to decide which tools were eligible for our study. As it was mentioned in the previous section, we selected 5 Python libraries: boilerpy3 [8] (version 1.0.6), newspaper [10] (version 0.2.8), readability-lxml [11] (version 0.8.1), jusText [9] (version 3.0.0) and trafilatura [3] (version 1.4.0). Each tool focuses on removing boilerplate while preserving the main text content of a web page, making them perfect candidates for scraping news articles from HTML pages. Python was chosen as a programming language because it is popular, has a large community, and offers a wide range of libraries for what we need.

# A. Dataset

Our dataset consists of 300 URLs containing news articles about climate change from websites in English, French, and Romanian; 100 URLs were considered for each language. All URLs were randomly selected from larger datasets: the URLs for English articles from [1], the URLs for Romanian articles from [15], while the URLs for French articles from the dataset of Meddeb et al. [21]. The texts of English and Romanian articles were manually extracted and verified; in contrast, the texts from French news articles were extracted based on custom crawlers, but they were also human-validated, as presented in Meddeb et al. [21]. Table I presents mean word counts and standard deviation of the human-validated texts by language.

Along with the human-validated texts for each URL, the dataset also included text automatically extracted via the five text extractors (i.e., “boiler,” “justext,” “newspaper,” “readability,” and “trafilatura”). We used a Chrome extension to get the

TABLE I DATASETS STATISTICS   

<html><body><table><tr><td rowspan="2">Dataset</td><td colspan="2">Statistics</td></tr><tr><td>Mean length</td><td> Standard deviation</td></tr><tr><td>English</td><td>714.95</td><td>557.90</td></tr><tr><td>Romanian</td><td>695.50</td><td>660.75</td></tr><tr><td>French</td><td>1053.52</td><td>940.32</td></tr></table></body></html>

HTML of web pages for the crawling part. The Webbrowser 5 module was used to automatically open each URL in the browser. The following section describes the process and the developed extension in detail. The scraping was done with all 5 Python libraries applied to the extracted HTML pages.

# B. Processing Flow

The overarching flow of this study is presented in Fig. 1. It can be noted that there are 3 main components: data collection, running models, and results analysis. All components will be detailed in the following subsections. Briefly, the procedure aimed to evaluate the performance of each text extractor when compared against the human-validated text in different NLP tasks.

![](img/2b6a73cde0c1c132c9d506af5ae758d5aef88bfc8c3935f498ade278d336ed38.jpg)  
Fig. 1. General flow and main components

# C. Web Browser Extension

The purpose of using a web browser extension was to avoid web pages that either blocked automated access or made the user play a captcha game to get access. Running directly in the browser, the developed extension accesses the complete HTML pages without triggering alarms about being an automation tool. Thus, the developed extension saves every visited URL alongside its HTML page. The extension uses events that appear in the browser, such as opening new windows or tabs, reloading pages, resizing windows, closing windows or tabs, etc. Every event triggers the main function of the extension, which firstly checks what windows are visible on the screen and which tabs are active inside those windows. Then, the URL and HTML page are saved for every active visible tab. To locally get data saved in the extension, a file is downloaded every time the active tab is changed. Therefore, when opening URLs with the Webbrowser module, the active tab changes with every open URL, and a new file with data is downloaded every time. To create one dataset for each language, we appended the results of individual files containing URLs and HTML pages.

# D. Comparison Metrics between Text Extractions

Every candidate text extracted via the web browser extension and the 5 automated text extractors (i.e., “boiler,” “justext,” “newspaper,” “readability,” and “trafilatura”) was compared to the reference human-validated text using ROUGE [19] metric, which stands for Recall-Oriented Understudy for Gisting Evaluation. It is a metric usually used in summarization and translation tasks to evaluate results. ROUGE compares an automatically produced summary against a humanproduced summary measuring the number of matching ngrams or based on the longest common subsequence. Even if ROUGE is usually used in summarization tasks, it is suitable in this case because it measures overlapping n-grams. This is our intent to check between automatic and human-validated extracted texts as we assess how similar they are in terms of matching words. F1-score was chosen for the analysis using the longest common sequence (LCS) of ROUGE, referred to as Rouge-l.

# E. Machine Learning Models

Three separate tasks were chosen to check the relevance of input data in different situations: NER (Named Entity Recognition), classification using sentiment scores, and abstractive summarization. The focus was not to train the best models or to get the best results for models but to make a correlation between the results of the models and the input data, establishing if the quality of input texts plays a role in the final scores. In the current study, the best results are assumed to be those obtained from the human-validated reference texts, while the scores obtained from candidate texts are better when they approach the reference scores. The purpose is to check whether the differences given by the ROUGE on input texts keep in the scores obtained after running the models.

As the name suggests, NER has the purpose of identifying the entities in a text. Named entities could be a person, time, organization, place, object, or geographical entity. Spacy [7] was used to find entities from our texts. Table II shows which model was considered for each language together with their performances as declared by the developers of the models. A list of identified texts as entities alongside their type was obtained for every article. The comparison between a candidate and a reference list of entities was done using exact matching, meaning that both text and type should be the same. The evaluation metric was F1-score.

TABLE II NER MODELS AND PERFORMANCES   

<html><body><table><tr><td rowspan="2">Dataset</td><td colspan="2">Statistics</td></tr><tr><td>Model</td><td>F1-score</td></tr><tr><td>English</td><td>en_core_web_trf</td><td>0.90</td></tr><tr><td>Romanian</td><td>ro_core_news_lg</td><td>0.76</td></tr><tr><td>French</td><td>fr_core_news_lg</td><td>0.84</td></tr></table></body></html>

For the sentiment scores regarding English and French articles, we considered the model proposed by Barbieri et al. [4] that generates 3 scores: positive, neutral, and negative, while a custom-built sentiment analysis model was used for Romanian to generate only 2 scores: positive and negative. The Romanian model was based on RoBERT-Large [20] and was finetuned on the public dataset LaRoSeDa [27]. Performances of the models are presented in Table III. We considered sentiment scores for reference text and any other candidate text as two distributions. We computed the Kullback-Leibler (KL) divergence [5], quantifying how much one probability distribution differs from another probability distribution. Unlike the F1-score, which is better when closer to 1, the KL divergence score is better when closer to 0. In $0 . 2 7 \%$ of cases, one of the HTML extractors did not return any content; as such, a uniform distribution was chosen for these exceptions.

TABLE III SENTIMENT MODELS AND PERFORMANCES   

<html><body><table><tr><td rowspan="2">Dataset</td><td colspan="2">Statistics</td></tr><tr><td>Model</td><td>F1-score</td></tr><tr><td>English</td><td>XLM-T</td><td>0.70</td></tr><tr><td>Romanian</td><td>RoBERT-Large</td><td>0.96</td></tr><tr><td>French</td><td>XLM-T</td><td>0.71</td></tr></table></body></html>

Regarding the summarization task, a different model was considered for each language. The model proposed by Zhang et al. [30] was used for English, the one developed by Hasan et al. [13] was applied to French texts, whereas the model proposed by Niculescu et al. [22] was considered for Romanian. As declared by their authors, the performances of the considered model can be viewed in Table IV. For evaluation, the F1-score of Rouge-l was computed between the candidate and reference summaries for each tool.

# IV. RESULTS

The first part of our analysis focused on the performance of the text extractors for Romanian $( \mathrm { N } = 1 0 0 )$ , English (N $= ~ 1 0 0 $ ), and French $( \mathrm { N } ~ = ~ 1 0 0 )$ ) online news articles. The analysis was conducted using the F1-score for Rouge-l. The F1-score for Rouge-l was computed by comparing the results of the text extractors against the human-validated text. This analysis aimed to identify, for each language, whether there are significant differences between the Rouge-l scores of the five text extractors when compared against the humanvalidated text. Due to a non-parametric distribution of data, the comparison was conducted using the Friedman test from the ”stats” R package [24]. Based on the work of Pereira et. al [23], the Friedman test was followed by Wilcoxon post-hoc pairwise comparisons from the ”rstatix” R package [17], using the Bonferroni adjustment.

TABLE IV SUMMARIZATION MODELS AND PERFORMANCES   

<html><body><table><tr><td rowspan="2">Dataset</td><td colspan="2">Statistics</td></tr><tr><td>Model</td><td>ROUGE-L</td></tr><tr><td>English</td><td>PEGASUS</td><td>41.30</td></tr><tr><td>Romanian</td><td> RoSummary</td><td>34.67</td></tr><tr><td>French</td><td>XL-Sum</td><td>27.53</td></tr></table></body></html>

The Friedman test suggested significant differences between the automated text extractors for Romanian articles $\mathrm { w } = 0 . 2 3 8$ , $\mathsf { p } < . 0 0 1 \ r )$ , English articles $\mathbf { w } = 0 . 3 0 2$ , $\mathsf { p } < . 0 0 1 ,$ ), and French articles ( $\mathrm { ~ w ~ } = \mathrm { ~ } 0 . 3 1 6$ , $\mathsf { p } < . 0 0 1 $ . The results, as well as the post-hoc pairwise comparisons between the text extractors, are presented in Fig. 2, Fig. 3, and Fig. 4.

Fig. 2 illustrates that the “newspaper” automated text extractor obtained for Romanian online news articles the most performant median results when compared against the humanvalidated text, followed by “readability”, “trafilatura”, “boiler”, and “justext.” The post-hoc analysis illustrated that the “newspaper” text extractor was consistently better than all the other text extractors.

![](img/00c177b6be46833fbbe7a2a012b4cdb892c1c54adcbfa415e93dbc7767703e7c.jpg)  
Fig. 2. Distribution of Rouge-l score by text extractor applied on Romanian news articles; the starred lines between the boxplots illustrate statistically significant differences $\ast < . 0 5$ , $* * < . 0 1$ , $^ { * * * } < . 0 0 1$ ).

Fig. 3 illustrates that the “newspaper”, “readability”, and “trafilatura” automated text extractors consistently obtained better results for English online news articles than “boiler” and “justext”, when compared against the human-validated text. Nonetheless, no significant differences were found between these three text extractors.

Fig. 4 illustrates that the “justext” text extractor consistently obtained poorer results for French online news articles than all the other text extractors. Moreover, “trafilatura” was consistently better than “boiler”, while “readability” was consistently better than “newspaper”. However, there was no evidence that “trafilatura” was consistently better than “newspaper” or that “readability” was consistently better than “boiler”. Likewise, there was no evidence for statistically significant differences between “trafilatura” and “readability”.

![](img/614263916c7502b6c2c97a9c2bd504a945320f78784293b71377283c70025384.jpg)  
Fig. 3. Distribution of Rouge-l score by text extractor applied on English news articles; the starred lines between the boxplots illustrate statistically significant differences $^ { * } < . 0 5$ , $* * < . 0 1$ , $* * * < \overbar { . } 0 0 1$ ).

![](img/2975999ffdb1e1b3c842a0fb62c3826192f290666ab3274628bc865e1813fbbe.jpg)  
Fig. 4. Distribution of Rouge-l score by text extractor applied on French news articles; the starred lines between the boxplots illustrate statistically significant differences $^ \ast < . 0 5$ , $* * < . 0 1$ , $* * * < \bar { . 0 0 1 }$ ).

These results suggest that the “newspaper” automated text extractor is the best alternative for Romanian online news articles compared to human-validated text. Nevertheless, any of the “newspaper”, “readability”, and “trafilatura” text extractors should be preferred over “justext” or “boiler” for extracting text from English online news articles. As regards the comparison of French online news articles, our results suggest that “justext” should be avoided.

The second part of the analysis examined the influence of each text extractor on the results of three NLP tasks: NER, sentiment analysis, and text summarization. More precisely, we aimed to understand whether there are significant differences between the automated text extractors in terms of NLP task results compared to the human-validated text. The comparisons were conducted using a similar evaluation procedure based on the Friedman test, followed by Wilcoxon post-hoc pairwise comparisons.

Additionally, the comparison aimed to identify the most performant alternative for each language by NLP task based on metrics such as the median score, interquartile score range, and the number of errors, here defined in terms of scores above or below a certain evaluation threshold. As such, the first step in identifying the most accurate text extractor involved analyzing statistically significant differences between the NLP task results. When no single solution could be identified, the interquartile range (IQR) and the number of errors were used to define the most appropriate solution.

The NER task was compared based on the F1-score. The F1- score was computed using, as the correct prediction, the results of the NER model applied to the human-validated text. The Friedman test applied on the NER task F1 scores suggested statistically significant differences between the automated text extractors for Romanian articles $\mathrm { w } = 0 . 1 8 9$ , $ { \mathbf { p } } < . 0 0 1  { \mathrm { \Omega } }$ ), English articles $\mathrm { w } = 0 . 2 0 8$ , $\mathsf { p } < . 0 0 1 ,$ , and French articles $\mathrm { w } = 0 . 2 7 4$ , $\mathsf { p } < . 0 0 1 ,$ . In this regard, the largest effect was observed in the case of French articles. Table V introduces the results by text extractor and language. Scores with bold highlight the most performant alternative resulting from the pairwise post-hoc comparisons, median score, IQR, and the number of errors, here defined as the number of articles that registered an F1- score lower than 0.5.

TABLE V NER TASK: MEDIAN F1-SCORE, INTERQUARTILE RANGE, AND ERRORS BY TEXT EXTRACTOR AND LANGUAGE.   

<html><body><table><tr><td rowspan="3">Text extractor</td><td colspan="9">Language</td></tr><tr><td colspan="3">RO</td><td colspan="3">EN</td><td colspan="3">FR</td></tr><tr><td>Mdn</td><td>IQR</td><td>*</td><td>Mdn</td><td>IQR</td><td>*</td><td>Mdn</td><td>IQR</td><td>*</td></tr><tr><td>boiler</td><td>0.88</td><td>0.42</td><td>24</td><td>0.80</td><td>0.36</td><td>21</td><td>0.78</td><td>0.34</td><td>18</td></tr><tr><td>justext</td><td>0.81</td><td>0.23</td><td>9</td><td>0.78</td><td>0.35</td><td>25</td><td>0.61</td><td>0.32</td><td>37</td></tr><tr><td>newspaper</td><td>0.94</td><td>0.08</td><td>9</td><td>0.88</td><td>0.26</td><td>17</td><td>0.75</td><td>0.36</td><td>19</td></tr><tr><td>readability</td><td>0.93</td><td>0.16</td><td>15</td><td>0.88</td><td>0.17</td><td>16</td><td>0.78</td><td>0.37</td><td>21</td></tr><tr><td>trafilatura</td><td>0.92</td><td>0.20</td><td>11</td><td>0.90</td><td>0.10</td><td>7</td><td>0.84</td><td>0.35</td><td>14</td></tr></table></body></html>

The pairwise comparisons for the Romanian articles on the NER task revealed that “newspaper” was consistently better than all the other text extractors except “trafilatura”. As “trafilatura” was statistically significantly better only than “justext”, the possibility for “trafilatura” to be as performant as “newspaper” could not be ruled out. Nonetheless, the IQR and number of online news articles with an F1-score lower than 0.5 (here regarded as errors of the text extractor) suggested that “newspaper” is the most appropriate choice for extracting text from Romanian news articles to conduct NER tasks; these findings are consistent with the results of the text extraction evaluation (see Fig. 2).

The pairwise comparisons for the English articles on the NER task were consistent with the results of the task extraction evaluation (see Fig. 3): the “newspaper”, “readability”, and “trafilatura” automated text extractors consistently obtained better results than “boiler” and “justext”. Unlike the text extraction evaluation, “trafilatura” performed consistently better than “newspaper” on this task. Based on these results, the analysis of the IQR, and the number of online news articles with an F1-score lower than 0.5, “trafilatura” was regarded as the most appropriate choice for extracting text from English online news articles to conduct NER tasks.

As regards the results for the NER task conducted on French articles, the pairwise comparisons are consistent with the results of the text extraction evaluation (see Fig. 4), with “justext” having consistently poorer results than the other text extractors. Unlike the text extraction evaluation, “trafilatura” was consistently better than “readability”. Based on the latter observation, as well as on the median score, IQR, and the number of online news articles with an F1-score lower than 0.5, “trafilatura” was defined as the most appropriate choice for extracting text from French online news articles to conduct NER tasks.

Concerning the sentiment analysis task, the text extractors were compared based on the Kullback-Leibler (KL) divergence scores. These scores were computed using the distributions of sentiment scores generated from automatically extracted and human-validated texts. Here again, statistically significant differences were recorded for Romanian articles $\mathbf { \check { w } } = 0 . 1 6 3$ , $\mathsf { p } < . 0 0 1 )$ , English articles ( $\mathrm { \Delta w } = 0 . 0 9 1$ , $\begin{array} { r } { \mathbf { p } < . 0 0 1 } \end{array}$ ), and French articles ( $\mathbf { w } = 0 . 0 4 3$ , $\mathsf { p } < . 0 0 5$ ). Table VI introduces the results by text extractor and language. Scores with bold highlight the most performant alternative resulting from the pairwise post-hoc comparisons, median score, IQR, and the number of errors defined as the number of articles that registered a KL score higher than 0.05. While the results are consistent with those from Table V, it should be noted that both “boiler” and “newspaper” obtained similar performance with “trafilatura” for French articles.

TABLE VI SENTIMENT ANALYSIS TASK: MEDIAN KL SCORE, INTERQUARTILE RANGE, AND ERRORS BY TEXT EXTRACTOR AND LANGUAGE.   

<html><body><table><tr><td rowspan="3">Text extractor</td><td colspan="9">Language</td></tr><tr><td colspan="3">RO</td><td colspan="3">EN</td><td colspan="3">FR</td></tr><tr><td>Mdn</td><td>IQR</td><td>*</td><td>Mdn</td><td>IQR</td><td>*</td><td>Mdn</td><td>IQR</td><td>*</td></tr><tr><td> boiler</td><td>0.005</td><td>0.045</td><td>24</td><td>0.003</td><td>0.03</td><td>19</td><td>0.001</td><td>0.11</td><td>10</td></tr><tr><td>justext</td><td>0.021</td><td>0.087</td><td>34</td><td>0.015</td><td>0.07</td><td>18</td><td>0.004</td><td>0.12</td><td>9</td></tr><tr><td>newspaper</td><td>0.001</td><td>0.014</td><td>12</td><td>0.003</td><td>0.02</td><td>11</td><td>0.001</td><td>0.009</td><td>9</td></tr><tr><td>readability</td><td>0.003</td><td>0.011</td><td>14</td><td>0.002</td><td>0.02</td><td>16</td><td>0.002</td><td>0.19</td><td>10</td></tr><tr><td>trafilatura</td><td>0.005</td><td>0.022</td><td>16</td><td>0.001</td><td>0.01</td><td>10</td><td>0.001</td><td>0.008</td><td>9</td></tr></table></body></html>

Concerning the text summarization task, the text extractors were compared using the F1-score of the Rouge-l. The F1- score of Rouge-l was computed using the results of the summarization model, utilizing the human-validated texts’ summaries as reference. Statistically significant differences between the text extractors were identified for Romanian articles $\mathbf { \tilde { w } } = 0 . 0 2 8$ , $\mathsf { p } < . 0 5 ,$ ), English articles $\mathbf { w } = 0 . 1 2 3$ , p $< . 0 0 1 $ ), and French articles $\mathrm { w } = 0 . 0 8 9$ , $\mathsf { p } < . 0 0 1 $ ). Table VII introduces the results by text extractor and language. Scores with bold highlight the most performant alternative resulting from the pairwise post-hoc comparisons, median score, IQR, and the number of errors defined as the number of articles that registered an F1-score of Rouge-l lower than 0.25. While the results are consistent with those from Table V and Table VI, it should be noted that “newspaper” obtained a better median performance than “trafilatura” for French articles. However, it also resulted in more errors, thus making “trafilatura” a more suited technology for automatically extracting text from French online news articles.

TABLE VII TEXT SUMMARIZATION TASK: MEDIAN F1-SCORE OF ROUGE-L, INTERQUARTILE RANGE, AND ERRORS BY TEXT EXTRACTOR AND LANGUAGE.   

<html><body><table><tr><td rowspan="3">Text extractor</td><td colspan="9">Language</td></tr><tr><td colspan="3">RO</td><td colspan="3">EN</td><td colspan="3">FR</td></tr><tr><td>Mdn</td><td>IQR</td><td>*</td><td>Mdn</td><td>IQR</td><td>*</td><td>Mdn</td><td>IQR</td><td>*</td></tr><tr><td>boiler</td><td>0.47</td><td>0.46</td><td>26</td><td>0.53</td><td>0.61</td><td>31</td><td>0.48</td><td>0.30</td><td>12</td></tr><tr><td>justext</td><td>0.52</td><td>0.33</td><td>20</td><td>0.36</td><td>0.20</td><td>23</td><td>0.40</td><td>0.22</td><td>14</td></tr><tr><td>newspaper</td><td>0.56</td><td>0.43</td><td>15</td><td>0.54</td><td>0.61</td><td>18</td><td>0.54</td><td>0.51</td><td>17</td></tr><tr><td>readability</td><td>0.46</td><td>0.40</td><td>27</td><td>0.62</td><td>0.64</td><td>26</td><td>0.46</td><td>0.37</td><td>16</td></tr><tr><td>trafilatura</td><td>0.48</td><td>0.40</td><td>17</td><td>0.59</td><td>0.47</td><td>17</td><td>0.50</td><td>0.34</td><td>11</td></tr></table></body></html>

# V. CONCLUSIONS

The current study reveals that selecting a particular technology for HTML text extraction can impact the quality of extracted data. Based on our analysis, the decision to choose one text extraction technology over another can lead to statistically significant differences in data quality. Here, it should be noted that according to Kendall’s $\mathbf { W }$ value described by Tomczak & Tomczak [28] and using Cohen’s interpretation guidelines embedded in the rstatix $\mathbf { R }$ package, these differences are moderate for French and English articles $( \mathbf { w } > 0 . 3 )$ ), but small for Romanian articles $\mathrm { w } = 0 . 2 3 8$ ). The latter effect size gives reason to refrain from drawing definitive practical conclusions as regards the most performant text extraction method in the case of Romanian articles. Nonetheless, the relative close value to that of English and French articles gives reason to conduct further studies specifically focused on Romanian articles.

Focusing on the moderate effect in the case of English and French articles, this can be explained through the interaction between the text extractors and the HTML structure of the websites from which the texts were extracted, or it may be due to the presence of ”cookies” pop-ups extracted during the automated text extraction process. A second Friedman analysis was conducted to test the latter hypothesis in which the ”cookies” were accepted during text extraction. The results of this analysis illustrated that the effect size dropped from moderate to low in the case of French articles but not for English ones. This suggests that the ”cookies” pop-ups represented an impactful factor in the case of French articles. Moreover, these results highlight the HTML structure as a plausible explanation for the moderate effect in the case of English articles.

Furthermore, the evaluation of each NLP task demonstrated a statistically significant difference between the quality of the NLP results based on the choice of text extractors. However, it should be noted that in all NLP tasks, regardless of language, the effect size of choosing one text extractor over another was small, especially for sentiment analysis and summarization tasks. This finding gives reason to consider that NLP models can reduce the impact of the text extractors on data quality, particularly in NLP tasks that involve identifying emotional tone and summarization. While the effect size was also small in the case of the NER task, the effect size difference gives reasons to consider further research on how the choice of text extractors influences NER performance compared to the other two NLP tasks.

Although the small effect size demonstrates certain practical limitations in terms of NLP performance differences among the text extractors, analyzing the NLP results enabled us to identify the most suitable text extractor for each language and NLP task. This conclusion was drawn based on criteria such as post-hoc pairwise comparison results, median scores, IQR, and the number of errors. Our findings indicate that text from Romanian online news articles should be extracted with “newspaper”, whereas “trafilatura” should be used for English and French articles, regardless of the NLP task. However, it should be noted that the performance of the ”newspaper” extractor in the case of French articles was also noticeable, thus giving reason to consider further experiments with a larger dataset of French articles.

To conclude, this study contributes to the literature by elaborating on the notion of ”garbage in, garbage out” in the context of NLP models by exploring the extent to which the selection of text extraction methodology influences the model performance. Our findings suggest that while the choice of an extraction technology has a minor to moderate impact on the quality of extracted text depending on the data source, the impact on the performance of NLP models is rather small, with NER tasks being the most susceptible to the choice of text extraction methodology. This aspect is of particular importance in light of the extensive adoption of large-scale language models trained on corpora extracted from the Internet and advancements in the field of AI/NLP.

# ACKNOWLEDGEMENT

This work was funded by the “Innovative Solution for Optimizing User Productivity through Multi-Modal Monitoring of Activity and Profiles – OPTIMIZE” / “Solut,ie Inovativa˘ de Optimizare a Productivitat˘ ,ii Utilizatorilor prin Monitorizarea Multi-Modala a Activit˘ at˘ ii si a Profilelor – OPTIMIZE” project, Contract number 366/390042/27.09.2021, MySMIS code: 121491.

# REFERENCES

[1] ADGEfficiency. Adgefficiency/climate-news-db: A database of climate change newspaper articles.   
[2] Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov, Vanni Zavarella, Erik Van Der Goot, Matina Halkia, Bruno Pouliquen, and Jenya Belyaeva. Sentiment analysis in the news. arXiv preprint arXiv:1309.6202, 2013.   
[3] Adrien Barbaresi. Trafilatura: A web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122–131, 2021.   
[4] Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258–266, 2022.   
[5] Jason Brownlee. How to calculate the kl divergence for machine learning, Oct 2019. [6] Orphee De Clercq, Luna De Bruyne, and V ´ eronique Hoste. News ´ topic classification as a first step towards diverse news recommendation. Computational Linguistics in the Netherlands Journal, 10:37–55, 2020.   
[7] Explosion. spacy, 2016-2021.   
[8] Github-boilerpy3. GitHub - jmriebold/BoilerPy3: Python port of Boilerpipe library — github.com. [Accessed 23-Feb-2023]. [9] Github-justext. GitHub - miso-belica/jusText: Heuristic based boilerplate removal tool — github.com. [Accessed 23-Feb-2023].   
[10] Github-newspaper. GitHub - codelucas/newspaper: News, full-text, and article metadata extraction in Python 3. Advanced docs: — github.com. [Accessed 23-Feb-2023].   
[11] Github-readability. GitHub - buriy/python-readability: fast python port of arc90’s readability tool, updated to match latest readability.js! github.com. [Accessed 23-Feb-2023].   
[12] Saram Han and Christopher K Anderson. Web scraping for hospitality research: Overview, opportunities, and implications. Cornell Hospitality Quarterly, 62(1):89–104, 2021.   
[13] Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Samin, Yuan-Fang Li, Yong-Bin Kang, M Sohel Rahman, and Rifat Shahriyar. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages. arXiv preprint arXiv:2106.13822, 2021.   
[14] Yin-Fu Huang and Po-Hong Chen. Fake news detection using an ensemble learning model based on self-adaptive harmony search algorithms. Expert Systems with Applications, 159:113584, 2020.   
[15] Denis Iorga. Climate change in Romanian online news, February 2023.   
[16] Kaggle. Getting Real about Fake News — kaggle.com. [Accessed 20- Feb-2023].   
[17] Alboukadel Kassambara. rstatix: Pipe-Friendly Framework for Basic Statistical Tests, 2022. R package version 0.7.1.   
[18] Moaiad Ahmad Khder. Web scraping or web crawling: State of art, techniques, approaches and application. International Journal of Advances in Soft Computing & Its Applications, 13(3), 2021.   
[19] Chin-Yew Lin and FJ Och. Looking for a few good metrics: Rouge and its evaluation. In Ntcir workshop, 2004.   
[20] Mihai Masala, Stefan Ruseti, and Mihai Dascalu. Robert–a romanian bert model. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6626–6637, 2020.   
[21] Paul Meddeb, Stefan Ruseti, Mihai Dascalu, Simina-Maria Terian, and Sebastien Travadel. Counteracting french fake news on climate change using language models. Sustainability, 14(18):11724, 2022.   
[22] Mihai Alexandru Niculescu, Stefan Ruseti, and Mihai Dascalu. Rosummary: Control tokens for romanian news summarization. Algorithms, 15(12):472, 2022.   
[23] Dulce G Pereira, Anabela Afonso, and Fatima Melo Medeiros. Overview´ of friedman’s test and post-hoc analysis. Communications in StatisticsSimulation and Computation, 44(10):2636–2653, 2015.   
[24] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2022.   
[25] Vignesh Rao and Jayant Sachdev. A machine learning approach to classify news articles based on location. In 2017 International Conference on Intelligent Sustainable Systems (ICISS), pages 863–867. IEEE, 2017.   
[26] Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. Fake news detection on social media: A data mining perspective. ACM SIGKDD explorations newsletter, 19(1):22–36, 2017.   
[27] Anca Tache, Gaman Mihaela, and Radu Tudor Ionescu. Clustering word embeddings with self-organizing maps. application on LaRoSeDa - a large Romanian sentiment data set. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 949–956, Online, April 2021. Association for Computational Linguistics.   
[28] Maciej Tomczak and Ewa Tomczak. The need to report effect size estimates revisited. an overview of some recommended measures of effect size. Trends in sport sciences, 21(1), 2014.   
[29] Erdinc¸ Uzun, Tarık Yerlikaya, and Oguz Kırat. Comparison of python ˘ libraries used for web data extraction. Journal of the Technical University-Sofia Plovdiv Branch, Bulgaria, 24:87–92, 2018.   
[30] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328–11339. PMLR, 2020.