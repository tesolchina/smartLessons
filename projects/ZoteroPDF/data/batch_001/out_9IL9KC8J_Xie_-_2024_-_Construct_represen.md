# Construct representation and predictive validity of integrated writing tasks: A study on the writing component of the Duolingo English Test

Qin Xiea,b, \*

a Faculty of Education, University of Macau, Macau b Faculty of Humanities, Education University of Hong Kong, Hong Kong

# ARTICLEINFO

# ABSTRACT

Keywords:   
Construct representation   
Integrated reading-to-write tasks   
Predictive validity   
Duolingo English Test

This study examined whether two integrated reading-to-write tasks could broaden the construct representation of the writing component of Duolingo English Test (DET). It also verified whether they could enhance DET's predictive power of English academic writing in universities. The tasks were (1) writing a summary based on two source texts and (2) writing a reading-to-write essay based on five texts. Both were given to a sample $( \Nu = 2 0 4 )$ of undergraduates from Hong Kong. Each participant also submitted an academic assignment written for the assessment of a disciplinary course. Three professional raters double-marked all writing samples against detailed analytical rubrics. Raw scores were first processed using Multi-Faceted Rasch Measurement to estimate inter- and intra-rater consistency and generate adjusted (fair) measures. Based on these measures, descriptive analyses, sequential multiple regression, and Structural Equation Modeling were conducted (in that order). The analyses verified the writing tasks' underlying component constructs and assessed their relative contributions to the overall integrated writing scores. Both tasks were found to contribute to DET's construct representation and add moderate predictive power to the domain performance. The findings, along with their practical implications, are discussed, especially regarding the complex relations between construct representation and predictive validity.

# 1. Introduction

Existing research on the Duolingo English Test (DET), mostly conducted by Duolingo's in-house research team, reported establishing its concurrent validity with wellrecognized English proficiency tests, notably TOEFL bt and IELTS (Brenzel & Settes, 2017) LaFair & Settles (2020, p.21), for instance, reported a strong positive correlation between DET and TOEFL iBT $( \mathrm { N } = 2 3 1 9$ $r = . 7 7 $ and IELTS scores $( \mathrm { N } = 9 9 1$ .A $r = . 7 8 \AA$ . Similarly, Brenzel & Settle (2017) reported a high correlation of DET with TOEFL iBT $[ \mathrm { N } = 1 0 5 7$ $r =$ .71) and IELTS $( \mathrm { N } = 3 0 4$ $r = . 7 0$ . The constructs assessed by DET, as described in Settles et al. (2020), however, do not appear to adequately represent the target domain of reference (i.e., language-related academic tasks in a universty context), indicating a concern about inadequate construct representation voiced elsewhere in association with automated tests and the limits of technology (Khabbazbashi et al., 2021; Isaacs et al., 2023).

Construct underrepresentation could threaten the validity of test cores for making high-stakes decisions (Messick, 1989) and lead to negative washback on teaching and learning (Messick, 1996). Although English proficiency tests adopted for universty admission purposes have rarely, if eer, achee ful onstrct rerentation, there has beo lack f chlarly atmps to idtfy constructs that are essential to successful academic performance.

The writing component of DET entitled Extended Writing Tasks (DET-EWT thereafter) is of particularly interest to the present research- especiall its construct representation and predictive validit of academic writing. According to LaFlar & Settes (2020), DET-EWT consists of three picture description tasks, each of which elicits short responses of one or more sentences, and one independent writing task, which requires test takersto describe, recount, or make an argument (p. 12)in at last 50 words. Fig. 1 presents two DET-EWT sample tasks.

The written responses to DET-EWT are scored at the portfolio level automatically by a scoring algorithm; one overall score is produced to represent the test-taker performance on all four tas. Acording to Laair & Sette (2020, p.18), the scoring alorithmwas trained on 3626 writing samples marked by human raters and achieved a high human-machine agreement (Cohen's K: Human: Machine $= . 8 2$ vs. Human: Human $= . 6 8 \mathrm { \ ' }$ . In another study on DET subscores (of production and literacy), LaFlair (2020, p.12-13) reported moderate to strong positive correlations $( r = 0 . 4 0$ to 0.70) with the writing scores of TOEFL and IELTS.

![](img/32114154bd6ae8de23c27a46a387df3c023ea1563c639a4e6bdf11e08c95c61a.jpg)

Write one or more sentences that describe the image

![](img/3cc1138d56ee8d395ef0120cb6b4d644541d9aa3aeac924b839cf906b7df4498.jpg)

# NEXT

# 1:32

![](img/9f042335a5c727a2a08124c6017579ac3a43fe8b2d90247697c61a3db9a3f9d5.jpg)  
Fig. 1. Sample writing tasks for EWT (Extracted from p.14, LaFair & Settles, 2020).

Written responses to ET-EWT are automaticl scored based on six linguistic features, namely, gammatical accuracy, gammatical complexity, lex sophistatio, exl divrty, sk rce, and egth Thee fre, illy the first four, e ssocit with core language-use competence fundamental to general and academic writing. However, it could be argued that language use alone cannot represent the wide range of constructs underlying academic writing, such as gere structures cohesion and coherence, and use of source materials.

Existing studies (e.g., Gardner & Nesi, 2013) of academic writing in English-medium universities identified a wide range of aca demic asignments that student wrot for assesment. They could be classied into 13 genre families, of which the academic esay (AE) was the most common. Within the AE genre family, the writing samples usuall share a genre structure consisting of an introduction, a series of arguments, and a conclusion involving component nres such as "challenge, commentary, consequential, iscusson, exposition, and factorial' (Gardner & Nesi, 2013, p. 38). Moreover, common academic writing tass at university require students "to demon state/delopthe alit  conct  cohent en nd mp crtl thinking kill an reig and iting informationdense source materials (Gardner & Nesi, 2013). The writing samples are often assessed in terms of content (i.e., isplay of topic knowledge, fulfillment of task requrement), organization (adherence to expected genre structure coherence, and cohesion), language use (lexical) syntactic accuracy and sophistication), and compliance with academic conventions (e.g, citations and references of source materials) (Authors 1, 2).

Integration of source materials and citation practices are important indicators of academic litracy skill, differentiating academic from general writing. Therefore, the use of sources in As has attracted a great deal o attention from language-testing and L2-writing researchers (umming, 2013), as well asfrom large esting institions. An example of source-based writing is sen in the OEFL iBT's integrated listening-reading-writing tasks (see Cumming et al., 2005] for the theoretical positioning).

In light of the above, i can be argued that ading integrated reading-to-write tasks to DET-EWT could extend its core to include discourse competence (e.g., cohesion and coherence) and basic academic literacy skill (e.g, citation and uses of source ideas and language), thereby, broadening is construct representation of the domain and narrowing the gap between what is ssessed in the test and what is to be predicted, i.e., English academic writing in universities.

The present research developed two integrated reading-to-write tasks to examine whether they could enhance DET-EWT's construct representation and predictive validit of the target domain. The first task (Task 1) required test-takers to read two texts on the same topic but with opposing views and to write a summary of 150-200 words. The second (Task 2) required them to read another three texts on the same topic, to choose stance, and write an argumentative esay of no more than 500 words based on l five texts. Both tasks assessed discourse organization and source use, two essential component constructs of academic writing.

The two tasks were developed based on insights from existing research (to be reviewed in the next section) on integrated essay writing assessment (e.g., Cumming et al., 005; Cumming et al., 2016; Plakans & Gebril, 2013; Sawaki e l., 2013; Zhu et al., 2020) and summary writing (e.g., Sawaki, 2005; Yang, 2014; Yu, 2009). Accordingly, the following research questions were formulated to guide the investigation:

RQ 1: What e the component costructs underlying the summary and the reding-to-write argumentative esay asks and academic writing at university?   
. RQ 2: To what extent if at all could the two integrated writing tasks enhance the Duolingo writing test (DET-EWT)'s construct rpresentation and predictive validity on undergraduates' academic writing performance?

In both RQs, academic writing at universit is represented by the academic asignments wrten by the participants for the assesment of their disciplinary courses. These assignments covered various topics but shared the features of AE as described in Gardner and Nesi (2013) (Appendix 3 presents sample assgnments at thre levels) and involved reading and integrating source texts. They were produced individuall, outside the classoom, with no time constraint, and representing the participants' best efforts.

By examining the constructs of integrated writing through different task formats (RQ1) and investigating their prediction on domain performance (RQ2), the study builds on prior research, such as Plakans & Gebril (2013) and Kyle (2020), to enhance our understanding of the constructs underlying integrate writing tass and contribute to the ongoing research into integrated writig assessment, especially as it becomes more prevalent in language testing and assessment.

Additionall, this reearch explores the les charted concepts of onstruct representation and predictive validity through innovative experimentation with two thematicall related integrated writing tasks alongside DET-EWT. Considering the paucity of empirical studies in these specific areas within the language testing litrature, our work is set to provide meaningful new insights. The study's fresh examination of these ideas is particularly timely, as Al-based online testing platforms, which rely on predictive validity for accurate and relevant assessments, are gaining increasing popularity.

# 2. Literature review

# 2.1. Construct representation vs. prediction

The term construct has been defined variously in the scholarly literature but al defintions center on a foundational idea, that is construct is an abstract theoretical entity or postulated atribute of people (Cronbach & Meel, 1955, p. 283), which i not directly observable but can be inferred from consistent patterns of behavior, performance, or responses (Ebel & Frisbie, 1991, p.108) Bachman (2007) defined the construct as what i is that we want to asses (p. 42). Yin and Mislevy (2022) surveyed diffrent approaches to define a construct and noted they all centered on aspect of test-takers' stable capacities acros different situations or consistent observable behaviors on specific tasks. Fulcher and Davidson (2007, p. 36-37) used construct to refer to component knowledge, skill and abilities of a model theorized to underpin test performance. Fulcher and Davidson's defintion of construct is adopted in this article.

Constructs are central to the validity of assessments. According to Messick (1989), construct under-representation and construct-irrelevant variance are two central threats to test validit. Construct representation refers to the extent to which a test can represent all the oeses, kowege, sills nd bilities undring the tgt mance. onstct-releant vriae rer o the other atributes that have been unintentionally measured and have afected test scores (se also Haladyna & Downing, 2004). Existing research into construct representation aims to identify the conceptual cognitive mechanisms underlie the target performance (Embretson, 1983, 2006; Messick, 1989). These underlying mechanisms, also called component constructs, trit, factors, or variabes, are fundamental to performance and could apply to a wide range of tasks. Their varied combinations define the validity of tes scores or what a test assesses or represents.

How well test can reflect the taget construct(s) is highly valued in test development. Full representing the target construct(s) can not only enhance the validity of test score interpretation but also enable score-based prediction of the target domain performance. In conceptualizing test authenticity, Bachman (1991), for instance, note both inferences and predictions about test-taker capacit to use language for future tass and the decisions made based on test scores rest on the correspondences between performance on alanguage test and performance on non-test language use contexts. Such correspondences between a test and its target domain of rference underpin construct representation.

Score-based prediction is of considerable utilit value to test users such as university admissions officers, who have an interest in foreseeing test-takers future academic performance based on their current test scores. A writing tes could have geat utility when its scores can predict important future writing performance.

Representing the target constructs in a test development project, however, i a long-standing chllenge because \*tests are imperfect measures of onstruct..they either leave out something tha should be included.. or else include something that should e left out, or both' (Messick, 1989, p. 34). Leaving out essential constructs leads to inadequate construct representation, whereas unintentionally including things that should not be included leads to constructirrelevant variance; both threaten the construct validity of test scores (Haladyna & Downing, 2004; Sireci & Zenisky, 2006).

Empirical verification of construct representation could be achieved through means of curriculum analysis, domain theory (Messick, 1995) or task analysis through decomposing the target domain performance (Embretson, 1983, 2006) such as the analysis outlined in the introduction to DET-EWT. Here, potential underreresented constructs necessary to academic writing in university are identified. It is important to point out, however, that the components identified i task decomposition do not necessarily reflect individual dfferences (Embretson, 1983); that is a proces or strategy may be an essential construct to the target performance, but the population under study may not vary systmaticll in their aility to perform it or instance, the proces may e so easy that virtually everyone can perform it without dificult, or it may e so difficult that few can perform it. In either case, the construct will not be associated with systmic indvidualdifferences or, technicaly, the variable's variance. Consequently, despite its necessit to the overall performance the construct may be of lite utility for predicting future eformance eause statistical prediction is based on the nomothetic span of correlations, which depend on the variability of measures.

Nomothetc pan refers to the network of correlations of a test with other measures (Embretson, 1983; Messick, 1989, 1995) Whereas construct reresentation concerns the intenal validity of atest, nomothetic span i related to tets external validity, that is how a test iscorrlated with other related measures of similar constructs. The correlation matrix among atest and a network of other measures is built on many possble commonalities underlying these measures, such as the population's shared prior knowledge, ed. ucatin, and even gentic traits (e., inellince), as el as the measures common underlying thortical construct or their hared construct representation. The nomothetic span of correlations with other measures isthe basis of a test's predictive validity.

# 2.2. Predictive Validity

Predictive validity is a form of criterion-related test validity defined as the extent or degree to which a test or a psychological measure can predct criterion (Cronbach & Meehl, 1955; Ihlenfeldt, 2023). The criterion could be academic succes or, in the case of the present research, academic writing performance in university. Existing predictive validty studies in language testing (Bridgeman, Cho, & DiPietro, 2015; Cho & Bridgeman, 2012; Ginther & Yan, 2018; Ihlenfeldt & Rios, 2022; Isacs, Hu, Trenkic, & Varga, 2023) tended to fous on the overall scores of Eglish proficiency t dopted for admission puroses, usually TOF or IE, and how wel they could predict academic sces in university. The atter was usually operationalized as students' Graduate Point Average (GPA). Most studies (e.g., Bridgeman, Cho, & DiPietro, 2015; Cho & Bridgeman, 2012; Ginther & Yan, 2018) conducted corrlation analysis based on linear (Pearson) or rank-ordere (Spearman) correlation cofficients between the overall English proficiency scores and GPA. A recent meta-analysis of IELTS predictive validity studies (Pearson, 2021) found that $9 0 \%$ of studies utilized GPA to indicate academic success, and $8 4 \%$ adopted correlational methods.

Their findings are mixed: researchers identified from strong to weak or none, and even negative correlations (Ihlenfeldt & Rios, 2022). Three meta-analyses on TOEFL's predictive vlidit f academic success (Abunawas, 2014; hlenfeldt & Rios, 2022; Wongtrirat, 2010) based on a total of 95 primary studies found small but positive correlations (Abunawas, $r = 0 . 2 1 \AA$ ; Ihlenfeldt, $r = 0 . 2 3 1$ Wongtriat, $r = 0 . 1 8$

Only a few studies focused on DET predictive validity of academic performance, notably Isac et al. (2023) and Ishikawa et al. (2016). Similar toTOEFL and IELS predctive validity studies, Isacs et al. (2023) adopted correlation methods and found DET overal scores had positive but small correlations $( r = 0 . 1 9 5$ ) with first-year academic grades for postgraduate but not for undergraduate students. This is contrary to the findings of Ishikawa et al. (2016), which found DET oveall scores to have stong predictive power to faculty asssment (r between 0.39 to 0.62). Furthermore, Ishikawa et al. also found DET to have stronger predictive validity than TOEFL for faculty assessment of students' written (DET: ${ r = } 0 \mathrm { { . } } 6 2 $ ; TOEFL: $r = 0 . 4 1$ ) and spoken performance (DET: $r = 0 . 4 9$ ; TOEFL: $\pmb { r } = 0 . 3 8 $ . Ishkawa et al. (2016), however, was based on a small sample of 77 students.

Except for Ishkawa et al. (2016), no studies have reported the predictive validity of component scores on respective criterion performance, e.g., academic writing and speaking performance in academia. Of particular interest to the present study is that the study reported TOEFL writing scores to correlate strongly with faculty assessment of student written performance $( r = 0 . 5 2 )$

Although the findings of the above predictive validity studie are not directly comparable tothe present study, they set a context for understanding our findings and informed us that the magnitude ofthe predictive power of DET-EWT on academic writing performance is likely to be small.

# 2.3. Source-based integrated writing: the construct represented

Integrated writing tests rests on a common, albeit not always explicit assumption, that they are a closer, fuller or better repre. sentation of academic writing performance than the traditional approach that asesses independent essay writing without sources. They are, therefore, considered capably of increasing test validity (Cumming et al., 2005; Kyle, 2020) and potentiall bring more power to predict the domain writing performance.

Both summary and integrated reading-to-write essay writing belong to source-based integrated writing, which involves a complex set f linguistic,cognitive, metacognitie, and academic literacy skill (mming et al., 2016) that ar esential for student writers to succeed in academia (Wette, 2017).

Summary writing, or summarization, is described as the paraphrasing of original texts with fewer words as ales-detailed recall" (Yu, 2009, p. 116). Summarization involves comprehension of the original texts main ideas and is afected by the length, types, and numer of source texts. Summarizing a single source, therefre, diffrs from summarizing multiple sources. In the latr, in addition to the coverage and accuracy of main source ideas, diffrent sources need to be connected and combined through deleting unnecessary detail, selecting, reordering and combining different source dea units (Sawaki, 2005; Sawaki, Quinlan, & Lee, 2013). Source content integration, defined as the coverage and accuracy of source ideas is the primary construct that differentiates summary from independent writing. Sawaki (2005) asesed ths construct with four subscales: one refers to a general content frame and the other three to specific main points of the source material. Yang (2014) asessed his summary writing samples in terms of content (i.e., covering principal source ideas and analysis of the connections between the sources), form (i.e, coherence and cohesion), and language use (including features of lexis and syntax).

In addition to source integration, integrated reading-to-write essays also asess argumentation or own opinions. A wellstudied example of this task type is the TOEFL iBT listening-reading-writing integrated test, where test-takers are required to read a passage, listen to a talk (on the same topic as the passage but from an opposite viewpoint), and write 150-225 words in $2 0 \mathrm { { m i n } }$ to summarize and explain the input and express their own stance (Plakans & Gebril, 2013; Yang & Plakans, 2012). The constructs assesed as operationalized in its rubrics consist of content (ie, the accuracy and completenes with which principal source ideas are incorporated, verbatim language taken from source, as wellas traditional constructsassessed via writig, namely, organization (i.e, cohesion and coherence), language use (i.e., lexical and syntactic accuracy and sophistication).

Another, perhaps les well-known, example of integrated reading-to-write task is from the GEPT advanced-level writing test produced by the Language Training and Testing Center in Taiwan (Qian, 2014; Weir, Chan & Nakatsuhara, 2013). The test has two writing tasks: Task 1 uses two input texts and requires test takers to write a summary and express their views in 250 words and $6 0 \mathrm { { m i n } }$ Task 2 provides non-verbal input of charts, graphs, tables or pictures, and requires tet takers to write a summary, analyze the problem, and provide solutions in 250 words and $4 5 \mathrm { { m i n } }$ . The rubrics consist of four criteria: Relevance and Adequacy, Coherence and Cohesion, Lexical Use (range and appropriateness, and Grammatical Use (range and accuracy. Summary of source ideas is assesed under the criterion of Relevance and Adequacy, whereas the use of source language is under Lexical Use, in particular, verbatim text borrowing (labeled as plagiarism) without appropriate signals is penalized (Qian, 2014, p.4; Weir, Chan & Nakatsuhara, 2013, p.5)

The reading-into-writing task was also adopted by Trinity Collge London (Chan, Inoue & Taylor, 2015) in their Integrated Skills of English paper. Similar to prior studie, the utilization of sources was operationalied as understanding source materials, choose pertinent infomatiofrom ource roniz eonshi  wh nd btw sores, modify the ourceonte suit th writives, nd summarizing and paraphrasing source ideas.

# 2.4. Integrated reading-to-write tasks: the construct assessed

Following the early research of Cumming et al. (2005), which compared integrated with independent essay writing tas, a line of research investigating various aspects of integrated writing has been accumulated in the literature of L2 writing assessment. In particular, many rcent studies long this line explored the constructs underlying integrated writing performance (Chan & Yamashita, 2022), among which, of direct relevance to the present research, are the studies that examined the part-and-whole relations between the features of integrated writing samples, uch as source integration, organization, and language use, and the overall writing scores. Most of these tdies reported overall scores had positie orelations with the quality of source use and interationg., Kyle, 2020; Geril and Plakans, 2009; Plakans & Gebril, 2013; Zhu et al., 2020) and other discourse and language use features (Galloway et al., 2020; Llosa & Malone, 2019).

Gebril and Plakans (2009) found that a positive correlation between the quality scores of integrated reading-to-write essays and the use of sources, both indirect source use via paraphrasing and the total amount of source use (including paraphrasing, quoting, and copying). In a later study (Plakans & Gebril, 2013), they manuall coded various source use features and found these features could explain a large portion $( 5 5 \% )$ of the overall score variance. Building upon these studies, Kyle (2020) adopted automated text analysis applications to code the linguistic overlaps between writing samples and source texts (e.g, word, n-gram, synonym and semantic overlaps). He found the textual indices related to content-word and n-gram overlaps explained a significant portion of the variance of overall integrated writing scores.

Zhu et al. (2020) focused on the features of discourse synthesi (i.e. quotation, summarization and connection). Unlike Gebril and Plakans (2009), Plakans & Gebril (2013) and Kyle (2020), they adoptd rubrics t asses thee features. They also found these features significantly contributing to the overall scores, explaining $6 3 . 6 \%$ of the variance of writing performance in the L1 (Chinese), and $4 7 . 9 \%$ in L2 (English).

Other features of integrated writing samples were also found to make positive contributions to overall writing performance, albeit at varying magnitudes. Both cohesion and coherence were found to make a positive contribution (Plakans & Gebril, 2016; Llosa & Malone, 2019), although ther relative importance remains inconclusive. Some studies (e.g., Llosa & Malone, 2019) reported cohesion had a larger correlation coefficient than coherence; some found cohesion made littl contributin (Galloway et al., 2020). The con. tributions of language use (including lexical complexity and diverity; syntactic complexity and accuracy) also remain inconclusive. In general, weak to moderate correlations were found. Glloway et al. (2020) and Ucclli e l. (2019) reported that lexical diversity had weak to moderate correlations; Llosa & Malone (2019) reported lexical and syntactic accuracy had moderate correlations with integrated writing scores. The differences in their findings are partiall dueto different operationalization of the features as well as whether they were scored based on textual or linguistic coding or against analytical rubrics.

The above research provides important insights to understanding the constructs underlying integrated reading-to-write tasks, however,to the best knowledge of the present researchers, research investigating the construct representation and predictive validty of source-based integrated writing remains in paucity. It is this research gap that the present study aims to fill.

# 3. The present study

The present study examined the construct representation and predictive validity of two integrated reading-to-write tasks, alongside DET-EWT, for academic writing at the university. One task onsisted of reading two passages and writing a summary; it design follows Yang (2014) and Zhang (2013). The second task consisted of reading five passages and writing an argumentatie essay; its design was inspired by Zhu et al., (2016, 2020).

The first task, i.e., the summary task, is similar to the listening-reading-writing task of TOEFL-iBT in that the two inputs pose conflicting views, but in TOEFL-iBT, one input is in the listening mode and the other in reading mode. It is also similar to the GEPTadvanced writing Task 1, which also involves two input texts of opposing views; diferig from GEPT Task 1, which also required test takers to state their own views, the summary task in the present research does not have this requirement.

The second task is more demanding as it involves the reading and synthesis of five input texts. It could be argued that this task, is closer to authentic academic writing situations, wherein students write from a number of source materials. For instance, the large-scale citation project reported by Jamieson (2013) found that undergraduate students in North American universities used an average of 10 sources in each academic paper. his task is better suited to tes-takers with abilities located at the higher end of the English language proficiency continum, which is the case of the present research. According to existing research (Bruce & Hamp-Lyons, 2015; Xie, 2020, 2023), undergraduate students in Hong Kong have a relatively high proficiency (above B1 of the CEFR ability spectrum).

The written responses to the two tass were assessed in terms of source use (oth source deas and language), discourse featres .. coherence and cohesion), and language use (i.., excal and syntactic accuracy and diversty). As the second task was areading-to-write argumentative essay, the strength of argumentation was also assessed.

# 4. Methodology

Three approaches were widely adopted to empirically asessing construct representation (Embretson, 1983): 1) the mathematical modeling approach (e.g, linear additive models such as regresion models and simple multiplicative models), 2) latent-trait modeling (e.g., Rasch modeling), and 3) multicomponent latent-trait modeling (MLTM). Mathematical models can verify the theoretical variables but are unable to parameterize personal abilitie; latent-trait Rasch modeling can parameterize both item and personal abilities but has a unidimensionalassumption and cannot parcel out the underlying component constructs. The MLTM approach combines the strengths of the first two approaches, but i requires a large sample size (often involving thousands of test-takers) to produce reliable estimates. Such a sample size is often unrealistic for L2-writing reearch. For research with modest sample sizes, a two-step method is recommended.

Specifically, the first step is to adopt the latent trait Rasch modeling approach to generate multiple-person measures based on subsets of items concerning diferent theoretical components. This method is conceptuallysimilar to the item-parceling approach (Purpura, 199; Xie, 2013, 2015a; Xie & Andrews, 2013) widely adopted by researchers in applied linguistics where composite variables are created based on aggregating items belonging to the same theoretical construct. The second step is to enter construct measures into mathematical models, which are specified based on a theory of construct representation, to estimate the constructs contributions to the overall task performance. The mathematical models thus show the constructs underlying the performance, allowing researchers to examine its construct representation.

The present study adopted the afore-mentioned two-step method to befit it relatively modest sample size. Specifically, the study first examined the performance data based on a prior theoretical conceptualization of the component constructs underlying integrated reading-to-write performance (to answer RQ1). Then it conducted corrlation-based statistical modeling of the relationships among different measures (to answer RQ2), including equential multiple regression (i.e., mathematical modelig) and Structural quation Modeling (SEM). The latter models the external relations between a test and other tasks asessing similar constructs based on the network of correlations among the measures, i.e., the nomothetic span (e.g., Xie, 2013, 2015a; Xie & Andrews, 2013.

Textual and linguistic analyses (such as Kyle, 2020) were also conducted to examine the linguistic features of the writing samples and their relations with performance scores, but due to space limitation, this part of the research is not reported in this article.

# 4.1. Research context and participants

The study was conducted among undergraduate students in Hong Kong universties. Participants were recruited from the humanities (language studies, history) and social sciences (education, psychology, social science) because these two disciplines tend to use academic esays as their primary mode of writing assessment (Gardner & Nesi, 2013). Moreover although research has identified distinct conventions of source use acros isciplines, humanities and social sciences share similar conventions and expectations of academic essay writing (Hu & Lei, 2012).

Specifically, the participants were recruited through campus ads and student social media networks. To have a sufficient variance of abilities in the student pool, we recruited 204 participants from the Education University of Hong Kong, the Chinese University of Hong Kong, and the University of Hong Kong. This sample size substantially exceeded our target sample of 136, which had been estimated at the project's initial stage based on power analyses (Soper, 2020) of a conservative model calibration of anticipated medium effect size at 0.25 and statistical power at 0.8, with two latent variables, eight observed variables, and a probability level at 0.05.

However, because only 154 participants completed the DET, data analyses involving DET scores inevitably involved a restricted sample. Sttisticall, thecorrelation coficients between D scores and ther measures ased on the restricted sample is maller than those based on an unrestricted sample (Ginther & Yan, 2018; saacs, Hu, Trenkic, & Varga, 2023; Thorndike, 1949). Table 1 presents the descriptive statistics of the 154 participants' DET overall score ranges and their corresponding IELTS band scores so that the readers can have a good understanding of the participants English proficiency levels. The mapping of DET to IELTS scores isbased on the concordance table from the DET official website. As Table 1 shows, except for $2 2 . 1 \%$ (34 out of 154) with DET scores lower than 100, or IELTS academic 5.5, the majority $( 7 7 . 9 \% )$ of the participants' English proficiency is above 105 or IELTS academic $6 . 0 \%$ , and $4 2 . 9 \%$ participants have DET scores of 120 and above, corresponding to IELTS 6.5 and above.

This distribution is largely in accordance with our knowledge about Hong Kong undergraduate students' English ability (Bruce & Hamp-Lyons, 2015; Author, 2), although probably because the participants were recruited from humanities and socio-sciences dis ciplines voluntarily, the sample appeared to have a higher-level English proficiency than the population.

In terms of the variance of English proficiency, the sample has $5 1 . 3 \%$ participants in the middle (with IELTS 6.0 to 6.5) as well a substantial percentages of low (IELTS $= < 5 . 5$ $2 2 \%$ ) and high proficiency (IELTS $> = 7 . 0$ $2 6 . 6 \%$ ) participants.

# 5. Data collection procedures

Data was colected during the pandemic (from 2020 to 2022). All participants registered online, signed a consent form, and submitted one rect academic esay. The alsofilled in a short questionnair rgarding ther biodata e.., gender, age, major, year of study). Briefl, the participants were from humanities and social science disciplines, especially language education and language studies majors; they were aged between 19 and 24 years and disributed evenly across diffrent years of study (Year 1: $2 5 . 2 \%$ : Year 2: $2 6 . 2 \%$ : Year 3: $2 5 . 7 \%$ : Year 4: $2 3 . 3 \%$ ). Regarding gender, the participants were predominantly female $\mathrm { F } { = } 8 0 . 6 \%$ vs $\mathbf { M } { = } 1 9 . 3 \%$ mirroring the gender (in)balance in student enrolment of the two disciplines in the research context.

During each data collction session, the participants watched a 30-minute video in which the firt author introduced the two writing tasks and possible ways of responding to them. Afterward, they wrote the first writing task in $3 0 \mathrm { { m i n } }$ , took a short break, and completed the second task in $6 0 \mathrm { { m i n } }$ . They were then introduced to the DET, especially the "dos and don'ts' of taking the online test. They took a quiz about the DET and only when they achieved a $1 0 0 \%$ score on the quiz, they were given a coupon to take the DET on their own as soon as possible, within one week.

Table 1 The English Proficiency Distribution of the Participants.   

<html><body><table><tr><td>DET score-range</td><td>IELTS</td><td>N</td><td>%</td><td>Accumulative %</td><td>Mean</td><td>SD</td></tr><tr><td>150</td><td>8</td><td>1</td><td>0.6%</td><td></td><td>150</td><td>NA</td></tr><tr><td>140-145</td><td>7.5</td><td>15</td><td>9.7%</td><td>10.4%</td><td>142</td><td>2.54</td></tr><tr><td>130-135</td><td>7</td><td>25</td><td>16.2%</td><td>26.6%</td><td>133.13</td><td>2.5</td></tr><tr><td>120-125</td><td>6.5</td><td>25</td><td>16.2%</td><td>42.9%</td><td>121.6</td><td>2.38</td></tr><tr><td>105-115</td><td>6</td><td>54</td><td>35.1%</td><td>77.9%</td><td>109.17</td><td>4.09</td></tr><tr><td>&lt;100</td><td>= &lt;5.5</td><td>34</td><td>22.1%</td><td>100.0%</td><td>92.65</td><td>9.55</td></tr><tr><td></td><td>Sum</td><td>154</td><td>100.0%</td><td></td><td>114.71</td><td>16.87</td></tr></table></body></html>

# 5.1. Measures, procedures, and variables

In addition to the participant' DET scores, the study also collected data through the following instruments, measures, and procedures.

The first instrument was the integrated reading-to-write summary task set (referred to as Task 1 hereafter), including writing instructions (see Appendix 1), two source texts, and a set of analytical rubrics (see Appendix 2a, ) adapted from previous research (Yang, 2014). Participants responded to Task 1 in $3 0 \mathrm { m i n }$ , five minutes more than in Cumming et al. (2005). The rubrics assess written responses against eight criteria grouped into thre component constructs: Component 1 entitled Source use (T1-sU) includes accurate (C1) and complete (C2) presentation of principal information of the source texts and (C3) clear and adequate presentation of the connections between the two source texts; Component 2 entitled Organization (T1-CC) has two criteria, namely coherence (C4) and cohesion (C5); Component 3 labeled Language use (T1-LU) has three criteria of vocabulary (C6), sentence skills (C7) and copying source language (C8). Allcriteria were rated on a scale from 0 to 5; a 0 point was given to rrelevant responses and responses with substantial copying from sources. The criteria were given an equal weight in computing component scores with MFRM (more in the data analysis section).

The second instrument was an integrated reading-to-write argumentative essay task set (referred to as Task 2 hereafter), including writing instructions, three more texts on the same topic as in the first task, and a set of analytical rubrics adapted from previous research (Yang, 2014; Plakans, 2009). Participants took $6 0 \mathrm { { m i n } }$ to complete this task. Appendix 3b provide the participants' writing samples of Tasks 1 and 2 at three levels of performance.

The two writing tasks were interconnected. After completing Task 1, participants read another three texts, decided on a stance, and wrote an argumentative essay based on all five texts in $6 0 \mathrm { { m i n } }$ (Task 2). Again, their responses were marked analytically in terms of Source Use (T2-sU, i.e., citation, coverage, and accuracy of main source ideas), Coherence and Cohesion (T2-c), the Strength of Argumenttion (T2-Arg), and Language Use i.., lexical and syntactic accuracy and sophistication) (T2-LU). Similar to Task 1,all criteria were given equal weight in computing component scores with MRFM.

For both writing tasks, two sets of reading texts were used in alternation by diffrent participants; each participant worked on one topic only. The first set was on Covid vaccination, the second was on global warming. The source materials used for input were taken from English newspapers and academic articles all matched for a comparable level of difficulty. More precisely, the sequence of word counts for the five texts on the topicof ovid were 245, 254, 242, 251, and 249 words. For the Global Warming topic, the first two input texts were taken from Yang (2014), the other three were of a similar genre and identified from English newspapers. The five texts contained 214, 245, 244, 248, and 251 words each. These texts recorded an average Flesch Reading Ease (FRE) score of 38.3, denoting that their complexity was on par with TOEFL iBT reading materials, which had an average FRE score of 39.96 and those used by Plakans (2009), which had an average FRE score of 40.5 (see also Xie, 2023).

Additionally, each participant submitted a recently completed academic asignment (referred to ask Task 3 hereafter, see Appendix 3b for writing samples of Tasks 1-3) that represented their best efort. These assgnments were analyzed against the categories and descriptions of Nesi & Gardner of academic writing. Owing to our participants' sample characteristics, consisting of the students from humanities and social sciences disciplines, most of the submission (113 out of 201) are academic esays. One example of these academic essas is submissions that responded to the fllowing assgnment intruction: \*Write an essay on a topic relevant to a curriculum and/or asssment issue that you are ikely to deal with as a new teacher in Hong Kong. escribe and discs the issue. analyze the problem(s) and propose a solution or solutions based on the content introduced in the course and/or your readings" In response, one student wrote an essy ted Effective teaching and positive classroom learning environment; another wrote under the title of \*The usage of technology in education in Hong Kong".

A total of 26 submissions could be classfied into the sub-genre of critique within the AE family, where students were required to choose a movie, a corpus, a website, or a given article/text to analyze and critique based on what they learned from the course. For example, students were required to select a piece of artwork from the contemporary Hong Kong art scene and analyze how it was influenced by Western and/or Chinese arts, and examine the creation of the selected work in the converging local historical, social and aesthetic context.

Another 18 submissions, which could be classified into the sub-genre of criticl reflections, had narrative recount as part of their essay. An example of this type i from a psychology course, which requires students to identify a past experience from his education journey, analyze the experience with the concepts introduced in the course, and draw implications for education.

Another 17 submissions are characterized as case study, where a case (or several cases) was given to students to (select and) analyze based on the theories or concepts introduced. An example of this ssignment is from a busines course, where students were given a case to analyze the ethical and businessses involved in prearation for a business meetig. Another example requires students to select a case from a collection of classroom misbehavior, describe the case and conduct an analysis based on theories and concepts introduced in the course. One response to this assignment is titled How to handle 'class clowns' based on Case $\# 5 . \mathrm { : }$

The remaining submissions are distributed to other sub-genres, including reports (15), explanations (9), literature survey (3), design and evaluate (3) (e.g., a lesson plan). Putting these submissions together, they could reflect the wide range of potentia aca demic assgnments that humanities and social science students (especially education) had to writ for asssment in universties. Importantl, all writing samples involved the use of source materials, albeit at varied degrees, showing various source use skill of paraphrasing, summarizing, connecting and synthesizing.

The writing samples were marked by the same raters against aset of analytical rubrics adapted from an existing set of in-house rubrics used in the humanities faculty of the first author's universty to mark academic assgnments. The rubrics consisted of 14 criteria distributed under four components, i.e., content fulfilment (T3-Con), source use (T3-SU), discourse (structure, coherence, and cohesion) (T3-CC), and language use (T3-LU). All criteria were given equal weight in computing component scores with MRFM. Further details about the writing tasks and rubrics can be found in Xie (2023).

# 5.2. Data processing and MFRM

Data included the participants' DET-EWT scores provided by Duolingo and human raters' marking of each participant's Task 1-3 writing samples. Two professional raters (pseudo-named Jess and Irene) and a research assistant (pseudo-named Ann) double-marked the samples acording to the rubrics. Inter-rater and intra-rater reliability were monitored closely via the linking sample coded and unknown to the raters); rater tendency was analyzed and corrected via Multi-Facet Rasch Measurement (MFRM) through FACETS (Linacre, 2020), which generated corrected measurement (termed fair scores in the original scale and fair measures in logits) for subsequent analyses. Details of marking and inter-rater reliability statistics can be found in Appendix 4.

# 5.3. Data analysis

A series of statistical analyses was conducted based on the MFRM adjusted measures (labeled as fair measures according to the Rasch conventions) of component constructs, e.g., source use (SU), coherence and cohesion (CC), language use (LU), and argumentation (Arg). To address RQ1 (\*what are the constructs underlying the summary and the reading-to-writ essay and academic essay writing in university?"), multiple regresson models were built to reveal the contribution of diffrent component construct to the overall performance on each task. MFRM-adjusted component scores were entered into the model to predict the overal task performance; their relative standardized weights indicated each component's importance to the overall peformance thereof, verifying the representation of the component constructs underlying the overall performance.

To address RQ2 to what extent, if a all could the two integrated writing tasks Tass 1 and 2] enhance the Duolingo Writing Tet (EWT)'s predictive validity on undergraduate students' academic essay-writing performance [Task 3]"), we first conducted sequential multiple regressions with the DET-EWT scores entering the model first a the independent variable (IV), followed by the MFRM measures of Task 1 and 2 overall performance (T1-O and T2-O), to predict the overall performance of academic essays (T3-0) The statistics of $\mathrm { R } ^ { 2 }$ change indicate additional variance explained by adding T1-O and T2-O, which entered the model in an alternating order of priority.

Lastly, we built a SEM model wherein the three writing tasks were given equal priorit (as shown in Fig. 2) to estimate their predictive power of AE (Task 3). In this model specification (Fig. 2), three main paths, $\beta _ { 1 : }$ 2 $\beta _ { 3 }$ were estimated; these indicated how the three tasks predicted the students performance on academic esays. Following the conventions of SEM studies four fi indices were used to assess model fitnes, including the Goodnes-of-Fit Index (GFI), Comparative Fit Index (CFI), Tucker-Lewis Index (TLI), and Root Mean Square Error of Approximation (RMSEA). RMSEA values of.05 or lower are considered a close it, and values of.08 or lower are considered good, whereas GFI, CFI, and TLI values above 90 suggest a good model fit (Bentler, 1990).

# 6. Findings

RQ1. What are the constructs underlying the summary and the reading-to-write argumentative essay tasks and academic essy writing in university?

# 6.1. Descriptive statistics

Descriptive statistics fall MFRM-adjusted measures of the component constructs underlying Tasks 1 to 3 are presented in Table 1. Sequential multiple regression models were built to decompose the three tasks' underlying constructs. As shown in Table 1, after cleaning extreme values, l component constructs of Tasks 1-3 had satisfactory distriution features resembling normal distribution, with Skewness and Kurtosis values within the range of - 1.0 and $+ 1 . 0$ ; therefore, they could be reliably used for correlation-based analysis.

Table 2 also reveals the relative difficultie of component constructs via the components' means. In Task 1, T1-SU appears to have been the most difficult component, with the lowest mean of the three components (Mear $\mathsf { 1 } = 0 . 1 5 3$ ), while LU appears to be the easiest, with the highest mean (Mean $\phantom { - } 1 { = } 1 . 8 6 5$ ). Similarly, T2-LU was also the easiest component of Task 2 (Mean $\it { \Omega } = 1 . 1 1 4 _ { . }$ ) and Task 3 (Mean $\scriptstyle 1 = 0 . 5 3 4$ ). In Task 2, LU was followed by T2-SU (Mean $= 0 . 7 6 5$ ) and T2-Arg (Mean $\it - 0 . 1 8 4 _ { . }$ ); T2-CC was the most difficult (Mean $\underline { { \underline { { \mathbf { \Pi } } } } }$ 1.299) and it also had the widest range $\scriptstyle \mathbf { S D = 3 . 8 3 4 }$ . In Task 3, LU was followed by T3-SU (Mean $_ { 1 = 0 . 3 7 7 }$ ), whereas T3-CC (Mean $\underline { { \underline { { \mathbf { \Pi } } } } }$ $- 0 . 5 4 5 )$ and T3-Con $\mathrm { M e a n } { = } - 0 . 5 8 5 )$ appear to be the most difficult and most varied ( $\mathrm { S D } { = } 2 . 4 5 2 $ , 2.943).

![](img/55f58227f7f2c14ab0fda28aebd34038e37395235f2bca9746cee9602f0e0841.jpg)  
Fig. 2. Initial Specification of the SEM Model.

In terms of the sequence of component difficulty, Task 2 (T2-LU, SU, Arg, and CC, in ascending order of dificulty) had a closer resemblance to Task 3 (T3-U, SU, CC, and Con, in asending order of dificulty). The order of difficulty was slighly ifference in Task 1 (T1-LU, C, and U in aending order of diffculty), where SU was more ifficult than CC. Acrossal thre tasks U was the easest component.

# 6.2. Mathematical modeling of construct representation

Figs. 3-5 present the construct representation modeling results for Regression Models 1-3. In Models 1 and 2, the source use component (T1-SU and T2-SU) contributed the most ( $\mathrm { \beta } \mathrm { = } 0 . 5 9$ and 0.55, respectively, $p < . 0 0 1 )$ to the overall task performance (T1-O and T2-O), whereas the language use component (T1-LU and T2-LU) contributed the least $\mathrm { \Delta } \mathrm { \cdot } \mathrm { \beta } = 0 . 2 8$ and 0.15, respectively, $p < . 0 0 1 )$ . In comparison, the coherence and cohesion component (T1-CC and T2-CC) contributed more to T1-O $\mathrm { \ B } = 0 . 3 1$ $p < . 0 0 1 \ r$ ) than T2-O $( \beta = 0 . 1 7$ $p < . 0 0 1 )$ . Lastly, the argumentation component (T2-Arg) contributed substantially to T2-O too $\mathrm { \ B } = 0 . 4 6$ $p < . 0 0 1 \mathrm { \AA }$ which makes good sense.

A slightly different construct representation was revealed for Task 3 (AE). As shown in Fig. 4,coherence and cohesion (T3-C) seems to have been the most important component construct, with the largest standardized beta coefficient $\mathrm { \Delta } \mathrm { \cdot } \mathrm { \{ } =  0 . 4 3$ $p < . 0 0 1 \mathrm { \AA }$ of the four components. This was followed by two components of similar importance, namely Source Use (T3-SU: $\beta = 0 . 3 7$ $p < . 0 0 1 \mathrm { \AA }$ and Content (T3-Con: $\beta = 0 . 3 2$ A $p < . 0 0 1 \ r _ { \cdot }$ . As in Tasks 1 and 2, the language use component (T3-LU) contributed least to the overallperformance of Task 3 ( $\mathrm { \beta } \mathrm { = } 0 . 0 8$ $p < . 0 0 1 $

To summarize this round of analysis, we found adierential representation of the component constructs in the three writing tasks. In the summary writing task (Task 1), the SU component was clearly the most important in explaining the overall writing performance. In the reading-to-writ esay task (Task 2), the SU and Arg components were decisive and far more important than CC and LU in explaining the overall performance. In the academic esay task (Task 3), CC, SU, and Con contriuted to the overall performance at a similar level, but LU contributed very little.

This finding recalls our discussion, in the first section of the iterature review, about the difference between theoretical decomposition and nomothetical span based on correlations; that LU being the easiest component of allthre tasks could be why it did not contribute much to the variance of writing performance.

RQ2. To what extent if at ll,could the two integrated writing tasks enhance the Duolingo Writing Test (DEr-EwT's construct rere sentation and predictive validity on undergraduate students' academic essay-writing performance?

# 6.3. Regression analysis

Two sequential regressions were conducted with DET-EWT, with the fair measures of the two tass (T1-O, T2-O) as the independent variables (IV) to predict the overal performance on academic esays (T3-O) as the dependent variable (DV). In sequential regression modeling, the order of entry determines the priority, as the first IV to enter the model removes all variances it could explain from the DV while the second IV to enter the model accounts for the DV's remaining variance, and so on and so forth.

Table 3 presents the zero-order Pearson correlation coeficients among the four measures. As noted earlier, the correlation :oefficients between DET-EWT and T1-O to T3-O would be larger if they are estimated based on an unrestricted sample size.

Table 2 escriptive Statistics of the Component Scores of Test-Taker Performance Components of Tasks 1-3 (MFRM-Adjusted Measures in Logits).   

<html><body><table><tr><td rowspan="2">Tasks and Components</td><td rowspan="2">N</td><td rowspan="2">Min</td><td rowspan="2">Max</td><td rowspan="2">Mean</td><td rowspan="2">SD</td><td colspan="2">Skewness</td><td colspan="2">Kurtosis</td></tr><tr><td>Stat.</td><td>SE</td><td>Stat.</td><td>SE</td></tr><tr><td>T1-Overall (O)</td><td>195</td><td>-4.54</td><td>3.49</td><td>0.031</td><td>1.369</td><td>-0.375</td><td>0.174</td><td>0.479</td><td>0.346</td></tr><tr><td>T1-Source Use (SU)</td><td>181</td><td>-5.80</td><td>4.82</td><td>0.153</td><td>1.969</td><td>-0.329</td><td>0.181</td><td>0.307</td><td>0.359</td></tr><tr><td>T1-Coh-Coh (CC)</td><td>178</td><td>-6.40</td><td>7.86</td><td>1.077</td><td>2.797</td><td>0.149</td><td>0.182</td><td>-0.150</td><td>0.362</td></tr><tr><td>T1-Lang Use (LU)</td><td>190</td><td>-4.78</td><td>8.54</td><td>1.865</td><td>2.299</td><td>-0.589</td><td>0.176</td><td>0.753</td><td>0.351</td></tr><tr><td>T2-Overall (O)</td><td>195</td><td>-2.51</td><td>2.52</td><td>0.296</td><td>0.874</td><td>-0.115</td><td>0.174</td><td>0.258</td><td>0.346</td></tr><tr><td>T2-Source Use (SU)</td><td>181</td><td>-1.72</td><td>3.30</td><td>0.765</td><td>0.983</td><td>-0.056</td><td>0.181</td><td>0</td><td>0.359</td></tr><tr><td>T2-Arg (Arg)</td><td>182</td><td>-5.37</td><td>4.91</td><td>0.184</td><td>1.893</td><td>-0.274</td><td>0.180</td><td>0.450</td><td>0.358</td></tr><tr><td>T2-Coh-Coh (CC)</td><td>191</td><td>-9.96</td><td>9.41</td><td>-1.299</td><td>3.834</td><td>0.869</td><td>0.176</td><td>0.527</td><td>0.35</td></tr><tr><td>T2-Lang Use (LU)</td><td>181</td><td>-3.45</td><td>5.76</td><td>1.114</td><td>1.697</td><td>-0.025</td><td>0.181</td><td>-0.224</td><td>0.359</td></tr><tr><td>T3-Overall (O)</td><td>188</td><td>-3.26</td><td>4.20</td><td>-0.050</td><td>1.438</td><td>0.633</td><td>0.177</td><td>0.527</td><td>0.353</td></tr><tr><td>T3-Content (Con)</td><td>175</td><td>-7.49</td><td>8.02</td><td>-0.585</td><td>2.943</td><td>0.177</td><td>0.184</td><td>-0.301</td><td>0.365</td></tr><tr><td>T3-Source Use (SU)</td><td>193</td><td>-3.84</td><td>5.53</td><td>0.377</td><td>1.978</td><td>0.337</td><td>0.175</td><td>0.201</td><td>0.348</td></tr><tr><td>T3-Coh-Coh (CC)</td><td>184</td><td>-6.70</td><td>6.10</td><td>-0.545</td><td>2.452</td><td>0.249</td><td>0.179</td><td>-0.015</td><td>0.356</td></tr><tr><td>T3-Lang Use (LU)</td><td>164</td><td>-3.80</td><td>4.29</td><td>0.524</td><td>1.622</td><td>-0.095</td><td>0.19</td><td>-0.267</td><td>0.377</td></tr></table></body></html>

Note: Arg: Argument; Con: Task content; CC: Coherence & Cohesion component; Lang Use: Language Use

![](img/af86ccc1f3a052f386e524cec2df8b29ec8313137dc342600f3afbb269ff060b.jpg)  
Fig. 3. Mathematical Model 1 for Task 1 Construct Representation, ote. All path coefficients are statistically significant, $p < . 0 0 1$

![](img/59c34143a8868d81199e2c5fd3666befa605bda9e609755d5c6905d063026ece.jpg)  
Fig. 4. Mathematical Model 2 for Task 2 Construct Representation, Note. All path coeficients are statistically significant, $p < . 0 0 1$

![](img/9119eae5bf817df5c5b41e8dea9a3d36c1f72e734e9bd05af2edbc2e3d19ebff.jpg)  
Fig. 5. Mathematical Model 3 for Task 3 Construct Representation, Note. All path coeficients are statistically significant, $p < . 0 0 1$

In the first sequentil regression analysis (Table 2), DET-EWT was entered into the model first (Model 1 in Table4); therefore, it was given the highest priority in the model, fllowed by T1-0 (Model 2) and T2-0 (Model 3). In the second regresson model (Table6), T2- O was entered after DET-EWT but before T1-O; therefore, T2-O was iven higher priority than T1-O. In both modeling proceses, DETEWT was given the highest priority.

In both Tables 4 and 6, T1-O and T2-O were found to give DET-EWT significant additional power in predicting students academic essay performance ( $\mathrm { \mathrm { R } } ^ { 2 }$ change ${ \scriptstyle : = 0 . 0 3 5 }$ , 0.019, $p = . 0 0 5 , . 0 3 7 )$ ; the corresponding standardized beta coefficients (Table 5) were also statistically significant $\mathrm { \Delta } \mathrm { \cdot } \mathrm { \{ } =  0 . 2 3 7$ , 0.162, 0.144, $\begin{array} { r } { p = . 0 0 1 , . 0 1 9 } \end{array}$ , and.037). The added predicting power ( $\mathbf { \mathrm { R } } ^ { 2 }$ changes $= 0 . 1 2 7 – 0 . 0 8 1 =$ 0.046) was modest in size but proportionally rather substantial because it stood for alittl over half of DET-EWT's original predictive power (i.e., the total $\mathrm { R } ^ { 2 }$ changes vs. DET-EWT adjusted R-square: 0.46 vs.081).

Table 3 Correlation matrix among the four measures.   

<html><body><table><tr><td></td><td>DET-EWT</td><td>T1-O</td><td>T2-O</td></tr><tr><td>T1-O</td><td>0.212 * *</td><td></td><td></td></tr><tr><td>N</td><td>150</td><td>195</td><td></td></tr><tr><td>T2-O</td><td>0.225 * *</td><td>0.228 * *</td><td></td></tr><tr><td>N</td><td>151</td><td>191</td><td>195</td></tr><tr><td>T3-O</td><td>0.357 * *</td><td>0.243 * *</td><td>0.248 * *</td></tr><tr><td>N</td><td>144</td><td>181</td><td>181</td></tr></table></body></html>

$^ { \ast } \mathrm { ~ } ^ { \ast } p < . 0 0 1$

Table 4 Model Summary of Sequential Regression 1.   

<html><body><table><tr><td>Model</td><td>R</td><td>R Square</td><td>Adjusted R Square</td><td>S.E</td><td colspan="5">Change Statistics</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td> R Square Change</td><td>F Change</td><td>df1</td><td>df2</td><td>Sig. F Change</td></tr><tr><td>1</td><td>.293a</td><td>0.086</td><td>0.081</td><td>1.330</td><td>0.086</td><td>18.767</td><td>1</td><td>200</td><td>.000</td></tr><tr><td>2</td><td>.347b</td><td>0.121</td><td>0.112</td><td>1.307</td><td>0.035</td><td>7.905</td><td>1</td><td>199</td><td>.005</td></tr><tr><td>3</td><td>.374c</td><td>0.140</td><td>0.127</td><td>1.296</td><td>0.019</td><td>4.387</td><td>1</td><td>198</td><td>.037</td></tr></table></body></html>

aPredictors: (Constant), DET-EWT bPredictors: (Constant), DET-EWT, T1-O cPredictors: (Constant), DET-EWT, T1-O, T2-O

Table 5 Regression Coefficientsa.   

<html><body><table><tr><td rowspan="2" colspan="2">Model</td><td colspan="2">Unstandardized Coefficients</td><td rowspan="2"></td><td rowspan="2">t</td><td rowspan="2"></td><td colspan="2">95.0% CI for B</td><td colspan="2">Collinearity Statistics</td></tr><tr><td></td><td></td><td></td><td>Lower</td><td>Upper Tolerance</td><td>VIF</td></tr><tr><td rowspan="2">1</td><td>Constant</td><td>-1.899</td><td>.437</td><td></td><td>-4.345</td><td>.000</td><td>-2.760</td><td>-1.037</td><td></td><td></td></tr><tr><td>DET-EWT</td><td>.319</td><td>.074</td><td>.293</td><td>4.332</td><td>.000</td><td>.174</td><td>.464</td><td>1.000</td><td>1.000</td></tr><tr><td rowspan="3">2</td><td>Constant</td><td>-1.693</td><td>.436</td><td></td><td>-3.885</td><td>.000</td><td>-2.552</td><td>-.834</td><td></td><td></td></tr><tr><td>DET-EWT</td><td>.282</td><td>.073</td><td>.259</td><td>3.840</td><td>.000</td><td>.137</td><td>.427</td><td>.969</td><td>1.032</td></tr><tr><td>T1-O</td><td>.196</td><td>.070</td><td>.190</td><td>2.812</td><td>.005</td><td>.058</td><td>.333</td><td>.969</td><td>1.032</td></tr><tr><td rowspan="4">3</td><td>Constant</td><td>-1.617</td><td>.434</td><td></td><td>-3.728</td><td>.000</td><td>-2.472</td><td>-.761</td><td></td><td></td></tr><tr><td>DET-EWT</td><td>.257</td><td>.074</td><td>.237</td><td>3.486</td><td>.001</td><td>.112</td><td>.403</td><td>.944</td><td>1.060</td></tr><tr><td>T1-O</td><td>.167</td><td>.070</td><td>.162</td><td>2.368</td><td>.019</td><td>.028</td><td>.306</td><td>.931</td><td>1.074</td></tr><tr><td>T2-O</td><td>.232</td><td>.111</td><td>.144</td><td>2.095</td><td>.037</td><td>.014</td><td>.450</td><td>.925</td><td>1.081</td></tr></table></body></html>

aDependent Variable: T3-O

Tables 6 and 7 present the results of the second sequential rgression analysis, where T2-O entered the model ahead of T1-0. The patterns f results are rather similar to those of the firt analyses, which suggests that 1-O and 2-O were rather unique tass and did not share much covariance. Their order of entry into the model, then, did not matter too much.

In both sets of regression models, T2-O was found to have slightly less predictive power than T1-O $\mathrm { \Delta } \mathrm { \cdot } \mathrm { \{ } =  0 . 1 4 4$ vs. 0.162). This finding is sightl surprising because we considered Task 2 (reading-to-writ argumentative essay) to share more of the underlying construct with Task 3 (academic essay writing) than did Task 1 (summary writing). Specificall, T2-0, as shown in Fig.3, had four underlying component construct, namely, SU, Arg, CC, and U, whereas T1-O, ashown in Fig. 2, only had three i.e., SU, CC, and LU. Task 2 has an additional Arg component, which it shares with Task 3; however, this additional component did not seem to bring more power to T2-O to predict T3-O (the domain performance).

# 6.4. SEM modeling

In the sEM model, all three writing tasks were treated equall, with no assigned priority. The initial model achieved excellent fitness (Fig. 5) with RMSEA:.035; NFI:.951; CFI:.989, and TLI:.973. The path cofficients i the model were consistent with those from the sequential multiple regressin analyses rported earlier. With the correlations among the DET-EWT and the two integrated writing tasks (T1-O and T2-O) controlled, DET-EWT made the biggest contribution to predicting Task 3 latent scores $( \beta _ { 1 } = 0 . 2 7 , p = . 0 0 2 )$ . The two integrated writing tasks (T1-O and T2-O) made additional contrbutions to predicting students performance in academic essay writing ( $\mathrm { \beta } \mathrm { _ { 2 } } = 0 . 1 6$ $p = . 0 3 6$ $\beta _ { 3 } = 0 . 1 9$ $p = . 0 1 6 )$ . All three path coefficients were statistically significant; they jointly explained $1 9 \%$ of the latent academic essay performance (T3-O).

One interesting observation, shown in Fig. 5, is that T2-O made a slightly greater contribution $( \beta _ { 3 } = 0 . 1 9 , p = . 0 1 6 )$ than T1-O to the latent T3-O $( \beta _ { 2 } = 0 . 1 6$ $\begin{array} { r } { p = . 0 3 6 ) } \end{array}$ I, thus reversing the order we observed in the previous section based on sequential regression analysis, where T1-O's contribution was slightly greater than T2-O's $\beta = 0 . 1 6 2$ vs 0.144). This could be because the latent T3-O in

Table 6 Model Summary of Sequential Regression 2.   

<html><body><table><tr><td>Model</td><td>R</td><td>R Square</td><td>Adjusted R Square</td><td>S.E</td><td colspan="5">Change Statistics</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td> R Square Change</td><td>F Change</td><td>df1</td><td>df2</td><td>Sig. F Change</td></tr><tr><td>1</td><td>.293a</td><td>.086</td><td>.081</td><td>1.330</td><td>.086</td><td>18.767</td><td>1</td><td>200</td><td>.000</td></tr><tr><td>2</td><td>.340b</td><td>.115</td><td>.107</td><td>1.311</td><td>.030</td><td>6.663</td><td>1</td><td>199</td><td>.011</td></tr><tr><td>3</td><td>.374c</td><td>.140</td><td>.127</td><td>1.296</td><td>.024</td><td>5.609</td><td>1</td><td>198</td><td>.019</td></tr></table></body></html>

a. Predictors: (Constant), DET-EWT b. Predictors: (Constant), DET-EWT, T2-O c. Predictors: (Constant), DET-EWT, T2-O, T1-O

Table 7 Regression Coefficients of Sequential Regression 2.   

<html><body><table><tr><td colspan="2"></td><td colspan="2">Unstandardized Coefficients</td><td></td><td>t</td><td>Sig.</td><td colspan="2">95.0% CI for B</td><td colspan="2">Collinearity Statistics</td></tr><tr><td></td><td></td><td></td><td>S.E</td><td></td><td></td><td></td><td>Lower</td><td>Upper</td><td>Tolerance</td><td>VIF</td></tr><tr><td>1</td><td>Constant</td><td>-1.899</td><td>.437</td><td></td><td>-4.345</td><td>.000</td><td>-2.760</td><td>-1.037</td><td></td><td></td></tr><tr><td></td><td>DET-EWT</td><td>.319</td><td>.074</td><td>.293</td><td>4.332</td><td>.000</td><td>.174</td><td>.464</td><td>1.000</td><td>1.000</td></tr><tr><td>2</td><td>Constant</td><td>-1.768</td><td>.434</td><td></td><td>-4.075</td><td>.000</td><td>-2.624</td><td>-.913</td><td></td><td></td></tr><tr><td></td><td>DET-EWT</td><td>.282</td><td>.074</td><td>.259</td><td>3.810</td><td>.000</td><td>.136</td><td>.427</td><td>.962</td><td>1.039</td></tr><tr><td>3</td><td>T2-O</td><td>.283</td><td>.110</td><td>.175</td><td>2.581</td><td>.011</td><td>.067</td><td>.500</td><td>.962</td><td>1.039</td></tr><tr><td></td><td>Constant</td><td>-1.617</td><td>.434</td><td></td><td>-3.728</td><td>.000</td><td>-2.472</td><td>-.761</td><td></td><td></td></tr><tr><td></td><td>DET-EWT</td><td>.257</td><td>.074</td><td>.237</td><td>3.486</td><td>.001</td><td>.112</td><td>.403</td><td>.944</td><td>1.060</td></tr><tr><td></td><td>T2-O</td><td>.232</td><td>.111</td><td>.144</td><td>2.095</td><td>.037</td><td>.014</td><td>.450</td><td>.925</td><td>1.081</td></tr><tr><td></td><td>T1-O</td><td>.167</td><td>.070</td><td>.162</td><td>2.368</td><td>.019</td><td>.028</td><td>.306</td><td>.931</td><td>1.074</td></tr></table></body></html>

aDependent Variable: T3-O

Fig. 5, derived from the four component constructs (T3-Con, SU, CC, and LU), differed from the T3-O scores generated directly by MFRM measurement.

In both the regression analyses (Tables 3 and 5) and the Structural Equation Modeling (Fig. 5), DET-EWT was found to be the strongest predictor of the domain academic writing performance (T3-O) although the language use component (T3-LU) remained T3- O's weakest contributing component. (Fig. 6).

# 7. Discussion

The present study explored the theoretical relationships between two important concepts in educational testing, namely, construct representation and predictiv validity, based on the nomothetic span of correlations among Tasks 1, 2, 3 and DET-EWT. Following an analysis of DET-EWT, which identified potential construct underreresentation, two integrated reading-into-write tasks (T1 & 2) were constructed to enhance the test's representation of the target domain performance. The lattr was indicated by the academic as signments written by the participants for asessment in universities (T3). The ensuing statistical analyses first verified the construct representation of the three writing tass, namely, the summary writing (T1), the integrated reading-to-write esay task (T2), and the academic writing in university (T3), by decomposing the overalltask performance into component constructs and comparing their relative contributions. This set of analyses responded to Research Question 1 concerning the threetasks' underlying construct representation. Next, the analyses verified Research Question 2, namely, whether or not, and, if s, to what extent the two added tasks could enhance DET-EWT's statistical power to predict test-taker performance of academic writing in universities (T3).

# 7.1.  Construct representation

Regarding RQ1, namely, what constructs underly the summary, the reading-to-write essay, and the academic writing tasks, the study examined all component onstructs and their relative contributions to the overal peformance. The findings verified Source use (SU) and Coherence & Cohesion (CC) and Language use (LU) to be important component constructs underlying al three writing tasks. Because Task 3 represents academic writing in universities, ths finding provides support that Tass 1 and 2 could enhance DET-EWT's representation of the targt domain performance. In additionto Language Use, which s ssessed by DET-W, the two task alo asses (and represent) other important component constructs necessary for academic writing, namely, Source Use, Coherence & Cohesion, and Argumentation. Incorporating them into DET-EWT, therefore, could enhance its construct representation.

![](img/1323b54c1fc7bc1880845f95facdc8e92852bf5b2dfdc422d9a9b9725fac7d44.jpg)  
Fig.6. Structral Model, Note. Model fitnes: Chi-quare: 13717; df: 11; NFI:.951 CFI:.989; TLI.973; RMSEA:035. Al path cofficients are sta tistically significant: $p = . 0 0 2 , . 0 3 6 , . 0 1 6$ from top to bottom.

Consistent with prior research (e.g., Plakans & Gebril, 2013; Zhu, Li, Cheong, Yu, & Liao, 2020), Source Use, which was oper. ationalized in Task 1 as (C-1) accurate and (C-2) complete representation of principal information of the source texts and (C-3) adequate presentation of the connections between the source texts, and in Task 2 as accurate citation, source coverage, and accuracy of main source ideas, was found to be the most important component construct to the overall integrated writing performance. It explained $3 4 . 8 \%$ of the total variance of Task 1 $( \beta = 0 . 5 9 )$ and $3 0 . 3 \%$ of Task 2 $( \beta = 0 . 5 5 )$ overall scores. The numbers are smaller than those reported in Plakans & Gebril : $5 5 \%$ ) (2013) and Zhu et al. (2021: $4 7 . 9 \%$ . One important reason for the differences is that the two prior studies did not include other criteria (i.e. LU and C&) in their regresion model as the present study did; therfore, the larger variance they found may partly stem from the shared variance between SU and the other criteria (CC and LU). Also an important reason to account for the discrepancies were the differences in the scoring method and the operationalization of the source-use construct.

Plakans & Gebril (2013)difere from the present study in both. First, they manually coded the writing samples and modeled the count data, while the present research adopted rubrics for scoring. Second, they operationalized source use diferently in terms of source idea origin from either the reading or the listening input), integration styles (explict or implicit) and importance (principal or supporting deas). The importance criterion was similar to C-2 in Task 1 and source coverage in Task 2 of the present research. In their study, the importance scores explained $2 2 \%$ of the overall integrated writing scores (p.226), which is smaller than the variance explained by SU in the present study, probably because our source use construct also covered the accuracy of main source ideas and the connections of sources. Not having the accuracy of source ideas coded was considered a limitation by Plakans & Gebril (2013).

Similar to the present research, Zhu et al. (2020) adopted analytc rubrics o score the features of source use, ut they focused onthe use of quotation, summarization and connections among source texts. The later two features were similar to C-2 and C-3in Task 1 and source coverage in Task 2. In their study, the two features jointly explained $2 1 . 1 6 \%$ of overall integrated writing scores (p.63), a figure closer to ours $( 3 4 . 8 \%$ for Task $1 \%$ and $3 0 . 3 \%$ for Task 2) but again smaller probably because they did not consider the accuracy of source idea.

An intriguing finding in this round of analysis was associated with the Language Use component (including the criteria of lexical and syntactic accuracy and diversity). When it contributions were compared with the other components, LU was found to make the least contribution to the overall performance across all three tasks. Specificall, its contributions were from very weak $\mathrm { \beta } \mathrm { = } 0 . 0 8$ to T3- O) to moderate $\mathrm { \Delta } \mathrm { \cdot } \mathrm { \Delta } \mathrm { \cdot } \mathrm { \Delta } \mathrm { \Delta } \mathrm { \cdot } \mathrm { \Delta } \mathrm { \Delta } \mathrm { \cdot } \mathrm { \Delta }$ and 0.28 to T2-O and T1-O respectively). This finding is intriguing for two reasons.

First, this finding indicates that the two writing tass are highly complementary to DET-EWT, which has LU as its core construct. Adding these two tasks to DET-EWT could strengthen its representation of the domain performance (by incorporating the constructs of Source Use, Coherence and Cohesion, and Argumentation). In the meanwhile, they willnot overlap too much with DET-EWT's existing core.

In addition to its weak to moderate contributions, LU was also found to be the easiest component for all threetasks. This finding could be related to the generally high English proficiency of the participants; i could als lead readers to wonder whether the weak to moderate contributions of LU is also related to ths sample characteristic. It is necessary to clarify, however, the contributions of components were computed based on variance (e.g. SD) but not means. In both Tasks 1 and 2, LU was the easiest component but, interestingly, its variance was not the smallest: in Task 1, its $\mathrm { S D } = 2 . 2 9 9$ vs. 2.797, 1.969 of the other two components; in Task 2, its SD $= 1 . 6 9 7$ vs. 3.834, 1.893, 0.983 of the other three. Only in Task 3, the three statistics of mean, variance, and standardized beta coefficient align (Mean $\scriptstyle 1 = 0 . 5 2 4$ $\mathrm { S D } { = } 1 . 6 2 3$ $\beta = 0 . 0 8 \mathrm { , }$ , which suggests that LU was the easiest, the least varied and the least important component construct of Task 3 academic writing.

Given the above, the participants' generall higher language proficiency may not be the only reason to explain LU's small to moderate contributions. In fact, it may be shared feature of interated writing tass because this finding is consistent with some prior research. For instance, alloway et al. (2020) and Ucceli et a. (2019) also reported that overall integrated writing scores had weak to moderate correlations with features of language use for lexical complexity, r ranged from 0.023 to 0.170; for syntactical complexity, from 0.200 to 0.355). Similarly, Chan & Yamashita (2022) reported small mean zero-order correlations with lexical $( r = 0 . 1 4 )$ and syntactic complexity $( r = 0 . 2 5 )$ . Clearly, further research is warranted to understand the comparative value of LU in integrated writing, but the findings of the present research and those from the literature sem to suggest that LU plays a les important role than SU in the quality of integrated writing performance.

# 7.2. Predictive validity

Regarding RQ2, namely, to what extent, if a all the two integrated writing tasks could enhance DET-EWT's predictive validity on academic writing, we conducted two rounds of statistical analyses, namely, sequential regressions and SEM. In both analyses, the findings confirmed both tass could make statisticall significant aditional contributions to DET-EWT's predictive power. Although the contributions were modest, they accounted for over half of its original predictive power $( R ^ { 2 }$ increased from 0.081 to 0.127).

As Ginther & Yan (2018) pointed out, in stdies of predictive validity in language testing, interpretation of efect sizes was difficult and complex and should not be based on the magnitude alone. Rather, the efects should be undersood in reference to their expected magnitude. If a weak correlation is expected, then even a smallincrease could be considered strong and meaningful. In their study of the predictive validity of TOEFL ibt, they observed correlations ranged from 0.15 to 0.36 between the writig component of TOEFL ibt and GPA, equivalent to $2 . 2 5 \%$ to $1 2 . 9 6 \%$ of variance, which are similar to the findings of the present study. Similarly, Cho & Bridgeman (2012) reported a total TOEFL ibt score and GPA corrlation of 0.18 for undergraduates and 0.16 for graduates (equivalent to $3 . 2 4 \%$ and $2 . 5 6 \%$ of the total variance), which are again similar to the findings of the present study.

Similar findings are also reported in Weir et al. (2013), who investigated the predictive validity of GEPT-advanced writing and found the total scores of writing had significant but weak to moderate correlations . $( r = 0 . 1 3 6$ to 0.294) with real-life academic writing

performance (p.26), explaining only $8 . 6 4 \%$ of the total variance.

Putting the statistic of the present study within the above context, the contributions made by the two writing tasks and DET-EWT (i.e., $1 2 . 7 \%$ variance of academic writing performance) and their individual effect size ( $\mathrm { \beta } \mathrm { = } 0 . 2 7$ , 0.19, 0.16) in predicting academic writing are not so small but rather meaningful.

In relation to RQ2, taking all thre tasks together, the predictive power of DET-EwT to the domain performance has been strengthened, therefore, supporting the theoretical relation between beter construct representation and stronger prediction of the domain performance. However,this theoretical relation was not supported when we compared Tasks 1 and 2 with DET-EWT.

Both the regression analyses and the SEM model found DET-EWT to be the strongest predictor of the three $( \mathrm { D E T - E W T } > \mathrm { T } 1 . 0 > \mathrm { T } 2 -$ O), which is surprising for two reasons. First, as mentioned at the outset, DET-EWT primarily asseses LU but LU was the weakest contributing component acrosall three writing tasks, including the domain peformance (Task 3). Therefore, if the positive relationship between construct representation and predictive validity holds, DET-EWT should have the weakest predictive power for the domain. Ina similar vein, because Tass 1 and 2 shared more common constructs (SU, CC and LU) with the domain, they are, arguably, closer representations of the domain and should have a stronger predictive power for Task 3. However, they were not; instead, DETEWT was found to be the strongest predictor. This contradicts to the hypothetical positiverlation between construct representation and predictive power.

# 7.3. Construct representation vs. Predictive validity

The rather unexpected findings noted in the previous section draw attention to the logic gap between construct representation and predictive validit, a pair of importance concepts often discussed side by side (e.g., Bachman, 1991). Corroborating with the earlier theoretical discussions regarding construct representation via task analysis and statistical prediction based on a nomothetic span of correlations (Embretson, 1983; Messick, 1995), the finding provides empirical evidence for the complex relations between the two concepts.

According to the unified validity theory (Messick, 1989 and 1995), construct validity subsumes content-related validity (including evidence of content relevance and representation), substantive or theory-related validity (e.g., theoretical process models of task performance as in Embretson [, 1983]), as wellas criterion-related validity (including criterion relevance and predictive utility). A sufficient representation of the target domain (both in content and cognitie proceses) enables a test to predic or make inferences of the target domain performance (Bachman, 1991; Embretson, 1983; Messick, 1995) and provide the scores meaning. Bachman (1991, p.681), for instance, noted the fundamental role of fully representing constructs in order to predict or make inferences about future performance. He elaborated on two types of construct representation: the first was correspondence between the abilities measured and the abilities involved in non-test language use situations, which is similar to the substantive or theory-based abilit validit in Embretson (1983) and Messick (1995). The second was the corrspondence between tes tas characteristics and features of the target language use contexts, which is similar to the content-related validity of Messck (1995) concerning content relevance and representation.

Following this ine of thought, some researchers, including the present ones, expected a better construct representation to bring about strengthened score validity and greater predictive power of th criterion performance. This expectation, however, as Embreston (1983, 2007) pointed out, confounds theoretical construct representation based on task decomposition or process modeling with predictive power based on the nomothetical span of correlations. More comprehensive construct representation could, and indeed does, bring about more predictive power, as attested by the findings of the present research that the two integrated tasks added a substantial portion of power to the DET-EWT to explain academic writing performance. However, the added power was not proportionate in magnitude to the substantial increase in construct representation through adding two tasks. This evidence demonstrates that theoretical construct representation and statistical predictive power based on a nomothetical span of correlations are related but different. Statistical predictive power reflects more than the common constructs shared by the tasks. Although the two added tasks shared more underlying constructs with the domain, individuall, they did not render larger predictive power than DET-EwT. Two other reasons may also play a role.

First, it may be related to measurement errors. Tasks 1 and 2 are single writing tasks, but EWT has four tasks as the number of tasks is generally associated with higher measurement reliability, EWT scores may have smaller measure errors.

Second, perhaps EWT assesses more than language use. Among the six features it automatically asses, namely, grammatical accracy, complexity, lexical sophistication, divrst, task relevance, and length, four belong to language use. In hindsight, however, we reconsidered essy length as part of the EWT construct. Essy length is a deceptively simple yet strong indicator of L2 timed writing performance, which was consistently linked with writing proficiency (e.g., Galloway, Qin, Uccell, & Barr, 2020; Grant & Ginther, 2000; Xie, 2015b). Between 0.26 and 0.53 correlation coefficients were reported between essay lengths and integrated writing performance for researcher-made tasks and even higher corrlations for standardized writing tass (Chan & Yamashita, 2022). By including essay length into its constructs, EWT may have tapped into other constructs beyond language use.

Lastly, the three writing tests jointly only predict about $1 9 \%$ of the total variance of academic writing, which is a fairly small proportion in magnitude. This finding, however,aligns with prior research about the weak to moderate predictive power of major gatekeeping English proficiency tests (e.g., TOEFL, IELTS or GEPT) on academic suces (Bridgeman, Cho, & DiPietro, 2015; Cho & Bridgeman, 2012; Ginther & Yan, 2018; Ihlenfeldt & Rios, 2022). Itis also consistent with research on GEPT (Weir et al., 2013), which reported that GEPT-advanced writing test only explained $8 . 6 4 \%$ variance of real-life academic writing performance.

Similar to these studies, the complexity of the outcome variable, i.e., GPA in most prior research and academic writing in the present research and Weir t al. (2013), is probably an important reason for the modest predictive power. This finding, along with those in the literature, reveals the academic writing involves a wider range of influences, processes, skill, and abilities beyond the constructs asessed by the aforementioned writing tasks. Genre knowledge, disciplinary conventions, problem-solving skill, motivation, and self-regulation skill could ll have affcted academic writing outside of classrooms. These influences have not been considered in existing research of L2-integrated writing and could also be a worthwhile venue for further research.

# 8. Limitations and significance

This study provides important and fresh insights about the component constructs underlying integrated writing; it also provides empirical evidences to deepen our understandings about the complex relations between construct representation and predictive validity. As a pair of important concepts in assessment, they are often mentioned in tandem in discussions of score validit (e.g, Bachman, 1991; Embretson, 1983; Messck, 1995), however, their differences, conceptual or statisical, are seldom investigated or discussed in the literature.

The findings of the research, however, warrnts further investigation due to the limitations of the reearch. The first set of limiting factors is assciated with the sample size and characterisics. A two-step method was adopted to verify construct representation because of a moderate sample size. The substantial workload expected from the research participants, each contributed more than three hours to the research, means it was extremely challenging to gather more data beyond what we did. Although our sample of 204 participants was larger than most existing studies on L2 integrated writing and sufficient for the analyses, larger sample sizes would be more desirable, especially for estimating prediction validity.

Another sample-related limitation is the restricted sample range. Because only 154 participants completed the DET, regression analysis and SEM analysis were based on a truncated sample, which was likely to lead to under-estimation of the standardized path coefficients. The present study was unable to correct the effects of restricted samples according to the Thorndike Case I formula (Thorndike, 1949) because we did not have accessto the parameter associated with unrestricted samples. This could be considered another limitation of the research.

Moreover, the component constructs covered by the analytic rubrics are not particularly specific linguisticall. While the analytic scores provide a general understanding of the contributions of dferent component constructs crossdifferent interated writing tasks, they cannot profile the specific linguistc featres typical f responses at different leels and across tasks. Although linguistic rofiing is not the focus of the reearch reported in this artice, it has been an important area for the line of research on integrated writing. As such, linguistic analyses on this data set was planned but to be reported elsewhere. In general, future research of integrated writing could benefit from taking, and even integrating, avariet of perspectives, ., reearch of the early TOEF based on analytical rubrics and (automated) textual and linguistic analyses.

Despite these limitations, this study explored an interesting and important pair of concepts, i.e., construct representation and predictive validit, via experimentation with two thematically-linked integrated writing tass along with DET-EWT. To the best of the authors knowledge, empirical reearch with these foci is scant, and that within the field of language tsting is unknown Therefore, we believe this study can bring fesh insights to the field, and hopefull, stimulate a renewed interes in the validit concerns associated with construct representation. The later is particularly important in light of the increasing popularity of I-based online tests and their reliance on predictive validity.

# Funding

The research reported in this article received financial support in the form of a research grant from Duolingo English Test Project #R2266), a Hong Kong Government Matching Grant (Project # R2A01) and a Research Grant (GRF ref#18600121) from the Research Grant Council of Hong Kong Government.

# Data Availability

The authors do not have permission to share data.

# Appendix A. Supporting information

Supplementary data associated with this article can be found in the online version at doi:10.1016/j.asw.2024.100846.

# Acknowledgement

The author gratefully thanks Anthony Kunnan, who gave constructive feedback on the first draft of the article.

# References

octoral s.  . p/s..//9.1/141/2--14.a! sequence=1. Bachman, L. F. (1991). What does language testing have to offer? TEs0L Quarterly, 25(4), 671-704. Bachan .00 th a    n    o c  i L. Cheng (Eds.), Language testing reconsidered (pp. 41-71). University of Ottawa Press. Breel, J e,017Tinsh t, t,  lt ied r/ino.cm resources). Brid   r. 015). Pig   ish t   g h . n 3, 307-318. Brue,  amp-on  015.ig tesof  d tiond fo  w   w ag o   ih for Academic Purposes, 18, 64-77. https://doi.org/10.1016/j.jeap.2015.03.003 Chan, ., Inoue, .,  Taylor  (2015. eveloing rubrics to aes the reding-into-writing kills case study. sssing writng, 26, 20-37. Chan, . hi, . (2). Inra wting an its r  mtai. sin ing 54, ile 10662. hp/i./10.1016/. asw.2022.100662 Cho, Y., & Briman, . (2012). Rtionship f   ces to  pmae: ome ide fromAmrica rsite. ge tin, 29, 421-442. a y   0f Next Generation TOEFL. Assessing Writing, 10(1), 5-43. https:/doi.org/10.1016/j.asw.2005.02.001 , 23, 47-58. https://doi.org/10.1016/j.jeap.2016.06.002 Embretson, S. (1983). Construct validity: Construct representation versus nomothetic span. Psychological Bulltin, 93(1), 179. mbreson. 006h     iv. e e d.)t rin Rasch models: Extensions and applications (pp. 235-254). Springer. Fulcher, G., & Davidson, F. (2o07). Language testing and assessment: An advance resource book. Routledge,. the roleof ore   kill in  o or m w. g  in 33, 13. h/.g/07/145 019-09942-x r,  1   i45. /m02 Gei 9 Foreign Language Assessment, 7(1), 47-84. er,   018) theo t   , c,  .   352) 271-295. (https:/doi.org/10.1177/026553221770401). Grant  ter,  (20 ing mer-gd nsti  to drie win if.  of d  ng 9, 13-145. https://doi.org/10.1016/S1060-3743(00)00019-9 Hladyna,  & Dwg, . 200. -r via n higta ig ti   d Prtic, 1, 17-27. Hu, G.W   .12tiii iry s  f ai ds ri r n netiv e Learning, 62(3), 813-850. 40(2), 276-299. (https://doi.org/10.1177/02655322221112364). Iscs, ,   kic    2023). n the ti vt  thish t  r o t. nge Testing, 40(3), 748-770. (https://doi-org.ezproxy.eduhk.hk/10.1177/026553222311585). Ih  6 1) 3 mazonaws.com/duolingo-papers/reports/DRR-16-01.pdf). Jami 03.    s     n .  the  1 doi.0rg/10.37514/ATD-J.2013.10.4.15). Khashi  J i,.2021) thek :  e .) Challnges in Language Testing Around the World: Insights for language test users (pp. 333-343). Springer Singapore Kyle, K (2020). The relationship betweenfeatures of source text use and integrated writing qualit. Asssing Writing, 45, Article 100467. LaFlair, G.T. (2020). Duolingo English Test: Subscores (DRR-20-03). (https://englishtest.duolingo.com/research). LaFlair, G.T. & Settes, B. (2020). Duolingo English Test: Technical Manual. Duolingo. (https://englishtest.duolingo.com/research. Losa  e,  019. mat o  wn   ad urd t wig r  ig 62, 235-263. Linacre, J. M. (2020). Faces computer program for many-facet Rasch measurement (Version 3.83.3). Beaverton, Oregon: Winsteps.com. Messck, S. (1989). Validity. In R. L. Linn (Ed.), Educational measurement (3rd ed., p. 13-103). American Council on Education. Mcillan. Meick        g American Psychologist, 50(9), 741-749. Messick, s. (1996).Validity and washback in languag eting Lnguage Testing 13), 241-256. htps/doi.org/10.117/026553229601300302 Plakans, L (2009). Dicrse snthes in inteatd second languag writing asment. Lnguag ting, 26(4), 561-587 hp/doi.rg/10.1177 0265532209340192 (3), 217-230. https://doi.org/10.1016/j.jslw.2013.02.003 Purura, .199.r       m..mr t rs, Qian D. 01e   t  w h4).  tig Center,. (https://www.lttc.ntu.edu.tw/en/LTTC-GEPT_Research_Reports). 21 Sawaki,   , e .013 nng rghs  ws ce o  inawing s nge Assessment Quarterly, 10(1), 73-95. https://doi.org/10.1080/15434303.2011.633305 Setes,  r  0h  di cti of io or osis, , 247-263. https:/doi.org/10.1162/tacl_a_00310 Sirei . G., & iky, L (206). Iative item fomats in comuter-ased teting I usut f mroved ostct rretatio. n S. wng, & T. M. Haladyna (Eds.), Handbook of test development. Lawrence Erlbaum Associates. Soper, D.S. (2020). A-priori Sample Size Calculator for Sructural Equation Models [Software]. (htp://ww.danielsoper.com/statcalc. Thorndike, R. L. (1949). Personnel selection; test and measurement techniques. New York: Wiley.

Wer . life adc pe L h Rt . 1). g  i r h/.-PT Research_Reports). Writing, 37, 46-58. https://doi.org/10.1016/j.jslw.2017.05.015   
Wongtra 010ih aiey   h  it   yis isi. ld Dominion University. https:/digitalcommons.odu.edu/cgi/viewcontent.cgi?article=1183&context=efl etds.   
Xie, Q. (2013). Dos tt prartion work? mplicatios fr cor valdt. gage sment Quarterl, 102), 196-218. htps://do.org/10.1080/ 15434303.2012.721423   
Xie Q. 2015) n whing ad stmtft im mt adc t t ptiot nh k n yse 50, 56-68.   
Xie, . (2015). I mst mes the raer!" n iestgtinf he ts-kers staes to mag rar mesin.sin wing 25, 2-3.   
Xie Q. 20 ig stc   s  ig f t ds n i k    Qly, 172) 183-203. https://doi.org/10.1080/15434303.2019.1691214   
Xie . 2023).s. - 5715./0.005   
e   3    t  ng 30 (1), 49-70. https://doi.org/10.1177/0265532212442634   
Yng, H (2014). d  d f strat ad sar witing poce.  A Qrly 11(4), 03-431 htp:/.g/0.1080/ 15434303.2014.957381   
Yng  Pa  012) d       d -i  y,461) 80-103. https://doi.org/10.1002/tesq.6   
Yin, C.  . 22  i r  ., o f   2nd ed..,). Routledge.   
Yu, G. 009  ing  t e f  ait   n  6-37./.106. asw.2009.04.002   
Zang, C. (2013). Efts o inttin on  tts s wiig. Jdl f Sod ng wiing 2(1), 51-67. htp://i.0.1016/] jslw.2012.12.001   
hu,  i,    Cg   016)  th h   ig  isttin  he lag   t  w.  3 678./o. org/10.1080/15434303.2016.1210609   
h,      s h s ate writing assessments. Reading and Writing, 34, 1-30. https://doi.org/10.1007/s11145-020-10065-x