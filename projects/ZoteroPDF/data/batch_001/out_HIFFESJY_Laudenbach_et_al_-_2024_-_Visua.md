# Visualizing formative feedback in statistics writing: An exploratory study of student motivation using DocuScope Write & Audit

Michael Laudenbach\*, David West Brown, Zhiyu Guo, Suguru Ishizaki, Alex Reinhart, Gordon Weinberg

Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15219, USA

# ARTICLEINFO

# ABSTRACT

Keywords:   
Automated Writing Evaluation (AWE)   
Formative feedback   
Statistics   
Writing in the Disciplines (WID)   
Writing assessment

Recently, formative feedback in writing instruction has been supported by technologies generally referred to as Automated Writing Evaluation tools. However, such tools are limited in their capacity to explore specific disciplinary genres, and they have shown mixed results in student writing improvement. We explore how technology-enhanced writing interventions can positively affect student attitudes toward and beliefs about writing, both reinforcing content knowledge and increasing student motivation. Using a student-facing text-visualization tool called Write & Audit, we hosted revision workshops for students $\left( \mathbf { n } = 3 0 \right)$ in an introductory-level statistics course at a large North American University. The tool is designed to be flexible: instructors of various courses can create expectations and predefine topics that are genre-specific. In this way, students are offered non-evaluative formative feedback which redirects them to field-specific strategies. To gauge the usefulness of Write & Audit, we used a previously validated survey instrument designed to measure the construct model of student motivation (Ling et al. 2021). Our results show significant increases in student self-efficacy and beliefs about the importance of content in successful writing. We contextualize these findings with data from three student think-aloud interviews, which demonstrate metacognitive awareness while using the tool. Ultimately, this exploratory study is non-experimental, but it contributes a novel approach to automated formative feedback and confirms the promising potential of Write & Audit.

# 1. Introduction

Like other STEM disciplines, statistics has a history of advocating for more writing instruction while confronting structural and logistical barriers: large class sizes, already limited instruction time, or apprehensions surrounding instructors perceived lack of writing expertise, to name a few (Hayden, 1989; Woodard et al., 2020). The exigence for statistics and data science is arguably even more acute than most, as statisticians are ofen asked to communicate their results to non-statistician scientists, managers, and journalists (Nolan & Stoudt, 2021). Indeed, tatistical analyses appear in a variety of interdisciplinary and professonal contexts, and recent call for curricular shfts to promote dat cience roficiency havewidened the responsibilitie of sttistics dartment- and y extension, responsiblities of statistics instructors (Diggle, 2015; Hardin et al., 015; Horton et al., 2014). Stil, Gooding et al. (2022)

have outlined the relative dearth of writing intruction in various data science programs across higher education, perhaps owing to the elusive definition of data science currcula itsef (Donoho, 2017; Gere et al., 2015; Gooding et al., 2022). Cllge-level tatistics de partments often house data science programs, or, at the very least, design and teach much of the curriculum. The disciplinary appeal here partly rests on the history of written assessment in statistc, relevant to the apication-level, or client-facing, expectations pervading data science work.

Accordingly, the present study looks at user- or client-facing reports produced by students in a foundational statistics course called Reasoning with Data, offred by the Department of Statistics & DataScience at Carnegie Mellon University. The foundational course emphasizes the thinking-through of empirical problems from beginning to end, using statistical evidence to make a clear argument. Applications are largely drawn from interdisciplinary case studies spanning the humanities, social sciences, and related felds. Methodological topics include basic exploratory data analysis, probability, significance tests, and empirical research methods. As the primary course satisfying the college-wide required learning outcome of statistical thinking, Reasoning with Data is taken by the majority of students in the College of Humanities and Social Sciences at Carnegie Mellon.

In addition to weekly labs, weekly homework, and thre exams, each student completes two wrien data analysis project, both of which ask them to produce astatistical report for an interested non-statistician reader (.g., arport stff and arline administratrs f the students are analyzing a dataset of plane departure and delay times). Meant to asess a student's comprehension and ability to apply and synthesize key course concepts, project 1 is an exploratory data analysis, and project 2 asks students to use inferential statistics. Both projects require the articulation of research questions relevant to potential readers. Yet while the course is writingintensive, students receive little to no formative feedack on their disciplinary writing skills, whichis common in other techncal, content-driven courses - individualized, process-oriented feedback on writing is not always logisticall feasible. Our learning intervention attempts to remedy the scarcity of feedback in these course contexts.

Recent work in language processing has led to a wider proliferation of writing technologies in higher education, which can deliver feedback in large, content-driven courses. These technologie represent just one approach to the instructional hurdles mentioned, and these tools include Automated Writig Evaluation (AWE), Automated Essay Scoring (AES), and Intlligent Tutoring Systems (ITS). In fact, early generations of AES tools were motivated by a desire to reduce instructor labor and create more consistent scoring (Strob! et al., 2019). While most have maintained those original commitments in some way, many have also expanded their ssessment capabilities to focus on formativefedback. Frequently cited AWE tols include Grammarl, Pigai, AcaWriter, and Criterion, each of which provides fedback for their users as they write albeit summative or error-driven. Supported primarily by early cognitive and more recent sociocognitive modeling of writing (Burstein et al., 2020; Corrigan & Slomp, 2021; Flower & Hayes, 1984; Hayes, 2012; Flower, 1994), AWE tools include features which offer student-facing summative and formative feedack, though the type and scaleof that feedback is widely varied (Allen et al., 2016; Strobl et al., 2019).

Still, despite promising use cases, these technologies have mixed record of demonstrating improvements in measurable outcomes, whether in students atitudes toward writing or in the writing itelf (Nunes et al., 2022; Zhai & Ma, 2023). That record is partly attriutable tothe divee range f tchlogies, choie in rearch dei and stdy seig, and h instrtiona support or lackof it - accompanying a given technology's implementation. Other reasons, to, likel attenuate the demonstrable effectivenes of many systems. For one, most prioritize summatie asessments, even f they provide other forms feedack (Strol e al., 2019). Summative assesments evaluate learning outcomes rather than foster students' development toward those outcomes, which is the role of formative asesments (Dixson & Worrll, 2016). Itis unsurprisig, then, that systems with a relative focus on summative assesment might not consistently document learning gains, as their value to the classroom is primarily elsewhere. In fact, the rarer IS and AWE systems that do prioritize formative assessment, such as Writing Mentor (Hazelton et a., 2021), appear to show encouraging results e. g., Wang et al., 2020; Zhai & Ma, 2023).

Another common design decision is more of an issue for post-secondary contexts specifically: that i, to target more generalized genres, like the argumentative essay, detached from disciplinary or social milieus (Deane, 2013; Zhai & Ma, 2023). While this is sensible in order to foster wide applicability, that generality can actually make systems les relevant in post-secondary contexts - particularl in the content areas, which emphasize specialized, field-specific genres. Thus, technologies that promote student writing have a stilluncertain place at the post-secondary level. On the one hand, they appear to be promising tools both for supporting writing in the content aeas and for moderating some of the logistical and pedagogical challenges that face instructors (Zhai & Ma, 2023). On the other hand, there remain questions as to what type of technology is most appropriate for a given context, how a technology can be successfully implemented, and, most importantly, what effects it is likely to have on learning.

The current study contributes to these conversations by presenting a new digital tool capable of targeting specialize, disciplinary genres with formative feedack. We piloted the implementation of a system called Docuscope Writ & Audit o support writing in a college-level tatistics course. We describe the technology at greater length in a subsequent subsection. However, it is useful to note here that our tool does no core students papers, as do most WE systems. Insted it i designed o visualize and  highlight students linguistic and rhetrical choice both in geeral and in reation to task-specific objetives set ut by the instuctor. s focus, therefre. is formative and process-oriented, supporting students as they edit and revise their drafts.

The aim of this preliminary study is exploratory and descriptive. Our goal isto investigate students experiences with and reactions to technologically asisted formative fdack on their dat science reports. Meta-analys of formative aessment have demonstrated its importance to students development as writers at every level (e.g., Graham et al., 2011, 2015). In fact, a recent meta-analysis focusing on L2 learners revealed differing effcts on student motivation depending on the type of feedback offred, concluding that feedback from multiple sources (e.g., instructor, pers, assistive-technology, etc.) has the greatest positie efect on motivation, as opposed to single-source feedback from an instructor or teaching asstant (Cen & Zheng, 2024). Yet, as one group observes about writing instruction in STEM, large class szes and limited instructor backgrounds often make formative feback impossible (Davies et al., 2021, pp. 373). As those same authors note, asstive technologies ffer a potential solution. Of the various kinds of assistive technologie, formative automated writing evaluation (fAWE) systems would seem to be logical candidates to support writing in STEM given their stated purpose. However, AWE is relatively rare, and reerch into it application in post-scondary content course is rarer still.

The purpose of this articl i to detail the preliminary implementation of an interactive too that fers automated and scalable formative feedback. In contributing to that smallbut growing body of research, ths study presents findings from think-aloud interviews with three student participants to provide insight into their composing processes as those processes are mediated by our technology, Write & Audit. That qualitative data is complemented by survey data from a larger cohort of students $\left( \mathbf { n } = 3 0 \right)$ . The role of the survey is two-fold. First, because it isa widely used and previously validated instrument, our findings can be juxtaposed with the results of other studies (Ling et al., 2021; MacArthur et al., 2016; MacArthur & Philippakos, 2013). Those juxtapositions can sugest whether the technologically assted instructional experience has benefits for students commensurate with the efets of other learning initiatives designed to support students writing. Moreover, the results of the survey can underscore pattrns that emerge from the think-aloud interviews, providing inight into the relationship between an fAWE-assted process and students dispositions toward writing.

# 2. Literature review

Central to the approaches presented in this study are theories surrounding the cognition of reading and writing, which se writing as an iterative problem-solving proces, one that requires complex decision-making to meet a shifing set of goals (Flower & Hayes, 1981, 1984; White et al., 2015). More recent updates to (socio)cognitive models of writing knowledge have emphasized the importance of motivation in writing performance, as wellas individual dispositions towards the social conditions in which writing is produced (Aull & Aull, 2021; Corrigan & Slomp, 2021; Driscoll & Wells 2012; Hayes, 2012; Perkins & Salomon, 2012). Importantly too, previous research has found positive correlations between varying dimensions of self-eficacy and writing performance (Bruning & Kauffman, 2016; Ling et al., 2021; Pajares et al., 206; Traga Philiakos e al., 2023). As we explain in latr sections, these theoretical commitments, which inform our decision toasses the construct of student motivation, are what compelled us to turn to think-aloud protocols to capture users thought processes as they interact with our automated formative fedback tool (Flower, 1994; Flower & Hayes, 1984; Reinhart et al., 2022).

The present study contributes to the growing body of research on systems that prioritize formative feedback and their effects on students' motivations, elf-reulation, and writing processes (e.g., Butterfuss et al., 2022; Roscoe et al., 2013, 2014, 2015; Roscoe & McNamara, 2013; Wang et al., 2020). In their work on learning transfer, Perkins and Salomon (2012) state that detecting, or discerning, the possibilit f connection in th first plac i diffiult task, butthey aso mention that schmas can assist in th proces (p. 253) Similarl, Corrigan and Slomp (2021) note the effectiveness of "assesment a learning in generating metacognitie knowledge of writing choce kill hch is key no oy tothe transfer of writing sklls acrosshrical conxts, t which hs a en inked to increased student achievement and motivation (pp. 180). We suspect that by visualizing formative feedback for specific rhetorical schemas - framed in the tol as audience expectations - Writ & Audit can boost student motivation by confirming or challenging their own, often tacit ideas about their choices (Flower & Hayes, 1984).

Our study's specific contributions, particularly the affordances and limitations of Write & Audit and our approach, are perhaps further carified by understanding the construct models against which the survey that we used was validated, and with which the thinkalouds were coded. With that in mind, we first discussthose writing constructs before describing Docuscope Write & Audit, detailing our research design, and presenting our findings.

# 2.1. Writing construct model for student motivation

To gauge the efects of automated formative feedback on students' attitudes toward writing, we turned to construct models commonly used in program asessment (Bruning et al., 2013; Ellio, 2007; Ling et al., 2021; Traga Philipakos, 2020; Whit et al. 2015). In their discussion f Automated Essay Evaluation (AEE) and it relationship to writing instruction, Ellio and Klobucar (2013) argue for the importance of construct models in asessment. Stemming from the field of educational psychology, a writing construct refers o theretically coherent matrix of variables that function as  proxy for the assment of sklls and lening utcomes Dean, 2013; White et al., 2015). The construct, the authors suggest, serves to validate any automated outputs from a system. It also has the potential to serve as a bridge between the automated analyses of students texts and instructional interventions (Deane, 2013; Hazelton et al., 2021).

Construct modeling is not simply meant to explain facets of communication, but to help educators analyze and teach the various domains of knowledge employed in both written and multimodal communication. White et al. (2015), noting that any construct model's constituent variables are \*categories that vary across time and circumstance but that can nevertheles be identified" (p. 76), have outlined the need for mapping domains of knowledge across multiple disciplines in local campus environments. They go on to state that \*unles such a figure is available for all stakeholders as a representation, subject to revision and improvement, of the construct that consumes so much time, atention, and resources," the validity of writing program assessment would be questionable (pp. 77). The commitment to such a taxonomy influences longitudinal curricular alignment and, the authors argue, encourages attention to interpersonal and intrapersonal competencies, including teamwork and communication (interpersonal) as wel as ethics and lifelong learning (intrapersonal). These competencies were outlined by the National Academies of Sciences, Engineering, and Medicine (2017), drawing from decades of work by educators and policy makers. Recently, composition scholars have established unified models of knowledge domains to help steer asessment practices. Specifically, Aull and Aull (2021) combined Corrigan and Slomp's (2021) map of knowledge domains with that of White et al. (2015). Though no model i ever perfect, this unified map offers potential areas of assessment for the present project, though we focus mostly on metacognitive knowledge and its subcomponent, motivation.

However, as our study was meant to be exploratory, we did not set out to directly assesstudent learning or writing quality. Additionall, we did not cllect any data related to students papers or their interactions with Write & Audit for our pilo study. (We intend to collect those dat as part of fuure rech, which we dussin the conclusion to this aticl.) Insted, ur studys data come from surveys that measure motivatio as i relae to writing, which has been connected to improvements in student achievement Cen & Zheng, 2024; Ling et al., 2021; MacArthur et al., 2016).

As detaled in Table 1, the motivation construct is important in and of itself, as students' learning outcomes have been linked to their belie out what ucsul writing is, wh  i for, and how thy undrnd thr rlationsh toi runng t al., 2013; llt 2007; Elliot & Church, 1997; Pajare, 2003; Traga Philippakos, 2020; White & Bruning, 2005). Whil not included in early cognitive models of the writing process (Bereiter & Scardamalia, 1987; Flower & Hayes, 1984), motivation has been the focus of recent sociocognitive approaches to the individualized teaching and asessment of student writing (Corrigan & Slomp, 2021; Hayes, 2012; Kellogg, 2008). Therefore, any instructional interventions that prompt changes to one or more of the subconstructs lite in Table 1 have implications for students' development as writers and their academic success.

We chose to pursue the construct of motivation based on the theoretical foundation of Write & Audit, one that engages students in becoming more responsible agents of their text, thus fostering a metacognitive awarenes of themselves as decision-makers - a key component of adaptive transfer (DePalma & Ringer, 2011; Helberg et al., 2018). This view is comparable to more actor-oriented models of learning transfer (Lobato, 2012), in which the student is labeled not as a novice writer but as a locus of transfer, an active participant in the ecology of discourse that shapes and is shaped by it participants (DePalma & Ringer, 2011). The emphasis on writing sils as dynamic,cros-contextual, and idiosyncratic (orrowing DePalma & Ringer's terms) avoids implicitly privileging "standard' modes of "good" writing. By encouraging students to "audit their own writing choices, our software complicates pre. conceived notions about so-called "good writing, pushing leaners t consider how low-lel eror corrections contribute to high-level rhetorical reasoning--supported by the fact that higher-achieving students have been found to focus more on content than on conventions (Graham, Schwartz, & MacArthur, 1993; Ling et al., 2021; Philippakos and MacArthur, 2015).

The specific survey that we used has several key attributes. First, it has been validated for latent motivational subconstructs that include goals, slf-fficacy, eliefs, and affect (MacArthur et al., 2016; Traga Philippakos et al, 2023; Philippakos & MacArthur, 2015). Additionall, the survey has been validated against an AWE construct model, whose feature components are lited in Table 2. Although our tool engages the user with feature sets from the constructs of organization and development, as well as sentencestructure, it does not asses those choices like other AWE systems might; Write & Audit can be tailored to a particular writing genre, where certain choices are viewed as successful or not according to the immediate social and rhetorical context (Brown & Wetzel, 2023). In their evaluation of the model in Table 2, Ling et al. (2021) found statistically ignficant, positive correlations between belief aout content and features measuring reflection and those measuring conventions (pp. 8-9). Mastery goals were also found to positively correlate with conventions. Thus, they were able to demonstrate relationships between the motivational subconstructs measured by the survey and the features students produce in their writing, although the correlations were weak.

In light of these findings, we can further elaborate the potential implications of our study's results and clarify some of its limitations. As noted, students dispositions toward writing affct academic uces. I adition, the dispositions measured by the survey appear to influence the production of specific and measurable discoursal features in students writing. In sum, then, we can draw inferences from the survey data at diffring level of scal from more general lerning outcomes to more ine-grained writing patterns

That said, it is important to recognize some gaps in the construct model, as it is currently constituted. As the authors themselves observe, the AWE construct as described in Table 2 comprises only a small subset (albeit a carefully considered one) of potential variables. As such, it understandably does not capture the same breadth of knowledge domains necessary for writing expertise as do more theoretical, sociocognitive construct models (e.g., Corrigan & Slomp, 2021). Our tool engages with some of the features in Table 2, but it does not sses them. Another gap is particularly relevant to writig in the content areas. The argument fr including disciplinary writing instruction - whether pecifically instisics or the content areas more broadly - have tended to coalesce around its role in some combination of the following:

Table 1 Motivational construct model from Ling et al. (2021) & Traga Philippakos et al. (2023).   

<html><body><table><tr><td>Goals</td><td>Mastery</td><td>: Goals focused on understanding, learning, personal growth, and self-improvement</td></tr><tr><td></td><td>Performance</td><td>: Goals focused on demonstrating improvement in relation to others</td></tr><tr><td></td><td>Avoidance</td><td>: Goals focused on avoiding failing in front of others and engaging in tasks that learners believe they cannot</td></tr><tr><td></td><td></td><td>attain</td></tr><tr><td>Confidence (Self- efficacy)</td><td>Tasks &amp; Processes Grammar</td><td>: Writers&#x27; beliefs about their ability to successfully complete a task using specific strategies : Writers&#x27; beliefs about their ability to use the correct spelling, punctuation, and choice of words</td></tr><tr><td></td><td> Self-Management</td><td>: Writers&#x27; beliefs about their ability to successfully and attentively plan and complete their writing</td></tr><tr><td>Beliefs</td><td>Content</td><td>: Good writing is represented by its capacity to generate, shape, and clarify ideas</td></tr><tr><td></td><td>Conventions</td><td></td></tr><tr><td>Feelings (Affect)</td><td></td><td>: Good writing is represented by its adherence to perceived correctness and conventional norms</td></tr><tr><td></td><td></td><td>: Writers&#x27; feelings or attitudes toward writing</td></tr></table></body></html>

Table 2 AWE construct model with feature sets from Ling et al. (2021).   

<html><body><table><tr><td>Argumentation</td><td>: Use of claim terms (e.g., suggests that)</td></tr><tr><td>Organization and Development</td><td>: Average number of claim verbs suggesting personal belief (e.g., I believe, I think) : Discourse text segments (such as thesis statements)</td></tr><tr><td></td><td>: Length of discourse text segments</td></tr><tr><td></td><td>.Argumentative sentences</td></tr><tr><td></td><td>: Topic development</td></tr><tr><td></td><td>: Discourse coherence quality score</td></tr><tr><td></td><td>: Words associated with the largest topic</td></tr><tr><td></td><td>: Semantically related words.</td></tr><tr><td>Reflection</td><td>: Argumentative connectives (e.g., furthermore)</td></tr><tr><td></td><td>: Personal reflection vocabulary</td></tr><tr><td></td><td>: Words expressing reference to self-address (e.g., I) and other people (e.g., anyone)</td></tr><tr><td>Sentence Structure</td><td>: Longer prepositional phrases</td></tr><tr><td></td><td>: Longer sentences</td></tr><tr><td></td><td>: Complex verbs</td></tr><tr><td></td><td>: Complex noun phrases</td></tr><tr><td></td><td> Relative clauses</td></tr><tr><td></td><td>: Passive sentences</td></tr><tr><td></td><td>: Sentence variety</td></tr><tr><td>Vocabulary</td><td>. Average word length</td></tr><tr><td></td><td>: Homonym sets</td></tr><tr><td></td><td>: Inflected word forms</td></tr><tr><td></td><td>: Derivational word forms</td></tr><tr><td></td><td>: Pronoun usage</td></tr><tr><td></td><td>. Stative verbs</td></tr><tr><td></td><td></td></tr><tr><td></td><td>: Positive and negative sentiment words</td></tr><tr><td></td><td>. Vocabulary richness</td></tr><tr><td>Conventions</td><td>: Grammar errors</td></tr><tr><td></td><td>: Mechanical errors : Word usage errors</td></tr><tr><td></td><td></td></tr><tr><td></td><td>: Overall errors in grammar, mechanics, and usage</td></tr><tr><td></td><td>: Contractions</td></tr><tr><td></td><td>: Correct collocation and propositional use</td></tr></table></body></html>

1. rehearsing specialized communicative forms, structures, and conventions;, 2. attuning students to the communicative expectations of audiences in their field; 3. catalyzing the development of content area skills; 4. re-enforcing problem-posing, problem-solving, and analytical schema; and 5. preparing students to participate in their field as experts or professionals.

The last three objectives are often emphasized by advocates with a writing-to-learn orientation. According to this view, practicing writing makes students not only better communicators, but also better engineers, chemists, or mathematicians. In promoting writing in statistics, Hayden (1989) epitomizes this persective when he writes: \*Thus writing becomes not just another subject to teach, nor even a tool for achieving traditional gals, but rather a necessary path to developing higher-level quantitative skills (p. 5). These kinds of writing-to-lean ojectives are at least partiall captured in the motivational construct particularly in the domains of belef related to content (reflecting students' understandings of writing as it can be used to generate, shape, and clarify ideas) and mastery goals (reflecting students focus on writing as a means for processing and generating knowledge). However, there are no comparable domains in the AWE construct model. Similar gaps have been noted by other researchers (e., Deane, 2013; Ling et al., 2021), and it underscores the need to harvest richer feature sets that can model en more robust sociocognitive domains for users of AWE systems. How to best acomplish this is beyond the scope of our study; however, the encouraging findings support call to enrich fAWE and ITS construct models, particularly for systems like Write & Audit that prioritize formatie asessment, target communication task processes, and scaffold content area objectives.

# 2.2. DocuScope Write & Audit

The tool presented here addresses some aspects of the AWE construct model mentioned above - it visualizes topic development, for example - but it does not evaluate them. Strobl e al.(2019) propose a taxonomy that organize systems by their orientation (product versus process and the type and amount of student-facing fedback the systems provide (see Fig. 1). According to their taxonomy, Write & Audit would be clssfied as both a proces-oriented Interactive Writing Platform (IWP) and an Intelligent Tutoring System (ITS). The authors identify only one ITS system, WritingPal (Roscoe et al., 2014), which ofers instructional modules, interactive practice games, and formative writing feedback produced using Coh-Metrix. However, another ool that mets  criteria is Research Writing Tutor, which provides formative feedback - modeled after Swales (1990) creating a research space moves -for IMRD reports using natural-language processing (Cotos et al., 2020). till, we contend that Write & Audit offers a more flexible, les-evaluative experience for teachers and students. ITS is a sparsely populated category, but Write & Audit's unusualness is perhaps unsurpris ing, given its origins in rhetoric and composition rather than assessment. Those origins inform Writ & Audit's focus on visualizing students' composing decisions in order to increase their metacognitive awareness of rhetorical strategies.

The Write & Audit application is web-based and integrated into the learning management system, Canvas, where it can be made accessible to students though an assignment. Once the tool is opened, students can copy and paste their text into a field and submit it for processing. That processing includes acombination of dependency parsing by a speciall trained language model and dictionarybased patten matching. Processig takes only a second or two, and afer i is complete, students are presented with four interactive panels: Expectations, Coherence, Clarity, and Impression. Each addresses a diffrent set of discoursal features orstructures and redirects students learning through a combination of visualization, exmplification, and task decription. Formative feedack delivered in written comments requires the student's cognitive effort in decoding, while the visualizations produced by Write & Audit offer alternative, les cognitiely demanding reresentations of the languagelements decribed in Fig. 2. Measuring the specific ffects that feedback visualization has on student witing processes is outside the scope of our study. Stil, we otin here the theoretical asis for each of Write & Audit's interactive functions, providing justification for the tool's design.

Fig. 2 shows the components of the writing processtargeted by each of the four interactive panels in Write & Audit: audience expectations (invention), organization (cohesion), sentence-level readability (carity), and rhetorical awareness (mpressins). These panels are intended to lesen the conitive lod of ther respective tass with the use of visualization so that the student i able to focus on higher-order rhetorical concerns (Flower & Hayes, 1981). For instance, the Coherence panel visualizes the topical structures across paragraphs as wellas sentences within each paragraph in the uer's text allowing them to quickly se the topical organization i their draft without having to engage in the cognitively demanding task of close reading.

As its name suggests, the Expectations panel is designed to activate students awareness of the expectations readers are likely to have for acommunicative task or genre (see e.g., Hyland, 2002). For example, the students participating i our study were ssigned to write IMRD-like statistical reports (Bazerman, 1997; Harmon, 1992). The expectations that the reports conventionall fulillinclude "describing real-world importance" and "describing concerns of the data and methods." Such content can be tailored to specific tasks and customized by the instructor. Crucially, each expectation is linked to topic clusters" or associated phrases (usually noun phrases) that likely ignal their presence, analogous to the way that wales (1990) describes the expectations of a discourse community, which "involve appropriacy of topics, the form, function and positioning of discoursal elements' (pp. 26). Those phrases can be either predefined for an assgnment or added by the students. For example, one expected topic for a research paper may be the goal of the research (see Fig. 3.1). Phras that sigal this topic may include the goal, our goal the urpose, or purpose, etc. In adition to this ist of expected topics, the intrfce incude  brief dcrtionf each expctation, as wel as aset of sample sentence that would satisfy the expectation. It also displays how many sentences in the text satisfy each expectation by searching for associated topics. If the student clicks an expectation, allthe sentences that satisfy it are highlighted in the text editor. With the use of interactive visualization, therefore, the tol allows students to examine if draft ddreses each topic expected in a specific writing assignment as they move from pre-writing, to drafting, to revising. The corresponding topic cluster is highlighted in the student text on the right.

The specific set of Write & Audit Expectations used in our study was tailored to the written assignments of the statistics course (Reasoning with Data). The creation of these expectations was an iterative process that incorporated aspects of the assignment prompt, input from statistics instructors, and the results from our corpus analysis of student writing that we colected. This process takes researchers' and instructors' own task representations and transforms them into a workable schema for students. In Write & Audit each expectation has its own description, which can be written by the instructor, and sample sentences, which we excerpted from student writing samples and published statistic articles. Table 3 displays the high-level questions that appear in the Expectations tab of Write & Audit as well as student writing excerpts with predefined topics in bold.

![](img/37276705eff597cd2bd39cd2e8a8f2657298c40b0548c28fee9e799d55c0be5b.jpg)  
Fig. 1. Taxonomy of online toos for writing support adapted from Strobl et al. (2019), with Write & Audit situated between IWP and ITS.

![](img/ebeb77e5fd66facdc5aa3f92b6bedc4e2034896fd02fa1439fa77d6b9a82639d.jpg)  
Fig. 2. Rhetorical knowledge sets addressby the four panels of Writ  Audit. The our items here correspond to the Expectations, Coherence. Clarity, and Impressions panels, respectively.

![](img/fbfea6c79d2dae6090f911ad2641c97d08fa92fc1c222d7028557db763e65bb2.jpg)  
Fig. 3.1. Screenshot of the Expectations panel with the first expectation highlighted on the left.

Table 3 Expectations for a User-Facing Statistics Report i 36-200: Reasoning with Data with student writing excerts with predefined words and phrases in bold included.   

<html><body><table><tr><td>Describing Real-World Importance</td><td>What is motivating this research? What is its point, or real-world importance?</td><td>This work may be educational for those in the diamond market...</td></tr><tr><td></td><td>Can you interpret results for a real-world audience?</td><td>Our results find that the optimal time for pulling the goalie is 3 minutes earlier.</td></tr><tr><td></td><td>What are some suggestions for future research/ action?</td><td>Future research should investigate the association between the clarity of diamonds and their prices.</td></tr><tr><td>Describing concerns of the data &amp; methods</td><td>Can you summarize the data?</td><td>The mean number of passes in the competition was 84.52 with a standard deviation of 77.31.</td></tr><tr><td></td><td>Do you have any concerns with the methods used to collect this data?</td><td>Possible limitations in the study include the relatively small dataset that we used.</td></tr><tr><td></td><td>How might you justify your techniques for your readers?</td><td>Yet, even with these limitations, this rather simplistic and restrictive model produces fairly reasonable approximations.</td></tr><tr><td>Statistical Analysis &amp; Interpretation</td><td>What can the reader learn from your figures? What can the reader learn from your statistical results?</td><td>Figure 2 shows the data for relationship status and income. With the results of the chi-square test, we can reject the null hypothesis since our p-value is smaller than our a level of 5%.</td></tr><tr><td>Title and Citations</td><td>Do you include an informative title? Optional: Do you include citations that serve a rhetorical purpose?</td><td>N/A References, Works Cited, Bibliography</td></tr></table></body></html>

These expectations are by no means perfect, and they're meant to be refined to better mee student needs, as we continue to do. The flexibility of these expectations allows for the tool's potential adaptability to other content areas.

The second panel, Coherence, alows students to see the topical organization within the text s a whole and within each paragraph. The ool visually maps how topics that address each expectation are distriuted across paragraphs, as well as acros sentences within each paragraph (se Fig. 3.2). Danes (1970) and the linguists and compositionists he inspired (e.g., Wite, 1983; VandeKopple, 1985) used static visualization of topical progressions acrossentences in a short text segment. Leong (2015, 2016, 2018) developed a systematic way to generate a visualization of much longer academic papers using spreadsheets, representing each sentence as a dot that aggregated into meaningful wholetext lines when viewed holistically. Inspired by Leong's general approach, we developed an algorith that automatically visualizes topical organization at the text-level across paragraphs, as wlas at the paragraph-leel across sentences. However, our design prioritized tex-level coherence and is based on Crossley et al. (2016) who sugest that the reader's perception of coherence and qualit of writing is influenced primaril by the presece of topics acros paragraphs. The goal, therefore, is to guide students'atention to lexical overlapsaross paragraphs and sentences willhlp them become more aware of how they are implementing coherence through topic progression.

The third panel, Clarity, is designed to help students notice sentences that may be diffcult t read. Although many researchers have proposed formulae for measuring readability, Bailin and Grafstein (2016) argue that developing a computational method for accurately measuring the readability of a text is complex; thus, existing general-purpose formulae are not suted for task-specific fedback. In place of a measure, we created a simple visualization that guides students attention to sentences that are potentiall difficult to read. Sentences with long noun phrases (topics) before (and sometimes after) the main verb can pose challenges to readers, particularly in informationall dense texts (Gopen & Swan, 1990). Following this logic, the tool shows the length of noun phrases before and after the main verb of each sentence (see Fig. 3.3). Students, then, can decide which sentences might warrant revision.

The fourth and last panel, Impression, directs student' atention to the rhetorical effects of their communicative choices (see Fig.3.4). The tol is buil on dictionary-based systm clled Docuscope (Ishizaki & Kaufer, 2011. The dictionary matches millons of Ianguage patterns and clasifies them into a small number (between roughly 20 to 120 depending on the configuration) f top-level rhetorical functions. For example, s configured in Writ & Audit, the dictionary captures a category Confidence, which highlights two groupings i students' papers.: Conidence Hedged e., tend to get, maye, i ems that) and Conidence High ., most likely, ensue that, know that, obviously). The category Actors can be similarly broken out into Actors Abstraction (e.g., market price, storage capacity, regulatory, distriutio), Actors First Person (e.g, , we), Actors Peple (e., Pauine, hr, peronel, rersentives), and Actors Public Entities (e., govement, mendment, corporation). As a taxonomy, Docuscope categorie asst text analyst in uncovering how writers implement genres using their language choices (Brown & Wetel, 2023). For students, sing categories like Confidence and Actors can help them attend to the ways in which they are framing their interpretations and modulating their claims, as wel as what they are grammatically and rhetorically centering (whether themselves or astractions and proceses). These kinds of decisions leave impressions on readers and can even influence the perceived effctiveness of a given text (Brown & Aull, 2017; Hyland, 1996, 1998)

![](img/562434f00f79a8008024bb9a47862712a8623a4ea43f7acc08d2afb132b2719a.jpg)  
Fig. 3.2. Screenshot of the Coherence panel with the text editor omitted.

![](img/dbafd6c75f2571a8b48968ed9e321c20e227aafb9c8c4cb19fe9f741208d592b.jpg)  
Fig. 3.3. Screenshot of the Clarity panel with the text editor omitted.

![](img/bc9eced232ea1cebe74d07fc29777168f1bd538d886f77f0ce95757c325d6b04.jpg)  
Fig. 3.4. Screenshot of the Impressions panel with the categories Confidence and Planning selected and highlighted in the text.

# 3. Methods

We describe our methods in the subsequent sections, though we recognize that our research does not meet the criteria for a single. case study design established by the National Center for Education Evaluation and Regional Asstance (Kratochwill & Hitchcock, Horner et al. 2010). Instead, the present study establishes our technology-enhanced learning intervention so that we can better prepare our evaluation of its manipulation in future studies involving replication and randomization (Kratochwill & Levin, 2010)

The conclusions drawn from this preliminary study are meant to inform the design and implementation of the tol in future it erations. Such an approach has been specificall referred to as design research (see Anderson & Shattuck, 2012; MacArthur & Phil. ippakos, 2013, pp. 181), where pedagogical theories and goals are adapted and tested in natural ettings - like a revision workshop, in the case of the present study. Through different methods for collecting qualitative and quantitive data, design research directly informs the revisions and future iterations of the intervention or instruction, or in our case, the Write & Audit tol. So, although our study is imited by is small smple size and the absece f a ontrol goup, dissed in a later section, the preent study represents the first step in this systematic proces for refining our learning intervention (MacArthur & Philipakos, 2013, pp. 186). The preliminary results presented below are, nevertheless, encouraging for further iterations of our process.

# 3.1. Study participants and workshop procedures

In the Fall 2022 semester, we presented students enrolled in Reasoning with Data (36-200) with the opportunity to participate n two revision workshops, as wellas pre- and post-study surveys, in exchange for a fifty-dollar cash compensation. Thistudy was approved by Carnegie Mellon University's Institutional Review Board (IRB). We did not cllect detaled demographic information from our participants atthe time of the stdy, they were al first-yr studnts at theuniverity. Participation was volunary, which partally explains our relatively small sample size $\left( \mathbf { n } = 3 0 \right)$ , but students did seem genuinely interested in the Write & Audit tool. The course itself does not include writing instruction in the curriculum; instead, students are given detailed written instructions and a modl paper. So, participants also seemed to appreciate the dedicated time to simply focus on their writing. Though these observations preclude the generalizability of our findings - which, again, i not the goal of the present study - they remain important for under. standing the appeal of the tool for those who chose to participate.

During the two weeks preceding the first workshop, participants $\left( \mathtt { n } = 3 0 \right)$ ) completed our pre-study self-efficacy survey. They were then given access to $2 0 { - } 3 0 ~ \mathrm { m i n }$ of asynchronous introductory materials for using the tool, explaining its core functions and concepts described in the previous section. Consulting these materials prior to the workshop was optional for participants. At the start of each revision workhop, the PI explained the purpose f the study, reitrated points from the introductory materials (similar t section 1.3), and demonstrated potential uses f the tol with a sample paper. Using a model paper wrtten by the course intructor, the PI interacted with Write & Audit's four panels, explaining the key concepts depicted in Fig. 2 and sharing his thoughts aloud as a hypothetical author-user.

Participants were then asked to analyze their own drafts using Write & Audit - starting solely with the expectations panel for the first ten minutes, then including the coherence panel, and so on. They were also asked to share their thoughts with the PI throughout the workshop, but these comments were not recorded. Each workshop ran for about $6 0 \mathrm { { m i n } }$ while students revised their papers, using the Write & Audit interface to varying degres. During the week after the second workshop, students completed a post-study survey. Students were given no specific training for the pre- or post-survey, and we did not collect additional demographic information.

# 3.2. Think-aloud protocols

Since six of the domains of expertise articulated in Corrigan and Slomp's (2021) sociocognitive construct model can be asessed, at least partiall, in surveys and written data, we used think-aloud interviews to capture the seventh: metacognitive knowledge, unique for its proces- rather than product-oriented modes ofassesment o addressthe domain of metacogntive knowledge, and to provide more context to students user experiences, we conducted seven IRB-approved think-aloud interviews with students as they used Write & Audit to revise their paper, and we present data from thre of these sessions. These think-aloud revision sessions followed the same structure as the group workshops, but included only the student participant and the interviewer.

In these sessions, the participant was asked to verbalize their thought processes while the interviewer continuously prompted them with the phrases "please remember to think aloud" and \*an you tell me what you're thinking?" Likewise, the interviewer only responded to students questions about the functionality of the software itself to keep participants focused on their thought processes While writing. At the end of each session, the interviewer and participant debriefed for ten to fteen minute. In contrast to the concurrent think-alouds, this debrief involved a semi-structured retrospective portion of the interview. Participants were asked to reflect and expand on their specific statements or actions as noted by the interviewer (Leighton, 2017; Reinhart et al., 2022)

Finall, transcripts of each interview session were cleaned and coded by the PI for the subconstructs of the motivation construct model described in Table 1. We present data from three f the seven interviews which contained the highest frequencies of self-fficacy and beliefs about content - the two subconstructs which showed a significant increase in our survey. We refer to participants by pseudonyms to preserve the confidentiality of this study.

# 3.3. Survey procedures

To further explore our intervention, we turned to a previously validated survey instrument designed to probe student motivation (Ling et al., 2021; MacArthur et a., 2016; Philippakos & MacArthur, 2015). Traga Philppakos et al. (2023) used confirmatory factor analysis to verify the internal structural consistency of the survey items. Likewise, Ling et al. (2021), using the same methodology, analyzed data from six US post-secondary institutions and identified positive correlations between self-eficacy and writing quality. Both of these studies established the reliability of this survey as an instrument for examining the relationship between motivation and writing quality. Additionall, we were influenced by a central conceit of DocuScope Write & Audit that the development of metalinguistic awareness can demystify academic writing and, thus, increase a student's feeing of agency and confidence over their discursive choices (Helberg et al., 2018). We see these metacognitive skill as compatible with the ubfactors of the motivation survey, presented in Table 1. For these reasons, and because participants would be receiving supplemental writing instruction pertaining to a course where no such instruction is ofered, we expected to see an increase in some of the subconstructs of student motivation. In particular, we anticipated increases in self-eficacy. Since our intervention only consisted of two voluntary, paid workshops, without any considerable changes to curriculum, we did not expect to see much change in the goals or affect subconstructs.

The present survey (Ling et al., 2021) consists of 53 items across the four major subconstructs of motivation: writing goals, writing confidence (selfefficacy), writing beliefs, and felins about writing (affct, each of which is measured through different Likert ranking scales (see Appendix A for the entire survey, including questions and scales). Students took the motivation survey before our first revision workshop and after the second. Each student was assigned a randomized identification number so that we could align their pre- and post-study responses. Thus, we conducted two-sided paired t-tests to evaluate the change in each students individual score for each factor before and afer using Write & Audit. Using paired tes offers slightly more statistical power than a general two-sample test. Although we have a small sample size $\left( \mathbf { n } = 3 0 \right)$ , we meet the minimum for a Gaussian distribution and the rule of thumb proposed by Roscoe (1969). Furthermore, we calculated effect sizes of the differences using Cohen's $d _ { * }$ This metric is important for understanding the practical importance of statistically significant changes: the greater the effect size $( d )$ , the stronger the relationship between the treatment (our learning intervention) and the variables (our pre- and post-survey data), and vice versa (Ells, 2010). Cohen (1988) suggested the following thresholds for interpreting effect size $( d )$ , which we use to interpret our results: small (d $\leq . 2 0 )$ ; medium ( $. 2 0 \leq d \leq . 8 0 \}$ ; and large $( d \ge . 8 0 )$ , with effect sizes equal to or greater than $d = 0 . 5 0$ described as being noticeable or impactful. Again, the present study is non-experimental, but the efect sizes presented below help us interpret the importance of different survey measures.

Table 4 Pre-study and post-study means of motivation scores for each subscale $\left( \mathtt { n } = 3 0 \right)$   

<html><body><table><tr><td> Motivation Factor</td><td>Pre-study M (SD)</td><td>Post-study M (SD)</td><td>Cohen&#x27;s d (paired)</td></tr><tr><td>Self-Efficacy - Tasks and Processes</td><td>81.03 (12.29)</td><td>89.03 *** (10.64)</td><td>0.73</td></tr><tr><td> Self-Efficacy - Grammar</td><td>37.23 (9.69)</td><td>39.73 * (7.86)</td><td>0.42</td></tr><tr><td>Self-Efficacy - Self-Management</td><td>30.90 (4.06)</td><td>33.27 ** (3.90)</td><td>0.60</td></tr><tr><td>Goals - Mastery</td><td>12.83 (2.26)</td><td>12.70 (2.04)</td><td>-0.07</td></tr><tr><td>Goals - Performance</td><td>9.233 (2.65)</td><td>9.77 (2.80)</td><td>0.22</td></tr><tr><td>Goals - Avoidance</td><td>8.33 (3.13)</td><td>8.03 (3.30)</td><td>-0.12</td></tr><tr><td>Beliefs - Content</td><td>24.57 (3.41)</td><td>25.97 * (2.99)</td><td>0.50</td></tr><tr><td>Beliefs - Convention</td><td>11.00 (2.74)</td><td>11.77 (3.29)</td><td>0.24</td></tr><tr><td>Affect</td><td>16.03 (4.44)</td><td>16.97 (3.94)</td><td>0.29</td></tr></table></body></html>

Note: $^ { * } p < . 0 5$ $^ { * * } p < . 0 1$ $^ { * * * } p < . 0 0 1$

# 4. Motivation survey results

To better frame our analysis of the think-aloud interviews below, we first introduce our survey results. The results of our analysis show statistically significant positive changes in students confidence and beliefs about content after using Write & Audit ee Table 4). Particularly promising is the significant improvement of each self-efficacy subscale: tasks and processes, $t ( 2 9 ) = 4 . 0 5$ $p < . 0 0 1$ grammar, $t ( 2 9 ) = 2 . 3 2$ $p = . 0 2 7$ ; and self-management, $t ( 2 9 ) = 3 . 3 1 , p = . 0 0 3 .$ The gain in self-efficacy in tasks and processes has the largest effect size (Cohen's $d = . 7 4$ . As stated in Table 1, the subscales of self-efficacy in the present survey refer to writers' confidence inteir aility touccesfull plan and complete scific tasks. The ires sugget tht the proces of using Write & Audt hods, at least, some promise for student writing development.

In the same vein, Table 4 also shows a statistically significant increase in beliefs about content, $t ( 2 9 ) = 2 . 7 5 2$ $p = . 0 1$ , with a moderate effect size $( d = . 5 0 )$ , suggesting a recognizable difference between pre- and post-study surveys. Beliefs about the importance of content rf tothe idhat goodwriti i rent y is caacity o rate, shape, and larfy id(se Table 1).Mrver, previous studies using the same survey have shown a positive corrlation between beliefs about content and student confidence, and thus, academic performance (Ling et al., 2021; MacArthur e l., 2016). Ultimately, these findings are consistent with our think-aloud interview data, though we did not assess student learning or academic performance.

Although there are statistically significant differenes observed in golorienttion or afect both show small incrses in desired directions. We also note the sight increase in scores for student belies about the importance of conventions in sucessul writing. In the previous studies which validated this survey instrument (Ling et al., 2021; MacArthur et al., 2016),researchers found negative correlations between beliefs in conventions and essay quality. According to Ling et a. (2021), students struggling with writing tend to overemphasize the importance of conformity to perceived grammar rule (p. 11). Note that self-fficacy in grammar refr to students confidence in their own knowledge of writing mechanics, while beliefs about conventions refers to students perceptions about the effcts of mencs onitig qualit. ethes, the inrse in the prt st is insificant, ad it eft i mall Chen's $d = . 2 4 )$ . It might simply be a benign byproduct of increased attention to content and significant gains in student confidence (i., self-efficacy in grammar).

# 5. Think-aloud interviews

In this section, we rfer to interview data from three participants - Akira, Jaha, and Ellen - whose responses contained the highest frequency of "self-fficacy' and \*beliefs about content. Their thoughts and decisions while using Write & Audit help constitut the metacognitive counterpart to the aforementioned gains in the survey data. Our think-aloud interviews offer more individualized in. sights into the domain of metacognitive knowledge as wellas the gains recorded by the survey. Overall our results show the tool's formative feedback serving a confirmatory function, encouraging student sel-eficacy in their choices, as well as idea generation in relation to content. Ultimately, Write & Audit's customized topics and our devoted revision workshops provided supplemental writing instruction as a response to the various logistical and pedagogical barriers in technical course curricula, and this particular implementation appears to be beneficial for students for the reasons that follow.

# 5.1. Self-efficacy: formative feedback and validation of choices

In our think-aloud interviews, we noticed students verbalize their realizations about patterns i statistics writing, specifically those related to word choice and organization. The visualizations produced by Write & Audit appeared to triger these insights, sugested by Jaeha's statement after using the Expectations tab to make a small revision: \*it lik, triggered something in my mind that I was like, oh, I should include, ie, that specific vocab. Similarl, when Akira noticed how he had used present tense throughout his paper, he called a past tense verb a \*kind f.. grammar eror," which he revise in order to maintain consistency. These edits show students

Two participants, Akira and Jaeha, likewise referred to topic sentences and concluding sentences as they revised their papers. Thinking aloud as he navigated the Coherence tab of Write & Audit, Akira stated, \*you just start with the higher phrase, like the goal. Which is very clear indication of what the purpose of the paragraphs about. Here, Akira is reassuring himself f atopic sentence he has written, displaying his confidence in skill captured by the subconstruct of self-fficacy in tass and procese (ee Appendix A.

While our technology-enhanced workshops can clearly asist students in the revision process, Write & Audit seems to also serve students well i is cacity as a confirmatory too- that is, a way to valdate parts of their writing. This is especially interesting when students pause to reflet on their own intentions for a sentence, retrospectively realizing why they made a certain choice. For instance, using Writ & Audit Ellen notices a connection between expectation B1 - "can you summarize the data?" - and the apearance of the corresponding topic cluster - "summary of data" - in the middle of her paper:

"Okay, so for this category, which is the summary of data. It says there are eight sentences that meet that requirement. And so it's about the collection of data, the methods that I use to collect those data and let's go to coherence to se.. [switches to the Coherence Tab.]

So basically there, [data descriptions] are like at the middle part of the essay and- Id say its inspiring for me because like, the summary of data should be at the middle of the essay and that kind of makes sense for me because, like,for every research question I might want to interpret data somewhere, so I guessI won't modify my essay that much for this topic, or this category."

Here, Ellen notices a bridge between two tabs in Writ & Audit, which allows her to weigh her writing choices from the reader's perspective. Additionall, aclear connection to survey questions about confidence in organizational structures presents itself in this response - e.g., I can write a wellorganized essay with an introduction, body, & conclusion'" and I can tll when to use different writing strategies. This exemplifies a trend that we noticed in the workshops: Write & Audit encourages the student to notice and asses their writing choices and, in that relection, to understand more about the content and claims in their writing. Specificall, the tool allows Ellen to quickly the relationship between genre expectations and topical cohesion in her paper, drawing her own conclusions from the visualizations.

By scrutinizing their decision-making from a reader's perspective, students tend to cultivate a sense of assurance in their writing choices. This pattern also ilustrates the promise of using digital tool to facilitate the detection and connection of existing knowledge and new knowledge (Corrigan & Slomp, 2021; Perkins & Salomon, 2012).

# 5.2. Beliefs about content: idea generation & problem-solving

Think-aloud protocols also semed to elicit responses that display student beliefs in the importance of content, a onstruct related to students substantive knowledge: an understanding of real-world happenings and the ability to craft exigent rhetorical responses (Corrigan & Slomp, 2021). Beliefs about the importance of content - in which we saw a statisticall significant increase from pre- to post-study survey scores - is particularl interesting for is relevance to writing-to-learn assignment, like those in the current study. Specificall, in their revision processes students engaged in problem-solving linked to ther quantitative reasoning, which is precisely What we had hoped to encourage with this project (e.g., Wolf, 2010). The ollowing question from the beliefs" section of the survey (see Appendix A) correspond to these goals: \*writing helps make my ideas clearer," \*writing helps me think about my topic in a new way," and "revising helps me clarify my ideas."

Beliefs about the importance of content were observed when Akira and Jaeha both verbalized their apprehensions toward the omission of certain data point, a statment that aligns with course learning objectives. Akia and Jaeha specifically used phrases like omitted some of the sample" and "people from an academic background would want me to reaffirm that my sample is sill like. accurate'. Furthermore, as Akira was looking at Expectation B2 - "are there any concerns that you have with the data?" - he noticed how his discussion of the dataset could affect his reader's interpretation of the results:

Maybe I should also mention some of the problems because in my study I kind of exclude some of the sample size, so that might be a problem for the results. So maybe I can mention some of the keywords that are mentioned at the top of the cluster [quality of data] here."

Here, Akira recognizes new ideas in his paper, precipitated by the visualization of two topic clusters in Write & Audit: Quality of Data and Meaning of Results. Not only is he, as the writer of the data-driven report, clarifying the connection between these topics, but through his revision, he is also creting topical cohesion between separate sections of the paper. Later he stated that he hada totalf, like, five figures," and realized that Write & Audit was tagging only the captions of those figures:

"Man, so that means I only use the title on the figure name, but then I didn't really include them in my writing, so this might be something to fix later to try to include the figure names in my analysis more. So then, readers will know which one I'm talking about, and then which evidence I'm pointing to support my conclusion."

This may signify Akira's ability to cognitively represent the reader in his writing and revision process (Kellgg, 2008). This metalingustic rflection i the process wed ike to encourage with Writ & Audit, and, relatedy, points to the dded benefit of using multiple drafts in disciplinary writing contexts (Modiano & Bonanome, 2019).

# 6. Discussion

At the post-secondary level, content area instructors who recognize the pedagogical value of writing are often faced with barriers when trying to implement writing in their classes - limitations of time, perceived limitations of expertise, and the demands of many students, among them. The results of our exploratory study, set in an introductory statistics course, suggest how technologies that support student writing can not only mitigate some of those barrers but also promote learning gains by decreasing the cognitive burden of student writers. After using Write & Audit, the students participating in the study showed statisticly significant gains in their selfefficacy and their beliefs that good writig is repreented by its capacit to generate, shape, and clarfy ides. Those gains helped us to contextualize data from the think-aloud interviews, allowing for qualitative investigation of tol in its preliminary stage. In those interviews, students elaborate the ways in which Write & Audit seems to help them see the connections between ingustic choice and rhetorical effct, which, at its core, is an essential skill for moving toward reader-based prose and knowledge. transformation (Flower, 1979; Bereiter & Scardamalia, 1987; Kellogg, 2008).

Again, our study is descriptive and non-experimental, and these results therefore cannot be definitively linked to our learning intervention without controlling for confounding variables or including comparison groups. While we cannot directly tie the increases to the use of Write & Audit or our revision workshops, the results at least demonstrate changes in desired directions of student dis positions towards writig at our institution. The present study responds to the aforementioned dearth of formative feedback in content driven courses and the need to develop support trategie for task-specific kinds of writing. Write & Audit ffers one possible approach to this problem, expanding on recent cholarship that likewise employs technology-enhanced learning interventions in the classroom (e.g., Brown & Wetzel, 2023; Helberg et al., 2018). In their discussion of the \*integrative pedagogical potential" of fAwE tools, Hazelton t al. (2021) tres the ned for an itertive dei proes statd in a pcific institional site in ordr to better understand student responses to these digital tools (pp. 79-80), details which were and are integral to our preliminary study. Moreover, Write & Audit can serve as one component of the multiple source feedback model identified by Cen and Zheng (2024) as the most effective at promoting motivation for L2 leners. This suggest to us that our pursuits to incorporate formative fdback in content areas at a large scale are worthwhile as is our decision to assess the writing construct of motivation rather than summatively assess the written products.

As the think-aloud data seem to imply, improvements are perhaps due to the tool's non-evaluative ethos, as a software which only (re)presents the user's writing from the reader's perspective- providing formativ feedback and shfting the focus from writer-ased to reader-based prose, ideally (Flower, 1979; Kellgg, 2008). Representing students' writing and revisions choice in multiple ways, as Write & Audit does, can engage multipl types of learners1 (Dolmage, 2017). We therefore propose that by assessing their writing choices with interactive visualizations, students are pushed to engage in metacogntive reflection about the rhtorical efects of those choices, possibly resulting in a greater sense of gency over their written work. Some students seemed to demonstrate such an agency in their responses to think-aloud protocols.

Our work contributes to scholarship investigating the effectivenes of technologies that support student writing, particularly to the smaller but growing body of research on technologies focused on formative aessment. Additionally, the abilit to lign a system with atarget task specific to an assignment can make tchnologies more releant and useful in post-secondary content areas, where writing can become increasingly specialized. With thataffordance, the tool can visualiz for students the relationships between the rhetorical moves typical of the target genre schema (the reader expectations) and what is realized in their developing papers; in this way, it actively engages the user with the knowledge sets depicted in Fig. 2.

# 6.1. Limitations

Certain limitations are intrinsic to our datacollction methods. Think-aloud procedures, for instance, are useful for examining non. linguistic problem-solving processe inthe working memory of participants, which explains their wide aceptance in mathematics and statistics (Leighton, 2017; Reihart et al., 2022). Indeed, the writing processinvolves problem-solving, ut it also involves processes like idea generation, goal-setting, self-evaluation, and others (Hayes, 2012; Kellog, 2008) - and of course, writing while speaking is nearly impossible.

First, think-aloud protocols only capture a snapshot of writing in a controlled setting, and although such studies have provided useful insights (Flower & Hayes, 1984; Hayes, 2012), they do not account for writing scenarios which stretch beyond a singular moment of composition (Hart-Davidson, 2007). Second, participants may be burdened by the cognitive demands of the think-aloud interview itelf, leding to inaccurate results stemming from what critcs have called "reactivity (Bowles, 2010). Depending on the circumstances of the study - and especiall the individual - think-aloud protocos can either reflect the subject's thought process, or deeply disrupt it. Furthermore, some early cognitive perspectives can elide social and cultural contexts, particularly the different intersections of identty betwen socially constructed cateorie like race, clas, and gnder (Carillo, 2017). This has led recent writing studies scholars to pursue different methods for capturing individul cognition that better acount for social contexts (Driscoll  Well, 2012; Hort, 2017; Roderick, 2019). Finally, we did not check our coding scheme for interrater reliability due to logistical constraints and the preliminary nature of the study. Nevertheles, for the purposes outlined in earler ections, the data we collecte sufficiently illustrate students' revision processes, at least enough to shed some light on the effects of our technology-enhanced learning

intervention.

Our conclusions are also limited by the relatively small ize and uniformity of participants in our study. Because we engaged with design research in the present paper, our next step is to replicate the study in a controlled seting with the variables established here. Further improvement could be realized through a more experimental study, one which follows the National Center for Education Evaluation and Regional Assistance standards for a single-case study design (Kratochwill& Hitchcock, Horner et al., 2010, p. 14) ours will be complete with control groups, parameters to control for onfounding variables, and aditional datacollection, including writing qualit, at diffrent phases (before, durig, and afer the learning intervention, for instance). First, we plan to increase the number of student participants from 30 to closer to 500 by incorporating our proces into the statitis course curriculum. Additional data from students willbe ollected in think-aloud and retrospective interviews, and coding schemes checked for interrater reliability Using the same survey instrument and protocols, we wil be able to test whether the promising results from the preliminary research replicate at scale.

As we discussed earlier, the dispositions measured by the survey have important implications for learning outcomes. They have also been linked to changes i some features that students produce in their writing, though those linkages are weak. However, i remains an open and crtical question as t how revision proesss are afectd by technlogies like Writ & Audit. Wat kinds of featres are being edited? And to what degree? Answering these kinds of questions will provide new and more elaborated construct models, models that are likely t require the harvesting of new variables like proces logs or revisions btween drafts (Roderick, 2019). I might alo require the development and application of new ways of measuring the evolution of a document (e.g., Shibani, 2020; Wininger, 2014). Whatever emerges from that work can, f course, help us refine our technologies and the scaffolded instruction surrounding them. But perhaps even more importantly, it willbe key in informing the ways in which we implement those technologies, support instructors, and guide students in their use.

# CRediT authorship contribution statement

Michael Laudenbach: Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources,Validation, Writing - original draft, Writing - review & editing, Software, Visualization. David West Brown: Conceptualization, Data curation, Formal analysis, Funding acquisition, Methodology, Resources, Software, Validation, Writing - original draft. Zhiyu Guo: Formal analysis, Methodology. Suguru Ishizaki: Conceptualization, Software, Supervision, Visualization, Writing - original draft, Funding acquisition. Alex Reinhart: Conceptualization, Funding acquisition, Methodology, Supervision, Validation, Investigation. Gordon Weinberg: Conceptualization, Data curation, Investigation, Project adminstration, Supervision.

# Declaration of Competing Interest

None.

# Data Availability

Data will be made available on request.

# Acknowledgement

Funding for this research study was provided by Carnegie Mellon Universty through The Simon Initiative, funding source ID 00001.005.243.100008.01, and the Dietrich College Seed Grant, funding source ID 062900.001.210.241100.01. We would also like to express our gratitude to David Kaufer, Norbert Ellio, and the editorial board at Assessing Writing for their guidance i our study design and analysis.

# Appendix A

Table 5 The motivation survey used in the current study, adapted from Ling et al. (2021) & MacArthur et al. (2016).   

<html><body><table><tr><td></td><td>Subconstructs &amp; Questions</td><td>Subscales</td></tr><tr><td>Goals</td><td></td><td></td></tr><tr><td></td><td>When I&#x27;m writing in class, I&#x27;m trying to...</td><td></td></tr><tr><td>1</td><td>.improve how I express my ideas.</td><td>Mastery</td></tr><tr><td>2</td><td>.keep people from thinking I m a poor writer</td><td>Avoidance</td></tr><tr><td>3</td><td>.get a good grade in the class</td><td>Performance</td></tr><tr><td>4</td><td>.hide that I have a hard time writing</td><td>Avoidance</td></tr><tr><td>5</td><td>.become a better writer</td><td>Mastery</td></tr><tr><td>6</td><td>.have my classmates believe I can write well</td><td>Performance</td></tr><tr><td>7</td><td> .pass this class</td><td>Performance</td></tr><tr><td>8</td><td>.avoid making mistakes in front of my classmates</td><td>Avoidance</td></tr><tr><td>9</td><td>.complete all the assignments for the class</td><td>Performance</td></tr><tr><td>10</td><td>.persuade others with my writing</td><td>Mastery</td></tr><tr><td>11</td><td>.be a better writer than my classmates</td><td>Performance</td></tr><tr><td>12</td><td>.hide how nervous I am about writing</td><td>Avoidance</td></tr><tr><td>13</td><td>-get my teacher to think I am a good writer</td><td>Performance</td></tr><tr><td>14</td><td>.better organize my ideas</td><td>Mastery</td></tr><tr><td>1</td><td>Self-Efficacy I can write a paragraph with a clear topic sentence</td><td></td></tr><tr><td></td><td></td><td>Tasks &amp; Processes</td></tr><tr><td>2</td><td>I can write complex sentences without making grammatical errors.</td><td>Grammar</td></tr><tr><td>3</td><td>I can set goals for improving my writing</td><td> Self-management</td></tr><tr><td>4</td><td>I can think of a lot of ideas for my writing</td><td>Tasks &amp; Processes</td></tr><tr><td>5</td><td>I can write a well-organized essay with an introduction, body, &amp; conclusion</td><td>Tasks &amp; Processes</td></tr><tr><td>6</td><td> I can evaluate whether I am making progress in learning to write</td><td> Self-management</td></tr><tr><td>7</td><td>I can write a paper using correct grammar</td><td>Grammar</td></tr><tr><td>8</td><td>I can organize paragraphs with ideas to support the topic sentence</td><td>Tasks &amp; Processes</td></tr><tr><td>9</td><td>I can think of many words to describe my ideas</td><td>Tasks &amp; Processes</td></tr><tr><td>10</td><td>I can plan before I write using an outline or organizer</td><td>Tasks &amp; Processes</td></tr><tr><td>11</td><td>I can use punctuation correctly in all my sentences</td><td>Grammar</td></tr><tr><td>12</td><td>I can avoid distractions while I write</td><td> Self-management</td></tr><tr><td>13</td><td>I can end an essay with a strong conclusion</td><td>Tasks &amp; Processes</td></tr><tr><td>14</td><td>I can come up with original ideas for my writing</td><td>Tasks &amp; Processes</td></tr><tr><td>15</td><td>I can use commas and semi colons correctly in my sentences</td><td>Grammar</td></tr><tr><td>16</td><td>I can plan time to get my writing done by the deadline</td><td> Self-management</td></tr><tr><td>17</td><td>I can start an essay with an interesting introduction</td><td>Tasks &amp; Processes</td></tr><tr><td>18</td><td>I can focus on my writing for at least one hour</td><td> Self-management</td></tr><tr><td>19</td><td>I can find ideas to write about when I&#x27;m given a topic</td><td>Tasks &amp; Processes</td></tr><tr><td>20</td><td>I can write a paper without spelling mistakes</td><td>Grammar</td></tr><tr><td>21</td><td>I can think of the perfect words to express my ideas</td><td>Tasks &amp; Processes</td></tr><tr><td>22</td><td>I can tell when to use different writing strategies</td><td>Tasks &amp; Processes</td></tr><tr><td></td><td>Beliefs</td><td></td></tr><tr><td></td><td>Writing helps make my ideas clearer</td><td>Content</td></tr><tr><td>2</td><td>Revising is mostly about fixing errors in my grammar</td><td>Conventions</td></tr><tr><td>3</td><td>Writing helps me think about my topic in a new way</td><td>Content</td></tr><tr><td>4</td><td>Good writers do not make errors in spelling</td><td>Conventions</td></tr><tr><td>5</td><td>The main problem of poor writers is using incorrect grammar.</td><td>Conventions</td></tr><tr><td>6</td><td>I learn new things from writing</td><td>Content</td></tr><tr><td>7</td><td>Good Writers discover new ideas while writing</td><td>Content</td></tr><tr><td>8</td><td>Writing quickly is an important part of good writing</td><td>Conventions</td></tr><tr><td>9</td><td>Good writers need little revision because they get it right the first time.</td><td>Conventions</td></tr><tr><td>10</td><td>Good writers have to be able to write long sentences correctly.</td><td>Conventions</td></tr><tr><td>11</td><td>Writing is one of the best ways to explore new ideas</td><td>Content</td></tr><tr><td>12</td><td>Revising helps me clarify my ideas</td><td>Content</td></tr><tr><td></td><td>Affect</td><td></td></tr><tr><td></td><td>I usually enjoy writing</td><td></td></tr><tr><td></td><td>I don&#x27;t like to write</td><td></td></tr><tr><td>2 3</td><td>The process of writing is satisfying for me</td><td></td></tr><tr><td>4</td><td>I think that writing is interesting</td><td></td></tr><tr><td>5</td><td>I try to avoid writing as much as possible</td><td></td></tr></table></body></html>

# References

research (2nd ed., pp. 316-329). New York: Guilford.   
Andeo   n, 16.302 001318911428813   
Aul ,    o he 5(1), 215-258.   
Bailin, A., & Grafstein, A. (2016). Readability: Text and context. Springer.   
Bazerman. (997). Discursively structured activie. Mind, Cule and Actity, 4(4), 296-308. htps://doi.org/10.1207/s15327884mca04046   
Bereter, . i,  (197.  tlln   tg i en i.  led Pcsti, 2 142-175.   
Bowles, M. A. (2010). The think-aloud controversy in second language research. Routledge.   
Bron,   Al  2017).  iy vesmpic ty  cs- mparif hhrd r a Pacnt exams in English. Research in the Teaching of English, 394-417.   
Bon  e 223).a l  Asis itio  o  s Cmany (Ed.). Series in Corpus Linguistics (p. 109) (https://benjamins.com/catalog/scl.109) (Ed.).   
g   03f     , 10 (1), 25-38.   
Brug  016. ii  . o wn    6017. T Guilford Press.   
Bur , R,  Mffe, . 2020 dg a wt. o  e c  29-346.an d Hll/C.   
But ,     h  m . 202.   in Pa iv fk a nti J Educational Computing Research, 60(3), 696-721.   
Carlo,  . (2017). heoling ionhi bt oin nd otie stue: ng some istril perctive on or conrary moment. Contemporary perspectives on cognition and, writing, 39-55. Assessing Writing, 59, Article 100802.   
Corrigan, J.A., & Slomp, D.H. (2021). Articulating a sociocognitive construct of writing expertise for the digital age.   
Cotos,  fman ., in . 200. Undd gat wers intioth ad mc f th hritin or ring rei. Jo f Writing. Research, 12(1), 187-232.   
Crosle, .  yle, maa . (2016). Th  fr the ti alyis f tet cohsio mti at of l, goal, d et cohesion. Behavior Research Methods, 48, 1227-1237.   
Danes, . (1970). One instace of Prage School methodology: Functional alysis f utterance and text. Method and Thry in inguisic, 132-146.   
Dvies     021)l cq for i w  stis on Education, 65(3), 373-383.   
Dene .03.th      ti  8/.106. sw.2012.10.002   
DePala, MJ, & Rnger, JM (201). wad  thry f adaptive trasfer: xpading discliry ssons f tasfer n sond-ngge wting and composition studies. Journal of Second Language Writing, 20(2), 134-147. https://doi.org/10.1016/j.jsw.2011.02.003   
ige 5. t  f     3/. org/10.1111/rssa.12132   
Dixson, D. D. & Worrell F. C. (2016). Formative and summative assessment in the classroom. Theory into Practice, 55(2), 153-159.   
Dolmage, J.. (2017). Adc bl: sility and Hgher tion. Unirit of Michign Pr. t:/w.tor./table/j.c3350)   
Donoho . 2017.50 f ci J f il h tsics 64) 745-76. s/i.g/.00/0618600017.1384734   
Driscoll, . L., & Well, J. (2012). Beyond knowledge and kills: Writig tranfer and the role of student dispositions. Composition Forum, 26. Guilford Press.   
Ello, J ch,  (1997. hcha  f ah ad  hem ivatio  of iy  o, 721) 218-232. https://doi.org/10.1037/0022-3514.72.1.218   
Ellio, .   013.d onh   win  .  ., ok of automated essay evaluation: Current applications and new directions (pp. 16-35). New York: Routledge. Univ. Press. https://doi.org/10.1017/CB09780511761676   
Flower, L. (1979). Writer-based Prose: A cognitive basis for problems in writing. College English, 41(1), 19-37.   
Flower, L. (1994). The construction of negotiated meaning. Carbondale, IL: Southern Illinois University Press.   
Flower, L, & Hayes, J. (1984). Images lans, and prose: The representation of meaning in writig. Writte Communication, 1(1), 120-160.   
Flower, L, & Hayes, J. R. (1981). The prenant pause: n inquir into the nature of planing. Rearch in the Teachng of English 15(3), 29-243.   
Gere, A wfor, . ier,  Pg, . (2015). i iidiy i /D:n intio st. l tion and Communication, 67(2), 243-266.   
Gdg  .2   i . ic, (3) 110-118. https://doi.org/10.1111/test.12314   
Gopen, G. D., & Swan, J. A. (1990). The science of scientific writing. American Scientist, 78(6), 550-558.   
Gaa,  ris Hert,011. onting The ts of mti  fri tionf e Corporation of New York.   
a , ri,   15.   pti   oiss   co Journal, 115(4), 498-522.   
am, .  3       r  i and with- out learning disabilities. Journal of Learning Disabilities, 26, 237-249.   
H  l    ,     05.sti curricula: Preparing students to "Think with Data". The American Statistician, 69(4), 343-353.   
Hon 19.r n f th scic pa.  of  ng o (), 5735./.190/05] 8UKA-W8FJ-U54N   
Hart-Davidon, W. 007). tyig he medd action f mpsig wth time-sediries. Hcke N.  Dos (s., D wn h Technologies, methodologies and ethical issues (pp. 153-170). Broadway, NY: Hampton Press.   
Hayden, R. (1989). Using writing to improve student learning of statistics. Writing Across the Curiculum, 1(1), 3-9.   
Hayes, J. R. (2012). Modeling and Remodeling Writing. Written Communication, 29(3), 369-388. https//doi.org/10.1177/0741088312451260   
Hon , l      e  1) ie ti    f i  f e to Writing, 7(1), 37-91.   
Heerg,  k,  ski, , er,  , l,  (2018  l s w e:  si ls an reflection to support students' written decision-making. Assessing Writing, 38, 40-45.   
Hort . 2017. the e   n 1991730o f tie, 16(1) https:/doi.org/10.1177/1609406917734060.   
Hrn,  Che, ,   hw, , d, ,  , l, , Me , ,  n,  04) i ine for undergraduate programs in statistical science. American Statistical Asociation. https:/doi.org/10.13140/2.1.1730.0808   
Hyland, K. (1996). Writing without conviction? Hedging in science research articles. Applied Linguistics, 17(4), 433-454.   
Hyland, K. (1998). Boosting, hedging and the negotiation of academic knowledge. Text & Talking, 18(3), 349-382.   
Hyland,  (2002). Dctive: Amt and ent in amic wiing. Aplied nusic, 23(2), 215-239. hp/di.rg/10.1093/applin/23.2215   
Ishiaki, .er,  01.tr s P    P Identification, Investigation, and Resolution, 276-296.   
Kellogg  (208. tin skls ie l etiv.   n c, 1) 16./i./0139/jow2008.01.01.1   
Kril, T, hk, J r,  i, J  , Rop,   ds, . 2010    nti. Retrieved from What Works Clearinghouse website: (http://ies.ed.gov/ncee/wwc/pdf/wwc_scd.pdf).   
rai  t t 15(2), 124-144.   
Leghton, .. 017). is th b o  lhe r us hin-d an tiv ay in .. . n .) ng Think-ld ni nd gnive  i tioh. f rt P.h/./1093//780199372904.003.000.   
Lng, A Th  & hi, ..018). g       tht ion d mic .ien Communication, 35(3), 286-314.   
Leng, P.015th  ps     ng 535./.515/x-2015 0001   
Leong, P. A. (2016). Thematic density of rearch-artice abstracts: A systemic-functionl acount. Wrd, 62(4), 209-227 http:/doi.org/10.1080/ 00437956.2016.1248668   
Ling, G, Ellot  Brstin, J, Mcffrey,  Macrtur . A Hlman . (2021). iting mivatio:  alatio td f sefen and performance. Assessing Writing, 48, Article 100509.   
Lobato, J. 2012). ear-rid sfer petive  it ctis to h d ptic l ist 473), 2-247.   
MacArr a 013  ini.   ew, 4 (2), 176-195.   
acrr,  a  06   i .  , 39(1), 31-43.   
od 0y  tiisi  m. ee, 25(2), 55-63. https://doi.org/10.53841/bpsptr.2019.25.2.55   
2017 competencies. The National Academies Press. https://doi.org/10.17226/24697.   
Nolan, D., & Stoudt, S. (2021). The promise of portfolios: Training modern data scientists. Harvard Data Science Review, 3(3). 2000 to 2020. Journal of Computer Assisted Learning, 38(2), 599-620.   
Pajaes,  (2003.lfica e, miatio and he  i wti i f t it n  Wig Qly, 192), 139-158.   
Pe,   0   i tir,  iion an perspective. Writing and Motivation, 19, 141.   
Perkins, 0   al ditio  tfr. iost 43, 48258./o.g 10.1080/00461520.2012.693354 international association for the improvement of mother tongue education.   
rt    ,        2 student statisticl rsoning. Jonal of Sttisics and Data ciece Edcation, 30(2), 100-113. htps://doi.org/10.1080/26939169.2022.2063209   
Roderick  01. fion ia pom ig w  t at o air wiin t. w on 36(3), 410-436. https://doi.org/10.1177/0741088319843511   
Roscoe, J. T. (1969). Fundamental research statistics for the behavioral sciences. New York, NY: Holt, Rinehart and Winston.   
Rosoe, R   ,  r,  . 014) n   rngtat sing elme. Computers and Composition, 34, 39-59.   
Roscoe, D. Mcmara . (2013. Witing pal it of anl wtig st or inth high choaso. Jol of tin Psychology, 105(4), 1010-1025.   
Rocoe    . 5. ii n n tin tutor. Technology, Instruction. Cognition, and Learning, 10, 59-79.   
Roscoe, R e  oe  . 013. gd t o ni wi fk on Journal of Learning Technology, 8(4), 362-381.   
hi   w .t Intenational Conference, AIED 2020, Ifrane, Morocco, July 6-10, 2020, Proceedings, Part I (p. 285-290). Springer Intenational Publishing.   
S     0  . Computers & Education, 131, 33-48.   
Swales, J. (1990). Genre analysis: Eglish in academic and rearch settig. ambridge, United Kingdom: Camridge University Pres (Print).   
Traga h) -24   
Tra Pha2ii i  wn  rs- aory factor analysis of scales on goals, self-efficacy, beliefs, and affect. Journal of Learning Disabilities, 56(1), 72-92   
VandeKopple, W. (185). Sentce topics, syntactic subjecs, and domains in texts. Writen Cmnictin, 2(), 339-357. htps:/oi.g/10.1177 0741088385002004001   
n      ue in an automated writing evaluation system. Assessing Writing, 44, Article 100449.   
White,  llot, ,  . (2015). Vy lik  ale: Thea of wing ps. th Stat eit , tps/.org/0.730 9780874219869   
White, M. J.  Bruning, R. (205). Implict witig elefs and their reation  writin qualit. empoy dio Pychogy, 30(2), 166-189.   
Wininger, M. (2014). Measuring the evolution of a revised document. Journal of Writing Research, 6(1), 1-28.   
Wite, . (983. o ste and eisio: An eaory std.l omtin nd ction 343), 313-341. tps/o./0.07/358262   
Wolf, J. (2010. Rhetorical numbers A case for quantitative writing in the composition cassoom. olle ompostion and Comnicatio, 452-475.   
Woodard, V, Le, H., & Woodard, R. (2020). Writig assinments to asess stistical thinking. Jonal of Statistic Edcatio, 28(1), 32-44.   
ZapataRivra,  A-d, J. & liri,  2021). mntg m on the contt of wkle tie task Th Jo Writing Analytics, 5, 324-341.   
Zhai  23ti i its    n 4) 875-900. https://doi.org/10.1177/07356331221127300   
Michael Laudenbach is a PhD Candidate in Rhetoric at Carnegie Mellon University.   
avidet  s  - r   iest.   
Zhiyu Guo is a PhD Candidate in Statistics & Public Policy at Carnegie Mellon University.   
Suguru Ishizaki is Professor of English, Director of the MAPw Program at Carnegie Mellon University   
Alex Reinhart is Assistant Teaching Professor at Carnegie Mellon University.