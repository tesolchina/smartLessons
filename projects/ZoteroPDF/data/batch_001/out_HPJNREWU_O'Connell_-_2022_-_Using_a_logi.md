# Using a logic model to evaluate rater training for EAP writing assessment\*

Jeanne O'Connell a, b

a The Department of Educational Research, Lancaster University, Lancaster, UK b The Centre for English Language Education, The University of Nottingham, Ningbo, China

# ARTICLEINFO

# ABSTRACT

Keywords:   
Logic model   
Rater training   
EAP writing assessment   
Language testing and assessment

Assessment by written exams and coursework is common practice in pre-sessional and preliminary year EAP programmes, but the allocation of marks for written assessment is complex, as is training raters to apply specified assessment standards. This practitioner research uses a Logic Model, a visual diagram commonly used in programme evaluation, to evaluate the rater training procedure for writing assessment in an English-medium university department. This study integrates data from surveys, interviews and workshops with the stakeholders involved in the rater training procedure to develop a Logic Model as part of an ongoing theory of change evaluation. The final product is a Model that reveals the guiding principles of rater training in the department, text that describes the evaluation process, and a measurement plan. This paper showcases how practitioner research can enhance EAP practice by demonstrating how an essential component of EAP assessment, rater training, and the rationale behind it, can be made cogent to the various stakeholders involved in the procedure. This paper offers considerations for EAP practitioners, managers, and testing staff when developing or working with rater training, bridging the gap between EAP and language testing and assessment communities.

# 1. Introduction

Over the past two decades, the subjectivity of marking written asessment has been questioned, particularly in high-stakes assessment (Chowdhury, 2020; Knight, 2002), and the use of rater training as a method of improving rater reliability has been widely studied in the literature. From Pre-Sessional to Post-Graduate level, there is rater training for the subjective marking of written exams and coursework (Knight, 2002; Wingate & Harper, 2021). In English for Academic Purposes (EAP), the asessment of writing continues to be . plagued by concerns about the reliability of rating (which usually means, the reliability of raters) (Hamp-Lyons, 2007, 1). Although marks for assessment should be allocated based on a student'sperformance, the allocation of marks is often a matter of academic judgement for raters (Bloxham & Boyd, 2012). Research has shown that diffrent raters may alocate different marks to the same performance and the same rater may allocate different mars to the same performance at adifferent time (Myford & Wolfe, 2003; Price, 2005). When there is asignificant disparity in theallocation of marks between raters or a rater is not marking consistently, there may be concerns about the farness of the assessment. Raters should have a shared understanding of the levels of performance to ensure farnessfor the students; this will often be based on rating scales and samples of performance shared and reviewed in rater training.

In this paper, the focus is on asessment where there is scope for disagreement among raters or when a single correct' answer cannot be provided (CoE, 2011, 41). Those who allocate marks to students are henceforth known as raters. Drawing on a definition from Hamid et al. (2019), fainesscan be defined as ensuring students are ' . treated equally and are given equal oportunit to demonstrate thir best performance' (McNamara & Ryan, 2011). The staff members in a universty department (EAP teachers, Module Conveners and members of the Assessment Team) are referred to as stakeholders in the rater training process.

To examine the process f rater training for subjectively-marked written assessment, this evaluative reearch uses a Logic Model approach to depict the path ofa rater training procedure for EAP writing asessment in a university. The steps leading from how raters prepare for rater training, to the training itsef, to how raters perform after the training are not straightforward. A Logic Model illustrates the various steps in the rater training procedure, from inputs to outcomes, to make a programme, or procedure, and the rationale behind it, more transparent and more cogent to the various stakeholders, while recognising its limitations. The aim of this research is to use a Logic Modelto encourage stakeholder buy-in to the rater training procedure by outlining the guiding principles that define it, examining the perspectives of the multiple stakeholders involved, and acknowledging the variety of measurement approaches used to examine the efectivenessof the training. Whil other approaches are approriate for evaluating training (e.g. Kirkpatrick and Kirkpatrick, 2009, the Logic Model approach was selected for this evaluation as it llows for formative approach to a complex procedure (Hayes et l., 2016), considering the local context (Weis, 1997), the perceptions of the key stakeholders (Gugiu and Rodriguez-Campos, 2007), and the underlying assumptions inherent in what i being evaluated (Weiss 1998a). Although there are challenges and limitations to the approach, the findings from this study provide some support for the hypothesis that a Logic Model approach can be productively applied to a rater training procedure.

# 1.2. Research questions

. What in the rater training process is perceived as problematic by the stakeholders?   
: Can the development of a Logic Model support the evaluation of the rater training process? o What outcomes of rater training represent success? o What activities are required to achieve these outcomes? o What inputs are needed to deliver these activities? o Can the development of a Logic Model influence stakeholder buy-in to the rater training process?

This paper introduces the context of the rater training evaluation. Then, the methodology is outlined and the results of the eval. uative process are discussed. Finally, recommendations for EAP practitioners, managers and testing staff are presented.

# 2. Context

The context in which this research is carried out is Preliminary Year programme, the firs year of a four-year degree programme in an L2 (second or aditional language) teaching department in a Sino-foreign university in China. Approximately 2,00 students, primarily Chinese nationals $( 9 0 \% + )$ , study EAP and subject-content courses related to their degree in this programme and 82 teachers, primarily British nationals $( 5 0 \% + )$ , teach EAP. Eighteen of these teachers also teach on the summer Pre-Sessional programme, in which approximately 120 students, primarily Chinese nationals $( 9 5 \% + )$ , study EAP for ten weeks to prepare for Post-Graduate programmes, predominantly in British universities. The subjectively-marked writing asessment of two EAP module is examined; these modules focus on academic language and skills and are taken by allstudents on the programmes described above.

Rater training is conducted for raters marking end-of-programme coursework for these modules, the marks of which contribute towards progressin and scholarship decisions. The ssessment i considered high stakes, as it . i used to make decisions that affect an individual's life in significant ways' (Coniam & Falvey, 2007, 457); the results of the asessment determine entry (or not) onto degree or Post-Graduate programmes with English as the medium of instruction. The scoring of such assessment should be fair (Hamid et al., 2019) and rater training is fundamental in ensuring there is a fair procedure for allocating marks. The training iscrried out by the Module Conveners, Co-Conveners (who asst the Module Conveners in their convening duties), and the Assessment Team, composed of the Head of Assesment, the Team Leader and two additional team members. In an evaluation, understanding the context provides an important contribution to improvement (Weiss, 1997).

Raters are key stakeholders in rater training; communicating the purpose of rater training and the benefits of this training for students and involving the raters in the evaluation of this training are central in building buy-in to the rater training process. Rater training is an opportunity to educate stakeholders in Language Asesment Litracy (LAL), described as having the capacity to ask and answer critical uestions about the purpose for assment, about the fines of the too being used, about testing condtions, and about what is going to happen on the basis of the results' (Inbar-Lourie 2008, 389). LAL and rater buy-in are integral to rater training.

# 2.1. Rater training for writing assessment

The value of rater training continues to be questioned (e.g. Eckes, 2008; Elder et al., 2007; Lim, 2011; Schoonen, 2005; Wind. 2020). There are numerous small-scale studies on rater training for writig aessment focussing on rater background (Barkaoui, 2010; Shi, 2001), rater behaviour (Baker, 2012; Farrokhi & Esfandiari, 2011; Fox, 2003; Schaefer, 2008), and the impact of social context (Baker, 2010). Recent commissoned studies have evaluated large-scale rater training, such as the evaluation of the online rater training for Aptis commissioned by the British Council (Knoch et al., 2016). In the last two decades, the use of psychometric measurement tools such as Facets, has been a significant development in measuring the efectivenes of raters pre- and post-training (Eckes, 2015; Erguvan & Aksu Dunya, 2020; McNamara & Knoch, 2012).In summary, research has shown that ther i inevitably some variation in the mark allocated by different raters (Tengberg et al., 2018; Gwet, 2014; Meadows & Billngton, 205; Engehard, 2002), but it is generally recognised that rater training improves the reliability of marking (Brown, 2012; Kang et al., 2019).

In rater training, an act of socialization (Lumley, 2002), raters develop a shared understanding of the task requirements, the rating scales, and the expected performance standards athe various peformance levels (Hamlton et al., 2001; ondo, 2010). Training ratrs in the use of a rating scale does not eliminate variability in the scale's use (Weigle, 2002; Weir, 2005), but rating scales are commonly used in language assessment to .. reduce the variation inherent in the subjectivity of human judgements' (CoE, 2011, 41). The challenges involved in developing rating scales are well documented (Hudson et al., 2017; Bloxham et al., 2016; Bloxham & Boyd, 2012; Sadler, 2009; Orr, 2007; O'Donovan et al., 2004; Turner & Upshur, 2002).

The rating scales in this research are criterion-referenced analytic scales. The scales, together with sample marked scripts, are available to students and staff before the programmes commence. The scales feature in clas material to encourage student familiarity with the range f features and levels of performance. A mark out of 10 is allocated for each of the three components: Task Fulfilment, Cohesion and Coherence, and Grammar and Lexis. The scales are based on tasks which showcase what students have covered in the programme. The rater chooses the level that best describes the performance of the student in the task. Although the rating scales used in rater training are inherent in the rater training procedure in this evaluation, it is beyond the scope of this paper to discus their development.

The purpose of rater training in the department is to ensure raters consider the features in the rating scales when allocating marks and to ensure a high degree of comparability, both between raters (inter-rater reliability) and within raters themselves (intra-rater reliability) (rown, 2012). Large-scale testing instttions, such as the British Council (BC) carry ut extensive traning for their raters to ensure they can apply the asement criteria accurately and reliably (BC, 2020). For example, to mark writing in the Intenational English Language Testing System (IELs), BC require raters to undergo four days of training, certification and monitoring (IELTS, 2021). Rater training in EAP difers from the training provided by large-scale exam providers one difference is in EAP raters may be responsible for assessing the performances of their own students.

# 2.2. The effectiveness of rater training for L2 writing in higher education

There have ben several studies examining the efectivenes of online rater taining (Knoch et al. 2016; Knoch & Huisman, 2014; Erlam et a., 2013; Brown & Jaquith, 2007) and significant research projects on the rater training for a diagnostic English placement test, the Diagnostic English Language Nees Analysis (DELNA) (Elder et al., 2007; noch et al., 2007). However, outside of a study on teachers' perctions f online trainng (Hamiton et al., 2001), there remains a dearth of rerch on the relative eficacy of different rater training formats in higher education that may provide datato ass in improving thefectivenes of rater training (Brown, 2012, 421).

The rater training discussed in this evaluation is based on a recognise training procedure used by testing agencies such as BC and Cambridge, underlined in detail in Bachman and Palmer (1996) and reiterated in the Manual for Language Test Development by the Council of Europe (CoE, 2011). CoE recognise the principles of language testing are . the same for oth high- and low-stakes tests, even though the practical teps taken will vary' (CoE, 2021). The steps in the rater training procedure outlined below are generally accepted as good practice and may be applied to both large-scale commercial language testing and more localised institutional testing. A brief description of the procedure is as follows: Prior to the training, samples of performance at varying levels of proficiency are marked by a team, including experienced raters, henceforth known as the benchmarking team. The benchmarking team agrees on marks for the sample scripts. A sample of these marked scripts, known as anchor script, are made available to the raters before the training. The raters are encouraged to familiarise themselves with the cales and these scripts prior to the training. In a typical rater training session, ratrs iscuss and grade the anchor scripts i pairs or groups, greeing on a standard. The raters are also given specific guidelines on scoring atypical scripts. After the training, raters mark a further sample of cripts. If their marks are within a specified range, they start marking. If their marks are outside this range, they mark an additional sample of scripts and submit them to the benchmarking team to review. If further support is needed, there is face-to-face (f2f) meeting with a member of the benchmarking team. After a number of scripts are marked, they are submitted for moderation.

# 2.3. Stakeholder buy-in and LAL

The benefits of engaging stakeholders in the implementation of assessment are wellrecognised (Andrade, 2011; Lederman, 2010; Sujitparapitaya, 2014). For this paper, stakeholder buy-in refers to the support or aceptance f takeholders, incuding raters, Module Conveners and Assessment Team members, for the rater training procedure in the department. As mentioned above, unlike other large-scale assesment, such as IELTS and DELNA, the raters marking EAP asessment are also the teachers the Module Conveners and the assement delopers. In dissions on assesment in highr ecatio, t is recogned tht . esment is abt teching and learning, for which faculty are primaril responsible (Gray, 2002, 58). Stakeholder buy-in, which may be substituted with terms such as faculty support or faculty backing (Feldhaus et al., 2015; Mullaney, 2018),can have direct impact on teaching and learning in the universit. By involving takeholders in the evaluation proces, for example, canvasing their views on rater taning, the evaluator aims to elicit buy-in to rater training in the department. It is recognised that stakeholders may buy into the different components of rater training to varying degres. Thus, evaluating stakeholder satisfaction with the rater training procedure is complex. Rather than viewing the stakeholder concerns outlined below as negative, this paper attempts to reframe these concerns as an opportunity to

examine new possibilities.

LAL can be defined as the knowledge, skills, and principles required by the stakeholders involved in assment to perform assessment tass (Inbar-Lourie, 2013; Fulcher, 2012). While there is much debate on what each stakeholder group should know about language assessment in order to perform their assessment-rlated roles efectivel (Deygers & Malone, 2019; Popham, 2009), it is generally recognised that language testing professinals, such as rater trainers, should be knowledgeable in statistics, one of the components of LAL (Kremmel & Harding, 2020; Taylor, 2013). However, the professonal background of many language testing professionals is in language teaching rather than in statisics or psychometrics (Baker, 2016; McNamara & Knoch, 2012). In the development of rater training for high stakes testing, a potential lack in trainers' LAL may have an impact on the fairness of student results.

# 2.4. Logic models

Logic Models have been increasingly utilised in evaluation to met the rise in demand for formal ccountabilit in higher education (Kaplan & Garrtt 2005); for example, including a Logic Model in an application for the funding of a new programme has become commonplace. This paper does not provide a guide on how to build Logic Models as there are various comprehensive manuals (e.g. National Health Service, 2016; Kelogg, 2004). Instead, this evaluation emphasises how a Logic Model can be used to show how a process can work to solve an identified problem under certain environmental conditions. Although the literature on Logic Models tends to use the term programme to describe what is evaluated by the Logic Model, the procedure evaluated in this research meets the criteria for a Logic Modl as it describes an intentional transformation of specific resources inputs) in certain activities (processes to achieve desired outcome (results) within aspecific context (McLaughlin & Jordan, 2015, 63). The components typically contained in a Logic Model are outlined in Fig. 1 below.

A Logic Model was deemed appropriate for the evaluation of the rater training procedure for three key reasons. A Logic Model.

1. is accessible to the non-evaluation community (Conrad et al., 1999);   
2. can assist in identifying the essential components of a process so it may be replicated (Kellogg, 2004; Sherman, 2016);   
3. provides a visual representation to serve as a tool for communication in discussion with the stakeholders, helping to identify areas for improvement, develop a common language among stakeholders and offer clarity on terminology (McLaughlin & Jordan, 1999; Newton et al., 2013).

A further reason for choosing this model is based on its focus on continuous review; Logic Model development guides (e.g. Tay. lor-Powell & Henert, 2008; Kellogg, 2004; den Heyer, 2002) stress the importance of reviewing and updating the Logic Model to maintain its fidelity as changes occur. Using a formative evaluation approach in which evaluation cycles occur during the procedure can help uncover strengths and areas for improvement, which can facilitate desired training outcomes (Hayes et al., 2016). There are however, some limitations of logic models which are discussed at the end of the following section.

# 3. Methodology

The process of developing a Logic Model to evaluate the rater training procedure was informed by Saunders (2011), who recognises theory may provide an explanatory framework to asst in the organisation and presentation of data and .. enhance the chance of the evaluation making a contribution to positive developments (9). Although it cannot be assumed that the ratr training procedure in one university department will be the same as the rater training procedure in another department or university, fuzy generalizations can be made (Bassey, 1999, 12). The evaluator adopted an insider research' approach and attempted to exercise caution and self-awareness in conducting the data analysis Costey et l., 2010,3). Prior to the commencement of this study, ethical clearance and permission to collect data were obtained from the relevant University Research Ethics Committees.

![](img/3e7b7f57b3f69c7340dcebb296e9177957460d1afbf8ae1d3457d5f7ef59f897.jpg)  
Fig. 1. A typical Logic Model.

# 3.1. Developing a logic model

The typical stages in the development of a Logic Model as outlined by McLaughlin and Jordan (2015) were followed, considering the theory-based approach to evaluation.

Much of the literature recommends Logic Models are developed collaboratively with key stakeholders (Gugiu & Rodriguez-Campos, 2007; Kaplan & Garett 205; Knowlton & Philips, 2013; McLaughlin& Jordan, 199. Stakeholder involvement is particularly helpful in a complex environment with high levels of uncertainty (Reynolds & Sutherland, 2013), such as a rater training procedure. When stakeholders are involved, they are more likely to be engaged in the process (Kaplan & Garrtt, 2005; Kellog, 2004; Renger & Titcomb, 2002) and, as mentioned above, there are benefits to this buy-in in terms of teaching, learning and asssment. In addition, it is claimed that stakeholder buy-in may lead to a more comprehensive Logic Model than when there is les stakeholder involvement (Kaplan & Garrtt, 2005; Renger & Titcomb, 2002) and the Logic Model is more likely to be used after itis created when different stakeholders are involved in its development (Gugiu & Rodriguez-Campos, 2007).

The Logic Model was constructed in five stages:

Stage 1: Collecting the relevant information   
Stage 2: Defining the problem   
Stage 3: Defining the elements   
Stage 4: Drawing the model   
Stage 5: Verifying the model with stakeholders

Firstly, the various methods of data collction for Stages 1-5 are outlined. Then, the results for Stages 2-5 are presented and discussed under Results and Discussion.

Stage 1: Collecting the relevant information

Information was collcted in two ways to make the underlying rationale explicit: 1) analysing the literature and 2) soliciting opinions from those involved in the process. For the purpose of this paper, the term underlying rationale' refers to a visual repre sentation of the causes f the problem being targeted (Renger & Titcomb, 2002). It i essential that the evaluator collect information relevant to the proces from multipl sources (McLaughlin & Jordan, 2015, 71). As mentioned above, extensive literature on the value of rater training, ratr reliabilit, and Logic Models was considered in this evaluation. In adtion to published literature, departmental documentation, such as training manuals and asessment policies, were reviewed. However, it is recognised that documents are subjective and the creator of a document may participate in purposeful or non-purposeful deception' (Merriam, 2009, 154)

Prior to this study, the researcher conducted a departmental survey asking EAP teachers their perceptions on the reliability of their marking. This survey was sent to 63 EAP teachers, who attended rater training in the semester; 41 $( 6 5 \% )$ responded. These survey results indicated stakeholder dissatisfaction, discussed below.

Based on this initial survey, the researcher repeated the survey the following semester. In addition to the EAP teachers, two other stakeholder groups involved with rater training were identified:

1. Assessment Team members ${ \bf ( n = 3 ) }$ -   
2. Conveners ${ \bf ( n = 1 2 }$

For this study, primary data was gathered via surveys with 55 EAP teachers, also known as Tutors or raters, and 12 Conveners (2 Module Conveners and $_ { 1 0 \mathrm { { C o } } }$ Conveners). Due to the tight time frame between the rater training and annual leave and the number of potential participants $\mathrm { ( n = 6 7 ) }$ , surveys were chosen to furnish the views of the participants (Sue & Ritter, 2012). Surveys were written on Qualtrics and a link was sent via email. Eight of the Conveners $( 6 7 \% )$ and 28 $( 5 3 \% )$ of the raters completed the survey in full and agreed to participate. The aim of the surveys was to determine how the Conveners and raters perceive the rater training procedure, specificly criticisms of the procedure and the extent f it efetivenes. The survey questions can be found in Apendix . The urvey respondents were randomly ssigned a code (F1-F36) for the purpose of anonymity. The evaluator recognises the development of the Logic Model is an iterative process requiring the ongoing collection of information (McLaughlin & Jordan, 199.

The following semester, semi-structured interviews $( \mathtt { n } = 5$ were carried out with the Module Conveners and members of the Assessment Team. The interviews started with an explanation of the consent required and with the clarification and reiteration of the role of the evaluator, considered separate from the evaluator's position in the department. As the members of the Assessment Team share an office, a group interview was chosen for data collection $\left( \mathtt { n } = 3 \right)$ . However, one of the team members was unable to attend, so an interview was held with two team members initill and then the other member individually emi-tructured interviews were also used with the Module Conveners of the two principal EAP modules in the department to follow up on survey results.

The interview guide developed by Gugiu and Rodriguez-Campos (2007) facilitated the interviewing. The arrangement of the questions aimed to obtain an in-depth understanding of the perceptions of the current ratr training procedure, as well as their views on the purpose of rater training and measuring it effectiveness. The interviews were conducted and recorded on MS Teams. The interviewees were randomly assigned a letter I1-I5 for the purpose of anonymity.

Stage 2: Defining the problem

This stage examined the problem in two phases: 1) a departmental survey on raters' perceptions on the reliability of their marking $\left( \mathtt { n } = 4 1 \right)$ ; 2) a repeated survey on raters' perceptions on the rater training $\left( \mathtt { n } = 3 6 \right)$ . To outline the problem, 59 comments related to rater dissatisfaction with the rater training procedure were summarised and commonly mentioned isues are discussed below.

Stage 3: Defining the elements

This stage was carried out with members of the Assessment Team and Module Conveners on Microsoft Teams. For the purpose of this evaluation, the evaluator prepared a brief information sheet explaining Logic Models (Appendix B) and a template. The Information Shet briefl explained the focus of the evaluation, outlined the questions that would be discussed, and illstrated a simple Logic Model from Kellog (2004), including some notes on terminology and the proces of asking how and why questions (McLaughlin & Jordan, 2015, 77). These documents were sent to the interviewees prior to the interview. Thefllowing steps were discussed in the interviews:

1. the assumptions on which the process is based;   
2. the inputs or resources;   
3. the activities being conducted;   
4. the outputs of each activity;   
5. the expected outcomes.

The evaluation followed each of these steps in the sequence to discuss whether the procedure outlined i followed and the evidence that may be used to measure whether the outcomes have been achieved. The inputs, activities, outputs and outcomes are discussed under guiding principles behind the process.

![](img/ca77127838150b176ff6ddb668dbdb4b8760ce88c20126abac143bf37863e24c.jpg)  
Stage 4: Drawing the model   
Fig. 2. A Logic Model for rater training.

This stage was completed with the participants via MS Teams. The interviewees were asked to send their Logic Model drafts and notes to the evaluator to asst in the evaluation. The evaluator drafted a Logic Model based on an understanding of the process and informed by the literature. Then, stakeholder perceptions of the various steps of the Logic Model, and how they interact, were dded It was recognised that short-term outcomes must frst be achieved in order to set the stage for medium-term or long-term outcomes (McLaughlin & Jordan, 2015). To determine the outcomes, prior research as well as the stakeholders knowledge of the process, aims and context was examined.

Stage 5: Verifying the model with stakeholders

During the follow-up interviews with the Module Conveners and the Assessment Team, at each stage of the model, the question why' was asked until the ealuator and the interviewees were satisfied that the underlying rationale f the problem was made explicit (McLaughlin & Jordan, 199). A Logic Model can be adapted to the local context and include other components. For example, in Fig. 2, the problem that the evaluation seeks toaddress th assumptions that may impact the process and the unintended outcomes are also included (Kellogg, 2004). At each stage, measurement indicators, represented by stars predicting whether the desired outcomes willbe achieved, are included; these indicators function as a guide for improving the procedure and a tol for measuring to what extent the procedure has accomplished its goals.

In order to develop the final draft of the Logic Model, itis common to have multiple sessions with the stakeholders. This leads to a number of unique versions of a Logic Model as mentioned in Stage 4.

# 3.2. Limitations of the Logic Model

There are several limitations to Logic Models two are discussed in ths section. The main limitation in terms of ths evaluation was the model tends to focus on the positive, intended outcomes, not the negative or unintended outcomes (Rogers, 2008; Taylor-Powell & Henert, 2008). This limitation may lead to overstating causal contributions in the model, making it more dificult to replicate (Rogers, 2008). This was addressed in this research through in-depth discussions of the unintended outcomes in the stakeholder interviews. Recognising that unintended outcomes ocur and understanding how these can be managed was integral to this evaluation. This is further discussed below. A second limitation is the development of a Logic Model can be time consuming (Gugiu & Rodriguez-Campos, 2007; Renger & Titcomb, 2002; Weiss, 1997). This model was developed over six-month period and will continue to be periodically reviewed, a proces which has been incorporated in more recent Logic Models; for example, den Heyer's (2002) design includes the impacts on individuals and organisations as a result of review and discussion. Although the completed Logic Model may appear simple, it ook several drafts to describe the rater training procedure considering the perspectives of each stakeholder group. The final drat (Fig. 2) shows the most critical relationships and feedack loops, which allow the activities to be adjusted based on the outputs and outcomes.

# 4. Results and Discussion

This section defines the problem, the assumptions, and the guiding principles of rater training. A measurement plan, alongside the unintended outcomes asociated with the proces, and the fully developed final version of the Logic Model are presented.

# 4.1. The problem

As mentioned above, 41 of 63 raters $( 6 5 \% )$ responded to the survey on raters' perceptions on the reliability of their marking. In response to the question asking raters who had recently undergone rater training how they viewed the procedure (positively, netral. negatively), 16 $( 3 9 \% )$ described the procedure negatively. There was an open-ended question asking for comments: 'Good in principle, but confusing and demoralising in practice; I found the procedure to be intensely professionally insulting, unnecessary and a poor use of my time resources; I support anything that guarantees consistency and fairness for students which was clearly the intention. However, too much sleep was lost by too many experienced tutors. Raising doubts and losing confidence had the opposite ffect to What was intended. As a result of this feedback, it was recognised that some takeholders did not buy into the rater training procedure as it was.

In the surveys examining the negative responses to rater training mentioned above, when the raters were asked if the rater training process was sufficient, 24 of 28 raters $( 8 6 \% )$ answered positively. However, for the question, What, if anything, was not so good about the rater training procedure?, 'descriptors was frequently mentioned in the department where this research took place, rating scales are known as decriptors. This suggests that some dissatisfaction with the rater training proces may be due to the rating scales. This is supported by nine of 28 respondents mentioning the rating scales could be improved. A further identified ssue was communication between stakeholders (four of 28 respondents mentioned that communication could be improved). A lack of confidence in individual marking standards was also mentioned by three respondents. All eight Conveners who participated in the evaluation agreed the procedure was sufficient for the needs of their team and the marks of the anchor scripts werefair. The onveners also mentioned isues with the rating scales (thre of eight mentioned the rating scales could be improved); one respondent (F13) mentioned involving raters in the construction of the scales: Perhaps involve tutors in the construction ofassessment critri, although this opens another can of worms! In the interviews with Module Conveners and members of the Asessment Team who run rater training, four of the five in. terviewees mentioned rater issatisfaction or a synonym (unhappines, discontent, anger) as a problem with the rater training proces.

Interviewee I3 discused the size of the department as a challenge: We are a large department, and however experienced you are, there may be some anxiety when it comes to rater training. Interviewee I5 also mentioned the size of the department as a challenge for rater training: I think ratr traning is an extremely difficult proes and this s especill tre in a dartment as large and diverse as ours. Overall, rater dissatisfaction with the rater training procedure was explicit in the surveys and interviews.

# 4.2. Assumptions

Assumptions related to inputs and activities and how these may lead to intended, or unintended, outcomes may be included in the Logic Model. The assumptions on which a programme or procedure is based are crucial in developing the model. In this evaluation, the assumptions were initilly identified in the literature review, and then the participants reiterated these issues in the process f developing the Logic Model. Iti likely that differet stff in an institution will have different assumptions about aessment, given their particular perspective (Gray, 2002, 58). In evaluation, it is important to focus on what programme, or in this case, a procedure. expects to achieve and how it expects to achieve it (Weiss, 1998a). This evaluation focusses on two key assumptions: The first assumption is related to inputs; there is an assumption that assessment criteria can be transparent. The question of whether trans. parency in assessment criteria is achievable features in the literature (Bearman & Ajjawi, 2018; Bloxham et l. 2014; Jackel et al. 2017) and was restated in both the interviews and the surveys. To ensure transparency, the participants stated that i is not sufficient for Module Conveners to decide on marks for anchor scripts in iolation; a representative sample of saff teaching on the programme must be involved in the marking of anchor scripts and the process must be transparent toall stakeholders (Lowie, Haines, and Jansma, 2010; Weigle, 2002).

The second assumption is an assumption based on the activities on which the rater training procedure is reliant t is assumed that students' work can be accurately and reliably marked (Bloxham, 2009). As mentioned above, the standards aspired to in rater training are socily constructed and may be 'relative, provisional and contested' (Or, 2007, 647). The challenges associated with conveying these standards continue to be well documented, alongside the assumption: If they are written clearly enough by academic staf, standards wil necessrily be understood by others' (Ajjawi et al., 2019). However, this assumption that raters who undergo rater training wil be able to mark to the agreed upon standard is contested. Saunders (2011) recognises the danger in assuming that .. people on the ground wil act in logical' ways to achieve well-undersood goals (2). Raters may not continue to mark to the agreed upon standard after rater training. Rater training relies on these two assumptions.

4.3. The guiding principles of rater training (inputs, activities, outputs and outcomes)

Alongside the Logic Model, a departmental Rater Training Handbook was developed based on Council of Europe Test Development Manual (CoE, 2011) and feedback from Module Conveners and Assessment Team member. The handbook outlines the rater training procedure and the guiding principles behind it and is available to allstakeholders. The characteristics of good practce are explained and the recommended steps in the rater training procedure are outlined. The role of feedack in the learning proces, and how this might be included to enhance student learning, i also considered. In summary, the guiding principles of the online rater training procedure define it as a supportive procedure, with f2f meetigs where possible, and online forums for post-training and moderation. shared understanding of the standards is developed through a bottom-up collborative discussion around the various sample performances made avalableto raters prior to the training, where the Assessment Team and Conveners work closely with the raters with the opportunity for raters to discuss their rating decisions with other raters.

The inputs in the Logic Model are listed as: the marking scales, sample scripts (with marks and feedback), the training manual, which outlines the rater training procedure, and time. In the Logic Model, activities are identified that have been shown by prior evaluations to be predictive of the desired outcomes (Armstrong & Barsion, 2006). The activities in the Logic Model outline the processes implemented with the inputs above to fulfil the aim of the rater training. The following activities were discussed in the interviews:

1. ensure raters are familiar with the marking scale;   
2. offer a guided discussion of marking a sample;   
3. have raters work in a group to mark a sample, followed by comparison with the marks allocated by the benchmarking team;   
4. discuss reasons for discrepancies;   
5. have raters work in pairs to mark a sample, followed by comparison with the marks allocated by the benchmarking team   
6. discuss reasons for discrepancies;   
7. have raters work independently to mark several samples.

A further activity mentioned by the stakeholders was feedback, which can be included in discussion and marking. The outputs can be summarised: 1) updated training manual derived from the Logic Model; 2) agreement on a standard; and 3) trained participants. It was decided to include the updates to the training manual in outputs although this manual may not be updated following every training sesson. The outcomes, which focus on the beefits for participants during and after the activities are divided into short-term, medium-term and long-term. The short-term outcomes, measurable during or soon afer the process ends, are an increased awareness of the guiding principles and a greater capacity to mark reliably. Medium-term outcomes, changes in behaviour that resul from new knowledge, may be an increase in confidence and positive washback in teaching. It is rlevant to note that unintended outcomes may also result here; for example, raters may become les confident f they are not marking within the expected standards. This is discussed in Stage 4. A long-term outcome may be improved job satisfaction and staf potentially staying in their jos for longer as a result.

# 4.4. A measurement plan

The interviewee ent their Logic Model drafts and notes to the evaluator to assist in the evaluation. An example of a Logic Model from a stakeholder can be seen in Appendix C. The short-term outcomes had to first be achieved in order to set the stage for mediumterm or long-term outcomes; for example, in order for raters' confidence to improve, they must first be able to mark to the specified standard. If a rater is not able to mark to the pecified standard, then their confidence is unlikely to improve. In this evaluation, two short-term outcomes and two intermediate outcomes that most clearly reflect the mission of the rater training procedure are considered.

According to this research, there are two main reasons performance may be measured: accountability, or communicating the value to others, and improvement. Both of these reasons are valid in this ealuation, which ams to communicate the value of rater training to stakeholders while also improving the procedure for all stakeholders. The interviewees recognised the efectiveness of rater training can be measured in two ways: 1) quantitively; for example, by using MS Excel or Facets to measure rater reliability pre- and posttraining, and 2) qualitatively; for example, by interviewing stakeholders or by testing stakeholders' understanding of the guiding principles pre- and post-training.

In this Logic Model, for each outcome, a measurement indicator, represented by a star in Fig. 2 , provides evidence related to whether the outcome was achieved. To create a measurement indicator, the following steps were followed: 1) define the specific observable, measurable characteristic that represents achievement of the outcome (e.g. a rater's marks) and 2) identify the specific statistic, such as number and percentage of participants attaining the outcome (e.g. rater correlation or $g \mathrm { . }$ -coefficients); these steps can be used to set baselines and targets and recognise achievement (Armstrong & Barsion, 2006, 484). For example, to check whether raters have a greater capacity to mark reliably afer rater training, pre-training marks can be measured against the marks awarded by the benchmarking team, then the post-training marks can be collated and measured. The results can be correlated and the co-eficient of correlation denotes the reliabilit of the test. Rater reliability can also be estimated by subjecting a selected sample of returned scripts to econd marking by the benchmarking team (cf. Shaw, 2004, 5) or by including pre-marked scripts inarater's alocation and observing how closely their marks agree (CoE, 2011, 43). To measure knowledge of guiding principles and confidence, qualitative research in the form of interviews or surveys may be more appropriate.

# 4.5. Unintended outcomes

As mentioned earlier, the negative outcomes may not be explicit in a theory trying to make a positive change. However, the open. ended question in the surveys and the request to report unexpected results in the interviews made some unintended outcomes of rater training explicit. Displacement may be recognised as an unintended outcome of rater training, where raters use the sessions and the feedback loop as a sounding board for grievances unrelated to the procedure. For example, according to Interviewee 5, raters use the rater training sesion as an opportunty to air grievances that have nothing todo with rating as they may be unhappy with other aspects of their work/life. This is also apparent in the survey where respondents refer to ssues unrelated to rater training (e.g. annual leae, teaching workload). By recognising and discussing potential problems, the unintended effects may be manageable.

# 4.6. The final model

Due to multiple sessions with the stakeholders, there were a number of unique versions of the Logic Model. Fig. 2 serves as the finalised version. There was consensus among stakeholders that the model accurately decribes the procedure and its intended results

# 5. Recommendations

Based on the process of developing the Logic Model above, the recommendations to improve rater training and encourage stakeholder buy-in are as follows:

# 5.1. Open communication

The first recommendation is related to communication between stakeholders, giving raters and rater trainers the opportunity to share their views and be involved in the various steps of the rater training proces. Developing the Model opened lines of communication and encouraged stakeholders to share their views on each step of the rater training procedure. Through sharing experiences and greater stakeholder involvement during rater training development, current or conflicting practices were clarified and potential components noted by stakeholders as absent were added.

Through developing the Logic Model, it was recognised that communication among raters was integral to the training proces. Raters valued maintaining standards through collaborative marking, and agreed unambiguous terminology should be used. One respondent explained: I liked the group marking procedure .. when markers were assigned small groups with a marking coordinator Who double marked some scripts and gave eedback' (F2). F24 requested: Let the staff mark together . o that we get better feel of Why a certain mark is given and can consult about problemati scripts. As a result, marking pods, where raters mark and communicate in small groups, could be reintroduced. It was apparent in the survey results and the interviews that there was much variation in terminology. In a department of 90 EAP teachers, it needs to be ensured that the language used is unambiguous. For example, F11 requests carity I think that the rater traning processhould be made very clearto tutors. This makes things more transparent but also gives people more confidence in benchmarks, etc. As a result, terminology can be outlined alongside the guiding principles in the training manual.

To ensure effctive communication, evaluation should be ongoing; key stakeholders should continue to discuss and update the Model based on learning and feedback. The feedback oop included in the Logic Model ensures communication between all stake. holders will continue to improve. The EAP Conveners were involved in this evaluation, and the EAP modules wil implement this process, but there are $^ { 2 0 + }$ other modules in the department not involved in the evaluation. The training manual and training sessions can be made available to all Module Conveners, and potentially shared across the university.

# 5.2. Transparent guiding principles

A further recommendation is to ensure guiding principles are explicit, recognising stakeholders are more likely to react positively to a proces when they are clear on why it is organised and implemented in a certain way. Raters should be aware of how the training materials are prepared. Interviewee 13 explains: the prep for the sessin is huge; the raters talk for an hour and  half, but if you think about it in terms of the whole course, it is not a lot of time to share ideas with your olleagues'. Interviewee I5 reiterates:  think for many of our staff members, particularly those that haven't been convening .. a module or who haven't ben part of the Asesment Team, I don't think they understand how much work goes into even just building a standardisation session.'

In discussion with the stakeholders who run the training, it was made clear that raters may not be aware of the guiding principles of the rater training procedure. This was recognised by F6: I think we lack clear guiding principles. It may be argued that by making the guiding principles explicit in a manual, presenting on what the Assessment Team does in Induction and in Professional Development sessions, and informing raters of the guiding principles of rater training, stakeholder buy-in may increase.

This evaluation highlights that rater training should feature throughout the academic year, beginning with an introductory session in Induction. Interviewe stressed the importance of viewing rar traning as a process rather than a ession that happens in the final weeks of the semester. Thisis also recognised in the surveys: I think we need more tandardisation throughout a module, not just as something that happens towards the end' (F14); Standardisation seems to be seen as something that just happens at the end of a course' (F20). As a result, in Induction, a session on rater training will be included to introduce the procedure and its guiding principles.

# 5.3. Language Assessment Literacy

The final recommendation is related to LAL. This research supports the claim that the professional background of many language testing professionals is in languagetaching rather than in statistics. Asthe mesurement indictors explicit in the Logic Model for rater training tend to be reliant on a basic knowledge of statistics and assciated software, i is suggested that Assessment Team members and Module Conveners managing rater training should have some basic skill in this aea to measure and promote rater reliability and ensure finess in marking. Of the participants, in the dicussion on how to mesure the effctiveness frater training, sttistics were mentioned by Assesment Team members. Upon further questioning, the Module Conveners and teachers revealed they had littl or no training in ths area. The language testing professinals working on the Assessment Team can work with the other stakeholders to increase their knowledge in this area.

In terms of the short-term outcome, greater capacity to mark reliably, outlined in the Logic Model, the ways in which this can be measured require some statistical knowledge. For example, as outlined above in the measurement plan, measuring inter- and intrarater reliability, acorrelation coeficient is a common measure, which requires a knowledge of statistics. If a rater wishes to measure the efectivenes of raters pre- and post-training, a statistical tool, such as Facets, would commonly be used. Rater trainers would benefit from some knowledge and skils in statistics to measure the extent of rater variability and whether it flls outside of what is deemed acceptable by the asssment provider. Some level of monitoring and evaluation of raters, using correlations and reliability analyses, is recommended to improve rater training or demonstrate with evidence that the training needs to be improved.

# 6. Conclusion

The purpose of this evaluative research was to use a Logic Model to evaluate rater training procedure for EAP written assessment within a university department to encourage stakeholder buy-in to the procedure and ensure rater training, and the rationale behind it is transparent and cogent to the various stakeholders. The final products are a Logic Model that reveals the guiding principles of rater training in the department, text that describes the evaluation proces, and a measurement plan. With this information, recommendations were developed to improve the procedure and encourage stakeholder buy-in. This paper offrs considerations for EAP pratitioners, managers, and tesing staff when developing or working with rater training, bridging the gap between the EAP and the language testing and assessment communitie. It is recognised that EAP assessment theory tends to be developed by language testers rather than EAP practitioners (Schmitt  Hamp-Lyons, 2015), whereas this evaluative research is carried out in EAP programmes by an evaluator integrated in EAP and the language testing and asessment communities. In an era in which accountability in assessment is encouraged, the continuous evaluation of rater training can guide communication between stakeholders and bridge the gap between the EAP and the language testing and asessment communitie. To quote Weis (198b), We cannot leave the procesof evaluation utilization to chance or regard it solely as an in-house conversation among colleagues' (32).

A limitation of this evaluative research was the sample size. Only $4 4 \%$ of the raters participated in the research. This low response rate may have been due the urvey being sent out close to annual leave dates. Although allstakeholders were given the opportunity to be involved in the evaluation, a future study may run Logic Model workshops for al raer nd involve a higher percentage of raters in the development of the Model from the onset. A further limitation may be the dual role of the evaluator as manager and collague in the department and evaluator. This limitation was somewhat mitigated by ensuring the interviewees were satisfied with the inter pretation of the results and the Logic Model. The Logic Model approach could also be applied to the rating scales development procedure and item-writing development to ensure al stakeholders involved in the procedures understand the guiding principles and the limitations of these procedures. Future evaluative research could examine the unintended outcomes of rater training, for example, deadweight, describing outcomes that may have happened regardes of the procedure, or substitution, where one group's needs are prioritised over another group's needs. Measuring the LAL of the raters, the Module Conveners and the Ssessment Team members may also be an area of further research.

# Author statement

We the undersigned declare that this manuscript i original, has not been published before and is not currently being considered for publication elsewhere. We confirm that the manuscript has been read and approved by all named authors and that there are no other persons who satisfied the criteri for authorship but are not listed. We further confirm that the order of authors listed in the manuscript has been approved by allof us. We understand that the Corresponding Author is the solecontact for the Editorial process. He/she is responsible for communicating with the other authors about progress, submissions of revisions and final approval of proofs.

Declaration of competing interest

None.

# Appendix A. Supplementary data

Supplementary data to this article can be found online at https:/doi.org/10.1016/j.jeap.2022.101160.

# References

Ajawi R   d 019 g stddi petinty   d i t ch Education. https://doi.org/10.1080/13562517.2019.1678579   
Andrae, ..1)      .   io 6 173. /0.1007 s10755-010-9169-1   
Armstrong, . in,  . 206). g  o--od ach  t  fa dt prr or medr. adc Medicine, 81, 483-488. https://doi.org/10.1097/01.acm.0000222259.62890.71   
Bachman, L. F., & Palmer, A. (1996). Language testing in practice. Oxford: Oxford University Press.   
Ber 010  f    t eng , 3-53 https://doi.org/10.1016/j.asw.2010.06.002 doi.0rg/10.1080/15434303.2011.637262 19(1), 63-83. Retrieved from: https://journals.lib.unb.ca/index.php/CJAL/article/view/23033.   
Barkaoui,  2010)   y rar aio criria chnge wh expie m-mt, ctia sd.  Qly, 41), 31-5.   
Bassey, M. (1999). Case study research in educational settings. Oxford: Oxford University Press.   
B. 220    /0) org/10.3389/feduc.2018.00096   
m  00   . g  09-.s/ doi.org/10.1080/02602930801955978   
ham . yd .012). biit n gg td wk  mi dd  a w-rt y ab conxt. iih Educational Research Journal, 38, 615-634. https://doi.org/10.1080/01411926.2011.569007   
Blom, ,  014    ri   6) 655-670.   
Blom, , -tr,  n, J  ric016. s sh  se marg g t  i f  crieia Assessment & Evaluation in Higher Education, 41, 466-481. https:/doi.org/10.1080/02602938.2015.1024607   
Brow,012 r a  r  .)Th f. 32.   
Brow, J. 00    d        qu Council of Europe.   
Chd     ig (1), 32-41. https://doi.org/10.36892/ijlts.vli1.14   
C (2011 t manual-for-language-test-development-and-examining-for-use-with-the-ce/1680667a2b. (Accessed 3 October 2020).   
CoE. (2021). li s ad xg ailble at htp//.t//nemke-/-s examining. (Accessed 2 September 2020) Accessed:.   
Conm  e, . 20). g- t a   .) o o f s  thp 457-471). New York: Springer.   
Degrs,   9 t t i   3-36/ doi.org/10.1177/0265532219826390   
Eckes, T. (2008. Rater types in witing perfomance assments: A clsfication aroch to raer variabilit. Lguge esting 25(2), 155-185.   
Eckes, T. (2015). Introduction to many-facet Rasch measurement. Frankfurt: Peter Lang Edition.   
Elder, C., Bkize G, Knch, , on Rndw, . (207). ain rar reonses to n onine ratr trnng prr. ge eting 24(1), 37-64.   
Engelhard, G. (2002). Monitoring raters in performan ments. In G. Tindal, & T. M. Haladyna (Eds.), Large-scale assessment programs for all students: Validity, technical adequacy, and implementation (pp. 261-288). New York: Routledge.   
uan y0r ert    ioi  fa h  isi 1, 1. https://doi.org/10.1186/s40468-020-0098-3   
Erlam ,  . d  013). tig i rar     ps.  in  i , 1) 1-29. Retrieved from: https://arts.unimelb.edu.au/_data/assets/pdf_file/0007/1771342/2-erlam-et-al.pdf.   
Farrokh,  i, 011 h dt f i  f ra.T  ctic  e, (11) 1531-1540.   
Fes R -e   t  d, ., , . 2015    y in for student-centered pedagogy and asessment in STEM foundation course. Assesment Update, 27(2) https://di.org/10.1002/AU.30014   
Fox, J. D. (2003). From products to process: An ecological aproach to bias detection. International Jounal of Testing, 3(1), 21-47.   
Fulhr . 02 r  th  2, 132 /0330.01.64201   
ay, .  . 96. San Francisco: The Jossey-Bass Higher and Adult Education Series.   
Gugiu, P.. -m,  (007m-d ie prl for otin ic d. io d P Plng 30), 339350. https://doi.org/10.1016/j.evalprogplan.2007.08.004 Analytics. (16). https://doi.org/10.1186/s40468-019-0092-9   
Hmion , el, ,   (201). rs pf -i rar tn  m t 29 5-520./.01016/0346- 251X(01)00036-7   
Hamp-Lyons, L. (2007). Worrying about rating. Assessing Writing, 12, 1-9.   
Hayes, H S    cca, Stt Wna016 ive mi- ph  tg ion and Program Planning, 58, 199-207. https://doi.org/10.1016/j.evalprogplan.2016.06.012   
den Heyer, M. (2002). Modeling learning programs. Development inPractice 12(3/4), 525-530. htps://doi.org/10.1080/0961450220149861   
Hdon, .   e, Pri 017.  at  d  t    r Education, 42, 1309-1323. https:/doi.org/10.1080/03075079.2015.1092130   
ELS. 1).   /-ir 2021) Accessed:.   
Inbar-Lourie . (2008). onstructig a Language Aesment knowledge base:  fous on lnguageasement cores. nguage sting, 25(3), 385-402.   
Inbar-Lourie, O. (2013). Language assessment literacy. In C. A. Chapelle (Ed.), The encyclopaedia of applied linguistics (pp. 2923-2931). Oxford: Blackwell.   
Jackl,   , f,  d 07 ak  he  re  th gr i d. Available at ttps://www.heacademy.ac.uk/knowledge-hub/assessment-and-feedback-higher-education-1. (Acesed 3 October 2020).   
Kang, O. Rbin ., & Kad, 2019. h effec f tg d rat ifres o l pficiey aset. nge ting 36(4), 481-504.   
Kaplan, S. A., & Garrtt K. E. (2005). The use f Logic Models by community-base intiatives. Ealuatin and Pogram Planing, 28, 167-172.   
Kelog,  004). oic dllment ude alle at wkfgl e-tr/e/206/2/-dtion-ic-de Develop- ment-Guide.aspx Accessed: 20 Sept 2020.   
Kirkpatrick, D. L., & Kirkpatrick, J D. (209). Evalatin training rograms: Th for lel (3rd ed.). Oakland, CA: Berrtt-Koehler Publishers.   
Knight P. (2002. tive me in igher ci: Prcie in drray.e in Highr io, 27(, 275-286. t//o.g/10.1080/ 03075070220000662   
nch, U   (2016 aif ai rar t or the k awing u-s f tts et. Pr n Language Testing and Assessment, 5(1), 90-106.   
Knoch, U., & Huisman, A. (2014). Review of the British Council Aptis rater taining for new markers. Melbourne: University of Melbourne.   
Knoch, U d, ., n Rw  207).n wtin i  d t cm wth fa-asn 2(1, 2643. Publications. 1-23.   
rel, g  200  ampreiria   a tery asar p the language assessment literacy survey. Language Assesment Quarterly, 17(1), 100-120. https://doi.org/10.1080/15434303.2019.1674855   
Lederman, .2010). h ay Rle i smet aabl at h/.idghe.om/n/2010/05/28/c-lsmet. eed1 October 2020) Accessed:.   
Lim G..2011). The deme and mannnce f rat qualit n poae wiing amet  ongdl study f nw and expid rar. Language Testing, 28(4), 543-560. https://doi.org/10.1177/0265532211406422   
Lowie, W.  J. Ja 010 g thFR  he  d   . P d o Sciences, 3, 152-161. https:/doi.org/10.1016/j.sbspro.2010.07.027   
Lumley, T. (2002). Asessment criteria in a large-scale writig test: What do they rell mea to the raters?Languag Testing, 19(3), 246-276.   
cLaughlin, J., & Jordn, G B. (199. Lgic model: tlfr teing your proram's pomne stry. tio nd Prom Plng 221), 65-72.   
cagh   . 015).   d. r,   . .)  i p pp. 62-87). New Jersey: Wiley.   
camara, , Kch, . (2012). The Rch war heemece f Rach meement in langage teting. gge eting 294), 55-576. htps./oi.org. 10.1177/0265532211430367   
camara 11. s ut i  g   Esha nteli h te. esmen Quarterly, 8, 161-178.   
Meadows, M., & Billington, L. (2005). A review of the literature on marking reliability. London: National Assessment Agency.   
Merriam, S. B. (2009). Qualitative research: A guide to design and implementation. San Francisco: Jossey-Bass.   
Mullaney, . P. 2018.What we tlk about when e alk aout flty buy in. Aailble at hs://w.dmirieg.om/lrhip/skil-anddevelopment/talk-talk-faculty-buy/. (Accessed 4 October 2020) Accessed:.   
Myford, C. M  Wlf, . 203. tin a meri ratr efts usi may-aet h meme. Jl ofApld nt, 52, 89-23.   
National Health Service. (2016. Your guide to using logic models. Midlands and Lancashre commissioning support unit. Availble at: htps:/ww. midlandsandlancashirecsu.nhs.uk/. (Accessed 4 October 2020).   
n   L oe013cm   d P 36 88-96.   
oan Pr00    d rir n 9, 325-335. https://doi.org/10.1080/1356251042000216642   
Or, . (207. rt ig the m d tin th t  o  ge i 6, 45656 / doi.0rg/10.1080/02602930601117068   
Popha . 009.   c h  m  ic 41),1. //1008005753   
Price 005.  e o f tie  f a   g, 03) 215-230.   
Renger, R., & Titcomb, A. (2002). A three-step approach to teaching Logic Models. American Jounal of Evaluation, 23(4), 493-503.   
Reynolds,  &d, . 013. tch t, , mig, a i  MC Health Services Research, 13, 168. https://doi.org/10.1186/1472-6963-13-168   
Rogers, P. J. (2008). Using programme theory to evaluate complicated and complex aspects of interventions. Evaluation, 14(1), 29-48.   
Sader, DR 209. Ga inty ad th rion of a ahmt  i Hgher io 34, 807-26. ts/.g/10.100/ 03075070802706553   
Sdr11.    i  me education: The practice turn (pp. 1-18). Maidenhead: McGraw Hill Education.   
Schaefer,  (2008. Rtr ia atte in a  wing a.  ting 5(4), 465-493. /.rg/0.1177/026553208094273   
Schmitt D., & Hamp-Lyons, L. (2015). The need for EAP teacher knowledge in asessment. Joundl of English for Academic Puposes, 18, 3-8.   
Schoonen, R. (2005). Generalizability of writin cores: An application of structural equation modelling. Language Testing, 22(1), 1-30.   
Shaw, S. D. (2004). IELTS writing: Revising assessment criteria and scales (phase 3). Research Notes, 16, 3-7.   
Sheman, .D.016) i A d  L de for quat ara    ndt nirt pmtion d Pgm Planning, 55, 112-119.   
Shi, L. (2001). Native- and non-native-speaking EFL teachers evaluatio of Chinese students English witing. Language eting 18, 303-325.   
Sue, V. M., & Rittr, L. A. (2012). Conductig oline surveys (2nd ed.). Thousand Oaks, CA: Sage. htps://doi.og/10.4135/9781506335186   
Su 14h 3. Available at: http://www.aabri.com/manuscripts/121371.pdf. (Accessed 30 September 2020).   
Taylr, . 2013.tin th thy, rticed rci of  tg to s staks e etio.  etg 0(3, 403-412. https://doi.org/10.1177/0265532213480338   
Taylr-w,   et,  0 d  d  a  /.x/dts/201/ 03/Logic Model guidecomplete.pdf. (Accessed 4 October 2020).   
ger 18           2) 118-137.   
Trer,  s, .202).  ri r t  ff t  mker dth  e on an an tut scores. Tesol Quarterly, 36, 49-70.   
Weigle, S. C. (2002). Assessing writing. Cambridge: Cambridge University Press.   
Weir, C. J. (2005). Language testing and validation: An evidence-based approach. New York: Palgrave MacMillan.   
Weiss, C. H. (1997). How can theory-based evaluation make greater headway? Evaluation Review, 21, 501-524.   
Weiss, C. H. (1998a). Evaluation: Methods for studying programs and policies (2nd ed.). New York: Prentice Hall.   
Weiss, C. H. (1998b). Have we learned anything new about the use of evaluation? American Journal of Evaluation, 19, 21-33. 10.1016/j.asw.2019.100416 for Academic Purposes, 49. https:/doi.org/10.1016/j.jeap.2020.100948