# Writing a successful applied linguistics conference abstract: The relationship between stylistic and linguistic features and raters’ evaluations

Matt Kessler a,\* , Caitlin Cornell b , J. Elliott Casal c , Detong Xia d

a University of South Florida, USA b Michigan State University, USA c University of Memphis, USA d Xi’an Jiaotong-Liverpool University, China

# A R T I C L E I N F O

Handling Editor: Guangwei Hu

Keywords:   
Academic writing   
Assessment   
Conference abstracts   
Complexity   
Corpus   
Rhetorical moves

# A B S T R A C T

The conference abstract (CA) is a promotional genre that is vital to academic success. However, composing a high-quality abstract can be challenging for both first (L1) and second language (L2) writers. Some researchers have performed contrastive analyses of accepted/rejected CAs, yet few studies have analyzed the extent to which different stylistic and linguistic features can predict reviewers’ scores. The current study furthers this line of inquiry, while also responding to scholars’ recent calls for more replication work in the fields of applied linguistics and second language acquisition. Using a corpus of 304 abstracts from an applied linguistics conference, the current study is an approximate replication that analyzes the extent to which 32 variables are predictive of CA raters’ evaluations. Data analyses consisted of multiple stages, including examining the relationships between CA scores and nine stylistic variables (e.g., rhetorical moves, study completeness) and 23 linguistic variables (e.g., grammatical errors, and lexical and syntactic complexity measures). Statistically significant variables were then entered into a regression model. Results suggest that seven variables accounted for approximately 25 percent of CA scores. The pedagogical implications of these findings are discussed for L1/L2 writers, along with future research directions.

# 1. Introduction

Academic conference abstracts play an important role in disciplinary activities. They are generally the primary means by which an academic paper is accepted or rejected for inclusion in a conference (Payant & Hardy, 2016; Samar et al., 2014). Academic conferences are also important spaces for researchers to receive feedback, disseminate their work, and engage in professional networking activities (Borg, 2014; Simon-Maeda, 2016). In this sense, conference abstracts can be rather high stakes writing tasks that contribute to successful conference experiences and professionalization. Furthermore, even after a paper is accepted, many conferences distribute abstracts to assist conference attendees in deciding which talks to attend. Thus, conference abstracts have a strong promotional component not only in selling a project from a scholarly perspective, but also in making it interesting. Given its importance as a genre, conference abstracts have received considerable attention as a distinct and important genre practice from an English for Academic Purposes (EAP) perspective. Notably, the EAP perspective aims to inform pedagogy by providing research-based instruction that develops first (L1) and second language (L2) writers’ knowledge of a given genre’s audience, rhetorical moves, lexico-grammar, and multimodal features that may be integral to its use (Gray, 2021; Kessler & Casal, 2024).

Research on conference abstract writing has widely adopted both rhetorical and broad structural perspectives (e.g., Faber, 1996; Halleck & Connor, 2006; Payant & Hardy, 2016; Samar et al., 2014; Stein, 1997; Yoon & Casal, 2020), as well as more linguistically defined or integrated approaches (e.g., Cutting, 2012; Kaplan et al., 1994; Simon-Maeda, 2016). Among these analyses, two important comparative trends emerge. One such trend is cross-linguistic or cross-cultural comparisons (e.g., Martín & Burgess, 2023), while the other is the investigation of textual elements of conference abstracts in relation to acceptance/rejection (e.g., Berkenkotter & Huckin, 1995; Faber, 1996; Halleck & Connor, 2006; Kaplan et al., 1994; Stein, 1997). Of these, Egbert and Plonsky (2015) stands out methodologically for adopting a wide range of textual features and examining their impact “individually and collectively” (p. 296) on conference abstract raters’ scores through t-tests, correlations, and regression analysis, rather than on categorical accept/reject variables. Notably, their study also suggested important pedagogical insights for conference abstract writers.

Responding to growing calls for replication research in corpus-based inquiries (e.g., Baker & Egbert, 2016), in EAP research (e.g., Omidian et al., 2023), and in Applied Linguistics more broadly (see Porte, 2012; Porte & McManus, 2019), the present study is an approximate replication of Egbert and Plonsky (2015). Their study analyzed 21 textual features in 287 abstracts (accepted and rejected) from the 2009 Second Language Research Forum in relation to reviewer scores. Similarly, in this study, we analyze 32 textual features in a corpus of 304 abstracts that were submitted to the same conference 10 years later. Thus, the present study represents two important contributions. First, we have updated the measures by adjusting some binary variables to continuous variables and adding syntactic and lexical complexity indices that have gained prominence since their initial study was published. Second, we are able to broadly assess changes in reviewers’ evaluations a decade later.

# 2. Literature review

# 2.1. Writing a successful conference abstract

Numerous recent analyses of conference abstracts provide descriptive rhetorical profiles (e.g., Payant & Hardy, 2016; Yoon & Casal, 2020), often with a single conference in focus (e.g., the American Association of Applied Linguistics Conference; AAAL). The results point toward broad alignment with the major research article part-genres (i.e., Introduction-Methods-Results-Discussion; IMRD). Cross-culturally, some studies highlight differences in how results and implications are presented (e.g., Martín & Burgess, 2023; Samar et al., 2014), with English language abstracts generally adopting more direct presentations of findings and their implications or significance.

The importance of including and discussing results in English language conference abstracts resonates with the findings of Cutting (2012). Cutting conducted a unique and insightful analysis of the use of vague language (e.g., general nouns) and study completeness in 106 abstracts across two English language conferences in Applied Linguistics. ‘Completeness’ was self-reported through a three-question author survey (completeness in writing, analysis, and data collection) on a scale of 1–5 (with 5 being complete). While Cutting’s findings reflect the practices of just two conferences, the analysis uncovers both a willingness among some authors to openly reference the ongoing nature of their research, as well as some authors’ recurring strategies to use vague language to disguise study incompleteness. Cutting’s study suggests that conference abstract writers can be successful even without findings to present, but it is important to note that Cutting did not collect unsuccessful abstracts to allow for deeper analysis of this practice in relation to acceptance or reviewer scores.

Many studies of conference abstracts do provide contrastive analyses of accepted and rejected proposals. Halleck and Connor (2006) found overall length and rhetorical move inclusion rates to distinguish accepted and rejected abstracts of the 1996 TESOL convention. In another study, Kaplan et al. (1994) targeted nearly 300 abstracts from the AAAL Conference from a structural and linguistic perspective. Findings indicate that approximately $4 0 \%$ of accepted abstracts did not include methodological descriptions, and approximately $3 5 \%$ of accepted abstracts did not include results (supporting Cutting’s findings). Conversely, Yoon and Casal (2020) found that in the 2017 AAAL Conference, nearly $9 6 \%$ included methods and $9 0 \%$ included results. Thus, their study underscores the importance of revisiting reviewer assessments periodically in genre-based scholarship, as genre practices and reviewer/rater evaluations may change over time.

# 2.2. Egbert and Plonsky (2015) and the relationship between textual features and reviewers’ scores

In a step forward analytically from previous studies of accepted vs. rejected conference abstracts, Egbert and Plonsky (2015) analyzed numerous textual features of conference abstracts from the Second Language Research Forum (SLRF) 2009 – a mid-sized Applied Linguistics conference organized by graduate students and focused on SLA research – in relation to the overall score those abstracts received when rated by a pool of conference reviewers. Their study included eight categorical stylistic features based on previous conference abstract scholarship: 1) the presence or absence of an Introduction section, 2) Methods, 3) Results, and 4) Discussion; 5) following IMRD order; 6) presence or absence of research questions or hypotheses; 7) presence or absence of citations; and 8) presence or absence of errors (spelling and/or grammatical). The authors also included 13 continuous variables based on past scholarship, including developments in corpus-based linguistic complexity research that highlight the importance of noun-phrase density and elaboration in academic writing (e.g., Biber & Gray, 2010). These included 1) number of present tense verbs, 2)

past-tense verbs, 3) modals of prediction, 4) first-person pronouns, 5) all personal pronouns, 6) nouns, 7) adjectives, 8) prepositions (all normalized to occurrences per 1,000 words), as well as 9) Introduction section length, 10) Methods length, 11) Results length, 12) Discussion length, and 13) total length (of the entire abstract in terms of word count).

Egbert and Plonsky (2015) then used independent samples t-tests (to compare presence vs. absence of features) and Pearson r correlations (for continuous variables) to examine the relationships between these variables and the abstract scores. Four stylistic variables were found to be significant, along with nine linguistic variables. Afterwards, the researchers entered all 13 statistically significant variables into an exploratory stepwise regression model to estimate how much the variables could account for the abstract scores. Six variables were found to be significant predictors of the model, accounting for 31 percent of the scores abstracts received. These included: total length, presence of citations, presence of a Results section, more nouns, absence of errors, and fewer first-person pronouns.

The findings of this study provide useful insights for conference abstract writers. First, while Cutting (2012) provided examples of what vagueness and success of incomplete studies can look like in conference abstracts, Egbert and Plonsky (2015) suggest that complete (or at least partially complete) studies perform better in scoring (with the inclusion of a Results section). The second is the avoidance of a personal tone, as both types of pronouns were negatively correlated with ratings. The third was that total abstract length was the best linguistic predictor of ratings $\textstyle ( R ^ { 2 } = 0 . 1 4 )$ ), supporting Halleck and Connor’s (2006) findings. Citations and avoidance of errors were also important components of success. Taken together, these results provide a clear basis for an EAP approach to teaching conference abstracts by unifying important textual features of previous conference abstract research into a single study that examines the importance of such features for reviewers’ ratings individually and in tandem.

Although Egbert and Plonsky’s (2015) study provides valuable insights, it is in need of replication for multiple reasons. For instance, recent scholarship on conference abstracts has highlighted how rapidly the genre practice can change, even within a particular conference community (see Kaplan et al., 1994 and Yoon & Casal, 2020 for comparison of Methods and Results section inclusion rates). In addition, since Egbert and Plonsky’s results have important pedagogical implications for EAP pedagogy, such findings should be revisited and reviewed using a similar corpus as a means of verifying the extent to which these findings still hold true. Thus, with a growing interest in replication research in both EAP and Applied Linguistics more broadly, a replication of how conference abstracts’ textual features relate to reviewers’ ratings is a worthwhile endeavor.

# 2.3. The current study

The current study is an approximate replication of Egbert and Plonsky (2015). In an approximate replication, a study’s methods are duplicated as closely as possible with only some variables being altered (see Porte & McManus, 2019). In particular, we make use of an updated and expanded set of textual features (21 in the original study, and 32 in the current study), and we also use a comparable dataset of abstracts from the same conference 10 years later, following the same statistical procedures. All original stylistic and linguistic measures are retained, with some measures slightly modified (e.g., changing some categorical variables to continuous variables, which is further described in the Methods section). In addition, a number of rhetorical features were added, as well as syntactic and lexical complexity measures based on recent scholarship. In keeping with Egbert and Plonsky’s study, the research question (RQ) that guided the current study was:

RQ: To what extent can stylistic and linguistic variables predict the scores of abstracts submitted to an Applied Linguistics conference?

# 3. Method

In this section, information about the corpus of conference abstracts and data analyses are provided. Because this study is an approximate replication, the procedures are modeled largely after those in Egbert and Plonsky (2015). Similarly, our procedures involved four steps, including: 1) collecting the corpus, 2) manually coding the stylistic features, 3) automatically tagging the linguistic features, and 4) running the data analyses.

# 3.1. Corpus and abstract rating process

The corpus in this study is comprised of conference abstracts submitted to SLRF 2019. The conference theme was on the topic of Advancing Transdisciplinary Research in SLA. Proposals for papers were invited that leveraged a diverse range of methods (i.e., quantitative, qualitative, or mixed methods) as well as topics (e.g., assessment, bilingualism, computer-assisted language learning, corpus, identity, ideology, instructed SLA, psycholinguistics, reading, writing, and more). Notably, the university’s Institutional Review Board where the study/conference took place determined that the study was exempt from review. Regardless, the first and second authors of this study – who served as the co-chairs of the 2019 SLRF conference planning committee – wrote consent language into the submission process, thereby giving researchers the opportunity to opt out of having their abstracts included in a future study.

Following Egbert and Plonsky (2015), only abstracts of empirical studies were included in the final sample. In total, the sample consisted of 304 conference abstracts, which included proposals that were ultimately accepted $( n = 1 8 7 )$ and rejected $( n = 1 1 7 )$ ). The 2019 SLRF call for papers set the word limit for abstract submissions at 350 words. Despite this, the lengths (i.e., total words) of the abstracts ranged widely from 133 (min) to 389 words (max). The mean abstract length was 320.9 words $S D = 4 3 . 6 \AA$ .

Once authors submitted their abstracts for the conference, all abstracts were rated by a reviewer pool of 146 academics. These 146 academics ranged from advanced PhD students to faculty in the field of Applied Linguistics from numerous universities who were experts in a variety of areas. Each abstract was first assigned to two reviewers who were familiar with the abstract’s topic area. Then, the reviewers used multiple categories to guide their ratings (i.e., importance of topic, theoretical orientation, research design, and organization/clarity). Based on these categories, reviewers issued a final recommendation and rating using a 7-point Likert-scale. This scale ranged from $^ { - 3 }$ to 3 ( $\mathbf { - } 3 =$ reject strongly recommended, $- 2 =$ reject recommended, $^ { - 1 = }$ reject somewhat recommended, $0 = a c c e p t$ only if space, $^ { 1 = }$ accept somewhat recommended, $2 =$ accept recommended, and $3 =$ accept strongly recommended). All reviewer ratings were then evaluated internally for discrepancies. A third rater was assigned in the event that two reviewers’ ratings were off by 2 or more and if those reviewers disagreed in terms of recommending accept/reject. A third rater was used in 59 of 304 abstracts $_ { 1 9 . 4 \% }$ of cases).

# 3.2. Manual coding of stylistic features

In total, all 304 abstracts were manually coded for nine stylistic features. Table 1 lists these features, along with how they were operationalized. Notably, Egbert and Plonsky (2015) included two additional stylistic variables, 1) citations and 2) errors, both of which were operationalized categorically (i.e., coded for their presence vs. absence). However, we transformed these two variables into continuous variables (i.e., total number of citations and total number of errors). Arguably, these two textual features could be placed into either the stylistic or linguistic category. For errors, in particular, given that literature has suggested that raters may be impacted by the number of errors present in a text (e.g., Kim & Kessler, 2022; Fritz & Ruegg, 2013), we opted to operationalize them continuously. The choice to operationalize citations continuously was exploratory in nature, primarily as a means of examining whether more (or fewer) citations impacted reviewers’ ratings. We note that although citations and errors appear in the next section on linguistic features (since they were analyzed using Pearson r correlations like the other linguistic variables), for replication purposes, we also analyzed them as categorical variables during data analysis.

As Table 1 shows, three new variables were added to our approximate replication, including: 1) states gap or provides motivation statement, 2) research design, and 3) completeness. These variables were added for different reasons. For instance, for the feature ‘states gap or provides motivation statement,’ research in EAP and Applied Linguistics has stressed the importance of authors motivating their work, by situating it within prior literature and attempting to fill a need (e.g., Swales, 1990, 2004). For ‘research design,’ research has suggested that quantitative designs have long been preferred in Applied Linguistics, and particularly within SLA; however, in recent years, there has been growing acceptance of qualitative and mixed methods work (see Nassaji, 2020). As such, we were interested in investigating the extent to which the design of the study impacted reviewers’ ratings, if at all. Finally, the variable of ‘completeness’ was also added. This decision was made based on prior work (e.g., Cutting, 2012), which has suggested that some authors feel pressure for their studies to be finished or partially finished, fearing that otherwise, reviewers might provide negative evaluations.

For the manual coding, three of the authors first tested the coding scheme described in Table 1. This involved practice coding a small set of abstracts collectively. Then, for reliability, the three authors independently coded $1 0 \%$ of the entire corpus. For replication purposes, we first assessed inter-coder reliability using simple agreement. Simple agreement was high for all manually coded variables $( 9 0 . 3 \%$ total), with the reliability of each feature being at or above $8 0 \%$ . In addition, we performed an inter-coder agreement test using Fleiss’ kappa to assess the reliability of the categorical variables in the study. Once again, the reliability among the three coders was deemed high $\mathbf { \tilde { \kappa } } = 0 . 8 7 )$ . After the inter-coder reliability checks, the three authors met to discuss any/all minor discrepancies before dividing the remaining abstracts to be coded independently.

Table 1 Stylistic features (all categorical variables).   

<html><body><table><tr><td>Stylistic feature</td><td>Brief description</td><td>Included in Egbert and. Plonsky (2015)</td></tr><tr><td>Introduction</td><td>: An introduction to the topic is provided, such as any background info about its importance or need for investigation</td><td>Yes</td></tr><tr><td>Methods</td><td>: Information is provided about the design and scope of the study, such as info about the participants, instruments, and data analyses</td><td>Yes</td></tr><tr><td>Results</td><td>. Findings from the study are stated, such as info about themes (from qualitative studies) or statistics (from quantitative studies)</td><td>Yes</td></tr><tr><td>Discussion</td><td>: A statement(s) is provided about what the findings might mean, or the implications of the study for theory, research, or pedagogy</td><td>Yes</td></tr><tr><td>Follows IMRD order</td><td>All sections described above (i.e., Intro, Methods, Results, Discussion) are present and in this order in the abstract</td><td>Yes</td></tr><tr><td>States RQ or hypothesis</td><td>: A research question (RQ) is explicitly stated, or a hypothesis is given about the potential outcome of the study</td><td>Yes</td></tr><tr><td>States gap or provides motivation statement</td><td>: A statement(s) is given in the Intro section about a gap or motivation for the study in terms of it filling a hole in the literature.</td><td>No</td></tr><tr><td>Research design</td><td>: Whether the study was a) quantitative, b) qualitative, or c) mixed methods</td><td>No</td></tr><tr><td>Completeness</td><td>: Whether the author indicated the study was fully or partially complete (coded as &quot;yes/ partial&quot;) or whether the study was &quot;unclear/not started&quot;</td><td>No</td></tr></table></body></html>

# 3.3. Automatic tagging of linguistic features

Following the manual coding of the stylistic features, the next step involved tagging the corpus for a variety of linguistic features. In total, 23 linguistic variables were included. Of these, 15 were replicated, and eight new syntactic and lexical measures were added. A summary of these features is shown in Table 2, along with general information regarding the tools used in identification.

Word count indices (i.e., section lengths and total words) and particular linguistic features (i.e., first-person pronouns, all personal pronouns, present tense verbs, past tense verbs, modals of prediction, and normalized adjective, noun, and preposition frequency) were all included in Egbert and Plonsky (2015). Word counts were assessed by the three coders, who highlighted the individual section lengths (e.g., Intro, Methods) in Google Docs and used that interface’s word count tool. Apart from word count indices, the other features were identified and counted using Nini’s (2019) freely available Multidimensional Analysis Tagger (MAT), which replicates Biber’s (1988) tagger that was used in the original study. MAT uses the Stanford Part of Speech Tagger (Toutanova et al., 2003), cited as having $9 6 . 8 6 \%$ accuracy (Kyle, 2016), to tag features; however, Nini’s reliability analysis of the tool foregrounds replication of Biber’s dimensions in the Brown and Lancaster-Oslo/Bergen, rather than the specific accuracy of individual linguistic items. Nonetheless, Nini reports large discrepancies only for adverbs, which are not used in the present study.

Five syntactic complexity measures were added to reflect recent research in register- and genre-based analysis of writing practices. Mean length of sentence is a global perspective on complexity; recent research has highlighted associations between sentence length and rhetorical functions, such as social science writers’ tendency to produce their longest sentences when stating research aims (Lu et al., 2020). This measure was calculated via the Tool for the Automatic Analysis of Syntactic Sophistication and Complexity 1.3.8 (TAASSC; Kyle, 2016), which recreates the automated complexity indices included in L2SCA (Lu, 2010) alongside other composite complexity indices, fine-grained complexity indices, and indices associated with usage-based perspectives on linguistics. Lu (2010) reports 1.00 correlation between computed sentence length and two human annotators’ coding of 30 texts.

Average number of modifiers per noun phrase builds on the adjective, noun, and preposition counts included in Egbert and Plonsky (2015) and Biber and Gray’s (2010) important observation that written academic discourse is characterized by elaborated and dense noun phrases. This measure allows for insights into how many modifiers are present in noun phrases, rather than the types of modifiers. Biber and Gray also identify clausal complexity as a feature of spoken registers. Therefore, two broad clausal measures, dependent clauses per clause and average number of clausal dependents, were included to target writing style and the general complexity of sentences. These three measures were all calculated using TAASSC 1.3.8 (Kyle, 2016). Lu (2010) reports 0.954 correlation between dependent clauses per sentence and two human annotators’ coding of 30 texts. Although TAASSC has been widely used in prior research, reliability information for many of the numerous individual measures is not available to our knowledge.

Table 2 Linguistic features (all continuous variables).   

<html><body><table><tr><td>Linguistic feature</td><td>Included in Egbert and Plonsky (2015)</td><td>Identification tools and methodsa</td></tr><tr><td colspan="3">Replicated measures</td></tr><tr><td>Intro length</td><td>Yes</td><td>: Word count tool (Google Docs)</td></tr><tr><td>Methods length</td><td>Yes</td><td>: Word count tool (Google Docs)</td></tr><tr><td>Results length</td><td>Yes</td><td>: Word count tool (Google Docs)</td></tr><tr><td>Discussion length</td><td>Yes</td><td>: Word count tool (Google Docs)</td></tr><tr><td>Total words</td><td>Yes</td><td>: Word count tool (Google Docs)</td></tr><tr><td>Number of citationsb</td><td>Yes</td><td>: Manually assessed.</td></tr><tr><td>Number of errorsb</td><td>Yes</td><td>: Manually assessed</td></tr><tr><td>First-person pronouns</td><td>Yes</td><td>: MAT (Nini, 2019)</td></tr><tr><td>All personal pronouns</td><td>Yes</td><td>: MAT (Nini, 2019)</td></tr><tr><td>Present tense verbs</td><td>Yes</td><td>: MAT (Nini, 2019)</td></tr><tr><td>Past tense verbs</td><td>Yes</td><td>: MAT (Nini, 2019)</td></tr><tr><td>Modals of prediction</td><td>Yes</td><td>: MAT (Nini, 2019)</td></tr><tr><td>Adjectives per 10,000 wordsc</td><td>Yes</td><td>: MAT (Nini, 2019)</td></tr><tr><td> Nouns per 10,000 wordsc</td><td>Yes</td><td>: MAT (Nini, 2019)</td></tr><tr><td> Prepositions per 10,000 wordsc</td><td>Yes</td><td>: MAT (Nini, 2019)</td></tr><tr><td colspan="3">Syntactic complexity measures</td></tr><tr><td>Mean length of sentence</td><td>No</td><td>: L2SCA (Lu, 2010) via TAASSC 1.3.8 (Kyle, 2016)</td></tr><tr><td>Dependent clauses per clause</td><td>No</td><td> L2SCA (Lu, 2010) via TAASSC 1.3.8 (Kyle, 2016)</td></tr><tr><td>Average number of modifiers in a noun</td><td>No</td><td>: TAASSC 1.3.8 (Kyle, 2016)</td></tr><tr><td>phrase Average number of clausal dependents</td><td>No</td><td>: TAASSC 1.3.8 (Kyle, 2016)</td></tr><tr><td>Nominalizations per 1,000 words</td><td>No</td><td>: Custom Python and manual analysis based on Biber et al. (1999) and Lu et al. (2020)</td></tr><tr><td colspan="3">Lexical complexity measures</td></tr><tr><td>COCA academic range</td><td>No</td><td> TAALES 2.2 (Kyle et al., 2018)</td></tr><tr><td>COCA academic frequency</td><td>No</td><td>: TAALES 2.2 (Kyle et al., 2018)</td></tr><tr><td>Lexical decision reaction time</td><td>No</td><td>: TAALES 2.2 (Kyle et al., 2018)</td></tr></table></body></html>

a MAT $=$ Multidimensional Analysis Tagger (Nini, 2019); L2SCA $=$ Second Language Syntactic Complexity Analyzer (Lu, 2010); TAASSC $=$ Tool for the Automatic Analysis of Syntactic Sophistication and Complexity (Kyle, 2016); TAALES $=$ Tool for the Automatic Analysis of Lexical Sophistication (Kyle et al., 2018). b As mentioned, both citations and errors were included in the original study. However, these two features were operationalized in the current study as continuous rather than as categorical variables. c Egbert and Plonsky (2015) assessed these three measures per 1000 words, but the MAT tool available for use in the current study normalized them per 10,000 words.

Additionally, nominalizations have long been associated with academic style, particularly in discussion of methodological processes (Ryshina-Pankova, 2015); their inclusion reflects this association and the importance of linguistically economical methodological description in word-count limited abstracts. Following procedures identified in Lu et al. (2020), a list of nominalization candidates was created via python scripts using five productive suffixes (-ity, -ment, -ness, -tion, -sion; Biber et al., 1999) and the 1025 entry Nomlex (Macleod et al., 2001) nominalization dictionary. The list was then reviewed by two of the researchers to remove false positives (e.g., city) and highly lexicalized items that occur abundantly in Applied Linguistics (e.g., education).

Included lexical measures all assess lexical sophistication, which “pertains to what makes lexical items (single- and multi-word units) difficult or challenging to acquire and use” (Vitta et al., 2023, p. 374). These measures were calculated using the Tool for the Automatic Analysis of Lexical Sophistication 2.2 (TAALES; Kyle et al., 2018). Word frequency has played an important role in lexical sophistication research historically, and word range (number of texts a word occurs in) accounts for words that occur abundantly in a small number of texts. We adopt “register-specific” (Kyle et al., 2018, p. 1032) versions of frequency and range measures that use the academic sub-corpus of the Corpus of Contemporary American English (COCA; Davies, 2008). Other perspectives on lexical sophistication include psycholinguistic perspectives concerned with word processing. One such measure is lexical-decision reaction time, which assesses how long speakers of a language take to determine if a word is real or not, with longer latencies suggesting more sophisticated lexical items (Kyle et al., 2018).

# 3.4. Data analyses

Following the original study, our data analyses occurred in three stages. This involved 1) examining the stylistic features, 2) examining the linguistic features, and then 3) constructing the regression model. All statistical procedures were performed in JASP (see jasp-stats.org).

In stage 1 involving the nine stylistic features, all were categorical in nature. Thus, they were analyzed via the use of independent samples t-tests to explore whether their presence or absence resulted in higher or lower scores (e.g., comparing whether abstracts with a Methods section received higher scores than those abstracts without the section). Prior to running the t-tests for each stylistic variable, assumptions were checked. As discussed in the next section of this paper, non-parametric alternatives (i.e., Mann Whitney U tests) were run when statistical assumptions were violated.

For examining the 23 linguistic features in stage 2, all variables were continuous. Thus, we conducted a series of Pearson’s (r) correlations to examine their relationship with raters’ scores (e.g., examining whether total words in an abstract corresponded to higher/lower scores). For both the stylistic and linguistic features, descriptive statistics are provided (e.g., M and $s D$ ), along with confidence intervals (CIs) and effect sizes to aid in the interpretation of the results.

The third and final stage of data analysis involved constructing the regression model. Following Egbert and Plonsky (2015), we used an exploratory (stepwise) multiple regression. Only those stylistic and linguistic variables that yielded statistically significant results from the first two stages of data analyses were entered into the regression model (entered sequentially into the model based on their effect sizes). As further discussed in the next section, assumptions were checked prior to running the regression. Both standardized beta $( \beta )$ and $R ^ { 2 }$ values are used to interpret the findings of the regression and to estimate the extent to which certain variables may account for reviewers’ scores (see Loerts et al., 2020 for more).

# 4. Results

Following the data analyses, which occurred in three different stages, the results are also presented in this order. First, we describe

Table 3 Results of Mann Whitney U testsa for stylistic variables in abstracts $\left( N = 3 0 4 \right)$ ).   

<html><body><table><tr><td>Stylistic feature</td><td># of abstracts w/feature</td><td colspan="2">Presence (score)</td><td colspan="2">Absence (score)</td><td>Test statistic</td><td>Sig.</td><td>Effect size</td><td>CIs</td></tr><tr><td></td><td></td><td>M</td><td>SD</td><td>m</td><td>SD</td><td>w</td><td>p</td><td>rrb</td><td> lower, upper</td></tr><tr><td>Methods section</td><td>294</td><td>1.2</td><td>1.3</td><td>0.1</td><td>1.5</td><td>754.5</td><td>0.008b</td><td>0.49</td><td>0.71, 0.16</td></tr><tr><td>Results section</td><td>263</td><td>1.3</td><td>1.3</td><td>0.4</td><td>1.5</td><td>3382.5</td><td>&lt;0.001b</td><td>0.37</td><td>0.52, 0.20</td></tr><tr><td>Discussion section</td><td>243</td><td>1.3</td><td>1.4</td><td>1.0</td><td>1.3</td><td>6254.5</td><td>0.056</td><td>0.16</td><td>0.31, 0.01</td></tr><tr><td>Follows IMRD order</td><td>203</td><td>1.4</td><td>1.3</td><td>0.8</td><td>1.4</td><td>7422.0</td><td>&lt;0.001b</td><td>0.28</td><td>0.40, 0.14</td></tr><tr><td>Includes RQ or hypothesis</td><td>243</td><td>1.4</td><td>1.3</td><td>1.2</td><td>1.4</td><td>6713.0</td><td>0.249</td><td>0.09</td><td>0.25, 0.07</td></tr><tr><td>Gap/motivation statement</td><td>168</td><td>1.2</td><td>1.4</td><td>1.3</td><td>1.3</td><td>11931.5</td><td>0.50</td><td>0.04</td><td>0.09, 0.17</td></tr><tr><td>Completeness</td><td>251</td><td>1.3</td><td>1.3</td><td>0.6</td><td>1.6</td><td>4941.5</td><td>0.003b</td><td>0.26</td><td>0.41, 0.09</td></tr></table></body></html>

a The assumption of normality was violated for all stylistic variables; thus, non-parametric Mann Whitney U tests were run. The corresponding effect sizes are shown as rank-biserial correlation $( r r b )$ . b Indicates statistical significance with $p < . 0 5$ .

the findings for the analyses of the stylistic variables. Second, the findings of the linguistic variables are shown. Finally, the results of the multiple regression are provided, showing the extent to which those statistically significant variables accounted for variance in reviewers’ overall abstract scores.

# 4.1. Stylistic variables

In total, nine different stylistic variables were examined for their presence or absence in the corpus. In Table 3, both the descriptive statistics and the results of Mann Whitney U tests are shown for most of these variables. Notably, for the stylistic variable involving the presence/absence of an Introduction section, only 1 of the 304 abstracts did not have an Introduction. Thus, given the small sample size, we opted not run inferential statistics on this feature. However, as Table 3 shows, four of the stylistic features came back as being statistically significant. These features were the inclusion of a Methods section, a Results section, following IMRD order, and study completeness. In all four cases, the inclusion of these stylistic features resulted in significantly higher scores from reviewers. The effect sizes $( r r b )$ also ranged from small (completeness $= 0 . 2 6$ , follows IMRD order $= 0 . 2 8$ , Results section $= 0 . 3 7 $ ) to medium (Methods section $= 0 . 4 9$ ).

The final stylistic variable involved coding for the methodological design of the studies and identifying whether they were quantitative, qualitative, or mixed methods in scope. Since there were three categories, we ran a one-way ANOVA. However, the results indicated that there were no statistically significant differences among those scores on quantitative $( M = 1 . 3$ , $S D = 1 . 4 ) $ , qualitative $M = 1 . 1$ , $S D = 1 . 3$ ), and mixed method abstracts $M = 1 . 1$ , $S D = 1 . 3$ ), $F ( 2 , 3 0 1 ) = 0 . 9 1$ , $\begin{array} { r } { p = . 4 0 3 } \end{array}$ , $\eta ^ { 2 } = 0 . 0 1$ .

# 4.2. Linguistic variables

For the next phase, 23 different linguistic variables were examined. As mentioned, because these linguistic variables were all continuous in nature, they were measured using a series of Pearson’s $( r )$ correlations to examine their relationship with raters’ scores. For readability purposes, we have divided the different linguistic variables into two tables, Tables 4 and 5, which provide both the descriptive statistics and the results of the correlations for these linguistic variables. The first table, Table 4, provides seven linguistic variables, most of which deal with the length of different IMRD sections. The second table, Table 5, shows 16 additional linguistic variables, including a series of syntactic and lexical measures. Eight of the variables in this table are also taken from Egbert and Plonsky (2015) for replication purposes.

As shown, of the 23 linguistic variables, nine came back as being statistically significant in terms of being correlated with abstract scores. Of these variables, five were positively correlated with scores, meaning that reviewers’ scores tended to go up as these variables increased (e.g., more words). The five positively correlated variables included: total words in the abstracts $( r = 0 . 4 0 )$ , Results length (0.21), Methods length (0.19), average number of clausal dependents (0.18), and dependent clauses per clause (0.12). Conversely, four variables were negatively correlated with scores, which implies that as these variables increased (e.g., number of errors), reviewers’ scores tended to decrease. The four negatively correlated variables were: COCA academic range $( r = - 0 . 1 7 )$ , modals of prediction (− 0.16), COCA academic frequency $( - 0 . 1 5 )$ , and number of errors $( - 0 . 1 4 )$ .

# 4.3. Regression analysis

In this section, the results of the regression analysis are presented. The four stylistic variables and nine linguistic variables that yielded statistically significant results from the previous sections were entered into an exploratory stepwise multiple regression. The predictors were entered into the model sequentially based on their effect sizes. These 13 variables are summarized for readers in Table 6, listed in descending order of magnitude.

Prior to running the regression model, the assumptions were checked (i.e., the QQ plot for the standardized residuals, linearity, VIF values for multicolinearity, and homoscedacity). All assumptions were met. Then, the 13 variables were entered into the model. Of these, seven variables came back as being statistically significant predictors of abstract ratings. Those variables were:

1. Total words $( \beta = 0 . 2 8 )$ )

Table 4 Results of Pearson $r$ correlations for linguistic features and abstract ratings.   

<html><body><table><tr><td>Linguistic feature</td><td>M</td><td>SD</td><td>Min.</td><td>Max.</td><td>Pearson&#x27;s r</td><td>p-value</td><td>CIs</td></tr><tr><td>Intro length</td><td>140.8</td><td>54.5</td><td>0</td><td>345</td><td>0.03</td><td>0.59</td><td>0.14, 0.08</td></tr><tr><td>Methods length</td><td>89.4</td><td>47.7</td><td></td><td>263</td><td>0.19</td><td>&lt;0.001a</td><td>0.08, 0.30</td></tr><tr><td>Results length</td><td>61.1</td><td>43.2</td><td>0</td><td>235</td><td>0.21</td><td>&lt;0.001a</td><td>0.10, 0.32</td></tr><tr><td>Discussion length</td><td>29.6</td><td>26.4</td><td>0</td><td>161</td><td>0.03</td><td>0.595</td><td>0.08, 0.14</td></tr><tr><td>Total words</td><td>320.9</td><td>43.6</td><td>133</td><td>389</td><td>0.40</td><td>&lt;0.001a</td><td>0.30, 0.49</td></tr><tr><td>Number of citationsb</td><td>4.7</td><td>3.8</td><td>0</td><td>21</td><td>0.09</td><td>0.11</td><td>0.02, 0.20</td></tr><tr><td>Number of errors</td><td>1.1</td><td>1.8</td><td>0</td><td>14</td><td>0.14</td><td>0.017a</td><td>0.25, 0.03</td></tr></table></body></html>

a Indicates statistical significance with $p < . 0 5$ . b Egbert and Plonsky (2015) operationalized citations as a categorical variable (i.e., included/excluded). For comparative purposes, we also checked citations in this manner using a Mann Whitney $U$ test, but the results were still not statistically significant $( p = . 0 8 1$ , $r r b = 0 . 1 7 \mathrm { . }$ ).

Table 5 Results of Pearson $r$ correlations for syntactic and lexical measures and abstract ratings.   

<html><body><table><tr><td>Linguistic feature</td><td>M</td><td>SD</td><td>Pearson&#x27;s r</td><td> p-value</td><td>CIs</td></tr><tr><td colspan="6">Syntactic complexity measures</td></tr><tr><td>Mean length of sentence</td><td>27.4</td><td>5.6</td><td>0.07</td><td>0.198</td><td>0.04, 0.19</td></tr><tr><td>Dependent clauses per clause</td><td>0.4</td><td>0.1</td><td>0.12</td><td>0.03a</td><td>0.01, 0.23</td></tr><tr><td>Average number of modifiers in a noun phrase</td><td>1.6</td><td>0.2</td><td>0.04</td><td>0.477</td><td>0.15, 0.07</td></tr><tr><td>Average number of clausal dependents</td><td>2.5</td><td>0.2</td><td>0.18</td><td>0.002a</td><td>0.07, 0.28</td></tr><tr><td>Nominalizations per 1000 words</td><td>39.1</td><td>18.9</td><td>0.02</td><td>0.781</td><td>0.10, 0.13</td></tr><tr><td colspan="6">Lexical complexity measures</td></tr><tr><td>COCA academic range</td><td>0.5</td><td>0.04</td><td>0.17</td><td>0.003a</td><td>0.28, 0.06</td></tr><tr><td>COCA academic frequency</td><td>8735.8</td><td>1612.9</td><td>0.15</td><td>0.009a</td><td>0.26, -0.04</td></tr><tr><td>Lexical decision reaction time</td><td>652.5</td><td>9.0</td><td>0.04</td><td>0.546</td><td>0.08, 0.15</td></tr><tr><td colspan="6">Linguistic measures from Egbert and Plonsky (2015)</td></tr><tr><td>First-person pronouns</td><td>0.4</td><td>0.7</td><td>0.11</td><td>0.056</td><td>0.003, 0.22</td></tr><tr><td>All personal pronouns</td><td>1.3</td><td>1.2</td><td>0.07</td><td>0.215</td><td>0.04, 0.18</td></tr><tr><td>Present tense verbs</td><td>3.7</td><td>1.5</td><td>0.05</td><td>0.391</td><td>0.16, 0.06</td></tr><tr><td>Past tense verbs</td><td>2.6</td><td>1.5</td><td>0.08</td><td>0.153</td><td>0.03, 0.19</td></tr><tr><td>Modals of prediction</td><td>0.3</td><td>0.4</td><td>0.16</td><td>0.005a</td><td>0.27, 0.05</td></tr><tr><td>Adjectives per 10,000 words</td><td>10.6</td><td>2.7</td><td>0.10</td><td>0.079</td><td>0.01, 0.21</td></tr><tr><td>Nouns per 10,000 words</td><td>27.8</td><td>3.6</td><td>0.02</td><td>0.785</td><td>0.13, 0.10</td></tr><tr><td>Prepositions per 10,000 words</td><td>11.4</td><td>2.1</td><td>0.10</td><td>0.095</td><td>0.21, 0.10</td></tr></table></body></html>

a Indicates statistical significance with $p < . 0 5$ .

Table 6 List of 13 statistically significant variables from RQs1-2 for the regression analysis.   

<html><body><table><tr><td>Stylistic or linguistic feature</td><td> p-value</td><td>Effect size (rrb or r).</td></tr><tr><td>Methods section (presence vs. absence)</td><td>0.008</td><td>0.49</td></tr><tr><td>Total words</td><td>&lt;0.001</td><td>0.40</td></tr><tr><td>Results section (presence vs. absence)</td><td>&lt;0.001</td><td>0.37</td></tr><tr><td>Follows IMRD order</td><td>&lt;0.001</td><td>0.28</td></tr><tr><td>Completeness</td><td>0.003</td><td>0.26</td></tr><tr><td>Results length</td><td>&lt;0.001</td><td>0.21</td></tr><tr><td>Methods length</td><td>&lt;0.001</td><td>0.19</td></tr><tr><td>Average number of clausal dependents</td><td>0.002</td><td>0.18</td></tr><tr><td>COCA academic range</td><td>0.003</td><td>0.17</td></tr><tr><td>Modals of prediction</td><td>0.005</td><td>0.16</td></tr><tr><td>COCA academic frequency</td><td>0.009</td><td>0.15</td></tr><tr><td>Number of errors</td><td>0.017</td><td>0.14</td></tr><tr><td>Dependent clauses per clause</td><td>0.03</td><td>0.12</td></tr></table></body></html>

2. Methods length $( \beta = 0 . 1 8 )$ 3. Follows IMRD order $( \beta = 0 . 1 3 )$ ) 4. Average number of clausal dependents $( \beta = 0 . 1 3 )$ ) 5. Number of errors $( \beta = - 0 . 1 2 )$ 6. Dependent clauses per clause $( \beta = 0 . 1 2 )$ ) 7. Results length $( \beta = 0 . 1 2$ )

These seven variables accounted for just under 25 percent of the variance, $R ^ { 2 } = 0 . 2 4 8 , F ( 7 , 3 0 3 ) = 1 3 . 9 4$ , $p < . 0 0 1$ . Table 7 highlights the results of the full regression model with these variables, including each variable’s contribution to the cumulative $R ^ { 2 }$ value when added to the model.

Table 7 Results of multiple regression for seven predictor variables contributing to reviewer scores.   

<html><body><table><tr><td> Predictor variable</td><td></td><td>t-value</td><td> p-value</td><td>Cumulative R2</td></tr><tr><td>Total words</td><td>0.28</td><td>4.74</td><td>&lt;0.001</td><td>0.159</td></tr><tr><td>Methods length</td><td>0.18</td><td>3.05</td><td>0.002</td><td>0.166</td></tr><tr><td>Follows IMRD order</td><td>0.13</td><td>2.48</td><td>0.014</td><td>0.185</td></tr><tr><td>Average number of clausal dependents</td><td>0.13</td><td>2.40</td><td>0.017</td><td>0.210</td></tr><tr><td>Number of errors</td><td>0.12</td><td>-2.41</td><td>0.016</td><td>0.227</td></tr><tr><td>Dependent clauses per clause</td><td>0.12</td><td>2.24</td><td>0.026</td><td>0.237</td></tr><tr><td>Results length</td><td>0.12</td><td>2.03</td><td>0.043</td><td>0.248</td></tr><tr><td>Total =</td><td></td><td></td><td></td><td>0.248</td></tr></table></body></html>

# 5. Discussion

In the current study, we conducted an approximate replication of Egbert and Plonsky (2015), responding to calls for replication research in both the EAP and Applied Linguistics literature (e.g., Porte & McManus, 2019). In terms of the RQ, which involved examining the stylistic and linguistic predictors of conference abstract ratings, the results suggest that approximately one-quarter of reviewers’ scores could be attributed to seven variables. To better facilitate comparisons between our results and Egbert and Plonsky’s, Table 8 compares the standardized beta values from each study.

In terms of similarities, three variables were found to be statistically significant predictors of reviewers’ ratings in both studies. These included 1) total words, 2) a Results section (or Results length), and 3) fewer errors. The strongest predictor of ratings was total words, implying that the greater the word count, the more likely an abstract was to receive a higher score. This finding is not only consistent with Egbert and Plonsky’s original study, but also with other literature involving analyses of accepted/rejected conference abstracts (e.g., Halleck & Connor, 2006). Apart from total words, completeness also appears to be a feature that is valued by reviewers, since abstracts with (longer) Results sections tended to receive higher ratings. This finding, too, is consistent with existing research, which has suggested that most accepted or high-rated abstracts tend to include some mention of the results (e.g., Yoon & Casal, 2020). Finally, it is also apparent that reviewers are negatively impacted by errors (grammatical and/or spelling), a finding that has been reported more broadly across writing studies in L2 assessment (e.g., Kim & Kessler, 2022; Fritz & Ruegg, 2013).

In terms of differences, Egbert and Plonsky (2015) found three other variables to be significant predictors, which were the 1) presence of citations, 2) more nouns, and 3) fewer first-person pronouns. None of these features were statistically significant in our replication. Instead, these were replaced by four different features, which were 1) Methods length, 2) follows IMRD order, 3) average number of clausal dependents, and 4) dependent clauses per clause. Regarding why such differences emerged, for some features, it is difficult to speculate. For example, the fact that the presence of citations (operationalized as a continuous variable in the current study) was not significant was somewhat surprising, particularly since it was significant in the original study. For replication purposes, we also examined citations as a categorical variable, yet it was still non-significant.

Conversely, for other variables such as fewer first-person pronouns, there are some potential explanations. One possibility involves the evolution of language practices in research-oriented genres. Scholars have highlighted that over the past 30 years, there has been both a growing use and acceptance of first-person pronouns in academic writing, including in abstract and research writing more broadly (e.g., Jiang & Hyland, 2020, 2022). Thus, since the current study’s corpus was collected one decade after the original study, it is possible that writers’ practices and reviewers’ acceptance of such practices has changed during that time.

Regarding the linguistic predictors in our dataset, it is noteworthy that average number of clausal dependents $( \beta = 0 . 1 3 )$ and dependent clauses per clause $( \beta = 0 . 1 2 )$ both surfaced as significant predictors, with higher clausal complexity predicting higher reviewer scores. Although Biber and Gray (2010) found that clausal complexity was more associated with spoken registers and conversational speech than with more formal written academic discourse (although not with these complexity measures), they did not find or argue that there was no meaningful variation of clausal complexity in academic writing practices. Recent studies have found discipline- and function-specific variation of clausal complexity (e.g., Ziaeian & Golparvar, 2022) in written academic discourse. Liu and Li (2024) found that research article writers in marine science used significantly more clausal subordination (also measured through dependent clauses per clause) in public-facing summaries than in the original articles. This may be interpreted as evidence of a somewhat less formal academic style, though not necessarily an informal conversational one. Future research may examine what is communicated more locally through such clausal complexity or employ more fine-grained measures of clausal complexity for similar purposes.

Of course, while the current study has numerous strengths, it is not without its limitations. For example, although this study was an attempt to replicate a study with important findings for EAP pedagogy, it still represents the context and practices of only one conference. The conference itself (i.e., SLRF) was SLA-oriented, and therefore might not be representative of the community genre practices of other Applied Linguistics subdisciplines which may be less interested in L2 learning. Additionally, we note here that the $R ^ { 2 }$ value found in this study represents approximately 25 percent of reviewers’ scores, meaning that much of the abstracts’ total scores is still unaccounted for – (which we suspect has more to do with overall study quality and design, rather than style). Arguably, however, the fact that one-quarter of conference abstract scores can potentially be attributed to several linguistic and stylistic features is quite remarkable, with important implications for L1 and L2 writers.

# 6. Conclusion

The current study was an approximate replication that examined the stylistic and linguistic predictors of conference abstract ratings. In terms of the pedagogical implications, this study suggests that there are some basic, practical things that researchers can do to improve their chances of receiving a higher score when composing a conference abstract. For one, since total words is the greatest predictor of scoring, researchers should strive to write the maximum number of words permitted. That is, if a conference sets a word count limit to 300 words, then it would be wise for writers to attempt to reach that threshold. In our experience (and as reflected in our dataset), this is something that many researchers do not do. Additionally, when writing towards this maximum word count, researchers should attempt to follow the IMRD order, and be sure to devote ample room to both the Methods and the Results sections, since reviewers appear to be sensitive to their presence and length. In terms of writing style, it may be acceptable to use a somewhat less formal academic style, in that more clausal elaboration may be permissible. Finally, although it may seem somewhat obvious, it is notable that errors (grammatical and spelling) are negatively correlated with scoring. Thus, researchers should not only carefully edit their abstracts, but if needed, they also may wish to ask a peer or colleague to proofread their text prior to submission. By doing these basic things, one’s chances of receiving a higher score will likely improve.

Table 8 Comparison of $\beta$ values in the current study and the original study.   

<html><body><table><tr><td colspan="2">The current study</td><td colspan="2">Egbert and Plonsky (2015)</td></tr><tr><td>Predictor variables</td><td></td><td>Predictor variables</td><td></td></tr><tr><td>Total wordsA</td><td>0.28</td><td>Total words4</td><td>0.29</td></tr><tr><td>Methods length</td><td>0.18</td><td>Citations (presence)</td><td>0.22</td></tr><tr><td>Follows IMRD order</td><td>0.13</td><td> Results section (presence)C</td><td>0.18</td></tr><tr><td>Average number of clausal dependents</td><td>0.13</td><td>More nouns</td><td>0.16</td></tr><tr><td>Number of errorsB</td><td>0.12</td><td>Errors (presence) B</td><td>0.14</td></tr><tr><td>Dependent clauses per clause</td><td>0.12</td><td>Fewer first-person pronouns</td><td>0.11</td></tr><tr><td>Results lengthc</td><td>0.12</td><td></td><td></td></tr><tr><td>Cumulative R2 =</td><td>0.248</td><td>Cumulative R2 =</td><td>0.31</td></tr></table></body></html>

Note: Superscript letters denote similar findings between the two studies.

Crucially, this study highlights the value of replication work, as it found both similarities and differences in the results. In closing, we strongly encourage other researchers to pursue replication studies, whether they be close, approximate, or conceptual in nature (for more, see Porte & McManus, 2019). Similarly, we encourage future researchers to attempt to replicate our study. This could be accomplished by reproducing the methods and measures used in this study, while also expanding the number of measures used and/or by exploring abstracts submitted to other Applied Linguistics conferences. Doing so would potentially enhance the generalizability of the findings and potentially account for more variance, thereby improving future EAP genre pedagogy.

# CRediT authorship contribution statement

Matt Kessler: Writing – review & editing, Writing – original draft, Methodology, Formal analysis, Data curation, Conceptualization. Caitlin Cornell: Writing – review & editing, Writing – original draft, Methodology, Formal analysis, Data curation, Conceptualization. J. Elliott Casal: Writing – review & editing, Writing – original draft, Methodology, Formal analysis. Detong Xia: Methodology, Formal analysis, Data curation.

# References

Baker, P., & Egbert, J. (2016). Triangulating methodological approaches in corpus linguistic research. Routledge.   
Berkenkotter, C., & Huckin, T. (1995). Genre knowledge in disciplinary communication: Cognition/culture/power. Erlbaum.   
Biber, D. (1988). Variation across speech and writing. Cambridge University Press.   
Biber, D., & Gray, B. (2010). Challenging stereotypes about academic writing: Complexity, elaboration, explicitness. Journal of English for Academic Purposes, 9(1), 2–20. https://doi.org/10.1016/j.jeap.2010.01.001   
Biber, D., Johansson, S., Leech, G., Conrad, S., & Finegan, E. (1999). Longman grammar of spoken and written English. Longman.   
Borg, S. (2014). The benefits of attending ELT conferences. ELT Journal, 69(1), 35–46. https://doi.org/10.1093/elt/ccu045   
Cutting, J. (2012). Vague language in conference abstracts. Journal of English for Academic Purposes, 11, 283–293. https://doi.org/10.1016/j.jeap.2012.05.004   
Davies, M. (2008). The corpus of contemporary American English (COCA). Available online at: https://www.english-corpora.org/coca/.   
Egbert, J., & Plonsky, L. (2015). Success in the abstract: Exploring linguistic and stylistic predictors of conference abstract ratings. Corpora, 10(3), 291–313. https:// doi.org/10.3366/cor.2015.0079   
Faber, B. (1996). Rhetoric in competition: The formation of organizational discourse in Conference on College Composition and Communication abstracts. Written Communication, 13(3), 355–384. https://doi.org/10.1177/0741088396013003003   
Fritz, E., & Ruegg, R. (2013). Rater sensitivity to lexical accuracy, sophistication and range when assessing writing. Assessing Writing, 18(2), 173–181. https://doi.org/ 10.1016/j.asw.2013.02.001   
Gray, R. (2021). Multimodality in the classroom presentation genre: Findings from a study of Turkish psychology undergraduate talks. System, 99, Article 102522. https://doi.org/10.1016/j.system.2021.102522   
Halleck, G. B., & Connor, U. M. (2006). Rhetorical moves in TESOL conference proposals. Journal of English for Academic Purposes, 5, 70–86. https://doi.org/10.1016/ j.jeap.2005.08.001   
Jiang, F. K., & Hyland, K. (2020). Prescription and reality in advanced academic writing. Iberica, 39, 15–42. https://doi.org/10.17398/2340-2784.39.14   
Jiang, F. K., & Hyland, K. (2022). Changes in research abstracts: Past tense, third person, passive, and negatives. Written Communication, 40(1), 210–237. https://doi. org/10.1177/07410883221128876   
Kaplan, R. B., Cantor, S., Hagstrom, C., Khami-Stein, L. D., Shiotani, Y., & Zimmerman, C. B. (1994). On abstract writing. Text, 14(3), 401–426. https://doi.org/ 10.1515/text.1.1994.14.3.401   
Kessler, M., & Casal, J. E. (2024). English writing instructors’ use of theories, genres, and activities: A survey of teachers’ beliefs and practices. Journal of English for Academic Purposes, 69, 101384. https://doi.org/10.1016/j.jeap.2024.101384   
Kim, S., & Kessler, M. (2022). Examining L2 English university students’ uses of lexical bundles and their relationship to writing quality. Assessing Writing, 51, 100589. https://doi.org/10.1016/j.asw.2021.100589   
Kyle, K. (2016). Measuring syntactic development in L2 writing: Fine grained indices of syntactic complexity and usage-based indices of syntactic sophistication (Doctoral Dissertation).   
Kyle, K., Crossley, S., & Berger, C. (2018). The tool for the automatic analysis of lexical sophistication (TAALES): Version 2.0. Behavior Research Methods, 50, 1030–1046. https://doi.org/10.3758/s13428-017-0924-4   
Liu, Y., & Li, T. (2024). Comparing the syntactic complexity of plain language summaries and abstracts: A case study of marine science academic writing. Journal of English for Academic Purposes, 68. https://doi.org/10.1016/j.jeap.2024.101350   
Loerts, H., Lowie, W., & Seton, B. (2020). Essential statistics for applied linguistics: Using R or JASP (2nd ed.). Bloomsbury.   
Lu, X. (2010). Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15, 474–496.   
Lu, X., Casal, J. E., & Liu, Y. (2020). The rhetorical functions of syntactically complex sentences in social science research article introductions. Journal of English for Academic Purposes, 44, 100832. https://doi.org/10.1016/j.jeap.2019.100832   
Macleod, C., Grishman, R., Meyers, A., Barrett, L., & Reeves, R. (2001). Nomlex. New York University. Retrieved from https://nlp.cs.nyu.edu/nomlex/.   
Martín, P., & Burgess, S. (2023). “Our study offers insight into…”: Rhetorical promotion in English and Spanish conference abstracts. International Journal of Applied Linguistics, 34, 134–149. https://doi.org/10.1111/ijal.12483   
Nassaji, H. (2020). Good qualitative research. Language Teaching Research, 24(4), 427–431. https://doi.org/10.1177/1362168820941288   
Nini, A. (2019). The multi-dimensional analysis tagger. In T. Berber Sardinha, & M. Veirano Pinto (Eds.), Multi-dimensional analysis: Research methods and current issues (pp. 67–94). Bloomsbury Academic.   
Omidian, T., Balance, O. J., & Sitanova-Chanturia, A. (2023). Replicating corpus-based research in English for academic purposes: Proposed replication of Cortes (2013) and Biber and Gray (2010). Language Teaching, 56(1), 128–136. https://doi.org/10.1017/S0261444821000367   
Payant, C., & Hardy, J. A. (2016). The dynamic rhetorical structures of TESOL conference abstracts. BC TEAL Journal, 1(1), 1–17. https://doi.org/10.14288/bctj. v1i1.220   
Porte, G. (2012). Replication research in applied linguistics. Cambridge University Press.   
Porte, G., & McManus, K. (2019). Doing replication research in applied linguistics. Routledge.   
Ryshina-Pankova, M. (2015). A meaning-based approach to the study of complexity in L2 writing: The case of grammatical metaphor. Journal of Second Language Writing, 29, 51–63. https://doi.org/10.1016/j.jslw.2015.06.005   
Samar, R. G., Talebzadeh, H., Kiany, G. R., & Akbari, R. (2014). Moves and steps to sell a paper: A cross-cultural genre analysis of applied linguistics conference abstracts. Text & Talk, 34(6), 759–785. https://doi.org/10.1515/text-2014-0023   
Simon-Maeda, A. (2016). A corpus-based study of the AAAL conference handbook. Journal of English for Academic Purposes, 23, 71–82. https://doi.org/10.1016/j. jeap.2016.06.001   
Stein, W. (1997). A genre analysis of the TESOL conference abstract. Unpublished doctoral dissertation. Oklahoma State University.   
Swales, J. M. (1990). Genre analysis: English in academic and research settings. Cambridge University Press.   
Swales, J. M. (2004). Research genres: Exploration and applications. Cambridge University Press.   
Toutanova, K., Klien, D., Manning, C. D., & Singer, Y. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 conference of the North American chapter of the association for computational linguistics on human language technology – naacl ’03 (Vol. 1, pp. 173–180). Association for Computational Linguistics. https://doi.org/10.3115/1073445.1073478.   
Vitta, J. P., Nicklin, C., & Albright, S. E. (2023). Academic word difficulty and multidimensional lexical sophistication: An English-for-academic- purposes-focused conceptual replication of Hashimoto and Egbert (2019). The Modern Language Journal, 107(1), 373–397. https://doi.org/10.1111/modl.12835   
Yoon, J., & Casal, J. E. (2020). Rhetorical structure, sequence, and variation: A step-driven move analysis of applied linguistics conference abstracts. International Journal of Applied Linguistics, 30(3), 462–478. https://doi.org/10.1111/ijal.12300   
Ziaeian, E., & Golparvar, S. E. (2022). Fine-grained measures of syntactic complexity in the discussion section of research articles: The effect of discipline and language background. Journal of English for Academic Purposes, 57, Article 101116. https://doi.org/10.1016/j.jeap.2022.101116

Matt Kessler is an Assistant Professor of Applied Linguistics at the University of South Florida’s Department of World Languages. His research focuses on issues surrounding L2 writing, genre-based teaching and learning, and computer-assisted language learning.

Caitlin Cornell is the Assistant Director of the Center for Language Teaching Advancement and a PhD Candidate in Second Language Studies at Michigan State University. She researches and supports accessible and disability inclusive pedagogy for additional language learning, including explorations of student experience and instructor readiness.

J. Elliott Casal is an Assistant Professor of Applied Linguistics (Department of English) and a faculty affiliate of the Institute for Intelligent Systems at the University of Memphis. His research interests include corpus linguistics, L2 writing, genre-based teaching and learning, and computer assisted language learning.

Detong Xia is an Assistant Professor in the Department of Applied Linguistics at Xi’an Jiaotong-Liverpool University. Her research focuses on corpus linguistics, L2 writing, and English for Specific Purposes. Her work appears in journals such as International Journal of Learner Corpus Research and International Review of Applied Linguistics in Language Teaching, and Text & Talk.