# Establishing analytic score profiles for large-scale L2 writing assessment: The case of the CET-4 writing test

Shaoyan Zoua,\*,1, Xun Yanb,2, Jason Fan c,3

a University of Health and Rehabilitation Sciences, Qingdao, China b Universty of Illinois at Urbana-Champaign, $H _ { s }$ USA c Language Testing Research Centre (LTRc), University of Melbourne, Melbourne, Australia

# ARTICLEINFO

# ABSTRACT

Keywords:   
Large-scale L2 writing assessment   
Localization   
Analytic rating scale   
MFRM analysis   
Cluster analysis   
Score profile

This study addresses a critical need in large-scale L2 writing assessment by emphasizing the significance of tailoring assessments to specific teaching and learning contexts. Focusing on the CET-4 writing test in China, the research unfolded in two phases. In Phase I, an empiricallydeveloped analytic rating scale designed for the CET-4 writing test was rigorously validated. Twenty-one raters used this scale to rate 30 essays, and Many-Facets Rasch Model (MFRM) analysis was performed on the rating data. The outcomes demonstrate the scale's robustness in effectively differentiating examinees' writing performance, ensuring consistency among raters, and mitigating rater variation at both individual and group level. Phase II extends the research scope by applying the validated scale to score 142 CET-4 writing scripts. Utilizing Hierarchical and K-Means cluster analyses, this phase unveils three distinct score profiles. These findings are significant for both the CET-4 writing test and other L2 large-scale writing assessment. Theoretically, this study introduces a perspective that aims to enhance our understanding of learners' performance in large-scale L2 writing assessment. Methodologically, this study presents a framework that integrates the validation of the rating scale with the identification of distinct score clusters, thus aiming to provide a more detailed solution for tailoring assessments to specific learning contexts.

# 1. Introduction

In recent years, the localization of large-scale L2 tests has garnered increasing recognition among language testing researchers and practitioners (e.g, Fan et al., 2022; O'Sullivan, 2016a, 2016b, 2020; Toh, 2013; Wu, 2019), with the consensus that appropriate localization is crucial for the meaningful use of tests and, by extesion, for their validation. Sullivan (2020) further stesses that for a test to be appropriate to a specifi context of use it should report performance in a way that is meaningful o that context (p. xii). As such, localization pertains not only to the test taker and the test task, but also to the scoring system, including score generation and interpretation (OSullivan, 2020). Wthin the domain f writing aessment, this emphasis on localization is most evident in the scoring system (see ., Knoch, 2011; L & He, 2015). This is because the rating scale in the scoring system usually represents the de fcto test construct, and writing performances are stillpredominantly scored by human raters (e.g., Chon, Shin & Kim, 2021; Shermis et al. 2017).

To respond to these concerns regarding localization, an increasing number of studies have been undertaken to investigate how the scoring systems of large-scale L2 writing assessments can be tailored to meet the needs of local context (e.g, Knoch, 2011; Li & He, 2015; Liu & Huang, 2020. Whil the majority of these studies concentrate on examining the reliabilit or validit f the rating scales designed for local asessment contexts, few have delved into how to provide more explicit and useful score interpretations that can inform local language teaching curriculum and consequently addres local learning needs. As Wind and Peterson (2017) tres, rather than overemphasizing reliabilit or psychometric qualit, language assessment researchers should consider exploring assessment practices that generate useful diagnosis of individual learners or individualized information which can be incorporated in subsequent teaching and learning contexts.

One of these practices might be to establish core profile ased on test takers recurring pattens of subscale scores in language tests (Choi, 2017). Profile information, as Choi (2017) argues, has considerable potential to inform language teaching and learning in different contexs. To date, a few studies have inestigated score/proficiency profiles for L2 leaners (e.g., Ginther & Yan, 2018; Choi. 2017; Friginal, Li, & Wigle, 2014; Jarvis et al., 2003). For example, Choi (2017) explored oral English proficiency profile of otential international teaching asistants (ITAs)from an ITA screening tes administerd at a US universty. Ginther & Yan (2018) identified the TOEFL iBT core profile of the first-year Chinese students at acomprehensive university and investigated the relationship between students'score profiles and their subsequent grade point average (GPA). Due to the diverse asessment contexts and the score/- proficiency profiles related to diferent targeted language skill, further reearch is warranted to elicit L2 learners score profile in different asessment contexts, such as the context of large-scale L2 writing tests. Therefore, this study aims to advance the inquir of score profile in lage-scale wriing asesment, adapting ther coring systems to bter fit the requirements of specific teching and learning contexts. Situated in the context of the writing test of the Collge English Test Band Four (hereafter the CET-4 writing) in China, we explore the possibility of revising the CET holisic cale which is currentl in use into an analytic scale and extracting students' score profiles of CET-4 writing performances.

# 2. Literature review

# 2.1. The use of holistic or analytic scoring in L2 writing assessment

In assessing  writing performance, there are long-standing arguments over the usefulness f holistic versus analyti scorin see. g., Hamp-Lyons, 1991, 1995; Shaw & Weir, 2007; Weigle, 2002). Generall, holistic scoring involves assgning a single score to a writing scrit based on the rater's overallimpresion of the whole script (Weigle, 2002). However, analyti scoring takes intoaccount a list of elements, each of which has to be judged separately and assgned a single score (Hamp-Lyons, 2016). Over the past decades, holistic scoring has ben widely used in large-scale L2 writing asesment due to its time- and cost-eficiency, and the subsequent ease in score reporting. However, in recent years, holistic scoring has raise concerns in local writing asessments because test users (e.g, test takers and teachers) are increasingly interested in the meaningfulness of test scores (Chapell, Enright, & Jamieson, 2008) and how scores can inform language teaching and learning (e.g., Green, 2017; Jones, Savill, & Salamoura, 2016). One of the crucial concers is that holistic coring fals to adequately captre test takers strenghs and weknesss in their peformance ( et l., 2010; Park & Yan, 2019). This is particularl the case when it comes to L2 learners whose language skill are stil under development, and who are more prone to showing unequal performance on different dimensions of the target constructs (Le et al., 2010). In contrast, analytic scoring can better capture varied learner profiles, which can reflect the complex and imbalanced nature in L2 writing development (Park & Yan, 2019, se also Hamp-Lyons, 191, 1995; Jarvis et a., 2003; Yan et al., 2021). Analytic score reports can also provide more detailed diagnostic information for test takers with varied profiles, leading to positive washback on teaching and learning (Knoch, 2011).

Previous research has investigated the reliability and usefulnessof the two scoring approaches in L2 writing assessment. While mixed findings regarding the reliability of the two scoring approaches were reported (e.g., Ahmadi, 2013; Bacha, 2001; Barkaoui, 2007, 2011; Wiseman, 2008), a growing body of evidence highlights the effectiveness of the analytic scoring approach, especially in decreasing variability among raters in using the rating criteria e.g., Barkaoui, 2010; Li & He, 2015; Zou, 2022) and in diagnosing the varied writing performance of L2 learners (e.g., Knoch, 2009; Wang & Xie, 2022). This underscores the alignment of the analytic scoring method with the localization trend in large-scale L2 writing assessment as discussed earlier.

However, despite the burgeoning research focusing on the development and validation of analytic rating scales in large-scale L2 writing assessment (e.g., Becker, 2018; Deygers & Van Gorp, 2015; Janssen, Meier, & Trace, 2015; Knoch, 2009; Lallmamode, Daud, & Kassim, 2016), ittle progress has been made in leveraging the outcomes of this scoring to talor them to the specific teaching and learning nees. This would restrict the effectivenessof asessments in enhancing student leaning to some extent, as relying solely on test scores fails to yield detailed insights into test takers' specific strengths and areas for improvement (Tannenbaum & Cho, 2014)

# 2.2. Research on score profiles: procedures and methods

In the realm of language testing, the generation of test-takers' score profiles is increasingly recognized as a useful approach, bridging the gap between language assessment and pedagogical practices (Choi, 2017). Such score profiles, which iluminate a test taker's proficiency across various language skils or components, serve as nuanced feedback mechanism, fostering enhanced learning experiences (Choi, 2017; Ginther & Yan, 2018). To extract profileiformation from test results, according to Choi (2017) the frst crucial step is to decide on the components of the profile.

While there are limited discussions on the interrelationships among different L2 writing proficiency components in existing theoretical models, a growing body of research has delved into the score profiles of learners' performance in diffrent writing as. sessments (e.g., Friginal et al., 2014; Jarvis et al., 2003; Kim, 2011; Xie, 2017). These studies typicll reveal that L2 writing profiles are unbalanced, with varied performance across diffrent criteria (Xie, 2017). As such, understanding these unbalanced profiles be. comes incresingly vital in the context f large-scale 2 writing asesment, particularl in terms f taloring the asement to cater to specific teaching and learning needs, addressing the aforementioned localization concerns.

In identifying score profile, different tatistical and psychometric methods can be employed. Traditionally, subscores are derived from Classical Test Theory (CT), such as evaluating the reliability of subscales and reporting total or average scores. Moving beyond this conventional approach, we discuss two other approaches: cognitive diagnostic analysis (CDA) and cluster analysis (CA). CDA models, us mosty in retive skil assment, a latent trt mds that net probabilistic kill mastry profiles for individual test takers, based on theoretical models or educational taxonomies (Leighton & Gierl, 2007). Although these models have proven effectie for extracting skill profile infomation i listening or reading aessments .g, Jang, 2009; Kim, 2015; Li & Suen, 2013; Mn & He, 2022), their application in writing assessment sems limited. In writing, typicall fewer test items/tasks are used, making it challenging to utilize DA models efectively for score profile xtraction (see Weigle 202). Therefore, we did not employ CDA inthis study, although we acknowledge its analytic prowess in the context of reading and listening assessment.

Cluster Analysis (CA) ofers a flexible method for extracting learners skill profiles in performance assessment. It classifies observations into specific groups based on the measurable attributes or criteria, such as analytic scores achieved on different rating criteria (Friginal t al., 2014).  andul f stdes have used A toiscn anuage leer profiles , Friginal t al., 2014 Ginther & Yan, 2018; Jarvis e al., 2003). For instance, Friginal et al. (2014) used CA to establish multiple profiles of compositions produced by both non-native speakers of English and native speakers of English. Ginther & Yan (2018) also utilized CA to identify the profiles of first-year Chinese students' TOEFL subsection scores to investigate the corrlational patterns between diffrent score profiles and students grade average point (GPA). While the existing body of research has showcased robustnes of CA in deriving score profiles, its utilization in large-scale writing assessments remains limite. Moreover, there's a noticeable asence of studies that connect the extraction of score profiles with the validation of analytic rating scales. Therefore, our study addresses this void by leveraging the potentials of CA in extracting score profiles in the context of large-scale writing assessment. We introduce an approach that seamlessly combines the generation of core profiles and the validation of analytic rating scale. By doing so, the study aims to enhance the objectivity and richness of the derived score profiles, offering significant insights for pedagogical practices.

# 3. The present study

# 3.1. The CET-4 writing test

Initially launched in 1987, the CET-4 was developed under the requirements of National College English Teaching Syllabus (NCETS), now recognized as College English Curriculum Requirements (CECR) sanctioned by the Ministry of Education in China. Given the pivotal role of the CECR in shaping the teaching and learning of English at tertiary institutions across the nation, the primary aims of the CE-4 are twofold. irstly, t sees t provide an objective asssment of collee students' English proficiency. Secondy, it aims to exert positive impact on the teaching and learning of college English in China (Jin, 2008). With is evolution in over 30 years, the CET-4 has become one of the most inluential high-stakes Englishtes i China, exerting significant washback to the teaching and learning of Collee English in China (Fan & Frost, 2022). In its writing sectio, test takers are require to write either a narrative or an argumentative essay on a given topic with no more than 180 words. The essays are marked using a holistic rating scale. The holistic scale is comprised of five band levels (Band A-E) and three rating categories: Content rlvance, Language quality and Discourse coherence.

To date, the CET-4 writing test has undergone several revisions in accordance with the changes in CECR (see Jin, 2019 for a re. view). For instance, more types of input materials have been used as writing prompts: pictures, tables, charts, or quotations. In addition, the test has incorporated writing topics that are more relevant to college students campus life and their future work. Social issues such as e-shopping, shared bicycle, and environmental protection have been used as common topics of the CET-4 writing test. These revisions, acorin tullia (2020) eflect practiclfforts at liationf th-4 in hat th d f th et tass is more closely aligned with the test takers characteristics. Despite such localization endeavors, the holistic scoring system has barely been revised since the irs administration of the CET-4, which has aroused increasing concerns inthe recent two decades (e.g., Fei & Zhao, 2008; Huang, 2008; Zha0 & Huang, 2020; Li & Huang, 2022). One concern is that the holisticrating scal i highly generalized in terms of its scale descriptions, which might limit its potentials in offering useful feedback to the teaching and learning of CE programs (Zou & Fan, 2019; Fei and Zhao, 2008). Another concern is that in using the holisic rating scale, raters give diferential weightings to different dimensions f the writing construct (Li & Huang, 2022), which would compromise the validity of the rating scale (Weigle, 2002). To addressthese concerns, we outline in this study the validation of an analytic rating scale for the CET-4 writing test, which was subsequently used to establish analytc score profiles for the CET-4 testakers. It is hoped that the study can shed ight on how to localize the scoring systems of a large-scale L2 writing assessment to cater to the needs of specific teaching and learning contexts.

# 3.2. Localizing the rating scale of the CET-4 writing test

In developing the analytic rating scale, we started by creating a descriptor pol based on various resources related to CE teaching and learning in China, including the CET test syllabus, the CECR, and China's Standards of English Language Ability (CSE). Notably, the CSE is an English proficiency framework isued by China's Ministry of Education (see hps://ww.neea.edu.cn/html1/report/ 18112/9627-1.htm for an official introduction to the CSE). The CSE serves as a comprehensive guide, detailing proficiency benchmarks across various levels of English language learning within the country. The descriptors we collcted were meticulouslyscrutinized, edited, and revised in a rater workshop with the guidance of a panel of experienced CET-4 raters by taking into account the features of CET-4 writing performance. After careful evaluation, the descriptors that survived the workshop were grouped under four analytic ring cri: lingustc rang nd ccy, dcore ogzio,cont nd ds, and instic prricMrver,the descriptors were assigned to five scal levels based n the panel's udgment of their diffclty (se the Appendi for part of the analytic rating scale).

Despite the efforts involved in developing the analytic scale (se Zou & Fan, 2019; Zou & Fan, 2022) it remains unclear regarding how the scale can be further leveraged to provide meaningful information for the teaching and learning of CE writing in China. Therefore, the purpose of this study is two-fold ( to evaluate the psychometric qualit of the analytic scale for the CET-4 writing test; and (b) to identify score profile for the CET-4 test takers. Specifially, the study aimed to investigate the following two research questions (RQs):

RQ1: Is the localized analytic rating scale psychometrically sound for the CET-4 writing?

RQ2: What scoring profiles can be identified in the CET-4 writing test?

# 4. Methods

To address the two research questions, this study was conducted in two sequential research phases (see Fig. 1 for the design of this study). The first phase focused on evaluating the psychometic quality of the analytic rating scale (RQ1), while the second phase aimed to identify score profiles for the CET-4 writing test (RQ2). The insights gleaned from the irst phase not only laid a solid foundation for the second phase but also facilitated the extrapolation of these results to granular teaching and learning scenarios. Consequently, this research seamlessly transitioned from evaluating the psychometric quality of the rating cale to pinpointing core profiles, culminating in their alignment with pertinent educational contexts. Research procedures at each phase are delineated in the following sections, including research instruments, participants, methods of data collection and data analysis.

# 4.1. Phase I: Examining the psychometric quality of the analytic scale

In this phase, we recruited 21 CET-4 raters from different CET rating centers nationwide to fulfil th requirements of purposive sampling. Out of the 21 raters, 19 had prior experience in evaluating examinees' performance on the CET-4 writing test, and boasted over 5 years f teaching experience. The remaining two raters, whil les sesoned, were both certified C raters. To ensure the raters understanding of the the analytic rating scale as well as the analytc scoring pproach, thorough rater training sessions wereconducted prior to their commencement of essay scoring.

Endorsed by the CET commitee this study used thirty writing scripts from previous operational CET-4 tets. The scanned copies of the scripts were distributed to the raers via email, who were asked to rate each of these scripts using the analytic rating scale and send back their ratings along with signed consent forms within two weeks.

The rating data was subject to an MFRM analysis using FACETS version 3.80.0 (Linacre, 2013). The MFRM has been recognized for its effectiveness in both validating existing rating scales (e.g., Fulcher, 1993, 1996; Milanovic et al., 1995), and empiriclly con. structing language proficiency scales (e.g., North & Schneider, 1998). McNamara, Knoch and Fan (2019) provide a comprehensive overview of the capabilitie of MFRM in analyzing data from performance language assessments ike writing and speaking, underscoring "specific objectivity" as it salient advantage over other statistical models (p. 176). Ths suggests that, i our study, the Rasch model can adeptly align particular facets from the analytic rating processto a logit scale. This alignment is crucial for pinpointing potential biases in either rating criteria or raters. Consequently, thescores resulted are in line with the intended construct, reinforcing the validity of the score interpretations (see e.g., AERA, APA & NCME, 2014).

![](img/6fa6d772be412dbaf4296ba6644855dc86bf5f68a27e8429f542bc79c1207444.jpg)  
Fig. 1. The study design.

The key facets of interest in our analysis included raters, examinees, and the analytic rating scale. Specificll, we examined statistis reiox ait, rr erty, r iy, s el ity, r cri tonlt, as e as model-data fit of each facet.

The Rasch model adopted in this study was developed by Masters and Wright (1997), which represented an extension of the basic Rasch model (Rasch, 1960). The mathematical expression of the model is:

Log $( \mathrm { P } _ { n j m k } / \mathrm { P } _ { n j m ( k - 1 ) } ) = \mathrm { B } _ { n } \cdot \mathrm { C } _ { j } \cdot \mathrm { A } _ { m } \cdot \mathrm { F } _ { m k } .$

Where.

$\mathrm { P } _ { \mathit { n j m k } } =$ probability of examinee $n$ being rated as $k$ on criterion $m$ by rater $j$   
$\mathrm { P } _ { n j m ( k - 1 ) } =$ probability of examinee $n$ being rated as $k { - } 1$ on criterion $m$ by rater $j ;$   
$\mathtt { B } _ { n } =$ proficiency level of examinee $n$   
${ \mathrm { C } } _ { j } =$ severity of rater $j ,$   
$\mathsf { A } _ { m } =$ difficulty of criterion $m$   
$\mathrm { F } _ { m k } =$ difficulty of being awarded the rating of $k$ on criterion $m$

In conducting the MFRM analysis, several statistics were meticulously scrutinized (Bond & Fox, 2015), including: 1) model-data fi statistics; 2) examinee separation; 3) rater separation and reliaility; 4) rater variation; 5) band level functionality; 6) ratig criterion functionality.

# 4.2. Phase II: Establishing the analytic score profiles

In the second phase, the same raters asessed 142 CET-4 esays using the validated scale. These essays were carefull chosen from recent operational CET-4 tests, touching upon a wide array of topics. It is worth mentioning that these essays were previously designated as sample essays by the CET commitee for rater training and certification. The scanned copies of the 142 scripts were distributed via email to the raters. However, during this phase, a partilly crossed rating design was adopted wherein every script was rated by at last two raters (McNamara et l., 2019). The colection of both the scrits and rating dataoccurred one month later, and the consent forms signed by the raters were also recycled.

In analyzing the rating data, we first performed an MRM analysis to convert students' raw scores on the four analytic rating criteria into unified Rasch measures, which represented a more accurate and objective measurement of students writing abilit (Bond & Fox, 2015). This processcould provide a standardized and more objective assessment of the scripts, thereby enhancing the con. sistency of the logit scores for the 142 essays on the 4 analytic criteria. Following the score conversion procedures recommended by Linacre (2002), we converted the logit scores onto a conventional 100-point scale.

Using the transformed logit scores, we then conducted two cluster analyses with spss 26.0: hierarchical analysis and K-Means analysis. The advantage of using hierarchical and K-Means cluster analyses in conjunction is that the two clustering algorithms have different analysis foci, thus can provide a more robust understanding of scoring patterns (Ginther & Yan, 2018).

For hierarchical cluster analysis, we used Ward's method of minimum within-group variance which can help extract the number of clusters emerged from the logit scores. As hierarchical cluster analysis is largely exploratory, there is no absolutely objective way to determine the optimal number of clusters (Ginther & Yan, 2018; Jarvis t al., 2003). One of the most recommended ways is to observe the scre plot which provides a visual ilustration of the change in agglomeration coefficient and uses the elbow rule tofind out the number of clusters just before a big drop in the relative distance coeficient (e.g., Staples & Biber, 2015; Kassambara, 2017). Then, a K-Means clustr analysis was performed to extract the average analytc scores of each scoring profile and the number of observations in each scoring profile. Results of the two-step analyses can help address the second research question.

# 5. Results

# 5.1. Psychometric quality of the analytic rating scale

To address RQ1, we outline the findings from the MFRM, focusing on key aspects like global model-data fit, examinee abilit, rater severity, and the operational efficiency of the rating scale.

# 5.1.1. Global model-data fit

Our evaluation of the psychometric quality of the rating scale involved a thorough asessment of the separation indices and the reliability etimates for both examines and rater. This ealuation is crcil a it juxtaposes exmnee ability and rater everity against the scale's inherent measurement errors, thereby determining global model-data congruence.

The reliability estimate for examinee ability was 1.00, indicating a high level of precision in the ordering of examinee ocations on the logit cale. This suggest how accurately the test can differentiate between examinees based on their abilities. On the other hand the reliability estimat for rater severity was 0.94,reflectig the precision in differentiating between raters based on their severit in scoring. While both values exceed the recommended benchmark of 0.8 (see Bond & Fox, 2015), it is important to note that they represent distinct aspects of the asessment proces. The examinee ability reliability is about the acuracy in ranking examinees performances, while rater severity reliability pertains to the accuracy in identifying different rater judgment patterns.

The separation index was 15.01 for examinee ability, implying a substantial spread in examinee proficiency levels. For rater severity, the index was 4.12, signifying discernibl variation in rater stringency. Both these indice exceed the baseline value of 2.0, as suggested by Oon & Subramaniam (2011).

It's worth noting that the RMSE values for the examinee and rater facets were 0.2 and 0.16 respectively. This reflects a commendable fit of our model to the data. That is to say, the observed scores from raters and the expected scores from the Rasch model for examinees are closely aligned.

# 5.1.2. Examinee ability

\*Note: each star in the second column of the two figures represents one examinee; $\mathrm { L A N G } = \mathrm { j }$ Linguistic range and accuracy, CONT $=$ Content and idea, ORGA $=$ Discourse organization, $\mathbf { A P P R } = \mathbf { j }$ Linguistic appropriacy; ${ \bf s 1 = }$ the sub-scale of Linguistic range and accuracy, ${ \bf S } 2 =$ the sub-scale of Content and idea, ${ \bf S } 3 = { }$ the sub-scale of Discourse organization, $\mathbf { S 4 } =$ the sub-scale of Linguistic appropriacy.

![](img/b0fa3c62739de86321a22e09090363b0019cc9e05acdaa80c9b354c86acac28d.jpg)  
Fig. 2. Variable map.

To closely examine the examinee separation, the variable map (seeFig. 2) resulted from MFRM analysis was checked as the map could provide visual information which facilitates direct comparisons between and within the facets of interest (Eckes, 2011).

The frst column of Fig. 2 shows the logit scale, whilst the second column presents the estimates of the examinee aility which are ordered in such a way that examinees with higher ability estimates clusterat the top, and those with lower ones at the bottom. The third column displays the 21 raters in terms of their severity or leniency, with more severe raters at the upper end, and vice versa.

As can be seen from Fig. 2, the examinee ability measures span from $+ 4 . 9 9$ to -- 6.05 logits, showing a spread of approximately 11 logits. The examinee separation ratio is 15.01 and the strata index is 20.29 $( \chi 2 = 5 7 9 0$ $d f = 2 0$ $\mathbf { p } < 0 . 0 1 \mathrm { \check { . } }$ , indicating that the examinee ability could be distinguished into around 20 statistically different levels.

# 5.1.3. Rater severity

Knoch (2009) posits that in an ideally functioning rating scale, raters as a group should not have significant differences in their severity or leniency. Therefore, a smalle rater separation ratio would be more desirable. As mentioned earlier, our rater separation ratio stands at 4.12, the strata index is 5.83 $( \chi ^ { 2 } = 5 7 9 0$ $\mathrm { d f } = 2 0$ $\mathbf { p } < 0 . 0 1 $ ) with associated reliability index is 0.94. This indicates that the raters, as agroup, can be approximately cateorized into 5 stisticly distinct levels based on their severity. orover, as depicted in Fig. 2, rater pread is much smaller than the pread of examinee ability, indicating that rater erit had a minimal influence on the measurement of examinees' performance (Eckes, 2011).

In addition to raters group-level statistics, their individual statistics were also checked to pinpoint those with aberrant rating behaviors (see Table 1). According to Knoch (2009), a wellfunctioning rating scale would result in fewer raters who rate either inconsistently or unduly consistently. Given the high stakes asociated with the CET-4, we adopted a more stringent range of 0.7-1.3 for rater infit mean square (MnSq) values (Bond & Fox, 2015) to detect possble individual variation in rating. As shown by Table 1, there was only one misfitting rater (R18) with infi values slightly above 1.3, sugesting that this rater showed more variations than the Rasch model had predicted. Besides, there were two raters (R5 and R20) with infit Mnsq values slightly lower than 0.7, indicating that they exhibited les variation than the model had expected. Apart from these raters, the other raters statistics were fairly consistent with model expectation.

Taken together, these results showed that the raters' scoring pattrns at both group and individual level were generally consistent with the expectations of the Rasch model. Furthermore, the MFRM estimates allow for meaningful comparisons of examinee abilities across different raters.

# 5.1.4. Functionality of the rating scale

5.1.4.1. Functionality of the scale levels. In examining the functioning of the rating scale, we first checked the functionality of the scale levels. A set of indices as proposed by Linacre (2002) were adopted which include the observed counts of each level, the average measures, the outfit mean square values and the threshold calibrations. Table 2 summarizes the findings regarding these indices.

As Table 2 shows, the observations of the five levels on each sub-scale ranged from 35 to 195, al of which greatly exceeded the minimum requirements of 10. This indicates that the five levels on each sub-scale were consistently used by the raters. Besides, the average mesures incrased as the scale evels adanced. The outit mean square values were ther equal or close to the expected value of 1. Furthermore, the level thresholds advanced monotonically with the increase of the levels. Also, the thresholds between adjacent levels all stayed within the range of 1.4 to 5.0 logits as suggested by Bond and Fox (2015). Taken together, these findings indicate that the five levels on the analytic scale could function in line with the expectations set by the Rasch model.

Table 1 Measurement Results for the Rater Facet.   

<html><body><table><tr><td>Rater</td><td> Measure</td><td>Model S.E.</td><td>Infit MnSq</td><td>Outfit</td></tr><tr><td>7</td><td>1.25</td><td>0.16</td><td>1.20</td><td>1.26</td></tr><tr><td>6</td><td>1.17</td><td>0.16</td><td>1.23</td><td>1.25</td></tr><tr><td>14</td><td>1.01</td><td>0.16</td><td>1.11</td><td>1.03</td></tr><tr><td>4</td><td>0.72</td><td>0.16</td><td>1.02</td><td>0.94</td></tr><tr><td>16</td><td>0.64</td><td>0.16</td><td>1.03</td><td>1.62</td></tr><tr><td>11</td><td>0.57</td><td>0.16</td><td>0.86</td><td>0.82</td></tr><tr><td>2.</td><td>0.54</td><td>0.16</td><td>0.70</td><td>0.70</td></tr><tr><td>20</td><td>0.36</td><td>0.16</td><td>0.65</td><td>0.63</td></tr><tr><td>12</td><td>0.00</td><td>0.16</td><td>1.06</td><td>1.00</td></tr><tr><td>13</td><td>-0.10</td><td>0.16</td><td>0.74</td><td>0.73</td></tr><tr><td>18</td><td>-0.25</td><td>0.16</td><td>1.34</td><td>1.25</td></tr><tr><td>1</td><td>-0.28</td><td>0.16</td><td>0.96</td><td>1.16</td></tr><tr><td>17</td><td>-0.28</td><td>0.16</td><td>0.85</td><td>0.83</td></tr><tr><td>21</td><td>-0.28</td><td>0.16</td><td>1.09</td><td>1.11</td></tr><tr><td>3</td><td>-0.38</td><td>0.16</td><td>0.82</td><td>0.86</td></tr><tr><td>8</td><td>-0.56</td><td>0.16</td><td>1.00</td><td>1.04</td></tr><tr><td>15</td><td>-0.79</td><td>0.16</td><td>1.16</td><td>1.26</td></tr><tr><td>19</td><td>-0.79</td><td>0.16</td><td>0.99</td><td>0.95</td></tr><tr><td>5</td><td>-0.82</td><td>0.16</td><td>0.65</td><td>0.66</td></tr><tr><td>9</td><td>-0.82</td><td>0.16</td><td>0.94</td><td>1.08</td></tr><tr><td>10</td><td>-0.90</td><td>0.16</td><td>1.09</td><td>1.07</td></tr><tr><td>Mean (Count: 21)</td><td>0.00</td><td>0.16</td><td>0.98</td><td>1.01</td></tr><tr><td>S.D. (Sample)</td><td>0.70</td><td></td><td>0.19</td><td>0.24</td></tr></table></body></html>

Table 2 Statistics of the scale levels.   

<html><body><table><tr><td rowspan="2">Level</td><td colspan="4">Observed count (%)</td><td colspan="4">Average measure</td><td colspan="4">Outfit MnSq</td><td colspan="4">Threshold calibration</td></tr><tr><td></td><td> S2</td><td>s3</td><td> S4</td><td>S1</td><td> s2</td><td> s3</td><td> S4</td><td>S1</td><td> S2</td><td>S3</td><td> S4</td><td>S1 s2</td><td>S3</td><td></td><td>S4</td></tr><tr><td>1</td><td>101</td><td>57.</td><td>87</td><td>117.</td><td>-4.75</td><td>-4.27</td><td>-4.36</td><td>-5.06</td><td>0.9</td><td>1.2</td><td>1.0</td><td>0.9</td><td></td><td></td><td></td><td></td></tr><tr><td>2</td><td>(16%) 131</td><td>(9%) 121</td><td>(14%) 122</td><td>(19%) 143</td><td>-2.59</td><td>-2.37</td><td>-2.19</td><td>-2.70</td><td>1.0</td><td>1.1</td><td>1.2</td><td>1.0</td><td>-3.89</td><td>-4.19</td><td>-3.67</td><td>-4.04</td></tr><tr><td></td><td>(21%)</td><td>(19%)</td><td>(19%)</td><td>(23%)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>3</td><td>169</td><td>181 (29%)</td><td>169 (27%)</td><td>195 (31%)</td><td>-0.16</td><td>0.13</td><td>0.05</td><td>-0.13</td><td>0.8</td><td>1.2</td><td>1.1</td><td>0.9</td><td>-1.55</td><td>-1.56</td><td>-1.40</td><td>-1.66</td></tr><tr><td>4</td><td>(27%) 182</td><td>189</td><td>162</td><td>140</td><td>2.25</td><td>2.51</td><td>2.33</td><td>2.24</td><td>0.8</td><td>1.1</td><td>1.1</td><td>0.9</td><td>0.92</td><td>1.28</td><td>1.21</td><td>1.34</td></tr><tr><td></td><td>(29%)</td><td>(30%)</td><td>(26%)</td><td>(22%)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>5</td><td>47</td><td>82</td><td>90</td><td>35.</td><td>4.09</td><td>4.47</td><td>4.01</td><td>3.64</td><td>0.9</td><td>1.1</td><td>1.2</td><td>1.0</td><td>4.51</td><td>4.47</td><td>3.86</td><td>4.36</td></tr><tr><td></td><td></td><td></td><td>(14%)</td><td>(6%)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>(7%)</td><td>(13%)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

\*Note: ${ \bf S 1 = }$ the sub-scale of Linguistic range and accuracy, ${ \bf S } 2 = { }$ the sub-scale of Content and idea, ${ \bf S 3 = }$ the sub-scale of Discourse organization, ${ \bf S } 4 = { }$ the sub-scale of Linguistic appropriacy. a) Functionality of the rating criterion

In using analytic scale to rate ESL writing performance, one of the long-standing issues is the halo effect' which is caused by difficultie in differentiting varous raing critria e, Wege, 2002; Shaw & Weir, 2007). herfore, one the analytic scale was established, a primary concen was to evaluate the distinctivenes of each rating criterion. In our study, statistics relating to the rating criteria are presented in Table 3.

As shown by the table, the four rating criteria differed significantly in terms of their dificulty measures $\textstyle ( \chi ^ { 2 } = 2 3 3 . 5$ $d f = 3 .$ $p < 0 . 0 1 \dot$ , with Linguistic appropriacy being the most difficult (0.75 logits), while Content and idea being the least difficult $_ { ( - 0 . 6 8 }$ logits). This indicates that it i rlatively more diffiult for examinees to obtain  high score on Linguistic apropriacy than on Content and idea. Meanwhile, the infit Mnsq values of the four criteria all stayed within the control range of 0.7 to 1.3, suggesting that the performance f the four rating critria ll fit with the Rasch model expectations. verall, these findings showed that the four rating criteria could function as distinctively as expected. No group-level halo effect was detected.

In summary, results from the MFRM analysis indicate that the analytic rating scale could robustly discriminate the CET-4 writing performance. Notably, the scale was used by the CET-4 raters both reliably and consistently, and no significant group-level halo ffect was detected. Additionall, both the scale levels and rating critria functioned distinctiely as anticipated. Collectively, these findings provided compeling evidence for the sound psychometric quality of the localized analytic rating scale in the CET-4 writing, thus providing answers to RQ1.

# 5.2. Score profiles of the CET-4 writing

To address RQ2, we first present the scree plot for the herarchical custer analysis. As shown in Fig. 3, the line indicates a demarcation point at three clusters on the transformed logit scale, suggesting that three analytic scoring profiles emerged from the CET-4 writing performance.

When the optimal number of clusters was entered in the subsequent K-Means cluster analysis, thre score profiles were extracted (see Table 4). This confirms the existence of the three score profiles among the CET-4 writing performance.

The three profiles were also graphically presented in Fig. 4, labeled as Discriminant high (DH), Discriminant medium (DM) and Discriminant low (DL). Its essential to understand that since the clusters were based on thessays logit scores derived from MFRM, a higher score indicates greater writing proficiency, and vice versa. As presented in Table 4:

For the DH profil, essays notably achieved a higher average logit score of 78.66 in Linguisic range and accuracy, whil Linguistic appropriacy was the lowest at 72.12 logits, while scores for Content and idea and Discourse organization stayed close to each other.

For the DM profil, the range of average oit scores acrossthe four criteriabegan at 39.37 for Linguistic appropriacy. Linguistic rang and accuracy came in second lowest at 44.21, but Content and idea and Discourse organization exhibited notably higher scores, regis tering at 48.73 and 48.23 logits respectively.

Regarding the DL profile, scores ranged from 9.58 logits for Linguisic appropriacy to 19.90 for Content and idea. Linguistic range and accuracy had the second-lowest average at 13.14 logits whereas Discourse organization was second-highest with 16.64 logits.

\*Note: The logit scale in the figure is a transformed logit scale, not the original logit scale.

Table 4 further reveals that among the three core profiles, DH and DM profiles were more dominant than the DL profil. Across these profiles Linguistic appropriacy consistetly reeived the lowest ratings, implying that it's a univerally challenging area for the CET-4 examinees irrespective of their proficiency level. Notably, examinees in the DH profile exhibited pronounced strengths in Linguistic range and accuracy, suggesting their need to focus on improving the other criteria. Conversely, DM students displayed balanced proficiencies in Content and idea and Discourse organization, necesstating concentrated efforts on enhancing Lingustic range and accuracy and Linguistc appropriacy. Lastl, for the DL group, while they exhibited slight strengths in Content and idea and Discourse organization, they should prioritize improving Lingusic range and accuracy and Lingusic appropriacy to elevate their overall writing

Table 3 Statistics of the Rating Criterion.   

<html><body><table><tr><td>Criterion</td><td> Measure</td><td>S.E.</td><td colspan="3">Infit</td><td>Outfit</td></tr><tr><td></td><td></td><td></td><td>MnSq</td><td>Std</td><td>MnSq</td><td>Std</td></tr><tr><td>Linguistic appropriacy</td><td>.75</td><td>.07</td><td>.87</td><td>-2.2</td><td>.93</td><td>-1.1</td></tr><tr><td>Linguistic range and accuracy</td><td>.24</td><td>.07</td><td>.84</td><td>-2.9</td><td></td><td>-2.0</td></tr><tr><td>Discourse organization</td><td>-.31</td><td>.07</td><td>1.07</td><td>1.1</td><td>.88</td><td>1.7</td></tr><tr><td>Content and idea</td><td>-.68</td><td>.07</td><td>1.11</td><td>1.9</td><td>1.11</td><td>2.3</td></tr><tr><td>Mean</td><td>.00</td><td>.07</td><td>.98</td><td>-.5</td><td>1.14</td><td>.2</td></tr><tr><td>S.D.</td><td>.54</td><td>.00</td><td>.12</td><td>2.1</td><td>1.01 .11</td><td>1.9</td></tr></table></body></html>

![](img/9fe15dbdc3e50b0d5188682a43bee0a666c00be7ababd9044fd25b425e58599d.jpg)  
Fig. 3. Scree plot for hierarchical cluster analysis of the CET-4 writing scores.

Table 4 The analytic score profiles among the CET-4 writing performance.   

<html><body><table><tr><td> Scoring profile</td><td>N</td><td>Linguistic range and accuracy</td><td>Content and idea</td><td>Discourse organization</td><td>Linguistic appropriacy</td></tr><tr><td>Discriminant high LANG&gt;CONT&gt; ORGA&gt;APPR</td><td>46</td><td>78.66</td><td>76.36</td><td>75.07</td><td>72.12</td></tr><tr><td>Discriminant medium ORGA, CONT&gt;LANG&gt;APPR</td><td>60</td><td>44.21</td><td>48.73</td><td>48.23</td><td>39.37</td></tr><tr><td>Discriminant low CONT&gt;ORGA&gt;LANG&gt;APPR</td><td>36</td><td>13.14</td><td>19.90</td><td>16.64</td><td>9.58</td></tr></table></body></html>

\*Note: 1. LANG $=$ Linguistic range and accuracy, CONT $=$ Content and idea, ORGA $=$ Discourse organization, APPR $=$ Linguistic appropriacy. 2. All the logit scores are achieved through transformed logit scale, not the original logit scale.

![](img/4457e53b4b6f5bc398dba74bbe2596267f1f188d84189f8f04d0eb42b6611012.jpg)  
Fig. 4. The three CET-4 analytic scoring profiles.

proficiency.

To sum up, results from the hierarchical cluster analysis and K-Means analysis revealed three distinct core profiles within the CET4 writing scripts: Discriminant high, Discriminant medium, and Discriminant low. These profiles underscoredifferent areas of trength and challenge among examnees with dfferent writing proficiency. Overall these findings can offr a comprehensive understanding of examinee performance across various rating criteria in the CET-4 writing assessment, hence addressing RQ2.

# 6. Discussion and implications

This study reports on the validation of a localized analytic rating scale (Phase 1) and the establishment of analytic core profile for the CET-4 writing (Phase H). In Phase I, we analyzed data from 21 raters who used the analytic scale to rate 30 CET-4 writing scripts. The rating data were subject to an MFRM analysis. In Phase I, we explored the core profiles in the CET-4 writing performance by using the analytic scale to rate 142 CET-4 writing scripts. Hierarchical cluster analysis and K-means analysis were conducted based on the Rasch measures obtained from the MFRM analysis of the rating data.

In Phase I, the results showed that the analytic rating scale was psychometrically sound. The scale could distinguish examinees abilities in the CET-4 writing test into statistically diffrent levels, while also leading to desirable consistency among raters and reducing rating discrepancies. The distinctiveness of both the scale levels and rating criteria has been confirmed to meet expected standards. This suggests that a localized analytic ratig scale can offr reliale informatin for both the tet takers and the test users of the CET-4 writing. These outcomes are largely consistent with previous iterature in that analytic rating scale can better control rater variability, thus esuring rating reliabilit (.g., Park & Yan, 2019; Zou, 2022; Barkaoui, 2007, 2010). Such findings are also signficant as they provide useful insights into tackling score variability and reliabilit ssues asociated with holistic scoring systems used in large-scale L2 writing assessments (see, for example, Zhao & Huang, 2020).

Another noteworthy finding from Phase I concerns the absence of significant group-level halo effcts when utilizing the analytic scale. This result contrasts with some previous studies (e.g., Farrokhi & Esfandiari, 2011; Lai, Wolfe, & Vickers, 2015). However, it aligns with Knoch's (2009) conclusion that an empirically-developed ratig scale can effctively mitigate the halo effect compared to an intuitively-developed rating scale. This finding implies that the halo efect, a long-standing isue that has plagued the users of analytic rating scales, can be addressed or even avoided by validating the rating scale in advance based on actual test performance. Such a validation processensures a more accurate understanding of the analytic rating criteria and associated descriptions, thereby minimizing the potential for halo effects.

In Phase I of our study, three distinctive score profiles were identified for the CET-4 writing tet. A consistent observation was the students' noticeable deficiency in Lingustic opropriacy. Echoing Liu and Zhang's (2018) findings, it reveals that English leaners in China often grale with pragmatic knowledge, underscoring the importance of nurturing their pragmatic appropriacy skills. An additional key observation is the variances in strengths exhibited by students at different CET-4 writing proficiency levels. For instance, while those t higher proficiency levels displayed commendable linguistic range and accuracy, they stil facd challenges in areas such as content and ideas, discourse organization, and linguistic appropriacy. Conversely, examinees at the medium level performed better in content and ideas and discourse organization compared to other areas. This relative weaknessin other aspects is likely to exert an adverse impact on their overall performance on the CET-4 writing test. Thus, enhancing teaching and learning support in these weaker areas is recommended to improve their overall writing proficiency.

Given these observations, it is evident that standardized writing assessments such as the CET-4 writing test can hardly meet the needs of L2 learners with varied writing proficiency. Despite reform measures introduced in studies like Yang, Gu and Liu (2013) and Jin (2019), there are ongoing discussions about how the CET-4 can be further refined to provide more relevant feedback for CE programs, notably in the domain of L2 writing (e.g, Li, 2009; Zha0 & Huang, 2020). As such, our research provides a theoretical perspective that can deepen our understanding of learners performance in large-scale L2 writing asessments like the CET-4 writing, thereby laying the groundwork for developing context-specific assessment practices in L2 education. Specificall, the areas of improvement as identified in our study present meaningful insights for curriculum designers and educators to emphasize and bolster these aspects, ensuring pedagogical methods align closely with the needs of Chinese English learners. Moreover, the findings yielded from our study are also instrumental for CE program coordinators and educators in making informed curriculum decisions, directing content creation, and shaping teaching trategies to addres the specific challenges encountered by students. By prioritizing these areas, iti expected that learners can achieve a more comprehensive writing proficiency, ensuring their competence aligns not just with rating criteria but with practical real-world communication needs.

Meanwhile, although it is widely recognized that L2 learners often exhibit uneven development across various dimensions of English writing (Lee et al., 2010), comprehensive insights into the writing progression of Chinese English learners remain limited. Predominant reearch attaches more importance to a corpus-driven approach (se e.g., Bai & Ye, 2018; Zhu, Yang, & Liu, 2021), emphasizing quantitative metrics like complexity, accuracy, and fluency. Our research, however, introduces a methodological framework that merges the validation of the rating scale with the delineation of distinct performance clusters. By integrating Rasch measurement outcomes with cluster analysis, we found nuanced distinctions in the CET-4 writing performance acros varying proficiency levels. This method not only addresses the inquiry of the oalization of large-scale L2 ssessments, but more importantly, it offers a holistic strategy for tailoring such assessments to specific educational contexts.

# 7. Conclusions, limitations and future research

In conclusion, this study bridges the gap between research and practical applications by demonstrating the value of establishing score profile for large-scale L2 writing assessments like the CET-4 writing test. This ensures that such assessments lign more closely with the pedagogical and learning needs of distinct CE programs in China. From a methodological perspective, our research can provide a reference for L2 researchers and test developers aiming to tailor large-scale L2 assessments to local teaching and learning needs, enhancing the meaningfulness of the assessment outcomes for both test takers and L2 teachers.

While we emphasize the important insights generated by our study, we acknowledge the limitations of our study. The first limitation isrlated to the adoption of the widely-accepted range of 0.7 to 1.3 in using MFRM to evaluate model fit Although this range offers apractical benchmark, it lacks grounding in empirical statistical tes. Recent advancements in the educational measurement field suggest that empirically derived values, such as those from boostrapping procedures, fer greater precision (eg, Seol, 2016; Wolfe, 2013). While we did not use these procedures in our study, we recognize their potential in providing a more nuanced understanding of model fit. As such, future research may benefit from combining such empirically derived values with conventionally recommended values to render a more comprehensive evaluation of model fit In addition, the sample size used in constructing the analytic score profiles isrelatively small considering the large scale of the CET-4. This could, to some extent, limit the findings generalizability. Therefore, further tudies with larger sample sizes are recommended to provide more nuanced and refined profile information for the CET-4 writing or other L2 writing assessments.

Overall, the conclusions drawn from this study, though tentative, provide valuable insights for both the CET program and other large-scale assessment programs in terms of adapting their reporting oftes results to met the evolving needs of local teaching and learning contexts. Furthermore, our study underscores the significance of exploring the connections between test performance, scoring, and learning needs to comprehend the rationale and research perspectives in conducting localization studie inthe contexts of large-scale L2 writing assessments.

# Funding detail

This article is generously funded by University of Health and Rehabilitation Sciences under Grant number Kwxz2024005.

# CRediT authorship contribution statement

Zou Shaoyan: Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Writing - original draft, Writing - review & editing. Yan Xun: Conceptualization, Data curation, Methodology, Supervision, Writing - review & editing. Fan Jason: Methodology, Writing - review & editing.

# Declaration of Competing Interest

None.

# Data Availability

Data will be made available on request.

# Appendix: An example of the analytic rating scale for the CET-4 writing

<html><body><table><tr><td>Level 5</td><td>Linguistic range and control Can use a sufficient range of vocabulary to convey information and ideas effectively. Can use some fairly advanced words or phrases as well as common idioms, proverbs, sayings or expressions to convey meaning more precisely or to enhance the expressing effect. Both the choice of words and the use of collocations are accurate and idiomatic, thus making the expression clear and distinct. Can effectively and accurately use a variety of sentence patterns,</td><td>Content and idea The content is closely related to the topic and the opinions are expressed in a clear and distinct manner. Can explain or illustrate issues and ideas with adequate examples or in proper ways. Can effectively organize and use facts and details to support one&#x27;s arguments. Can produce clear, well- organized and well-developed text, demonstrating a sufficient understanding of the topic.</td><td>Discourse organization Can appropriately and flexibly use a wide variety of cohesive devices to indicate the logical relations between sentences and to make the text clear and well-organized. Can maintain the cohesion and consistency of the content through echoing the points mentioned before. Good paragraph structures, with clearly expressed main points and adequate supporting details. Has an adequate control of the connections between paragraphs as well as the logical relations between parts and the whole.</td><td>Linguistic appropriacy Can express appropriately and idiomatically in written language based on specific contexts. Has certain awareness of the audience, and the language style and register (formal or informal) are rather appropriate. Can express feelings or attitudes in a manner that is appropriate to the context. Can idiomatically use cultural references and figures of speech to convey information effectively.</td></tr></table></body></html>

# References

18(3), 279-293.   
Becker, A. (2018). Not to scale? An argument-based inquiry into the validit f an L2 writing rating scale. Asessing Writing, 37, 1-12.   
Bond, T.G., & Fox, C.M. (2015). Aplying the Rasch model: Fundamental measurement in the human sciences (3rd ed). New York: Routledge.   
hapele , ight,   on, J . (208. udn alidt mt for he e f ish  F .  or e.   
Choi I. (2017). mrica ef a lsh ici fro n nth cg tes.  g 341), 4982.   
Chon . hi .  021).m  i t  h x  t, sti ct and errors. System, 96, Article 102408.   
Deyger, B, & Van Gorp, K (2015). Dening th scoring validity f aco-constructed CEFR-based rating scal. Langug. Testing, 32(4), 521-541.   
Eckes, T. (2011). Frankfurt. Introduction to many-facet Rasch measurement. Peter Lang.   
an,t2 t   ice     ., 91, 76-89.   
Fan, J, Frot K, Jin . (2022). Loal eglish testin n hnstrtiary ctio: onxts, plcie, and prctice. ge ting 39(3), 453-473.   
Fei, Q. & Zhao, Y.Q. (2008). Problems in CET-4 writing rubric and coring method. Foreign Languag Teching Thorie and Practice, 4),45-52, 93.   
Frial,  i,  e .. 014sti me    tio r hy ra a.  of econ Language Writing, 23, 1-16.   
uer,     n   o  n   t  ster,.   
Fulcher, G. (1996). Does tick decription eadto smart tests? data-based aproach to rating cale onstructio. Languge etig 13(2), 208-238.   
iter,   018).i theoh    a  rii, cy, d p  ig 352, 271-295.   
Gren, A (2017) Lngr ge t Prin Matrils  codin in tes? P in gg tig d Aset, 61), 12-131.   
Hamp-Lyons, L. (1995). Rating nonnative writing: The trouble with holistic scoring. TEs0L Quarterly, 29(4), 759-762.   
Hamp-Lyons L. (2016). Farewell to holistic scoring. Part Two: why build a house with only one brick?Assessng Writing, 29, A1-A5.   
m-y 91). . NJ: Ablex Publishing Corporation.   
Hn, .00  at   sic g r   t hy c.g 3) 201-218.   
Jang, 9.i    et Language Testing, 26(1), 31-73.   
Janssen, G., Meier,V., & Trace, J. (2015). Building a better rubric: Mixed methods rubric rvision. Asssing Writin, 26, 51-66.   
Jarvis, ., t , ki, ris  (203).  e   y r r tio.  f in 12(4) 377-403.   
Jin, Y. (2008). Powerful tests, powerles test designers? - Challenges facing the College English test. CELEA Jounal, 31(5), 3-11   
Jin, . (2019 ti tie s  l: Thleish  i  L, r . .  u, .R .), s Language Proficiency Testing in Asia (pp. 101-130). Routledge.   
Jones, N., Saville, N., & Salamoura, A. (2016). Learning oriented assessment (Vol. 45). Cambridge University Press.   
Kassambara, A. (2017). A practical guide to cluster analysis in R: Unsupervised machine learning. Sthda.   
m 15) i   i  .g 32(2), 227-258.   
Kim, Y. H. (2011). Diagnosing EAP writing ability using the reduced Reparameterized Unified Model. Language Testing, 28(4), 509-541.   
Knoch, U. (2009). Diagnostic writing assessment: The development and validation of a rating scale. Peter Lang.   
Knoch, U.11 nr sti  i he  ike ah riri  g 62) 81-96   
Lai,   . , .5.ff ir   ing n, 51 102-12.   
Lald   016.  t   u   the  ti electronic portfolios. Assessing Writing, 30, 44-62.   
Lee, .   010 -t srf t ins  istc ic,  f e. Applied Linguistics, 31(3), 391-417.   
Leighton, J., & Gl,M . (2007). nie dtic smet fr cio: hy d aptios. mbrig mbrige erst Pre.   
Li H   (015.ion  raersa-ng  as  f rg s.  rl, 2, 178-212.   
Li,  (0  c  th   t    f  (536.   
Li, H L ,  013). ttig d valtingQ-i fo tive stic alf a g t tiot, 11) 1-25.   
Li, J  .2   t tsi  t r c and national writing raters. Assessing Writing, 51, Article 100604.   
Linacre, J. M. (2002). Optimizing rating scale category effectiveness. Journal of Applied Measurement, 3(1), 85-106.   
Linacre, J.M. (2013). Facets Rasch measurement computer program (version 3.80.0). Chicago: Winsteps.com.   
Li, J.  018         f   . n Language World, 9(1), 79-87.   
Liu, Y.,  g, .00Te qualt fais i   imcin o qat mo  i on. Evaluation, 67, Article 100941.   
Masters, G., & Wight, D.(1997. Thepartil creit model. In Hdook of Modm tem Rponse hry (p. 101-121) ew York NY: pringer Nw York.   
McNamara, T., noch, U., & Fan, J. (2019). Faies, jstice, and language asessment The rol of meurement. Oxford: Xxford University Pres.   
Mivic  aill ,  ( in    .  k .), Validation in language testing (pp. 15-38). Clevedon: Multilingual Matters.   
Min .  2 l k  istn  d i ti e. Language Testing, 39(1), 90-116   
North, B. & Schneider, G. (1998). Scaling descriptors for language proficiency scales. Language Testing, 15(2), 217-262.   
ia 06   i   t  l 4-5: Japn Society of English Language Education & the British Council.   
Sullivan . (2020). Frword: Lcalization. In Su, Wr &u (Ed.), Enlish angug Poficiency etin i Asi p. xiixvi. Rutde.   
Osulia  6 i i  fo  .,hn ns h o o  . Taipei: Crane Publishing Company Ltd.   
On,P .  0h if  t tha   t the tifc R. F. Waugh (Eds.), Aplications of Rasch measurement in learning environments research (pp.119-139). Netherlands: Sense Publishers.   
Park H  a 2019).non  ra pecwthlstic le and a ary, ytic le on an  wtilmet t pers n Language Testing and Assessment, 8(2), 34-64.   
Rasch, G. (1960). Probabilistic Models for Some Inteligence and Attainment Tests. Copenhagen: Danish Institute for Educational Research.   
Sel, . (206. in the or mtoea thecii anof i for os rac i stitic.  t 18(3, 37-956.   
Shaw, S. D., & Weir, C. J. (2007). Examining writing: Reearch and practic in asessng second lnguage writing. Cambridge University Press.   
Shi     .1    a & J. Fitzgerald (Eds.), Handbook of writing research (2nd ed., pp. 395-410). New York: The Guilford Press.   
Staple,   Bbe . (2015.ster ai.  k .), ang qtive m sond   (. 243-274) ork, Routledge.   
Taeba J. h .014).iicl f t na dig  tmap g  co  rrs f luage proficiency. Language Assessment Quarterly, 11(3), 233-249.   
Toh, .13.t    t t   ihn  ein J Measuring up in Education, 251-226.   
Wang, Y., Xie, Q. (2022). Diagnosing EFL undergrduates discourse competence in acadic writing. Asesing Writing 53, Article 100641.   
Weigle, S. C. (2002). Cambridge. Assessing writing. Cambridge University Press,.   
Wind, ., & Ptron,   (2017.  yttic i f mets fo an ratg qualty i nge asmet nge tin 352), 161192.   
a  a University.   
Wolfe, E. W. (2013). A bootstrap approach to evaluating person and item fit to the Rasch model. Jounal of Applied Measurement, 14(1), 1-9.   
Wu R.019.  sh ici  t , d h i ti .1)..   
Xie . 2017)t  ain n is civei din th y  , 1, 264.   
an    ki, 1) f n  c i t admission ESL writing program at a USA University. In Challenges in language testing around the world (p. 529-546). Singapore: Springer.   
an, 013 k classes of a university. Chinese Journal of Applied Linguistics. 36(3), 304-325.   
ha, .    a  t  iin for assessment policy makers. Studies in Educational. Evaluation, 67, 1-9.   
Zhu,  g    2021). t  f sit  d g.    1), 4146.   
ou . (202h t f g  on th  wig d-s st     .)g thsh Chinese Learners' of English (pp. 11-28).   
Zou, S., & Fan, J. (2019). A prelinary examintion of the rating scale vlidity f the CET-4 writing. Forig ngge and Lie, 35(3), 148-156   
Zou, .  . 222. ic rtgcri w  e i. e, 133-43 60.

hayan  r   t    ) appeared in Language Assessment Quarterl, Assessing the English Language Writing of Chinese Learners of English (Springer).

a   t Chmpag   ber  th    d . s   k t p Assessing Writing, System, Journal of Second Language Writing, and Foreign Language Annals.

eboure ti s rhn  valt tr  diof    m thry ad ngae e   ti  o s    a e  t rs 2019) and Working Towards a Proficiency Scale of Business English Writing: A Mixed-Methods Approach (with Li Wang, Sringer, 2021)