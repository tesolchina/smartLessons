# A review of AWE feedback: types, learning outcomes, and implications

Qing-Ke Fu, Di Zou, Haoran Xie & Gary Cheng

To cite this article: Qing-Ke Fu, Di Zou, Haoran Xie & Gary Cheng (2024) A review of AWE feedback: types, learning outcomes, and implications, Computer Assisted Language Learning, 37:1-2, 179-221, DOI: 10.1080/09588221.2022.2033787

To link to this article: https://doi.org/10.1080/09588221.2022.2033787

# A review of AWE feedback: types, learning outcomes, and implications

Qing-Ke Fua $\textcircled { 1 0 } ,$ , Di Zoub $\textcircled{1}$ , Haoran Xiec $\textcircled{1}$ and Gary Chengd a School of Teacher Education, Huzhou University, Huzhou, PR China; bDepartment of English Language Education, The Education University of Hong Kong, Tai Po, Hong Kong; c Department of Computing and Decision Sciences, Lingnan University, Tuen Mun, Hong Kong; dDepartment of Mathematics and Information Technology, The Education University of Hong Kong, Tai Po, Hong Kong

# ABSTRACT

Automated writing evaluation (AWE) plays an important role in writing pedagogy and has received considerable research attention recently; however, few reviews have been conducted to systematically analyze the recent publications arising from the many studies in this area. The present review aims to provide a comprehensive analysis of the literature on AWE feedback for writing in terms of methodology, types of learners, types of feedback and its applications, learning outcomes, and implications. A total of 48 articles from Social Science Citation Index journals and four other important journals in the field of language education were collected and analyzed. The findings revealed that most previous studies on AWE applied quantitative research methods, rather than purely qualitative ones. The duration of the experiments in approximately $3 3 \%$ of the studies was shorter than ten weeks, and $1 0 \%$ of the studies were of one session only. The group size of over half of the studies had fewer than 30 participants, and $2 1 \%$ of the studies had medium to large group sizes (from 51 to 100). The focus of most of the articles was on L2 writers with little attention paid to L1 writers and K12 students. AWE feedback to some extent can improve students’ writing from the product-oriented aspect but is not as effective as human feedback (e.g. teacher or peer feedback). Students generally considered AWE feedback useful and were motivated when using it, although they noticed a lack of accuracy and explicitness as the feedback tended to be generic and formulaic. The results of the review have several implications for researchers, teachers, and developers of AWE systems.

# KEYWORDS

Automated feedback; writing; technology enhanced language learning; systematic review

# 1.  Introduction

Formative assessment is designed to evaluate students’ understanding of knowledge beyond fact-checking, and feedback is an essential part of this assessment process as it assists learning by evaluating their performance, ideally along with comments and suggestions for improvement (Zhu et  al., 2020). Feedback is normally provided by an agent who assesses and advises students; agents normally include teachers, peers, parents, or the learners themselves (Hattie & Timperley, 2007). Considering the importance and effectiveness of immediate feedback, as well as the difficulties teachers have in providing it both inside and outside of the classroom, researchers and educators have made continuous attempts to achieve real-time feedback using technological tools (Calvo & Ellis, 2010). With the development of artificial intelligence, timely feedback can now be automatically generated by software systems after learners complete their learning activities (Calvo & Ellis, 2010). Basic automated feedback or evaluation indicates only whether an answer is right or wrong, but with specific criteria, advanced computer systems are also able to provide rich individualized feedback (Mirmotahari et  al., 2018).

Compared to the feedback provided by humans, automated feedback can overcome knowledge barriers and offer consistent evaluation based on predetermined criteria (Cotos et  al., 2017; Hoang & Kunnan, 2016; Wilson et  al., 2014). Automated feedback systems can also respond instantly, which allows students to take immediate action to advance their learning (Cheng, 2017). Immediate feedback is essential for learning as it increases learners’ awareness of their mistakes (Leontjev, 2014) and helps them correct conceptual errors before they are internalized (Zhu, et  al., 2020). Automated feedback can also provide students with more personalized suggestions and assist them in strengthening their understanding of the learning content (Luo & Liu, 2017). Further, automated feedback systems can be used to capture human rater bias patterns (Liu & Kunna, 2016). As for the affective response, some students may be intimidated when receiving feedback from teachers and thus favor impersonal automated feedback (Hoang & Kunnan, 2016). In addition, automated adaptive feedback can improve students’ learning performance as it can adapt to their learning abilities, and customized comments tend to be conducive to effective learning (Leontjev, 2014; Ware, 2011). Leontjev (2014) compared adaptive corrective feedback, which incrementally adapts to students’ abilities by providing more details and explicit information, to simple feedback (simplified indications of whether students’ performance on items is correct or not) and found that the students who learned with adaptive feedback had significantly better achievement than those who learned with simple non-adaptive feedback.

# 1.1.  Applications of AWE feedback

AWE feedback has been applied in various contexts. For example, Wilson et  al. (2014) who investigated the use of AWE feedback on the writing of L1 students in grades 4–8, showed that students’ writing quality improved across revisions, although the gain was small and decreased over time with no significant transfer effects found. Sung et  al. (2016) also reported that AWE feedback could help grade 6 students develop their summary writing skills, and the improvement significantly increased across revisions. According to both Wilson et  al. (2014) and Sung et  al. (2016), repeated exposure to AWE feedback is conducive to young L1 students’ development of writing skills over time. Similarly, Kellogg et  al. (2010) argued that compared to no feedback or intermittent feedback, receiving continuous AWE can help college students effectively reduce errors in mechanics, grammar, and usage on a transfer test. Nonetheless, no reliable transfer effects were observed in holistic quality scores that were assessed from the aspects of grammar, usage, mechanics, style, organization, and development. In another study, Bond and Pennebaker (2012) found that AWE feedback improved adult writers’ pronoun use. Nevertheless, transfer effects on holistic writing quality seem difficult to achieve.

# 1.2.  Quality of AWE feedback

The accuracy of AWE feedback appears satisfactory in terms of linguistic forms (e.g. grammar, syntax, collocation, and mechanics) while relatively poor in terms of discourse-level feedback (e.g. content, cohesion, and structure) (Li et  al., 2015; Luo & Liu, 2017). Luo and Liu (2017) investigated Pigai (one of the most popular AWE systems in China) and found that this system provided less direct and indirect corrective feedback than individual or group peer feedback. AWE feedback was more helpful than teacher feedback for correcting linguistic errors and led to more content enrichment. However, Bai and Hu (2017) found that the accuracy of AWE varies across error types, with mechanics ranked top with an accuracy rate of 98, followed by grammar $( 5 8 \% )$ , and collocations $( 2 2 \% )$ . Further, Ranalli et  al. (2017) also showed that AWE is more suitable in lower-level writing courses.

# 1.3.  Types of feedback

In L2 writing classrooms, corrective feedback is common, consisting of direct and indirect feedback (Bruton, 2009; Luo & Liu, 2017). Direct feedback (direct comment or assistance for correcting errors or enhancing the deficiencies) is helpful for improving students’ deficiencies through direct assistance in correcting errors, while indirect feedback (indicating errors or deficiencies with no direct correction information provided) helps in developing the learners’ higher-order thinking skills (e.g. self-regulated learning) (Luo & Liu, 2017). Recently, researchers have begun to investigate and compare focused and unfocused feedback (Bitchener & Ferris, 2012). Focused feedback (comment on a single language feature in a written text) provides intensive advice on a limited number of language features to help learners continually improve over a period (Bruton, 2009). When corrective feedback is focused on targeted language areas, learners’ writing improved, and the effect lasted over time (Bitchener & Ferris, 2012).

There are no definite answers concerning which type of feedback is more effective and under what conditions it can best help students. In sum, diverse factors (e.g. individual characteristics and instructional contexts) tend to mediate the effects of feedback on learning (Shute, 2008).

# 1.4.  Previous review studies on AWE, their implications, and limitations

Compared to the large number of studies on AWE, attempts to synthesize the relevant literature have been limited. Among the few reviews, Graham et  al. (2015) examined the literature on formative assessment in student writing from grades 1 to 8 from various dimensions (e.g. grades, participant types, numbers of participants, methodology, genre types, study quality, and effect sizes). They found that the effect size of AWE feedback on students’ writing was 0.38, which was relatively small compared to the students who received feedback from adults (0.87), peers (0.58), and self (0.62; self-evaluate their own writing). Stevenson and Phakiti (2014) systematically reviewed the AWE literature in terms of diverse factors (e.g. the publication types, countries, programs, educational levels, language backgrounds, methodology, and outcomes). Their review results showed that the relatively few studies they found on AWE were highly heterogeneous across most of the aforementioned factors. Thus, they critically analyzed the effects of AWE rather than conducting a meta-analysis and concluded that there was little evidence to show student writing benefited from AWE feedback. Another most recently published review article by Nunes et  al. (2021) reviewed eight articles that investigated the effects of six AWE systems on writing. The results indicated that AWE feedback is effective for writing learning and instruction in K-12 classrooms, especially when the use of AWE systems was supported by teachers, and when students were provided with rich opportunities of writing practices.

In a synthesis of studies on how AWE is used in writing classrooms, Stevenson (2016) discussed three constructs: Purpose (why AWE is used); Action (what strategies teachers and students apply); and Use (how and when AWE is used). Findings revealed that saving teachers’ time, fostering autonomy, and developing writing processes were the most common objectives, while promoting social interaction and facilitating students’ understanding of conceptual knowledge were the least. Regarding Action, a combination of AWE feedback with teacher feedback was the most frequently adopted strategy, followed by scaffolding students’ use of AWE, embedding AWE in classrooms, and using AWE as an assessment tool. However, limitations brought by AWE included difficulties in understanding feedback, low accuracy and revision rates, and surface-level revisions. Stevenson suggested that contextual environments (e.g. genres and purposes) are important for AWE.

While both Graham et  al. (2015) and Stevenson and Phakiti (2014) compared AWE feedback to no feedback, they did not investigate teacher or peer feedback. Nunes et  al. (2021) only selected studies that assessed writing quality with quantitative measures. They only analyzed eight studies and did not cover all grade levels. Thus, a review study across different school levels is necessary. Moreover, Stevenson and Phakiti’s review examined only the effects of AWE on written production. Accordingly, in the present review of studies, we examine both the product and process of writing with AWE feedback from the perspectives of students’ cognitive, emotional, and behavioral engagement as AWE-assisted writing is generally considered a process that involves engagement in these three areas (Zhang, 2017; Zhang & Hyland, 2018), and the effects of AWE feedback are largely related to the extent to which an individual engages in the feedback in the three ways (Zhang, 2017). This complex interaction is underscored in a recent study by Ranalli (2021) who found that ‘engagement with (AWE) feedback is complex and multi-faceted’ (p. 13). Thus, learner engagement is a multidimensional construct that dynamically involves learners’ cognition, emotions, and behavior (Fredricks et  al., 2004), and technology is considered effective in promoting learning through learner engagement (Fatawi et  al., 2020).

With the rapid development of AWE-related technology, research on AWE has increased. Therefore, it appears appropriate to review the recent AWE research to note trends and generate implications for future work (Duman et  al., 2015; Zhang & Zou, 2020). Moreover, several previous review studies (e.g. Hsu et  al., 2012; Lin & Hwang, 2019) suggested that when reviewing the literature on technology-assisted learning, it is important to investigate the research trend from various perspectives such as the participants, research methods, applications of educational technology, and research outcomes (Chen et al., 2020a, 2020b; Zou et al., 2021). Both researchers and practitioners can benefit from such findings (Chen et al., 2021a, 2021b, 2021c). The following research questions are thus proposed:

1. How were AWE studies designed or implemented in terms of their methodology, learners, feedback, and applications?   
2. What kinds of learning outcomes were measured, and what were the main findings?   
3. What implications were suggested?

# 2.  Method

# 2.1.  Data search and collection

The reviewed articles were retrieved from Social Science Citation Index (SSCI) journals as they adopt stringent review criteria before accepting articles for publication and thus have a good reputation and broad influence in the academic community (Duman et  al., 2015). In recent years, researchers have increasingly used SSCI articles to conduct literature review research (e.g. Shadiev et  al., 2017; Zhang & Zou, 2021a; Zheng et  al., 2016; Zou et al., 2020). One benefit of this method is that the variation of quality in the selected articles is reduced which enhances the validity of the review findings. In previous reviews, for example, Su and Zou (2020), Zhang and Zou (2021b), and Zou et al. (2020) included book chapters, conference articles, and non-SSCI journal articles to increase coverage at the beginning stages; however, they later found that the quality of these studies were weak, so they decided to confine their sample to SSCI journal articles only. Similarly, Duman et  al. (2015) and Shadiev et  al. (2017) limited their reviewed articles to SSCI publications. We also included four non-SSCI journals (i.e. CALICO Journal, RELC Journal, The Journal of AsiaTEFL, and Writing & Pedagogy) in our search in consideration of their influential role in the field of writing education.

Informed by the terms used in Stevenson and Phakiti’s review article (2014) and after reading through some sample studies, we used the following search string: (automat\* OR automat\* OR generated) AND (score OR feedback OR assess\* OR evaluat\*) AND (computer OR technology OR mobile OR online) AND writ\*. We collected the potential target publications in SSCI journals using the ISI Web of Science as the search engine and found 1267 articles. We also searched the four other journals and found 167 more articles.

Inclusion criteria were then identified to further limit the target articles to those that were closely related to the purpose of the study. Specifically, the target articles had to meet the following requirements: a) be an empirical study; b) have completed at least one round of learning in the implementation (i.e. students wrote first, and then received some feedback that was automatically generated, based on which, they further revised); c) have at least one learning outcome measured after the implementation; d) be written in English; and e) be published before December 11, 2021. After excluding the articles that did not meet the above criteria, a total of 48 articles from 32 journals were selected for the subsequent data coding and analysis. Seven articles were published in Computer Assisted Language Learning, four in Computers & Education, three in Language Learning & Technology, two in Computers in Human Behavior, two in ELT Journal, two in System, two in Journal of Computer Assisted Learning, and two in Assessing Writing. The other 24 articles were published in 24 individual journals (please refer to the list of coded articles for more details).

# 2.2.  Coding scheme

Guided by the research questions, the Technology based Learning model from Lin and Hwang (2019), and several rounds of discussions, the first draft of the coding scheme was developed by the authors. One author is an experienced teacher and scholar in the field of language teaching, while the others specialize in educational technology. The researchers then coded several articles using the draft coding scheme during which they took notes on issues concerning the accuracy, comprehensiveness, and appropriateness of the coding scheme, and then further revised the scheme. After three rounds of revisions, the coding scheme was finalized with agreement among the researchers.

Table 1. Coding schemes for methodology, types of learners, feedback, and applications.   

<html><body><table><tr><td>Category</td><td>Subcategories</td><td>Coding items</td><td>References</td></tr><tr><td rowspan="4">1. Methodology</td><td>1.1 Type of research methods</td><td>1) quantitative, 2) qualitative, 3) mixed methods.</td><td>Hung et al. (2018)</td></tr><tr><td>1.2 Design</td><td>1) between-subjects, 2) within-subjects,. 3) between and within-subjects, 4) single group.</td><td>Stevenson and Phakiti (2014)</td></tr><tr><td>1.3 Duration</td><td>1) one session, 2) short term (&lt;10 weeks), 3) intermediate term (11weeks ~ 4 months), 4) long-term</td><td>Hwang and Fu (2019)</td></tr><tr><td>1.4 Sample size</td><td>(&gt;4 months), 5) not specified. 1) small (&lt;30), 2) medium (30~50), 3) medium to large (51~100), 4) large (&gt;100), 5) not specified.</td><td>Hwang and Fu (2019)</td></tr><tr><td>2. Types of learners</td><td>2.2 Educational background</td><td>2.1 L1 or L2 learners 1) L1 learners, 2) L2 learners. 1) elementary education, 2) secondary education, 3) higher education, 4) post-graduate education, 5) others, 6) not specified.</td><td>Hwang and Fu (2019)</td></tr><tr><td>applications</td><td>3. Feedback and 3.1 Feedback types 3.2 How the effects of AWE were examined 3.3 Genres of the. text</td><td>Please refer to Table 2 for details. 1) AWE alone, 2) AWE vs. no feedback, 3) AWE vs. instructor feedback, 4) AWE vs. peer feedback, 5) others. 1) narrative, 2) expository, 3) descriptive, Graesser et al. (2003) 4) persuasive, 5) others..</td><td>Shute (2008)</td></tr></table></body></html>

Table 2. Coding schemes for feedback types (adapted from Shute (2008)).   

<html><body><table><tr><td>Feedback type</td><td>Description</td><td>Example</td></tr><tr><td>1. Verification</td><td>Indicates correctness of the text through hints such as right/ wrong, scores, or correct rate</td><td>&#x27;The plagiarism ratio in the post is 70%&#x27; (adapted from Akcapinar (2015))</td></tr><tr><td>2. Correct response</td><td>Provides the correct answer to a specific problem without additional explanations or</td><td>&#x27;The older lady in the picture is... (adapted from Gao and Ma (2019))</td></tr><tr><td>3. Try again</td><td>guides Indicates incorrect response and allows students one or more attempts to answer the</td><td>&#x27;Are you sure about this?&#x27;</td></tr><tr><td>4. Error flagging</td><td>question Highlights errors in texts without providing correct answers</td><td>&#x27;The lady in the picture is..&#x27; (adapted from Gao. and Ma (2019))</td></tr><tr><td>5. Elaborated feedback</td><td>Provides information about the</td><td>Subject-verb-object (Svo) is a sentence</td></tr><tr><td>5.1 Attribute isolation 5.2 Topic</td><td>central attributes of target Ianguage knowledge or skills Provides information such as</td><td>structure where the subject come first, followed by the verb and object. &#x27;We need to add &quot;s&quot; or &quot;es&quot; at the end of the</td></tr><tr><td>contingent</td><td>explanations or guides related to the target language points, e.g. teaching materials</td><td>verb in &quot;Simple Present Tense&quot; when the. subject of the sentence is in &quot;third person. singular&quot; form!</td></tr><tr><td>5.3 Response contingent 5.4 Hints/cues/</td><td>Explains why the answer is right or wrong Guides students to revise in the</td><td>The dog play with the yarn. Feedback: the subject and verb do not agree.</td></tr><tr><td>prompts</td><td>anticipated direction, usually through strategic hints</td><td>&#x27;Make sure every sentence has a verb&#x27; (adapted from Wilson et al. (2014))</td></tr><tr><td>5.5 Bugs/ misconceptions</td><td>Presents information about specific errors or misconceptions to help students diagnose it</td><td>&#x27;These two sentences are squashed together without using a coordinating conjunction or proper punctuation; these are run-on sentences&#x27;</td></tr><tr><td>5.6 Informative tutoring</td><td>Provides the most elaborated feedback, usually including verification information, error flagging, and strategic information</td><td>&#x27;When the clause indicates that an event took place in the past, the predicates of the clause should adopt the past form. For example, correct: I had (Past Tense) a word. with Julia yesterday. Typical Learner Error: I have (Tense Error) a word with Julia</td></tr></table></body></html>

The coding scheme has five categories: methodology, types of learners, types of feedback and its applications, learning outcomes, and implications (Appendix A). The first three categories, their subcategories, coding items, and referenced articles are illustrated in Table 1.

In the ‘methodology’ category, the coding items of the ‘type of research methods’ were adapted from Hung et  al. (2018); those of ‘design’ were from Stevenson and Phakiti (2014); and those of ‘duration’ were from Hwang and Fu (2019). The ‘sample size’ refers to the number of participants in the experimental or control groups, depending on which one was the smallest, following Hwang and Fu (2019).

In the ‘types of learners’ category, ‘L1 learners’ refers to the participants who wrote in their L1, and ‘L2 learners’ refers to those who wrote in their second or foreign language. The coding items of ‘educational backgrounds’ were adapted from Hwang and Fu (2019).

In the ‘feedback and applications’ category, first, we categorized the types of feedback (Table 2). In addition to the five main types (i.e. verification, correct response, try again, error flagging, and elaborated feedback), we also included six subcategories for the fifth type (i.e. attribute isolation, topic contingent, response contingent, hints, misconceptions, and informative tutoring) based on Shute (2008). We then examined how the effects of feedback were investigated in the reviewed studies. When the research included only an experimental group of students who learned with AWE feedback without a control group, we coded the research as ‘AWE alone.’ Other typical approaches included comparing the AWE feedback condition with the no feedback, peer feedback, or instructor feedback conditions. Lastly, we coded the genres of the texts (e.g. narrative, expository, descriptive, etc.) using coding items developed by Graesser et  al. (2003).

In the ‘learning outcomes’ category, diverse subcategories are included (Table 3). These items were developed based on the belief that AWE-assisted writing is generally a cognitive, emotional, and behavioral engagement process (Zhang, 2017; Zhang & Hyland, 2018). Specifically, cognitive engagement refers to students’ revisions based on AWE feedback and use of cognitive strategies (Zhang & Hyland, 2018). We categorized cognitive engagement into the ‘higher-order thinking or competence’ subcategory and coded it based on research by Devitt (2004) and Cotos et  al. (2017). Emotional engagement, which refers to students’ feelings about AWE-assisted learning, was categorized into the ‘affective or psychological states’ subcategory (Hwang & Fu, 2019). Behavioral engagement, which refers to the actions involved in AWE-assisted learning, had two subcategories, ‘revising performance and efficiencies’ and ‘learning behaviors.’ Each sub-category further included a list of coding items, most of which were given definitions or examples to make them easily understandable (Table 3).

Another subcategory entitled ‘orientation of results’ includes the results of the learning outcomes. This subcategory consisted of two dimensions: ‘inferential statistics’ and ‘non-inferential statistics;’ each was composed of four coding items, namely, positive, negative, not significant, and mixed results. When a learning outcome was reported after undergoing an inferential statistical test (e.g. T-test, ANOVA, Pearson Correlation), it was coded as one of the four coded items in the inferential statistics dimension; otherwise (e.g. when the learning outcomes were measured by a descriptive statistical or qualitative analysis), it was coded as one of the four coded items in the non-inferential statistics subcategory.

In the ‘implications’ category, a thematic analysis of the text was conducted in the conclusion and/or implications sections of the selected articles to identify a collection of salient themes aiming to provide implications for future educational and research practices in the field. The coding was open-ended without pre-defined coding items.

<html><body><table><tr><td rowspan="5">1. Product</td><td>1.1 Language-related</td><td>1) Vocabulary form, 2) Vocabulary meaning, 3) Vocabulary use (e.g. collocation, association), 4) Mechanics (e.g. punctuation, capitalization, style), 5) Grammar (accuracy, complexity, and fluency), 6) Sentence (accuracy, complexity, and</td><td></td></tr><tr><td>1.2 Content-related</td><td>fluency), 7) Others (open-coded). 1) Focus (the central idea of a whole text), 2) Meaning, 3) Idea/text development, Hoang and Kunnan (2 4) Justification (e.g. evidence, warrant, backing, qualifiers, rebuttal), 5).</td><td></td></tr><tr><td>1.3 Organization-related</td><td>Conclusions. 1) Cohesion (constructing clear and readable texts through connecting texts together with reference words and/or conjunctions), 2) Coherence (constructing</td><td>Bailey (2014); Graesse</td></tr><tr><td>1.4 Genre-related</td><td>Iogical and smooth flow of ideas from one sentence or paragraph to the next) 1) Rhetorical knowledge (understanding of text intention and purpose, and awareness of socio-rhetorical interpretation among writers, texts, and readers), 2) Formal knowledge (linguistic conventions, prototypical forms, structural</td><td>Cotos et al. (2017); Ta</td></tr><tr><td>2. Revising performance and efficiencies.</td><td>conventions of the genres), 3) Process knowledge (procedural practices of composing the genre) 1) Rates of review/ numbers of revision, 2) Distribution or type of revision, 3)</td><td></td></tr><tr><td colspan="2">3. Writing skills or performance 4. Higher-order thinking or competence</td><td>Editing/revising time &amp; time costs, 4) Length of texts, 5) Others (open-coded) Writing skills or performance (global/whole)..</td><td>Devitt (2004); Cotos e</td></tr><tr><td colspan="2"></td><td>1) Writing strategies (e.g. self-monitoring/regulation), 2) Genre awareness (critical consciousness of communicative purposes and rhetorical functions), 3) Others (open-coded).</td><td></td></tr><tr><td colspan="2">5. Affective or psychological states</td><td>1) Perceived usefulness, 2) Perceived ease of use, 3.) Intention to use, 4) Satisfaction, motivation or interest, 5) Engagement and flow, 6) Confidence/ self-efficacy, 7) Cognitive load, 8) Learning anxiety, 9) General attitude/ evaluation/opinions, 10) Others (open-coded).</td><td>Hwang and Fu (2019)</td></tr><tr><td colspan="2">6. Learning behaviors</td><td>Open-coded 1) Prior knowledge/abilities or education level, 2) Language proficiency level, 3)</td><td></td></tr><tr><td colspan="2">7. Individual differences</td><td>Others (open-coded)</td><td></td></tr><tr><td colspan="2">8. Relationships 9. Other issues</td><td>Open-coded Open-coded</td><td></td></tr></table></body></html>

# 2.3.  Coding method

The 48 articles were coded according to the coding scheme and the rules as follows. First, if the participants who learned with AWE outperformed their counterparts who learned with instructor feedback, peer feedback, or without feedback in terms of specific learning outcomes, the orientation of the results of these outcomes would be coded as ‘positive;’ If there was no significant difference between groups, it was coded as ‘not significant.’ If the participants underperformed, it was coded as ‘negative.’ When more than one type of result was found, it was coded as ‘mixed results.’ If the participants who learned with AWE experienced or reported more negative feelings (i.e. frustration, sadness, pressure, and anxiety) than their counterparts in a control group, the orientation of the results would be coded as negative. If the research involved only a single group experiment, the pre-test performance of the participants became their notional control group.

When learning outcomes were measured by descriptive statistical or qualitative analysis, if the articles reported more positive learning outcomes of the participants who learned with AWE than those of their counterparts without AWE, the learning outcomes would be coded as ‘positive.’ If the articles reported that there were no differences between the groups, it was coded as ‘not significant.’ If the articles reported more negative learning outcomes, it was coded as ‘negative.’ Additionally, when the articles reported that AWE and its alternatives, such as instructor feedback, peer feedback, and no feedback, had both advantages and disadvantages, the learning outcomes were coded as ‘mixed results.’ In the case of a single group experiment, as above, the participants’ pre-test performance became its notional control group.

Concerning the coding of the learning outcomes related to writing performance or skills, if the learning outcomes were not measured in terms of specific language areas (e.g. vocabulary use, grammar, cohesion, and accuracy), they would be coded as ‘writing skills’ or ‘performance’ (global/whole).

A thematic analysis technique from the study of Denham (2019) was used to code the implications. All aspects of the selected studies were coded literally as reported by the articles. The researchers coded the data independently first, and then checked the results collectively. In case of any discrepancies, the researchers re-read the articles and discussed face-to-face until agreement was reached.

# 3.  Results

The distribution of the 48 target articles is illustrated in Figure 1.

# 3.1.  Research methods applied by the reviewed studies

The review results showed that 24 of the 48 studies $( 5 0 \% )$ adopted quantitative methods, 20 $( 4 2 \% )$ mixed methods, and four $( 8 \% )$ qualitative methods. Regarding the design of the experiments, 23 studies $( 4 8 \% )$ employed between-subjects design, 18 $( 3 8 \% )$ single group design, four $( 8 \% )$ within-subjects design, and three $( 6 \% )$ between and within-subjects design.

Regarding the duration of the experiments, 19 $( 4 0 \% )$ intermediate term (11weeks to 4months), 16 $( 3 3 \% )$ were short term $( < 1 0$ weeks), five $( 1 0 \% )$ were only one session (Figure 2), and one $( 2 \% )$ was long-term research $\mathrm { > }$ 4months). Seven articles $( 1 5 \% )$ did not specify the duration.

Concerning the group size, as illustrated in Figure 3, 26 studies $( 5 4 \% )$ were small $\left( < 3 0 \right)$ , eight $( 1 7 \% )$ medium (30 to 50), ten $( 2 1 \% )$ medium to large (51 to 100), and three $( 6 \% )$ large $( > 1 0 0 )$ . One study $( 2 \% )$ did not specify the minimal group size.

![](img/b7421245d5ca43583f3898ba4e4df10ae32b1afc0d7740afd19c3d541be3d892.jpg)  
Figure 1. Frequency of articles by year.

![](img/015c364834f2dab788e275d29dd36836f1d11b6ef6a514195d0aeb8518787634.jpg)  
Figure 2. D istribution of the duration of the experiments.

![](img/9e31a70c1d70aacc046e13c3aa5bfe07eca6599dccd3901bcaeb9b4e3ea1bc71.jpg)  
Figure 3. D istribution of the minimal group size.

![](img/bed0aa55df25a0ba559660556275273e0a14c2f911b654c71e1842884b32e06c.jpg)  
Figure 4. D istribution of the target languages used in writing.

# 3.2.  Learners

As for the language used by learners, 34 $( 7 1 \% )$ studies investigated L2 writing, and 12 $( 2 5 \% )$ examined L1 writing (Figure 4). Specifically, 33 studies investigated English as a foreign or second language (EFL or ESL), followed by English as a first language (eight studies, $1 7 \%$ ), German as a first language (two studies, $4 \%$ ), Chinese or Turkish as a first language (one study each, $2 \times 2 \%$ ), and Spanish as a foreign or second language (one study, $2 \%$ ).

Regarding the participants’ educational backgrounds, higher education students garnered the most attention (35, $7 3 \%$ ), followed by secondary school students (five studies, $1 0 \%$ ), elementary school students (three studies, $6 \%$ ), post-graduate students (two studies, $4 \%$ ), and working adults (two studies, $4 \%$ ). One study involved students with more than one kind of educational background $( 2 \% )$ .

# 3.3.  Feedback and application

Concerning feedback types, 42 of 48 studies $( 8 8 \% )$ employed elaborated feedback, within which, 29 $( 6 0 \% )$ utilized informative tutoring, nine $( 1 9 \% )$ utilized hints, cues or prompts, three $( 6 \% )$ utilized topic contingent, and one $( 2 \% )$ utilized bugs or misconceptions. Four studies $( 8 \% )$ adopted verification feedback, one study $( 2 \% )$ used correct response feedback, and one study $( 2 \% )$ adopted error flagging feedback.

Regarding how the effects of AWE were examined, 16 $( 3 3 \% )$ studies examined the effectiveness of AWE without control groups (AWE alone). Fifteen $( 3 1 \% )$ studies compared AWE with no feedback (AWE vs. no feedback). Thirteen $( 2 7 \% )$ studies compared AWE with instructor feedback, followed by three studies $( 6 \% )$ comparing with peer feedback. One study $( 2 \% )$ compared AWE with feedback from an online dictionary (Longman) or an online thesaurus (Thesaurus.com).

The genres of the writing text of the 48 studies were diverse. Persuasive essays were the most frequently investigated (19 studies, $4 0 \%$ ), followed by descriptive essays (five studies, $1 0 \%$ ), summary (three studies, $6 \%$ ), narrative (three studies, $6 \%$ ), reflective essays (two studies, $4 \%$ ), and expository essays (one study, $2 \%$ ). Four studies $( 8 \% )$ involved more than one genre, and 11 $( 2 3 \% )$ did not indicate which genre was investigated.

# 3.4.  Learning outcomes

The learning outcomes were reported from diverse constructs as mentioned in the coding scheme section. To demonstrate the effects of AWE precisely, in most constructs (e.g. product-related outcomes, revising performance and efficiencies, writing skills or performance, higher-order thinking or competence, affective or psychological states, and learning behaviors), the learning outcomes were categorized into five types depending on how the effects of AWE were investigated. The learning outcomes consisted of the outcomes when students learned under the conditions with AWE alone, AWE vs. no feedback, AWE vs. instructor feedback, AWE vs. peer feedback, and AWE vs. online dictionary/thesaurus/search engine. All details of the coding results are presented in the tables.

# 3.4.1.  Product-related outcomes

Of the 59 outcomes, 31 $( 5 3 \% )$ were positive, among which, 20 were measured by inferential techniques and 11 by non-inferential techniques (Table 4). Among these 31 positive outcomes, most (27, $8 7 \%$ ) were examined during conditions of learning with AWE alone (19 outcomes) and learning with AWE vs. without feedback (8 outcomes). Eleven outcomes $( 1 9 \% )$ were negative; all of these were measured in the condition of comparing AWE to peer feedback (nine outcomes) or instructor feedback (two outcomes). Fifteen outcomes $( 2 5 \% )$ had no significant differences across five types of conditions: learning with AWE alone (four outcomes), learning with AWE vs. without feedback (four outcomes), learning with AWE vs. peer feedback (one outcome), and learning with AWE vs. instructor feedback (six outcomes).

<html><body><table><tr><td rowspan="2"></td><td colspan="3">Inferential statistics</td><td colspan="4"> Non-inferential statistics</td></tr><tr><td> Positive</td><td>Negative</td><td></td><td>Not Significant</td><td>Positive Negative</td><td>Not Significant</td><td>Mixed</td></tr><tr><td>Language-related</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1. Vocabulary form</td><td></td><td></td><td>Liu et al. (2016)d</td><td>Liu et al. (2016)a</td><td>Bai and Hu (2017)a; Zaini (2018)a</td><td></td><td></td></tr><tr><td>2.  Vocabulary use</td><td></td><td>Lai (2010)c</td><td></td><td></td><td>Zaini (2018)a</td><td></td><td>Bai and Hu</td></tr><tr><td>3. Mechanics</td><td></td><td>Lai (2010)c</td><td></td><td></td><td>Bai and Hu (2017)a</td><td></td><td>(2017)a</td></tr><tr><td>4. Grammar</td><td></td><td>Liu et al. (2016)d</td><td></td><td>Liu et al. (2016)a</td><td></td><td></td><td>Bai and Hu (2017)a</td></tr><tr><td>5. Sentence</td><td></td><td></td><td></td><td>Liu et al. (2016)a</td><td>Zaini (2018)a</td><td>Liu et al.</td><td></td></tr><tr><td>6.</td><td>Pronoun use</td><td>Bond and Pennebaker (2012)b</td><td></td><td></td><td></td><td>(2016)d</td><td></td></tr><tr><td>accuracy</td><td>7.Accuracy: grammatical</td><td>Liao (2016)a; Gao and Ma</td><td>Shang (2019)c (0.44)</td><td></td><td></td><td></td><td></td></tr><tr><td>8. complexity</td><td>Complexity: sentence</td><td>(2019)b</td><td>Shang (2019)c</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>9. Lexical density</td><td></td><td>(0.19) Shang (2019)c</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>10. Accuracy: grammatical accuracy (long-term</td><td>Gao and Ma (2019)a (0.37)</td><td>(0.22)</td><td>Gao and Ma (2019)b</td><td></td><td></td><td></td></tr><tr><td></td><td>improvement) 11. Accuracy: sentence accuracye</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>style)</td><td>12. Accuracy: overall (grammar, usage, mechanics, and</td><td>Li et al. (2015)a (0.47)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>13. Accuracy: simple past tense. 14. Accuracy: weighted clause.</td><td></td><td>Gao and Ma (2020)b</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ratio</td><td>(2.26)</td><td>Barrot (2021)a (1.72); Barrot (2021)b</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>15. Error reduction (conventions, grammar,. punctuation, and spelling) 16. Lexical Complexity</td><td></td><td>Guo et al. (2021)a (0.87)</td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

<html><body><table><tr><td>Sub-total (n=29) Content-related</td><td>10 studies (34%)</td><td>7 studies (24%)</td><td>4 studies (14%)</td><td>5 studies (17%)</td><td>1 study (3%)</td><td>2 studies (7%</td></tr><tr><td>1. Focus</td><td></td><td>Lai (2010)c</td><td>Lee et al. (2009)b</td><td>Garcia-Gorrostieta et al. (2018)d</td><td>Liu et al. (2016)d</td><td></td></tr><tr><td>2. Meaning</td><td>Wade-Stein and</td><td>Lai (2010)c</td><td>Lee et al. (2009)b;</td><td>Chapelle et al. (2015)a</td><td>Liu et al.</td><td></td></tr><tr><td>3. Idea/text development</td><td>Kintsch (2004)b; Morch et al. (2017)c</td><td></td><td>Lai (2010)c</td><td></td><td>(2016)d</td><td></td></tr><tr><td>4. Justification</td><td>Garcia-Gorrostieta et al. (2018)d; Liu et al. (2016)a</td><td></td><td></td><td></td><td>Liu et al. (2016)d</td><td></td></tr><tr><td>5. Conclusion</td><td>Garcia-Gorrostieta et al. (2018)d; Liu et al. (2016)a</td><td></td><td></td><td></td><td>Liu et al. (2016)d</td><td></td></tr><tr><td>Sub-total (n=17) Organization-related</td><td>6 studies (35%)</td><td>2 studies (12%)</td><td>3 studies (17%)</td><td>2 studies (12%)</td><td>4 studies (24%)</td><td></td></tr><tr><td>1. Coherence</td><td>Liu et al. (2016)a</td><td>Lai (2010)c</td><td>Lee et al. (2009)b</td><td></td><td>Morch et al. Liu et al.</td><td></td></tr><tr><td>2. Cohesion</td><td>Liu et al. (2016)a</td><td></td><td></td><td></td><td>(2017)c (2016)d</td><td></td></tr><tr><td>Sub-total (n=6)</td><td>2 studies (33%)</td><td>1 study (17%)</td><td>1 study (17%)</td><td>1 study (17%)</td><td>1 study (17%)</td><td></td></tr><tr><td>Genre-related 1. Rhetorical knowledge</td><td colspan="6">Cotos (2011)a (0.77)</td></tr><tr><td>2. Formal knowledge 3. Formal knowledge: Causal</td><td colspan="6"> Saricaoglu (2019)a</td></tr><tr><td> language features</td><td colspan="6"></td></tr><tr><td>(immediate) 4. Formal knowledge: Causal</td><td colspan="6">Saricaoglu (2019)a</td></tr></table></body></html>

<html><body><table><tr><td colspan="4">5. Process knowledge</td><td colspan="4">Chapelle et al. (2015)a; Cotos et al. (2017)b</td></tr><tr><td>Sub-total (n=7)</td><td>2 (29%)</td><td></td><td>1 (14%)</td><td>4 (57%)</td><td></td><td></td><td></td></tr><tr><td>Sum</td><td>a: 12; b: 5; c: 1; d: 2</td><td>c: 8; d: 2</td><td>a: 4; b: 4; c: 1</td><td>a: 7; b: 3; d: 1</td><td>c: 1</td><td>d: 6</td><td>a: 2</td></tr><tr><td>59</td><td>20 (34%)</td><td>10 (17%)</td><td>9 (15%)</td><td>11 (19%)</td><td>1 (2%)</td><td>6 (10%)</td><td>2 (3%)</td></tr></table></body></html>

These findings indicated that AWE had overall mixed effects on the improvement of learners’ product-oriented knowledge in writing. Additionally, language-related outcomes were more frequently discussed $\left( \mathtt { n } = 2 9 \right)$ than organization-related $\left( \mathrm { n } = 6 \right)$ , genre-related outcomes $( \mathtt { n } = 7 )$ , and content-related ones $( \mathtt { n } = 1 7 )$ ).

# 3.4.2.  Revising performance and efficiencies

Among the 16 outcomes reported by the reviewed articles, seven $( 4 4 \% )$ were positive with three measured by inferential techniques and four by non-inferential techniques (Table 5). These results were obtained under the conditions of AWE alone (two outcomes) and AWE vs. no feedback (five outcomes). There were also two outcomes $( 1 2 \% )$ reported to be insignificant under the condition of AWE feedback vs. no feedback. Furthermore, one outcome $( 6 \% )$ comparing AWE feedback with peer feedback was significantly negative.

# 3.4.3.  Writing skills or performance (global/whole)

Table 6 shows 17 of 22 outcomes were positive when students learned under the conditions of AWE alone (seven outcomes), AWE vs. no feedback (six outcomes), AWE vs. instructor feedback (three outcomes), and AWE vs. online dictionary, thesaurus, or search engine (one outcome). Almost all these outcomes were measured with inferential techniques. Also, two studies found that the effects of AWE could last over the long-term rather than only an immediate period.

# 3.4.4.  Higher-order thinking or competence

Concerning the effects of AWE on students’ higher-order thinking or competence, out of four outcomes, three were positive, as measured by non-inferential techniques under the conditions of AWE alone (two outcomes) and AWE vs. no feedback (one outcome (Table 7). Compared to other aspects of knowledge and skills related to writing, the effects of AWE on students’ higher-order thinking or competence were less frequently investigated in the 48 articles.

# 3.4.5.  Affective or psychological states

The effects of AWE on students’ affective or psychological states are shown in Table 8. A total of 37 outcomes were reported in the reviewed studies, 14 of which $( 3 8 \% )$ presented positive results, 11 mixed results $( 3 0 \% )$ , eight negative results $( 2 2 \% )$ , two no significant differences $( 5 \% )$ , and two not available $( 5 \% )$ (as no quantitative methods were applied to measure the outcomes). Among these outcomes, most were measured via non-inferential techniques.

<html><body><table><tr><td rowspan="2"></td><td colspan="3">Inferential statistics</td><td colspan="2"> Non-Inferential statistics</td><td rowspan="2">Investigated qualitat</td></tr><tr><td>Positive</td><td> Negative</td><td>Not Significant</td><td>Positive</td><td>Mixed</td></tr><tr><td>Rates of revision/ number of revisions</td><td>Lachner et al. (2017)b</td><td>Lai (2010)c</td><td></td><td>Zhang (2017)a; Roscoe et al.  Bai and Hu (2017)a; Zhu (2017)b</td><td>et al. (2017)a</td><td>Garcia-Gorrostieta et (2018)d</td></tr><tr><td>Distribution or types of revision</td><td></td><td></td><td></td><td>Roscoe et al. (2017)b</td><td>Zhu et al. (2017)a</td><td>Saricaoglu (2019)a; Garcia-Gorrostieta (2018)d</td></tr><tr><td>Editing or revising time/ time costs</td><td>Proske et al. (2012)b; Wade-Stein and</td><td></td><td></td><td>Zaini (2018)a</td><td></td><td></td></tr><tr><td> Length of texts</td><td>Kintsch (2004)b</td><td></td><td>Proske et al. (2012)b;</td><td></td><td></td><td></td></tr><tr><td>6</td><td>b: 3 (19%)</td><td>c: 1 (6%)</td><td>Lee et al. (2009)b b: 2 (12%)</td><td>a: 2; b: 2 (25%)</td><td>a: 3 (19%)</td><td>a: 1; d: 2 (19%)</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2"></td><td colspan="4"> Inferential statistics</td><td>Non-Interential Statistics</td></tr><tr><td>Positive</td><td>Negative</td><td>Not Significant</td><td>Mixed</td><td>Positive</td></tr><tr><td>1. Writing skills or performance</td><td>Morch et al. (2017)a; Zhu et al. (2017)a; Landauer et al. (2009)b; Cheng (2017)b; Roscoe et al.</td><td></td><td>Morch et al. (2017); Sherafati et al. (2020)d (0.042); Wilson ande</td><td>Sung et al. (2016)a</td><td>Zaini (2018)a</td></tr><tr><td></td><td>(2017)b; Proske et al. (2012)b; Chen et al. (2015)e; Wang et al. (2013)a; Wang et al. (2013)d; Sherafati et al. (2020)a; Hassanzadeh and.e</td><td></td><td>Roscoe (2020)d</td><td></td><td></td></tr><tr><td></td><td>Fotoohnejad (2021)a (0.97); Hassanzadeh and Fotoohnejad</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>(2021)d (0.86); Reynolds et al.</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>(2021)d; Han et al. (2021)b</td><td></td><td></td><td></td><td></td></tr><tr><td>2. Writing skills or</td><td>Wade-Stein and Kintsch (2004)b;</td><td>Sherafati et al. (2020)d</td><td></td><td></td><td></td></tr><tr><td>performance (transfer effect)</td><td>Sherafati et al. (2020)a</td><td>(0.628)</td><td></td><td></td><td></td></tr><tr><td>22</td><td>a: 6; b: 6; d: 3; e: 1 (73%)</td><td>d: 1 (4%)</td><td>d: 2; c: 1 (14%)</td><td>a: 1 (4%)</td><td>a: 1 (4%)</td></tr></table></body></html>

Table 7. Learning outcomes in terms of higher-order thinking or competence.   

<html><body><table><tr><td rowspan="2"></td><td>Non-Inferential statistics</td><td>Investigated qualitatively</td></tr><tr><td>Positive</td><td></td></tr><tr><td>1. Writing strategiese</td><td>Zhang (2017)a</td><td>Lee et al. (2009)b</td></tr><tr><td>2. Genre awareness</td><td>Cotos et al. (2017)b</td><td></td></tr><tr><td>3. Autonomy awareness</td><td>Wang et al. (2013)a</td><td></td></tr><tr><td>4. Creativity</td><td></td><td></td></tr><tr><td>4</td><td>a: 2: b: 1 (75%)</td><td>b: 1 (25%)</td></tr></table></body></html>

# 3.4.6.  Learning behaviors

Regarding the effects of AWE on students’ learning behaviors, as illustrated in Table 9, six of 14 outcomes $( 4 2 \% )$ were positive, three $( 2 1 \% )$ were negative, and one $( 7 \% )$ had mixed results. These indicated the effectiveness of AWE in terms of promoting behavioral engagement, despite being limited to some extent.

# 3.4.7.  Individual differences

This section presents the review results concerning what and how individual characteristics impacted the outcomes of AWE-assisted activities. As exhibited in Table 10, students’ language backgrounds (whether they wrote in L1 or L2) did not influence their writing performance, but both their academic levels and learning motivation influenced their writing performance. Chen et  al. (2015) reported that students with lower language proficiency levels, lower levels of involvement (which refers to the number of paraphrased phrases in the context of this research), or greater motivation (in a pre-test) achieved greater improvement in their paraphrasing performance (restating information by using a variety of words), compared to those with higher language proficiency levels, involvement, or lower motivation.

In addition, students’ perceptions of AWE varied across different language levels or backgrounds. O’Neill and Russell (2019) found that international students perceived less satisfaction with AWE than domestic non-English speaking background ones (DNESB), who felt confidence in developing their language skills. Compared to DNESB and international students enrolled in an English Learning Center, local students considered AWE less useful in terms of developing their language skills. Also, students of lower language levels were more satisfied with AWE, although students with higher language levels were more likely to consider AWE easy to follow. Cotos et  al. (2017) reported that no significant difference was found between native and non-native speaking students in terms of their genre learning performance when using AWE. It was also found that the genre awareness of students at more advanced academic levels improved more than that of the students at lower levels partly because the former used more advanced rhetorical strategies.

<html><body><table><tr><td rowspan="2"></td><td colspan="2">Inferential statistics</td><td rowspan="2"> Not Significant</td><td colspan="3">Non-Inferential statistics.</td><td rowspan="2">Investigated qualitat</td></tr><tr><td>Positive</td><td>Negative</td><td>Positive</td><td>Negative</td><td>Mixed</td></tr><tr><td>1. Perceived usefulness</td><td>O&#x27;Neill and Russell (2019)d</td><td></td><td></td><td>Dikli and Bleyle (2014)a; Lai (2010)c</td><td></td><td>Bai and Hu (2017)a; Chen and Cheng (2008)a; Liu et al. (2016)d; Zhu</td><td></td></tr><tr><td>2. Perceived ease of use.</td><td>O&#x27;Neill and Russell (2019)d</td><td></td><td></td><td>Garcia-Gorrostieta et al. (2018); Cheng</td><td></td><td>et al. (2017)a</td><td></td></tr><tr><td>3. Intention to use</td><td></td><td></td><td></td><td>(2017)b Garcia-Gorrostieta et al.. (2018); Cheng</td><td></td><td>Roscoe et al. (2017)b</td><td></td></tr><tr><td>4. Confidence</td><td>O&#x27;Neill and Russell</td><td></td><td></td><td>(2017)b</td><td></td><td></td><td></td></tr><tr><td>5. Satisfaction, motivation or interest</td><td>(2019)d</td><td></td><td>Shang (2019)c</td><td>Landauer et al. (2009)b; Zhang (2017)a</td><td></td><td></td><td></td></tr><tr><td>6. General attitudes/ evaluation/opinions</td><td></td><td>Dwyer and Sullivan (1993)d</td><td></td><td>Landauer et al. (2009)b; Wang et al. (2013)a</td><td>Dikli and Bleyle (2014)d</td><td>Chen et al. (2015)e; Chen and Cheng (2008)a; Li et al. (2015); Roscoe</td><td>Cheng (2017)b; Lai (2010)c</td></tr><tr><td>7. Engagement &amp; flow</td><td></td><td></td><td>Bond and</td><td></td><td></td><td>et al. (2017); Shang. (2019); Barrot (2021)b</td><td></td></tr><tr><td>8. Learning anxiety</td><td></td><td></td><td>Pennebaker (2012)b</td><td></td><td></td><td></td><td></td></tr><tr><td>9. Perceived frustration and/or sadness</td><td></td><td></td><td></td><td></td><td>Zaini (2018)a Zhang (2017)a</td><td></td><td></td></tr><tr><td>10. Perceived control and surveillance</td><td></td><td></td><td></td><td></td><td>Zaini (2018)a</td><td></td><td></td></tr><tr><td>11. Perceived pressure</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>12. Perceptual adequacy</td><td></td><td></td><td></td><td></td><td>Zaini (2018)a Chen and Cheng</td><td></td><td></td></tr><tr><td>13. Perceived feedback quality</td><td>Roscoe et al.</td><td></td><td></td><td></td><td>(2008)a</td><td></td><td></td></tr><tr><td>14. Self-efficacy 15. Cognitive load</td><td>(2017)b Wilson and Roscoe (2020)d</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

<html><body><table><tr><td rowspan="2"></td><td colspan="3">Inferential statistics</td><td colspan="2">Non-Inferential statistics</td><td rowspan="2">Investigated qualitatively</td></tr><tr><td>Positive</td><td> Negative</td><td>Mixed</td><td>Positive</td><td>Negative Not Significant</td></tr><tr><td>1. Choosing the way of feedback (AWE or</td><td></td><td>Dwyer and Sullivan (1993)d</td><td></td><td></td><td>Sherafati et al. (2020)d</td><td></td></tr><tr><td>Teacher feedback) 2. Pre-writing behaviors</td><td>Proske et al. (2012)b</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>3. Revision behaviors (e.g. content addition, substitution, and deletion)</td><td>Roscoe et al. (2017)b</td><td></td><td></td><td></td><td>Wang et al. (2012) d</td><td></td></tr><tr><td>4. Submission behavior in terms of numbers of submissions</td><td></td><td></td><td>Sung et al. (2016)b</td><td>Zhang (2017)a; Sung et al. (2016)a</td><td></td><td></td></tr><tr><td>5. Engagement with feedback (fixation order for different categories of error)</td><td></td><td></td><td></td><td></td><td></td><td>El Ebyary and Windeatt (2019)a</td></tr><tr><td>6. Engagement with feedback (fixation</td><td></td><td></td><td></td><td></td><td></td><td>El Ebyary and</td></tr><tr><td>duration on different categories of error). 7. Reduced plagiarism behaviors</td><td>Akcapinar (2015)a</td><td></td><td></td><td></td><td></td><td>Windeatt (2019)a</td></tr><tr><td>8. Submission behavior in terms of numbers</td><td></td><td>Sung et al. (2016)a</td><td></td><td></td><td></td><td></td></tr><tr><td>of submissions (long-term) 9. The number of submissions over time.</td><td></td><td></td><td></td><td></td><td></td><td>Li et al. (2015)a</td></tr><tr><td>10. Accepting feedback across language areas 14</td><td></td><td>a: 1; d: 1 (14%)</td><td>b: 1 (7%)</td><td>Guo et al. (2021)a a: 3 (21%)</td><td>d: 1 (7%) d: 1 (7%)</td><td>a: 3 (21%)</td></tr></table></body></html>

Table 10. Learning outcomes in terms of students’ individual differences.   

<html><body><table><tr><td rowspan="2"></td><td colspan="2">Inferential statistics</td><td rowspan="2"></td><td colspan="2">Non-Inferential statistics</td></tr><tr><td>Not Significant</td><td>Mixed</td><td>Positive</td><td>Negative</td></tr><tr><td></td><td>1. Did students&#x27; perception of AWE differ across their language levels?</td><td></td><td>O&#x27;Neill and Russell (2019)</td><td></td><td></td></tr><tr><td>background?</td><td>2. Did students&#x27; perception of AWE differ across their language</td><td>O&#x27;Neill and Russell (2019)</td><td></td><td></td><td></td></tr><tr><td>academic levels?</td><td>3. Did students&#x27; genre learning performance differ across their</td><td></td><td></td><td>Cotos et al. (2017)</td><td></td></tr><tr><td>non-native)?</td><td>4. Did students&#x27; genre learning performance differ across their Ianguage background (native or</td><td>Cotos et al. (2017)</td><td></td><td></td><td></td></tr><tr><td></td><td>5. Did students&#x27; paraphrasing performance differ across their Ianguage proficiency levels?</td><td></td><td></td><td></td><td>Chen et al. (2015)</td></tr><tr><td>6. Did students&#x27; paraphrasing</td><td>performance differ across their involvement levels in pre-test?</td><td></td><td></td><td></td><td>Chen et al. (2015)</td></tr><tr><td>7. Did students&#x27; paraphrasing motivation levels?</td><td>performance differ across their</td><td></td><td></td><td>Chen et al. (2015)</td><td></td></tr><tr><td>7</td><td></td><td>2</td><td>1</td><td>2</td><td>2</td></tr></table></body></html>

# 3.4.8.  Relationships

Several studies investigated the kinds of relationships that occurred in the process of student learning with AWE. Roscoe et  al. (2017) found that students who perceived improvements in feedback quality over time were likely to use AWE in the future, and the quality of their writing also improved. Bai and Hu (2017) reported that the accuracy of AWE was significantly positively correlated with students’ uptake of the suggestions from AWE. Wade-Stein and Kintsch (2004) also found that when writing tasks were challenging, the participants perceived AWE as more effective because they did not need it if the tasks were easy to complete.

Zhu et  al. (2017) explored students’ revision patterns and discovered that the initial grades were not correlated with students’ engagement in the revision process, aspects of which included the number of revisions and the time on revision, whereas students with higher grades in their initial submissions were more likely to heed feedback suggestions.

Additionally, Roscoe et  al. (2017) found that superficial revisions (e.g. minor changes that did not significantly enrich the content, revisions at the level of mechanics or words, and simple restructuring) did not correlate with improved writing quality. The number of submissions also did not correlate with writing performance.

# 3.4.9.  Other issues

This section presents the review results concerning the students’ learning outcomes as well as research issues such as pedagogies involving AWE-assisted activities, influential factors, interactions with AWE, and reasons for revisions. Most of these issues were qualitatively investigated or discussed in the articles but not quantitatively measured.

Chen and Cheng (2008) found that instructors’ teaching could influence students’ perceptions of the effectiveness of AWE. Specifically, when AWE was used as a preliminary assessment tool in the early drafting and revising stages to help students advance to a preliminary required level and then followed by teacher assessment to help students improve their writing at the level of meaning, AWE was considered most effective by students. As to how AWE should be implemented, Chen and Cheng claimed that four issues ought to be considered, namely, using AWE at the appropriate time, the need for human feedback, the students’ language proficiency and individual learning needs.

El Ebyary and Windeatt (2019), who investigated the reasons for students’ fixation behaviors (a reader’ gaze distribution on the screen), showed that no matter how many errors were found in each language area, grammar was noticed first, followed by organization and development, while style was less of a focus. In another study, Bai and Hu (2017) reported how students successfully corrected their writing errors after several rounds of trials with the help of AWE. Zhang and Hyland (2018) and Zhang (2020) investigated how students interacted differently with teacher and AWE feedback. Tian and Zhou (2020) explored students’ interaction with AWE feedback and discussed the reasons for revisions. Jiang and Yu (2020) investigated how EFL students employed resources and strategies to use AWE feedback and found that students appropriated automated feedback differently. Additionally, Lee (2020) investigated L2 learners’ composing process, strategies, and mental aspects, focusing on their cognitive engagement while learning with AWE feedback.

# 3.5.  Implications suggested in the reviewed articles

Seven salient themes were identified based on an analysis of the Conclusion and/or Implication sections of the 48 articles.

# 3.5.1.  Fostering learners’ autonomy and responsibility

Seven of the reviewed studies (i.e. Bai & Hu, 2017; Chen & Cheng, 2008; Cheng, 2017; El Ebyary & Windeatt, 2019; Liu et  al., 2016; Wade-Stein & Kintsch, 2004) argued that students who wrote using AWE needed to be responsible for their own learning. Several articles claimed that too much feedback can overburden learners’ cognitive capabilities and discourage them while also decreasing their motivation (Liu et  al., 2016). El Ebyary and Windeatt (2019) suggested giving learners the choice of turning on or off certain aspects of feedback. Bai and Hu (2017) suggested that learners ought to actively evaluate the quality of AWE and analyze its advantages and disadvantages to use it effectively, adding that students’ attempts to identify errors on their own was also important.

# 3.5.2.  Exploring the effects of influential factors on writing

The authors of ten studies suggested that future research should focus on the factors that can influence the effects of AWE, including how and to what extent they can impact the effectiveness of AWE-infused writing activities. For example, Chen and Cheng (2008) argued that teachers’ practices and students’ learning goals could affect their perceptions of the value of AWE. Similarly, Liao (2016) believed that the effects of AWE can differ across students with varied ability levels. Bai and Hu (2017) also suggested that it is necessary to explore how students’ language proficiency, personal attitudes, and technological literacy affects their revision processes. Other factors such as genres, students’ learning motivation and interests were also suggested research issues for investigation (Shang, 2019).

3.5.3.  Investigating how learners interact with feedback and make revisions Eleven of the studies recommended more investigation on how and why learners interact and revise according to feedback. Exploring how students make revision choices can help us go deep into the process of knowledge construction among the AWE-facilitated learning activities (Cotos et  al., 2017). Examining how students interact with AWE (e.g. whether accept a suggestion or not and to what extent) and the underlying reasons can also reveal the complex nature of AWE-facilitated learning.

# 3.5.4.  Facilitating learners’ engagement with feedback

Ten studies claimed more research is needed on learners’ cognitive, emotional, and behavioral engagement with AWE. Saricaoglu (2019) argued that AWE should be delivered to learners in a way that it stimulates cognitive engagement rather than focusing on superficial information (e.g. right/wrong and correct rate). AWE also promoted behavioral engagement, and when learners had rich opportunities to interact with it, they were likely to revise their writing (Saricaoglu, 2019).

# 3.5.5.  Transformation to social constructivist pedagogy

Among the 48 studies, 16 studies suggested that future research should blend teacher and/or peer feedback with AWE to improve the quality of writing, especially meaning-focused writing, as teacher and/or peer feedback were considered effective in encouraging interaction and the co-construction of knowledge (Lai, 2010), which likely leads to more effective revisions (Shang, 2019).

# 3.5.6.  Examining the effectiveness of AWE across various contexts

Some studies proposed that future investigations be implemented in various contexts to further examine the effectiveness of AWE, for example, examining whether the effectiveness of AWE can be generalized to a variety of genres (Lachner et  al., 2017). Such efforts can assist us in discovering affordances of AWE across contexts, building our confidence in using it in language classrooms.

# 3.5.7.  More refinement of experimental designs for follow-up research on the use and validity of AWE

Twelve studies suggested improving the experimental design of follow-up research for assessing the validity of AWE. One suggestion was to include control groups to better investigate the effects of AWE alone rather than adopting only single group experiments (Cotos et  al., 2017; Li et  al., 2015). Another recommendation was to conduct longitudinal studies to examine the long-term effects of AWE-assisted writing processes (Chen & Cheng, 2008; Liao, 2016; Proske et  al., 2012; Roscoe et  al., 2017; Saricaoglu, 2019). Studies with larger sample sizes were also advocated to improve the generalizability of research findings (Proske et  al., 2012).

# 4.  Discussion

The results of our review showed that more research attention has been focused on AWE-assisted writing in recent years, possibly because of the rapid improvement of automated evaluation technologies such as machine learning and natural language processing (Zhu, et  al., 2020). Another possible reason is that an increasing number of teachers believe that AWE is beneficial (Link et  al., 2020). However, the number of AWE-related studies in the field remains relatively small, and more research is necessary.

# 4.1.  Methodological issues

The finding that half of the reviewed studies adopted quantitative methods while only four studies adopted qualitative methods further supports the observation that the scientific community tends to favor quantitative over qualitative methods (Lopez et  al., 2015; Twining et  al., 2017).

Qualitative and quantitative methods represent different epistemological beliefs (positivism for quantitative approaches vs. interpretivism for qualitative approaches) (Bahari, 2010), both of which appear to align with the complex nature of the vast variety of questions being studied in our scientific community (Lopez et  al., 2015). Almost half of the studies adopted mixed methods, which shows that mixed methods can make full use of the benefits of quantitative and qualitative approaches (Trotter, 2012), and are beneficial for generating rich findings (Bahari, 2010).

Concerning the design of the experiments, $3 3 \%$ of the studies did not use control groups for comparison so it is difficult to attribute the improvement in writing (if any) solely to AWE as practices of writing without AWE may also have contributed to the improvement (Li et  al., 2015; Stevenson & Phakiti, 2014). When it was difficult to include a comparison group in the research, adopting a within-subjects design could have been undertaken to address this shortcoming.

Most studies’ durations were short, and the group sizes were not large. According to Cook and Hatala (2015), smaller sample sizes generally lead to lower precision in experiments; adopting a repeated measures design, such as within-subjects and crossover designs, can achieve higher precision with small sample sizes. Also, the effects of treatment cannot be fully achieved over a short duration (Hwang & Fu, 2019). Therefore, larger sample sizes for better statistical power, longer duration for investigation of long-term effects of AWE, and power analysis are recommended.

# 4.2.  Learners

Most of the reviewed studies focused on tertiary education students, which is perhaps because researchers normally worked at universities or colleges (Hwang & Fu, 2019). However, K12 students also need feedback on their writing; however, it is difficult for teachers to provide every student with high quality feedback due to the large class size and the diversity of students (Grimes & Warschauer, 2010). Thus, we recommend more studies that introduce AWE to K12 students to supplement teacher feedback.

# 4.3.  Feedback and applications

The feedback provided by AWE in most studies included detailed information such as verification, error flagging, and strategic guides to help learners reflect on their errors and revise in the expected directions. This result dovetails with Liao (2016), who claimed that indirect feedback helps students reflect on the comments and learn critically. Notably, the kinds of feedback employed also depended on the AWE systems that were used; therefore, authors of these studies should specifically state which AWE systems were used. These frequently used (i.e. used in at least two studies among the 48 samples) systems include Criterion, Pigai (www.pigai.org), Grammarly, My access!, and Summary Street.

As presented in 3.4.1, ‘product-related outcomes,’ nine studies compared AWE to peer feedback, and two studies compared AWE to instructor feedback. These 11 studies all reported that human feedback (peer feedback and teacher feedback) was superior to AWE in enhancing writing development. Considering the different merits of AWE and human feedback, a possible direction for future research may be to blend AWE with human feedback to make full use of the benefits of both types of feedback.

In terms of genres, AWE was more frequently applied to persuasive essays than descriptive, summary, narrative, reflective and expository essays. As most of these findings focusing on only one genre, i.e. persuasive writing, cannot be generalized to other genres (Lachner et  al., 2017), more research is needed to explore the effects of AWE in assisting students with other genres.

# 4.4.  Learning outcomes

# 4.4.1. Writing performance

Our review results indicate that AWE feedback is generally considered effective in enhancing writing development (see Tables 4 and 6). The benefits of AWE feedback can be summarized as follows. AWE that precisely targets some specific areas (e.g. spelling, mechanics, usage, and grammar) can help students improve their writing (Cheng, 2017; Dikli & Bleyle, 2014; El Ebyary & Windeatt, 2010; Shang, 2019; Wade-Stein & Kintsch, 2004). Meanwhile, frequent access to AWE systems helped the students better understand both correct and incorrect usages of the target language. For example, Garcia-Gorrostieta et  al. (2018) found that AWE was more helpful than teacher feedback in terms of content-related outcomes (e.g. focus, justification, and conclusions) perhaps because AWE was accessible ubiquitously and identified gaps and examples that informed students how to revise; however, teacher feedback was usually only available during class hours (Garcia-Gorrostieta et  al., 2018).

Nevertheless, the finding that only $5 3 \%$ of the outcomes were positive indicates the limitations of AWE feedback (see Table 4); thus further improvements in AWE appear necessary. For example, because AWE is considered unreliable, students sometimes are reluctant to take up suggested changes (Bai & Hu, 2017). When a sentence is long and complicated (e.g. consisting of multiple elements), AWE systems have limited ability to analyze it (Bai & Hu, 2017). Students’ superficial use of AWE is yet another impediment (Lee et  al., 2009).

The superiority of human feedback over AWE, given that 11 studies reported that human feedback (peer feedback and teacher feedback) outperformed AWE, is perhaps because human feedback provided more context-specific (or explicit) information relevant to the problems in students’ writing, whereas AWE was often generic and indirect, which sometimes made it difficult for students to understand how to revise (Lai, 2010; Liu et  al., 2016; Shang, 2019). Moreover, peer feedback promoted social interaction and audience awareness, which encouraged students to engage in discussions and revision (Lai, 2010). Compared to AWE, peer feedback encouraged an atmosphere of mutual discussion and correction, enabling students to obtain more ideas and understand linguistic forms, resulting in better writing (Shang, 2019).

Evidence on transfer effects of AWE was extremely limited. Although Wade-Stein and Kintsch (2004) found that the effects of AWE could be transferred to subsequent learning tasks without AWE support (see Table 6), where the retention interval was only one week.

Several reasons may account for the limited effects of AWE on students’ writing performance. One is the inaccuracy of feedback primarily due to AWE systems’ limited text analysis capabilities. The natural language processing techniques behind most AWE systems are similar, that is, to extract linguistic, structural, semantic, and rhetorical features from the training samples or selected corpora to analyze input texts (Roscoe et  al., 2017). The accuracy of these analyses heavily depends on the span of the selected samples or corpora; thus, a transfer effect may be difficult to achieve in a wider sample. Furthermore, these techniques generally ignore the contextual nature of the text, possibly leading to lower effects of AWE at the level of meaning. Recently, neural network technology based on deep learning, which has been demonstrating its benefits in many fields such as speech recognition and image identification, could be an effective method to resolve the aforementioned problems. Another is students’ inability to understand the feedback message due to a lack of prior knowledge or their low level of language proficiency, which can lead to less cognitive involvement. Other possible reasons include inappropriate pedagogical design (Saricaoglu, 2019), i.e. students’ failure to recognize the advantages and disadvantages of AWE resulting in a low level of intention to use it and insufficient exposure to it. Trying to resolve these problems can strengthen the effects of AWE. Carefully designed AWE-involved pedagogies, such as adopting human feedback for meaning-level revisions and using AWE for form-level ones, can effectively compensate for the deficiencies of AWE.

It is noteworthy that most of the reviewed studies did not report the effect sizes (see Table 4), which seems consistent with Plonsky’s (2015) observation that many researchers in the areas of CALL and applied linguistics tend to overrate null hypothesis significance testing, while overlooking the possible significance of a difference even if it is not statistically significant. Moreover, some of the reviewed studies did not consider unperceived differences in the pre-test while simply using an independent t-test to evaluate the effects of the proposed intervention. These practices may impede the replication of previous work and make it difficult to integrate the findings of subsequent studies into an existing body of knowledge in the field (Plonsky, 2015).

# 4.4.2.  Behavioral engagement

The results presented in Section 3.4.2, Revising performance and efficiencies, and 3.4.6, Learning behaviors, indicate that AWE feedback can promote revision engagement. Specifically, AWE feedback encourages students to produce more revisions such as additions, substitutions, and reorganizations (Roscoe et  al., 2017; Sung et  al., 2016), and is helpful for stimulating students to spend more time revising (Zhang, 2017). Wade-Stein and Kintsch (2004) reported that students want to spend more time revising until their revisions are approved by the AWE system. Proske et  al. (2012) argued that AWE systems can guide students to manage their own writing processes (e.g. spend more time on pre-writing such as planning and producing ideas). When students understand the writing tasks and AWE, they tend to make more drafts (Zhang, 2017).

Nevertheless, as shown in Table 9, it seems that most students preferred teacher feedback when they are provided with a choice because it is easier to understand (Dwyer & Sullivan, 1993). Lai (2010) also argued that students considered peers as a real audience as opposed to computers, and this audience helps them interact thus producing more revisions.

As illustrated in Table 9, other issues such as students’ engagement with feedback (e.g. fixation behaviors across different areas of language) (El Ebyary & Windeatt, 2019) and the number of submissions over time (Li et  al., 2015) were investigated. Akcapinar (2015) also reported that AWE helped students avoid plagiarism. Future research may investigate topics concerning students’ behavioral engagement, for example, their interaction with feedback and revising behaviors.

# 4.4.3.  Cognitive engagement

Findings from Section 3.4.4, Higher-order thinking or competence, reveal that AWE feedback can also cognitively engage students in writing activities. For example, in a case study by Zhang (2017), to obtain higher scores, a participant tried to understand the feedback on her own, monitored her revision process (e.g. noticing the information from feedback and trying to revise based on the comments), and learned to use cognitive strategies such as self-regulation (e.g. adopting and assimilating the comments critically rather than receiving them passively). The participant’s writing strategies were thus cultivated; however, this kind of investigation of students’ cognitive engagement in writing is scarce. Given that creativity is an important part of writing (Onkas, 2015), and that fostering students’ creativity abilities can help them generate more unique, fluent, and elaborated ideas, future research may consider investigating the effects of AWE from this perspective using a case study approach. Further, cultivating students’ higher-order thinking can help them learn to meaningfully and critically respond to the AWE feedback while developing their own writing strategies, and even improving their creativity.

# 4.4.4.  Emotional engagement

The findings in 3.4.5, Affective or psychological states, indicate that students usually perceive AWE as easy to use and helpful and have intentions to continue using it because AWE is easily accessible, prompt, and is also perceived to be beneficial for improving writing, especially at the level of linguistic accuracy (Dikli & Bleyle, 2014; Shang, 2019). Students sometimes hold a more positive attitude towards AWE feedback because that they can receive more feedback than when learning with teacher feedback (O’Neill & Russell, 2019).

The findings, however, also showed that students usually hold negative attitudes toward AWE compared to instructor or peer feedback. For instance, students generally regard human feedback as more useful than AWE feedback because the former is direct and explicit with information or suggestions, whereas the latter is vague and sometimes not easily understandable (Lai, 2010). Also, the AWE systems sometimes fail to identify important errors or produce incorrect feedback (Dikli & Bleyle, 2014). Additionally, some students may feel pressured and controlled because they believe they must accept the corrections and/or suggestions from AWE (Zaini, 2018). Zhang (2017) also reported that when obtaining low scores from AWE, students feel frustrated and sad.

# 4.4.5.  Individual differences

The results in Section 3.4.7, Individual differences, indicate that many individual factors such as students’ language skills, prior knowledge, learning motivation, attitude, and technical ability can impede the effects of AWE (Bai & Hu, 2017; Liao, 2016; Shang, 2019). O’Neill and Russell (2019) suggested that more deliberated AWE-assisted teaching and learning approaches are necessary in future to adapt to the needs of diverse groups of students. It is thus necessary to conduct more research to explore how these individual factors impact the effects of AWE, the results of which may help teachers improve the flexibility of AWE-assisted writing activities for students from diverse backgrounds and those with special needs.

# 4.4.6.  Relationships and other issues

The effects of AWE are context-dependent. It is therefore necessary to investigate the relationships and the influencing factors involved in AWE-related writing so that more effective AWE-assisted writing practices can be designed to provide individualized learning for students with diverse backgrounds. The findings in 3.4.8 Relationships and 3.4.9 Other issues also support this suggestion.

# 4.5.  Implications for researchers, teachers and developers of AWE systems

Based on the discussion above and the seven identified themes from the reviewed articles, there are several implications for researchers, teachers and developers of AWE systems.

# 4.5.1.  Implications for researchers

As students’ engagement in writing often involves complicated processes involving both individual and contextual factors (Zhang, 2017), more research on the relationships or factors among them and how they may influence the effects of AWE-assisted writing activities are necessary.

As for interacting with feedback, we found little evidence about how students dealt with feedback and made revisions. Research on the underlying processes would enhance our understanding of how knowledge is accumulated through AWE-involved learning activities (Cotos et  al., 2017; Roscoe et  al., 2017) and how to better design AWE systems and learning activities.

Writing is generally a process of social interaction in which meaning is negotiated through interactions between the writer and the reader (Chen & Cheng, 2008; Nystrand, 1989). Teachers and/or peers normally play the role of the audience negotiating the meaning of a text with writers to help them improve their writing. However, peer or teacher feedback is not always available, while AWE is instant and attentive, and this immediacy can compensate for the asynchronicity of peer or teacher feedback (Hoang & Kunnan, 2016; Liu & Kunna, 2016; Shang, 2019).

We also contend that more research is needed to investigate which types of AWE (e.g. general or specific) are appropriate for specific language areas or students with varying language proficiency levels. Other ideas for future research included examining: the transfer effects of AWE over long durations (Wade-Stein & Kintsch, 2004); how AWE can be integrated into classroom instruction and how various integrations affect students’ perceptions, revision, and performance (Saricaoglu, 2019). As AWE-assisted writing is a complicated process, careful pedagogical design of the instruction is viewed as necessary, and more research in various contexts is necessary before generalizing its effects (Chen & Cheng, 2008). Additionally, future research should apply both quantitative and qualitative methods to collect data and provide a clear picture of the progression and effects of AWE-assisted writing (Chen & Cheng, 2008; Proske et  al., 2012).

# 4.5.2.  Implications for teachers

The results of our review suggest that AWE does not have a direct effect on learners’ writing performance; instead, AWE primarily relies on how teachers incorporate it into their teaching. Teachers’ pedagogical practice impacts what individual learners feel and how they react to the AWE-assisted writing activity, which is manifested in their cognitive, behavioral, and emotional engagement. These engagements are closely associated with their revision and/or writing performance. As cognitive, emotional, and behavioral engagements dynamically interact with each other (Zhang, 2017; Zhang & Hyland, 2018), facilitating learner engagement with AWE by simultaneously considering the three kinds of engagement is strongly recommended.

Teachers should also monitor students’ learning process and adjust the pedagogy accordingly. For example, if teachers find students are superficially dealing with the AWE feedback or do not know how to deal with it via checks on learning logs, the teacher can help them develop revising strategies, or pair them with a classmate. When students’ motivation for using AWE feedback is low, teachers can consider gamifying subsequent stages of AWE-assisted writing.

By fostering the learners’ sense of responsibility and autonomy, teachers can help students overcome the shortcomings of AWE, stimulating them to review the feedback critically and reflectively, leading to effective and creative revisions. Although AWE has its weaknesses and has received significant criticism, we tend to consider AWE-assisted writing as part of an ‘ecology of implementation’ (Cotos, 2014, p. 59) which teachers should make full use of.

# 4.5.3.  Implications for developers of AWE systems

Current AWE systems focus mainly on students’ writing, so the feedback is largely generated based on students’ written output while ignoring the learning process (e.g. learning improvement, learning behaviors, and affective reaction). However, students are usually inspired by their progress which is emotionally related to their learning engagement. Future AWE systems should take these factors into account to provide more personalized feedback to enhance students’ individual developments by integrating emerging technologies such as big data and deep learning algorithms into the process of system design. For example, after students achieve significant improvement in low level writing aspects (i.e. grammar, word choices, etc.), the AWE systems may provide them with feedback on higher-level writing ones (i.e. content, coherence, etc.) in the format of indirect or elicited feedback. However, if they fail to achieve the expected learning objectives, the system can produce lower-order and elaborated feedback to help learners keep pace with their development. Practitioners and researchers are also advised to recognize the social potential of AWE which can help students reinforce the effects of learning through teaching. Specifically, when students progress in a targeted language area, the AWE system should guide them to help those who fail to reach the expected objectives in the language area by way of peer instruction or feedback, thus personalizing the design of the AWE-assisted learning activity.

With further epistemic and pedagogical investigations of AWE-assisted writing, future research should explore the development and application of personalized artificial intelligence-enhanced AWE systems. Ideally, AWE systems should be adaptive to the learners’ individual situation, not only their writing performance but also their learning process and engagement.

# 5.  Conclusion

This review aimed to provide a comprehensive analysis of the literature on AWE-assisted writing in terms of the research methodology, types of learners, types of feedback and its applications, learning outcomes, and implications to inform future work in this field. The results showed that AWE feedback is to some extent effective for improving students’ writing, especially at the level of language mechanics, but is not comparable to teacher or peer feedback. We also found that interactions with AWE often involve students’ cognitive, emotional, and behavioral engagement, which further influences their revision performance. Thus, future research may investigate how students engage in AWE-assisted writing, what factors affect their engagement, and how and to what extent these factors may affect the engagement.

This review is not without its own limitations. First, only 48 studies were reviewed, mainly retrieved from SSCI journals, thus limiting the scope. Nevertheless, we reviewed the publications in these journals because they have been widely recognized as the most rigorous among research journals (Authors, 2020, 2021). Moreover, we developed the search terms based on Stevenson and Phakiti (2014) and applied rigorous inclusion and exclusion criteria to select only empirical studies on AWE-assisted writing. Thus, the findings and conclusions represent only a limited sector of published studies so we cannot claim to have investigated every issue of relevance in the area. Such a limitation, however, is common among review studies, e.g. Duman et  al. (2015), Shadiev et  al. (2017), and Hung et  al. (2018). Future research may consider enlarging the data sources and including more studies to strengthen the validity of review. One alternative is to follow the guidelines for selecting databases for meta-analysis or systematic reviews in applied linguistics proposed by In’Nami and Koizumi (2010), which would include more popular search engines such as Linguistics and Language Behavior Abstracts, ERIC, and ProQuest. The 48 reviewed studies were heterogeneous across many factors (e.g. the participants’ age, L1 or L2, the research design, and types of learning outcomes), and most of them did not report the effect size, so we could not conduct a meta-analysis. Because of this, we aimed to conduct a comprehensive and critical analysis of the relevant literature, rather than a rigorous meta-analysis that focused on the effects of AWE. Additionally, this review included studies on both first and second languages; future research may consider focusing on AWE use in L2 contexts only.

In conclusion, AWE is an important recent development for assisting writers who compose in either their native tongue or their L2. As time goes forward, our composing processes will increasingly be aided by technology, perhaps spreading beyond the domain of linguistic accuracy to that of meaning. Developing an understanding of the current state of the art, as the present study set out to do, helps to bring an awareness of AWE’s present usage and capabilities and where they may lead.

# Disclosure statement

No potential conflict of interest was reported by the authors.

# Funding

This research is financially supported by the Croucher Chinese Visitorships 2019-20 of the Croucher Foundation, Hong Kong SAR, the Research Cluster Fund (RG 78/2019-2020R) of The Education University of Hong Kong, and the Educational Science Development Program of Zhejiang Province (NO. 2020SCG030).

# Notes on contributors

Qing-Ke Fu is a lecture, his research interests include technology-assisted learning, game-based learning, Flipped classroom and STEM education.

Di Zou is the corresponding author of this paper. She is an Assistant Professor at the Education University of Hong Kong. Her research interests include technology-enhanced language learning, game-based language learning, and AI in English language education.

Haoran Xie is an Associate Professor at the Department of Computing and Decision Sciences, Lingnan University, Hong Kong. His research interests include artificial intelligence, big data, and educational technology. He is the Editor-in-Chief of Computers and Education: Artificial Intelligence.

Dr. Gary Cheng is an Associate Professor of the Department of Mathematics and Information Technology at The Education University of Hong Kong. With substantial years of work experience in Hong Kong academia, Dr. Cheng has built a wealth of knowledge and a network of support to unleash the potential of technology for teacher education.

# ORCID

Qing-Ke Fu $\textcircled{1}$ http://orcid.org/0000-0002-9032-2905   
Di Zou $\textcircled{1}$ http://orcid.org/0000-0001-8435-9739   
Haoran Xie $\textcircled{1}$ http://orcid.org/0000-0003-0965-3617   
Gary Cheng $\textcircled{1}$ http://orcid.org/0000-0002-5614-3348

# References

Bahari, S. F. (2010). Qualitative versus quantitative research strategies: Contrasting epistemological and ontological assumptions. Technology Journal, 52, 18–26.   
Bailey, S. (2014). Academic writing: A handbook for international students. Routledge.   
Bitchener, J., & Ferris, D. (2012). Written corrective feedback in second language acquisition and writing. Taylor & Francis.   
Bruton, A. (2009). Designing research into the effects of grammar correction in L2 writing: Not so straightforward. Journal of Second Language Writing, 18(2), 136–140. https://doi.org/10.1016/j.jslw.2009.02.005   
Calvo, R. A., & Ellis, R. A. (2010). Students’ conceptions of tutor and automated feedback in professional writing. Journal of Engineering Education, 99(4), 427–438. https://doi.org/10.1002/j.2168-9830.2010.tb01072.x   
Chen, X., Zou, D., Cheng, G., & Xie, H. (2020a). Detecting latent topics and trends in educational technologies over four decades using structural topic modeling: A retrospective of all volumes of Computers & Education. Computers & Education, 151, 103855. https://doi.org/10.1016/j.compedu.2020.103855   
Chen, X., Zou, D., & Xie, H. (2020b). Fifty years of British Journal of Educational Technology: A topic modeling based bibliometric perspective. British Journal of Educational Technology, 51(3), 692–708. https://doi.org/10.1111/bjet.12907   
Chen, X., Zou, D., Xie, H., & Cheng, G. (2021a). Twenty years of personalized language learning. Educational Technology & Society, 24(1), 205–222.   
Chen, X., Zou, D., Xie, H., & Su, F. (2021b). Twenty-five years of computer-assisted language learning: A topic modeling analysis: Twenty-five years of CALL. Language Learning & Technology, 25(3), 151–185.   
Chen, X., Zou, D., Xie, H., & Wang, F. L. (2021c). Past, present, and future of smart learning: A topic-based bibliometric analysis. International Journal of Educational Technology in Higher Education, 18(1), 1–29. https://doi.org/10.1186/s41239-020-00239-6   
Cook, D. A., & Hatala, R. (2015). Got power? A systematic review of sample size adequacy in health professions education research. Advances in Health Sciences Education: Theory and Practice, 20(1), 73–83. https://doi.org/10.1007/s10459-014-9509-5   
Cotos, E. (2014). Genre-based automated writing evaluation for L2 research writing: From design to evaluation and enhancement. Palgrave Macmillan.   
Denham, A. R. (2019). Using the PCaRD digital game-based learning model of instruction in the middle school mathematics classroom: A case study. British Journal of Educational Technology, 50(1), 415–427. https://doi.org/10.1111/bjet.12582   
Devitt, A. (2004). Writing genres. Southern Illinois University Press.   
Duman, G., Orhon, G., & Gedik, N. (2015). Research trends in mobile assisted language learning from 2000 to 2012. ReCALL, 27(2), 197–216. https://doi.org/10.1017/ S0958344014000287   
El Ebyary, K., & Windeatt, S. (2010). The impact of computer-based feedback on students’ written work. International Journal of English Studies, 10(2), 121–142. https:// doi.org/10.6018/ijes/2010/2/119231   
Fatawi, I., Degeng, I. N. S., Setyosari, P., Ulfa, S., & Hirashima, T. (2020). Effect of online-based concept map on student engagement and learning outcome. International Journal of Distance Education Technologies (IJDET), 18(3), 42–56.   
Fredricks, J. A., Blumenfeld, P. C., & Paris, A. H. (2004). School engagement: Potential of the concept, state of the evidence. Review of Educational Research, 74(1), 59–109. https://doi.org/10.3102/00346543074001059   
Graesser, A. C., McNamara, D. S., & Louwerse, M. M. (2003). What do readers need to learn in order to process coherence relations in narrative and expository text? In A. P. Sweet & C. E. Snow (Eds.), Rethinking reading comprehension (pp. 82–98). Guilford.   
Graesser, A. C., McNamara, D. S., Louwerse, M. M., & Cai, Z. (2004). Coh-Metrix: Analysis of text on cohesion and language. Behavior Research Methods, Instruments, & Computers: A Journal of the Psychonomic Society, Inc, 36(2), 193–202. https://doi. org/10.3758/bf03195564   
Graham, S., Hebert, M., & Harris, K. R. (2015). Formative assessment and writing: A meta-analysis. The Elementary School Journal, 115(4), 523–547. https://doi.org/10.1086/681947   
Grimes, D., & Warschauer, M. (2010). Utility in a fallible tool: A multi-site case study of automated writing evaluation. The Journal of Technology, Learning and Assessment, 8(6), 4–43.   
Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81–112. https://doi.org/10.3102/003465430298487   
Hoang, G. T. L., & Kunnan, A. J. (2016). Automated essay evaluation for English language learners: A case study of MY Access. Language Assessment Quarterly, 13(4), 359–376. https://doi.org/10.1080/15434303.2016.1230121   
Hsu, Y. C., Ho, H. N. J., Tsai, C. C., Hwang, G. J., Chu, H. C., Wang, C. Y., & Chen, N. S. (2012). Research trends in technology-based learning from 2000 to 2009: A content analysis of publications in selected journals. Educational Technology & Society, 15(2), 354–370.   
Hung, H.-T., Yang, J. C., Hwang, G.-J., Chu, H.-C., & Wang, C.-C. (2018). A scoping review of research on digital game-based language learning. Computers & Education, 126, 89–104. https://doi.org/10.1016/j.compedu.2018.07.001   
Hwang, G.-J., & Fu, Q.-K. (2019). Trends in the research design and application of mobile language learning: A review of 2007–2016 publications in selected SSCI journals. Interactive Learning Environments, 27(4), 567–581. https://doi.org/10.1080/ 10494820.2018.1486861   
In’Nami, Y., & Koizumi, R. (2010). Database selection guidelines for meta-analysis in applied linguistics. TESOL Quarterly, 44(1), 169–184. https://doi.org/10.5054/ tq.2010.215253   
Kellogg, R. T., Whiteford, A. P., & Quinlan, T. (2010). Does automated feedback help students learn to write? Journal of Educational Computing Research, 42(2), 173–196. https://doi.org/10.2190/EC.42.2.c   
Leontjev, D. (2014). The effect of automated adaptive corrective feedback: L2 English questions. Apples-Journal of Applied Language Studies, 8(2), 43–66.   
Lin, H. C., & Hwang, G. J. (2019). Research trends of flipped classroom studies for medical courses: A review of journal publications from 2008 to 2017 based on the technology-enhanced learning model. Interactive Learning Environments, 27(8), 1011– 1027. https://doi.org/10.1080/10494820.2018.1467462   
Link, S., Mehrzad, M., & Rahimi, M. (2020). Impact of automated writing evaluation on teacher feedback, student revision, and writing improvement. Computer Assisted Language Learning, 33, 1–30.   
Liu, S., & Kunna, A. J. (2016). Investigating the application of automated writing evaluation to Chinese undergraduate English majors: A case study of WriteToLearn. CALICO Journal, 33(1), 71–91. https://doi.org/10.1558/cj.v33i1.26380   
Lopez, X., Valenzuela, J., Nussbaum, M., & Tsai, C. C. (2015). Some recommendations for the reporting of quantitative studies. Computers & Education, 91, 106–110. https:// doi.org/10.1016/j.compedu.2015.09.010   
Luo, Y., & Liu, Y. (2017). Comparison between peer feedback and automated feedback in college English writing: A case study. Open Journal of Modern Linguistics, 7(4), 197–215. https://doi.org/10.4236/ojml.2017.74015   
Mirmotahari, O., Damsa, C., & Berg, Y. (2018). Individualized assessment and automated feedback in undergraduate computer science education. Proceedings of International Conference of the Learning Sciences, ICLS, 3(2018), 1577–1578.   
Nunes, A., Cordeiro, C., Limpo, T., & Castro, S. L. (2021). Effectiveness of automated writing evaluation systems in school settings: A systematic review of studies from 2000 to 2020. Journal of Computer Assisted Learning. https://doi.org/10.1111/jcal.12635   
Nystrand, M. (1989). A social-interactive model of writing. Written Communication, 6(1), 66–85. https://doi.org/10.1177/0741088389006001005   
Onkas, N. A. (2015). Interpretation theory and creative writing. The Anthropologist, 22(2), 196–202. https://doi.org/10.1080/09720073.2015.11891869   
Plonsky, L. (2015). Quantitative considerations for improving replicability in call and applied linguistics. Calico Journal, 32(2), 232–244. https://doi.org/10.1558/cj.v32i2.26857   
Ranalli, J. (2021). L2 student engagement with automated feedback on writing: Potential for learning and issues of trust. Journal of Second Language Writing, 52, 1–16. https:// doi.org/10.1016/j.jslw.2021.100816   
Ranalli, J., Link, S., & Chukharev-Hudilainen, E. (2017). Automated writing evaluation for formative assessment of second language writing: Investigating the accuracy and usefulness of feedback as part of argument-based validation. Educational Psychology, 37(1), 8–25. https://doi.org/10.1080/01443410.2015.1136407   
Shadiev, R., Hwang, W.-Y., & Huang, Y.-M. (2017). Review of research on mobile language learning in authentic environments. Computer Assisted Language Learning, 30(3–4), 284–303. https://doi.org/10.1080/09588221.2017.1308383   
Shute, V. J. (2008). Focus on formative feedback. Review of Educational Research, 78(1), 153–189. https://doi.org/10.3102/0034654307313795   
Stevenson, M. (2016). A critical interpretative synthesis: The integration of automated writing evaluation into classroom writing instruction. Computers and Composition, 42, 1–16. https://doi.org/10.1016/j.compcom.2016.05.001   
Stevenson, M., & Phakiti, A. (2014). The effects of computer-generated feedback on the quality of writing. Assessing Writing, 19, 51–65. https://doi.org/10.1016/j.asw.2013.11.007   
Su, F., & Zou, D. (2020). Technology-enhanced collaborative language learning: Theoretical foundations, technologies, and implications. Computer Assisted Language Learning, 1–35. https://doi.org/10.1080/09588221.2020.1831545   
Tardy, C. M. (2009). Building genre knowledge. Parlor Press.   
Trotter, R. T. (2012). Qualitative research sample design and sample size: Resolving and unresolved issues and inferential imperatives. Preventive Medicine, 55(5), 398–400. https://doi.org/10.1016/j.ypmed.2012.07.003   
Twining, P., Heller, R. S., Nussbaum, M., & Tsai, C. C. (2017). Some guidance on conducting and reporting qualitative studies. Computers & Education, 106, A1–A9. https://doi.org/10.1016/j.compedu.2016.12.002   
Ware, P. (2011). Computer-generated feedback on student writing. TESOL Quarterly, 45(4), 769–774. https://doi.org/10.5054/tq.2011.272525   
Wilson, J., Olinghouse, N. G., & Andrada, G. N. (2014). Does automated feedback improve writing quality? Learning Disabilities: A Contemporary Journal, 12(1), 93–118.   
Zhang, R., & Zou, D. (2020). Types, purposes, and effectiveness of state-of-the-art technologies for second and foreign language learning. Computer Assisted Language Learning, 1–47. https://doi.org/10.1080/09588221.2020.1744666   
Zhang, R., & Zou, D. (2021a). Types, features, and effectiveness of technologies in collaborative writing for second language learning. Computer Assisted Language Learning, 1–31. https://doi.org/10.1080/09588221.2021.1880441   
Zhang, R., & Zou, D. (2021b). A state-of-the-art review of the modes and effectiveness of multimedia input for second and foreign language learning. Computer Assisted Language Learning, 1–27. https://doi.org/10.1080/09588221.2021.1896555   
Zheng, P., Liang, X., Huang, G., & Liu, X. (2016). Mapping the field of communication technology research in Asia: Content analysis and text mining of SSCI journal articles 1995–2014. Asian Journal of Communication, 26(6), 511–531. https://doi.org/1 0.1080/01292986.2016.1231210   
Zhu, M., Liu, O. L., & Lee, H. S. (2020). The effect of automated feedback on revision behavior and learning gains in formative assessment of scientific argument writing. Computers & Education, 143, 103668. https://doi.org/10.1016/j.compedu.2019.103668   
Zou, D., Huang, Y., & Xie, H. (2021). Digital game-based vocabulary learning: Where are we and where are we going? Computer Assisted Language Learning, 34(5–6), 751–777.   
Zou, D., Luo, S., Xie, H., & Hwang, G. J. (2020). A systematic review of research on flipped language classrooms: Theoretical foundations, learning activities, tools, research topics and findings. Computer Assisted Language Learning, 1–27. https://doi.org/10.1 080/09588221.2020.1839502

# Appendix A. The coding schemes Coded papers

Akcapinar, G. (2015). How automated feedback through text mining changes plagiaristic behavior in online assignments. Computers & Education, 87, 123–130. https:// doi.org/10.1016/j.compedu.2015.04.007   
Bai, L., & Hu, G. (2017). In the face of fallible AWE feedback: How do students respond? Educational Psychology, 37(1), 67–81. https://doi.org/10.1080/01443410.2016. 1223275   
Barrot, J. S. (2021). Using automated written corrective feedback in the writing classrooms: Effects on L2 writing accuracy. Computer Assisted Language Learning. https:// doi.org/10.1080/09588221.2021.1936071

Bond, M., & Pennebaker, J. W. (2012). Automated computer-based feedback in expressive writing. Computers in Human Behavior, 28(3), 1014–1018. https://doi.org/10.1016/j. chb.2012.01.003

Chapelle, C. A., Cotos, E., & Lee, J. (2015). Validity arguments for diagnostic assessment using automated writing evaluation. Language Testing, 32(3), 385–405. https://doi. org/10.1177/0265532214565386   
Chen, C.-F E., & Cheng, W.-Y E. C. (2008). Beyond the design of automated writing evaluation: Pedagogical practices and perceived learning effectiveness in EFL writing classes. Language Learning & Technology, 12(2), 94–112.   
Chen, M. H., Huang, S. T., Chang, J. S., & Liou, H. C. (2015). Developing a corpus-based paraphrase tool to improve EFL learners’ writing skills. Computer Assisted Language Learning, 28(1), 22–40. https://doi.org/10.1080/09588221.2013.783873Cheng, G. (2017). The impact of online automated feedback on students’ reflective journal writing in an EFL course. The Internet and Higher Education, 34, 18–27. https://doi.org/10.1016/j. iheduc.2017.04.002   
Cotos, E. (2011). Potential of automated writing evaluation feedback. Calico Journal, 28(2), 420–459. https://doi.org/10.11139/cj.28.2.420-459   
Cotos, E., Link, S., & Huffman, S. (2017). Effects of DDL technology on genre learning. Language Learning & Technology, 21(3), 104–130.   
Dikli, S., & Bleyle, S. (2014). Automated Essay Scoring feedback for second language writers: How does it compare to instructor feedback? Assessing Writing, 22, 1–17. https://doi.org/10.1016/j.asw.2014.03.006   
Dwyer, H. J., & Sullivan, H. J. (1993). Student preferences for teacher and computer composition marking. The Journal of Educational Research, 86(3), 137–141. https:// doi.org/10.1080/00220671.1993.9941152   
El Ebyary, K., & Windeatt, S. (2019). Eye tracking analysis of EAP students’ regions of interest in computer-based feedback on grammar, usage, mechanics, style and organization and development. System, 83, 36–49. https://doi.org/10.1016/j.system.2019.03.007   
Gao, J. W., & Ma, S. (2019). The effect of two forms of computer-automated metalinguistic corrective feedback. Language Learning & Technology, 23(2), 65–83.   
Gao, J. W., & Ma, S. (2020). Instructor feedback on free writing and automated corrective feedback in drills: Intensity and efficacy. Language Teaching Research, https:// doi.org/10.1177/136216882091533.7   
Garcia-Gorrostieta, J. M., Lopez-Lopez, A., & Gonzalez-Lopez, S. (2018). Automatic argument assessment of final project reports of computer engineering students. Computer Applications in Engineering Education, 26(5), 1217–1226. https://doi. org/10.1002/cae.21996   
Guo, Q., Feng, R. L., & Hua, Y. F. (2021). How effectively can EFL students use automated written corrective feedback (AWCF) in research writing? Computer Assisted Language Learning. https://doi.org/10.1080/09588221.2021.1879161   
Han, Y. X., Zhao, S., & Ng, L. L. (2021). How technology tools impact writing performance, lexical complexity, and perceived self-regulated learning strategies in EFL academic writing: A comparative study. Frontiers in Psychology. https://doi.org/10.3389/ fpsyg.2021.752793   
Hassanzadeh, M., & Fotoohnejad, S. (2021). Implementing an automated feedback program for a foreign language writing course: A learner-centric study implementing an AWE tool in a L2 class. Journal of Computer Assisted Learning, 37(5), 1494–1507. https://doi.org/10.1111/jcal.12587   
Jiang, L. J., & Yu, S. L. (2020). Appropriating automated feedback in L2 writing: experiences of Chinese EFL student writers. Computer Assisted Language Learning. https://doi.org/10.1080/09588221.2020.1799824   
Lachner, A., Burkhart, C., & Nuckles, M. (2017). Mind the gap! Automated concept map feedback supports students in writing cohesive explanations. Journal of Experimental Psychology. Applied, 23(1), 29–46. https://doi.org/10.1037/xap0000111   
Lai, Y. H. (2010). Which do students prefer to evaluate their essays: Peers or computer program. British Journal of Educational Technology, 41(3), 432–454. https://doi. org/10.1111/j.1467-8535.2009.00959.x   
Landauer, T., Lochbaum, K., & Dooley, S. (2009). A new formative assessment technology for reading and writing. Theory into Practice, 48(1), 44–52. https://doi. org/10.1080/00405840802577593   
Lee, C. (2020). A study of adolescent English learners’ cognitive engagement in writing while using an automated content feedback system. Computer Assisted Language Learning, 33(1–2), 26–57. https://doi.org/10.1080/09588221.2018.1544152   
Lee, C., Wong, K. C. K., Cheung, W. K., & Lee, F. S. L. (2009). Web-based essay critiquing system and EFL students’ writing: A quantitative and qualitative investigation. Computer Assisted Language Learning, 22(1), 57–72. https://doi.org/10.1080/09588220802613807   
Li, J. R., Link, S., & Hegelheimer, V. (2015). Rethinking the role of automated writing evaluation (AWE) feedback in ESL writing instruction. Journal of Second Language Writing, 27, 1–18. https://doi.org/10.1016/j.jslw.2014.10.004   
Liao, H. C. (2016). Using automated writing evaluation to reduce grammar errors in writing. Elt Journal, 70(3), 308–319. https://doi.org/10.1093/elt/ccv058   
Liu, M., Li, Y., Xu, W., & Liu, L. (2016). Automated essay feedback generation and its impact on revision. IEEE Transactions on Learning Technologies, 10(4), 502–513. https://doi.org/10.1109/TLT.2016.2612659   
Morch, A. I., Engeness, I., Cheng, V. C., Cheung, W. K., & Wong, K. C. (2017). EssayCritic: Writing to learn with a knowledge-based design critiquing system. Educational Technology & Society, 20(2), 213–223.   
O’Neill, R., & Russell, A. (2019). Stop! Grammar time: University students’ perceptions of the automated feedback program Grammarly. Australasian Journal of Educational Technology, 35(1), 3795. https://doi.org/10.14742/ajet.3795   
Proske, A., Narciss, S., & McNamara, D. S. (2012). Computer-based scaffolding to facilitate students’ development of expertise in academic writing. Journal of Research in Reading, 35(2), 136–152. https://doi.org/10.1111/j.1467-9817.2010.01450.x   
Reynolds, B. L., Kao, C. W., & Huang, Y. Y. (2021). Investigating the effects of perceived feedback source on second language writing performance: A quasi-experimental study. The Asia-Pacific Education Researcher, 30(6), 585–595. https://doi.org/10.1007/ s40299-021-00597-3   
Roscoe, R. D., Wilson, J., Johnson, A. C., & Mayra, C. R. (2017). Presentation, expectations, and experience: Sources of student perceptions of automated writing evaluation. Computers in Human Behavior, 70, 207–221. https://doi.org/10.1016/j. chb.2016.12.076   
Saricaoglu, A. (2019). The impact of automated feedback on L2 learners’ written causal explanations. ReCALL, 31(2), 189–203. https://doi.org/10.1017/S095834401800006X   
Shang, H.-F. (2019). Exploring online peer feedback and automated corrective feedback on EFL writing performance. Interactive Learning Environments, 30(1):4–16.   
Sherafati, N., Largani, F. M., & Amini, S. (2020). Exploring the effect of computer-mediated teacher feedback on the writing achievement of Iranian EFL learners: Does motivation count? Education and Information Technologies, 25(5), 4591–4613. https://doi. org/10.1007/s10639-020-10177-5   
Sung, Y. T., Liao, C. N., Chang, T. H., Chen, C. L., & Chang, K. E. (2016). The effect of online summary assessment and feedback system on the summary writing on 6th graders: The LSA-based technique. Computers & Education, 95, 1–18. https://doi. org/10.1016/j.compedu.2015.12.003   
Tian, L. L., & Zhou, Y. (2020). Learner engagement with automated feedback, peer feedback and teacher feedback in an online EFL writing context. System, 91, 102247. https://doi.org/10.1016/j.system.2020.102247   
Wade-Stein, D., & Kintsch, E. (2004). Summary street: Interactive computer support for writing. Cognition and Instruction, 22(3), 333–362. https://doi.org/10.1207/ s1532690xci2203_3   
Wang, Y., Harrington, M., & White, P. (2012). Detecting breakdowns in local coherence in the writing of Chinese English learners. Journal of Computer Assisted Learning, 28(4), 396–410. https://doi.org/10.1111/j.1365-2729.2011.00475.x   
Wang, Y. J., Shang, H. F., & Briody, P. (2013). Exploring the impact of using automated writing evaluation in English as a foreign language university students’ writing. Computer Assisted Language Learning, 26(3), 234–257. https://doi.org/10.1080/09588 221.2012.655300   
Wilson, J., & Roscoe, R. D. (2020). Automated writing evaluation and feedback: Multiple metrics of efficacy. Journal of Educational Computing Research, 58(1), 87–125. https:// doi.org/10.1177/0735633119830764   
Zaini, A. (2018). Word processors as monarchs: Computer-generated feedback can exercise power over and influence EAL learners’ identity representations. Computers & Education, 120, 112–126. https://doi.org/10.1016/j.compedu.2018.01.014   
Zhang, Z. (2017). Student engagement with computer-generated feedback: A case study. Elt Journal, 71(3), 317–328. https://doi.org/10.1093/elt/ccw089   
Zhang, Z. (2020). Engaging with automated writing evaluation (AWE) feedback on L2 writing: Student perceptions and revisions. Assessing Writing, 43, 78–91. https://doi. org/10.1016/j.asw.2019.100439   
Zhang, Z. V., & Hyland, K. (2018). Student engagement with teacher and automated feedback on L2 writing. Assessing Writing, 36, 90–102. https://doi.org/10.1016/j. asw.2018.02.004   
Zhu, M. X., Lee, H. S., Wang, T., Liu, O. L., Belur, V., & Pallant, A. (2017). Investigating the impact of automated feedback on students’ scientific argumentation. International Journal of Science Education, 39(12), 1648–1668. https://doi.org/10.1080/09500693.2 017.1347303   
Zhu, M. X., Liu, O. L., & Lee, H. S. (2020). The effect of automated feedback on revision behavior and learning gains in formative assessment of scientific argument writing. Computers & Education, 143, 103668. https://doi.org/10.1016/j.compedu.2019.103668