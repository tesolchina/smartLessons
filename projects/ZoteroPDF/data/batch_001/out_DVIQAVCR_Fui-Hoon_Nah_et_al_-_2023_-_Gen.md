# Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration

Fiona Fui-Hoon Nah, Ruilin Zheng, Jingyuan Cai, Keng Siau & Langtao Chen

To cite this article: Fiona Fui-Hoon Nah, Ruilin Zheng, Jingyuan Cai, Keng Siau & Langtao Chen (2023) Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration, Journal of Information Technology Case and Application Research, 25:3, 277-304, DOI: 10.1080/15228053.2023.2233814

To link to this article: https://doi.org/10.1080/15228053.2023.2233814

# EDITORIAL PREFACE ARTICLE

# Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration

# Introduction

Artificial intelligence (AI) has elicited much attention across disciplines and industries (Hyder et al., 2019). AI has been defined as “a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation” (Kaplan & Haenlein, 2019, p. 15). AI has gone through several development stages and AI winters. In the first two decades (i.e., 1950s and 1960s), AI demonstrated success which included programs such as General Problem Solver (Newell et al., 1959) and ELIZA (Weizenbaum, 1966). However, limitations in processing capacity and reduced spending on AI turned its development into stagnation.

In recent years, AI has made a comeback with the introduction of AlphaGo in 2015 and ChatGPT in 2022. Following the release of the application named “Chat Generative Pre-trained Transformer” or ChatGPT by OpenAI in late 2022, AI has attracted worldwide attention. As Figure 1 shows, the term “ChatGPT” has attracted a growing search interest on Google since its release on November 30, 2022. ChatGPT belongs to a class of language models called Generative Pretrained Transformer (GPT). The category of GPT refers to Large Language Models (LLMs) that use deep learning techniques for extensive training with tremendous amounts of data (Cascella et al., 2023). ChatGPT is specifically designed and fine-tuned for conversational usage to produce human-like responses by drawing on its wealth of information and knowledge. The capabilities of ChatGPT are enabled by generative AI, which refers to a type of AI that can generate human-like text and creative content (e.g., music and images) as well as consolidate data from different sources for analysis (Dasborough, 2023). ChatGPT passed the Turing test by fooling people to believe that its responses were from humans rather than machines/AI (Woolf, 2022). The advent of this state-of-theart AI is expected to revolutionize society as well as the way we live, work, learn, and communicate.

The generative AI model is an example of highly promising unsupervised machine learning. Previous generative models, such as Restricted Boltzmann Machines (Smolensky, 1986), Deep Belief Networks (Hinton et al., 2006), and Deep Boltzmann Machines (Salakhutdinov & Larochelle, 2010), exhibit limitations as they lack generalization power (Pan et al., 2019). Featuring exceptional data generation capacity, Generative Adversarial Networks (GANs) were put forward as a novel generative model. GANs consist of two competing neural networks, one called the generator and the other called the discriminator (Goodfellow et al., 2020). The generator produces as realistic data as possible while the discriminator tries to differentiate synthesized data from real data. Another neural network architecture called Transformers was proposed in the paper “Attention Is All You Need” and is based on attention mechanisms that dispense between recurrence and convolutions (Vaswani et al., 2017). The concept of self-attention enables the model to attend to different parts of the input sequence while generating the output sequence. These breakthroughs brought generative AI to a new era of development and advancement.

![](img/e69fd7be127aa5faac1e2bd0ee7aad894b7c170e9de52f3a56d148c2cc6bf3f5.jpg)

Generative AI can generate multimodal contents, including but not limited to text, audio, image, video, and even three-dimensional models. Some representative applications include ChatGPT for text, Midjourney for images, and DeepBrain for videos. These models can be interrelated via text-to-image generative models (Qiao et al., 2022) and audio-visual correlation transformers (S. Wang et al., 2022). The diverse forms of AI-generated content (AIGC) enable a wide range of applications. AI can generate textual content, such as poems (Köbis & Mossink, 2021), political statements (Bullock & Luengo-Oroz, 2019), and academic papers (Hu, 2023), that can be hard to differentiate from human-generated content. Examples of AI-generated images include artworks (Gillotte, 2019), synthetic faces (Whittaker et al., 2020), and magnetograms of the Sun (J. Liu et al., 2021), which range from humanities to sciences. There are, however, potential legal, moral, and ethical issues created by generative AI, such as copyright infringement in AI-generated artworks (Gillotte, 2019), cheating and plagiarism at educational institutions (University of Cambridge, n.d..), data privacy and security (Siau & Wang, 2020), and malicious use of deepfakes and GANs (Whittaker et al., 2020).

AI has created both challenges and opportunities in various fields, including technology (explanation and information processing), business (decision-making and AI-enabled automation), education (intelligent tutor and personalized learning), healthcare (smart health and AI diagnosis) as well as arts and humanities (human-centered design and cultural proximity) (Siau, 2018; W. Wang & Siau, 2019; Yin et al., 2022). AI-human collaboration is the key to addressing challenges and seizing opportunities created by generative AI. There are numerous ways that humans can collaborate with generative AI. For example, educators can utilize ChatGPT in science pedagogy while carefully examining AI-generated resources before adapting them to the teaching context (Cooper, 2023). In problem-solving, generative AI can facilitate brainstorming as well as the generation or refinement of solutions. Hydrologists attempted to use ChatGPT for basic problem-solving but found the quality questionable due to ChatGPT’s imbalanced capabilities in responding to qualitative and quantitative questions (Halloran et al., 2023). ChatGPT has demonstrated potential for supporting medical practice (Cascella et al., 2023) and providing advice for public health (Biswas, 2023). Boßelmann et al. (2023) argued that advances in AI can improve the overall diagnostic and therapeutic accuracy of epilepsy. ChatGPT is also expected to create new trends in the nursing industry, such as increased use of digital tools and robotics (Gunawan, 2023). However, potential misuse can happen, such as offering diagnosis and treatment recommendations without proper validation or oversight. Hence, regulations and governance need to be in place (Wan et al., 2022).

In the following sections, we present examples of key applications of generative AI, challenges created by generative AI, and the importance of human-centered collaboration with generative AI. Human-centered AI collaboration is key to effectively utilizing generative AI applications. In this paper, we offer general guidelines and suggestions to facilitate human-centered AI collaboration.

# Generative AI applications

Generative AI, such as ChatGPT, offers potential applications in a variety of industries including business, education, healthcare, and content generation. In this section, we review examples of generative AI applications in these industries.

# Business

Generative AI, such as ChatGPT, can serve businesses in many ways ranging from marketing and sales, operations, IT/engineering, risk and legal, human resources, accounting and finance, to utility/employee optimization (Chui et al., 2022). For example, it can function as a chatbot to offer customer service, serve as a virtual assistant to help customers complete specific tasks, carry out accounting and human resource tasks, and generate advertisements or ideas for marketing. ChatGPT can also function as an internal or external collaborator for various company projects or campaigns. Hence, the potential business applications of ChatGPT are unlimited.

However, generative AI, such as ChatGPT, can be a double-edged sword for businesses. On the one hand, it can help businesses to increase efficiency and generate creative content such as advertisements or ideas for advertisements. With increased sales and marketing and lower costs, profitability increases. On the other hand, ChatGPT can create hallucinations and produce misinformation or fake information. Hence, users cannot fully trust the information. Further, businesses need to ensure that none of their proprietary information or trade secrets are shared with ChatGPT, as they could be released to the public. Mechanisms could be used to protect proprietary information by turning off the chat history so the conversations would not be used to train the ChatGPT models (https://openai.com/blog/new-ways-to-manage-yourdata-in-chatgpt) or by banning the use of ChatGPT completely. Due to concerns about privacy and information security, a growing number of ChatGPT bans or restrictions has been imposed by organizations and countries (e.g., Ray, 2023; Stancati & Schechner, 2023).

The ability to protect proprietary information and identify fake or false information are two essential characteristics that can enable successful applications of ChatGPT by businesses. AutoGPT uses ChatGPT as the language model foundation and builds an independent information base that is not part of ChatGPT. Increasing the benefits and values offered by generative AI to businesses while minimizing any harm or losses is crucial to businesses.

# Education

ChatGPT has created disruptions and major changes in the field of education. ChatGPT can serve as an assistant in learning and teaching activities. For students, ChatGPT can assist in a variety of tasks, including information search, answering questions related to specific subjects, and enhancing writing in a variety of languages. For teachers, ChatGPT can assist in generating teaching plans, preparing teaching materials (e.g., scripts, slides, and quizzes), reviewing and grading assignments, and providing feedback to students. Because ChatGPT is based on LLMs, it can be used to create educational content, personalize learning experiences, and improve student engagement (Kasneci et al., 2023), which can improve the overall efficiency and effectiveness of education delivery. In academic research, ChatGPT can assist with problem formulation, research design, data collection and analysis, as well as reviewing and critiquing the writing and composition (Susarla et al., 2023). Further, by providing tailored support, direction, and feedback, ChatGPT is a useful tool for autodidactic learners (i.e., self-learners) in open education (Firat, 2023).

Despite ChatGPT’s usefulness in the education context, concerns arise that some students may use ChatGPT to cheat in examinations or commit plagiarism in essay writing. The undesired consequences are that education norms are disrupted, students’ learning process is affected, and academic integrity is threatened. Facing these challenges, educational institutions across the world have devised responses after the launch of ChatGPT. For instance, the University of Cambridge stated that including content generated by AI platforms such as ChatGPT would be considered academic misconduct (University of Cambridge, n.d.). The University of Oxford holds a more open attitude toward ChatGPT and considers it a useful tool for both educators and students (University of Oxford, 2023). Recently, Turnitin, a wellknown plagiarism detection software service has included the feature of AIgenerated text detection in response to the prevailing use of generative AI (Turnitin, n.d.). Similarly, a range of AI content detection services is offered by

AI Writing Check,1 CatchGPT,2 Content at Scale,3 Copyleaks,4 GPT Radar,5 GPTZero,6 OpenAI’s AI Text Classifier,7 Originality.AI,8 Winston,9 and ZeroGPT,10 among others.

# Healthcare

Healthcare is another domain in which generative AI and ChatGPT can make a significant impact. With ChatGPT reported to pass the United States Medical Licensing Examinations (Kung et al., 2023), the world has seen a gradually clearer picture of how generative AI may reshape the healthcare industry. Empowered by LLMs, generative AI, such as ChatGPT, can hold the potential of transforming the healthcare industry in a variety of aspects such as patient interaction, clinical diagnosis support, telehealth services, health education, health advice, and health promotion.

However, it remains an open question as to whether clinical professionals and patients will accept the new advancement of generative AI. Strict healthcare regulations and high entry barriers to the industry have made it hard for digital innovations such as generative AI to penetrate the healthcare domain (Ozalp et al., 2022). Concerns including the ethical use of AI, information accuracy, privacy, cybersecurity, and risk potential are persistent (Siau & Wang, 2020). In the highly regulated healthcare industry where value creation is critical, relying too much on the content generated by AI may lead to catastrophes, such as the mistreatment of patients. Also, collaboration with generative AI in the healthcare domain comes with the risk that critical and sensitive health information could be leaked or compromised.

# Content creation

Generative AI can be used to create content and is making a profound impact in the content industry. For example, the marketing industry is taking on generative AI to produce synthetic and personalized advertisements for potential consumers. Synthetic advertisements comprise content based on artificial and automatic production and modification of data (Arango et al., 2023). With data and parameters provided by humans and AI technologies including deepfakes, the collaboration between humans and AI can depict an unreal yet convincing reality (Campbell et al., 2022). The journalism industry is another area that has been radically changed by generative AI. News robots such as Quill and Xiaomingbot have been used in news production. They mainly produce news that relies on data analytics and has somewhat fixed templates. With emerging capabilities to make logical inferences and process multimodal data in LLMs, the generation of goal-driven narratives becomes possible. Hence, generative AI can produce more complex news stories with text and videos (Wong et al., 2022). Art creation is also facing revolutions caused by generative AI such as ChatGPT, DALLE-2, and Midjourney. AI may help boost human-AI collaborative content creation. In the framework of Parallel Art proposed by Guo et al. (2023), humans and AI collaborate to improve artistic creation. Humans provide creative input, feedback, and guidance for the AI system, and AI assists humans in creation by providing linguistic, visionary, and decision-making support.

Generative AI has also brought changes to the gaming industry. In the first few months of ChatGPT’s launch, users have used it to create text-based games. AI has also been used to design game characters and generate character strategies in fighting games without any human designer’s intervention (Martinez-Arellano et al., 2016). Apart from plot and character designs, generative AI is also used in visual content generation of games such as in realtime 3D scene rendering and painting of characters. With the aid of generative AI in the creation of plots, characters, and scenes, there has been a significant increase in efficiency and creativity in game production that is made possible by AI algorithms.

Generative AI models largely increase generation efficiency by automating the creation process, which is analogous to the industrial revolution of steam power, internal combustion engines, and electricity in improving the efficiency of the production of goods. Certain trends can be implicated by drawing a parallel between the advancements in content generation automation and the industrial revolution. First, there is an increasing requirement for employees to collaborate with and embrace the use of generative AI to achieve higher productivity. The need for human producers of content may decrease and those who are not competent to collaborate with AI may become unemployed or even unemployable. Second, innovative business models may emerge from the revolution of content generation. For instance, the rise of personalized AIGC has the potential to emerge as the predominant source of content consumption due to its ability to tailor content to the specific needs and preferences of individual consumers. As a result, an expanding array of content, including advertisements, short videos, and narratives, is being generated by AI algorithms based on users’ tastes and requirements. Third, the online social network structure could be reshaped as algorithms become important nodes in the communication network. As AI algorithms are given increasing authority in determining user perception and content exposure, there is a potential shift of communication power from human users to these algorithms. Consequently, companies that possess the most advanced algorithms may wield greater influence and control over communication channels.

# Challenges with generative AI

Generative AI can bring many challenges to society. In this section, we discuss these challenges from four perspectives: ethics, technology, regulations and policies, as well as economy.

# Ethical concerns

Ethics refers to systematizing, defending, and recommending concepts of right and wrong behavior (Fieser, n.d.). In the context of AI, ethical concerns refer to the moral obligations and duties of an AI application and its creators (Siau & Wang, 2020). Table 1 presents the key ethical challenges and issues associated with generative AI. These challenges include harmful or inappropriate content, bias, over-reliance, misuse, privacy and security, and the widening of the digital divide.

# Harmful or inappropriate content

Harmful or inappropriate content produced by generative AI includes but is not limited to violent content, the use of offensive language, discriminative content, and pornography. Although OpenAI has set up a content policy for ChatGPT, harmful or inappropriate content can still appear due to reasons such as algorithmic limitations or jailbreaking (i.e., removal of restrictions imposed). The language models’ ability to understand or generate harmful or offensive content is referred to as toxicity (Zhuo et al., 2023). Toxicity can bring harm to society and damage the harmony of the community. Hence, it is crucial to ensure that harmful or offensive information is not present in the training data and is removed if they are. Similarly, the training data should be free of pornographic, sexual, or erotic content (Zhuo et al., 2023). Regulations, policies, and governance should be in place to ensure any undesirable content is not displayed to users.

Table 1. Ethical challenges.   

<html><body><table><tr><td>Challenges</td><td>Issues</td><td>References</td></tr><tr><td>Harmful or inappropriate content</td><td>Content produced by generative Al could be violent, offensive or (Zhuo et al., 2023) erotic</td><td></td></tr><tr><td rowspan="3">Bias</td><td>Training data representing only a fraction of the population may (Zhuo et al., 2023) create exclusionary norms</td><td></td></tr><tr><td>Training data in one single language (or few languages) may create monolingual (or non-multilingual) bias</td><td>(Weidinger et al., 2021)</td></tr><tr><td>Cultural sensitivities are necessary to avoid bias. Bias exists in employment decision-making by generative AI.</td><td>(Dwivedi et al., 2023)</td></tr><tr><td>Over-reliance</td><td>Users adopt answers by generative Al without careful verification (Ilskender, 2023; Van Dis or fact-checking</td><td>(Chan, 2022) et al., 2023)</td></tr><tr><td>Misuse</td><td>Plagiarism for assignments and essays using texts generated by Al (Cotton et al., 2023;</td><td>Thorp, 2023)</td></tr><tr><td rowspan="2">Privacy and security</td><td>Generative Al can be used for cheating in examinations or assignments</td><td>(Susnjak, 2022)</td></tr><tr><td>Generative Al may disclose sensitive or private information.</td><td>(Fang et al., 2017; Siau</td></tr><tr><td>Digital divide</td><td>First-level digital divide for people without access to generative.</td><td>&amp; Wang, 2020) (Bozkurt &amp; Sharma,</td></tr><tr><td rowspan="2"></td><td>Al systems Second-level digital divide in which some people and cultures</td><td>2023) (Dwivedi et al., 2023)</td></tr><tr><td>may accept generative Al more than others</td><td></td></tr></table></body></html>

# Bias

In the context of AI, the concept of bias refers to the inclination that AIgenerated responses or recommendations could be unfairly favoring or against one person or group (Ntoutsi et al., 2020). Biases of different forms are sometimes observed in the content generated by language models, which could be an outcome of the training data. For example, exclusionary norms occur when the training data represents only a fraction of the population (Zhuo et al., 2023). Similarly, monolingual bias in multilingualism arises when the training data is in one single language (Weidinger et al., 2021). As ChatGPT is operating across the world, cultural sensitivities to different regions are crucial to avoid biases (Dwivedi et al., 2023). When AI is used to assist in decision-making across different stages of employment, biases and opacity may exist (Chan, 2022). Stereotypes about specific genders, sexual orientations, races, or occupations are common in recommendations offered by generative AI. Hence, the representativeness, completeness, and diversity of the training data are essential to ensure fairness and avoid biases (Gonzalez, 2023). The use of synthetic data for training can increase the diversity of the dataset and address issues with sample-selection biases in the dataset (owing to class imbalances) (Chen et al., 2021). Generative AI applications should be tested and evaluated by a diverse group of users and subject experts. Additionally, increasing the transparency and explainability of generative AI can help in identifying and detecting biases so appropriate corrective measures can be taken.

# Over-reliance

The apparent convenience and powerfulness of ChatGPT could result in overreliance by its users, making them trust the answers provided by ChatGPT. Compared with traditional search engines that provide multiple information sources for users to make personal judgments and selections, ChatGPT generates specific answers for each prompt. Although utilizing ChatGPT has the advantage of increasing efficiency by saving time and effort, users could get into the habit of adopting the answers without rationalization or verification. Over-reliance on generative AI technology can impede skills such as creativity, critical thinking, and problem-solving (Iskender, 2023) as well as create human automation bias due to habitual acceptance of generative AI recommendations (Van Dis et al., 2023). Hence, AI literacy is critical for all users. Users should not blindly trust the answers provided by generative AI but are recommended to go through verification procedures before adopting them.

# Misuse

The misuse of generative AI refers to any deliberate use that could result in harmful, unethical or inappropriate outcomes (Brundage et al., 2020). A prominent field that faces the threat of misuse is education. Cotton et al. (2023) have raised concerns over academic integrity in the era of ChatGPT. ChatGPT can be used as a high-tech plagiarism tool that identifies patterns from large corpora to generate content (Gefen & Arinze, 2023). Given that generative AI such as ChatGPT can generate high-quality answers within seconds, unmotivated students may not devote time and effort to work on their assignments and essays. Hence, in the era of generative AI, the originality of the work done by students could be difficult to assess. Text written by ChatGPT is regarded as plagiarism and is not acceptable (Thorp, 2023). Another form of misuse is cheating in examinations. If students have access to digital devices during examinations, they can resort to using ChatGPT to assist them in answering the questions. To address potential misuse in education, AI-generated content detectors such as Turnitin could be used and strict proctoring measures will need to be deployed (Susnjak, 2022). However, the challenges go beyond content detection and examination proctoring as the line between what is considered appropriate versus inappropriate use of ChatGPT could be fuzzy. Researchers have offered suggestions and recommendations on how ChatGPT could be used in the responsible conduct of scholarly activities (Susarla et al., 2023).

# Privacy and security

Data privacy and security is another prominent challenge for generative AI such as ChatGPT. Privacy relates to sensitive personal information that owners do not want to disclose to others (Fang et al., 2017). Data security refers to the practice of protecting information from unauthorized access, corruption, or theft. In the development stage of ChatGPT, a huge amount of personal and private data was used to train it, which threatens privacy (Siau & Wang, 2020). As ChatGPT increases in popularity and usage, it penetrates people’s daily lives and provides greater convenience to them while capturing a plethora of personal information about them. The concerns and accompanying risks are that private information could be exposed to the public, either intentionally or unintentionally. For example, it has been reported that the chat records of some users have become viewable to others due to system errors in ChatGPT (Porter, 2023). Not only individual users but major corporations or governmental agencies are also facing information privacy and security issues. If ChatGPT is used as an inseparable part of daily operations such that important or even confidential information is fed into it, data security will be at risk and could be breached. To address issues regarding privacy and security, users need to be very circumspect when interacting with ChatGPT to avoid disclosing sensitive personal information or confidential information about their organizations. AI companies, especially technology giants, should take appropriate actions to increase user awareness of ethical issues surrounding privacy and security, such as the leakage of trade secrets, and the “do’s and don’ts” to prevent sharing sensitive information with generative AI. Meanwhile, regulations and policies should be in place to protect information privacy and security.

# Digital divide

The digital divide is often defined as the gap between those who have and do not have access to computers and the Internet (Van Dijk, 2006). As the Internet gradually becomes ubiquitous, a second-level digital divide, which refers to the gap in Internet skills and usage between different groups and cultures, is brought up as a concern (Scheerder et al., 2017). As an emerging technology, generative AI may widen the existing digital divide in society. The “invisible” AI underlying AI-enabled systems has made the interaction between humans and technology more complicated (Carter et al., 2020). For those who do not have access to devices or the Internet, or those who live in regions that are blocked by generative AI vendors or websites, the first-level digital divide may be widened between them and those who have access (Bozkurt & Sharma, 2023). For those from marginalized or minority cultures, they may face language and cultural barriers if their cultures are not thoroughly learned by or incorporated into generative AI models. Furthermore, for those who find it difficult to utilize the generative AI tool, such as some elderly, the second-level digital divide may emerge or widen (Dwivedi et al., 2023). To deal with the digital divide, having more accessible AI as well as AI literacy training would be beneficial.

# Technology concerns

Challenges related to technology refer to the limitations or constraints associated with generative AI. For example, the quality of training data is a major challenge for the development of generative AI models. Hallucination, explainability, and authenticity of the output are also challenges resulting from the limitations of the algorithms. Table 2 presents the technology challenges and issues associated with generative AI. These challenges include hallucinations, training data quality, explainability, authenticity, and prompt engineering.

# Hallucination

Hallucination is a widely recognized limitation of generative AI and it can include textual, auditory, visual or other types of hallucination (Alkaissi &

McFarlane, 2023). Hallucination refers to the phenomenon in which the contents generated are nonsensical or unfaithful to the given source input (Ji et al., 2023). Azamfirei et al. (2023) indicated that “fabricating information” or fabrication is a better term to describe the hallucination phenomenon. Generative AI can generate seemingly correct responses yet make no sense. Misinformation is an outcome of hallucination. Generative AI models may respond with fictitious information, fake photos or information with factual errors (Dwivedi et al., 2023). Susarla et al. (2023) regarded hallucination as a serious challenge in the use of generative AI for scholarly activities. When asked to provide literature relevant to a specific topic, ChatGPT could generate inaccurate or even nonexistent literature. Current state-of-the-art AI models can only mimic human-like responses without understanding the underlying meaning (Shubhendu & Vijay, 2013). Hallucination is, in general, dangerous in certain contexts, such as in seeking advice for medical treatments without any consultation or thorough evaluation by experts, i.e., medical doctors (Sallam, 2023). In the future, algorithms will need to be improved to prevent hallucinations.

# Quality of training data

The quality of training data is another challenge faced by generative AI. The quality of generative AI models largely depends on the quality of the training data (Dwivedi et al., 2023; Su & Yang, 2023). Any factual errors, unbalanced information sources, or biases embedded in the training data may be reflected in the output of the model. Generative AI models, such as ChatGPT or Stable

Table 2. Technology challenges.   

<html><body><table><tr><td>Challenges</td><td>Issues</td><td>References</td></tr><tr><td>Hallucination</td><td>Contents generated may be nonsensical or incorrect</td><td>(Alkaissi &amp; McFarlane, 2023; Azamfirei et al., 2023; Ji et al., 2023; Sallam, 2023)</td></tr><tr><td rowspan="2">Quality of training</td><td>Generative Al may provide fictitious information or information embedded with factual errors</td><td>(Dwivedi et al., 2023; Ji et al., 2023)</td></tr><tr><td>Difficult to obtain enough training data for generative Al to ensure the quality of the datasets.</td><td>(Dwivedi et al., 2023; Gozalo-Brizuela &amp; Garrido-Merchan, 2023; Su &amp; Yang,</td></tr><tr><td rowspan="4">data Explainability</td><td>Difficult to interpret and understand the outputs of</td><td>2023) (Dwivedi et al., 2023)</td></tr><tr><td>generative AI Difficult to discover mistakes in the outputs of</td><td>(Rudin, 2019)</td></tr><tr><td>generative Al Users are less or not likely to trust generative Al</td><td>(Burrell, 2016)</td></tr><tr><td>Regulatory bodies encounter difficulty in judging whether there is any unfairness or bias in generative AI</td><td>(Rieder &amp; Simon, 2017)</td></tr><tr><td>Authenticity</td><td>Manipulation of content (such as images and videos) causes authenticity doubts</td><td>(Gragnaniello et al., 2022)</td></tr><tr><td rowspan="2">Prompt engineering</td><td>Prompt engineering becomes a vital component of digital literacy for the full utilization of generative AI</td><td>(V. Liu &amp; Chilton, 2022)</td></tr><tr><td>Techniques such as brute-force trial with the prompt are (V. Liu &amp; Chilton, 2022) needed to improve the quality of content generated by AI</td><td></td></tr></table></body></html>

Diffusion which is a text-to-image model, often require large amounts of training data (Gozalo-Brizuela & Garrido-Merchan, 2023). It is important to not only have high-quality training datasets but also have complete and balanced datasets. To address the issue with quality datasets, data cleansing for the training datasets is necessary but overwhelmingly expensive given the massive amount of data. Synthetic training data could be used to not only ensure the diversity of the datasets but also to address sample-selection biases in the datasets (Chen et al., 2021).

# Explainability

A recurrent concern about AI algorithms is the lack of explainability for the model, which means information about how the algorithm arrives at its results is deficient (Deeks, 2019). Specifically, for generative AI models, there is no transparency to the reasoning of how the model arrives at the results (Dwivedi et al., 2023). The lack of transparency raises several issues. First, it might be difficult for users to interpret and understand the output (Dwivedi et al., 2023). It would also be difficult for users to discover potential mistakes in the output (Rudin, 2019). Further, when the interpretation and evaluation of the output are inaccessible, users may have problems trusting the system and their responses or recommendations (Burrell, 2016). Additionally, from the perspective of law and regulations, it would be hard for the regulatory body to judge whether the generative AI system is potentially unfair or biased (Rieder & Simon, 2017).

# Authenticity

As the advancement of generative AI increases, it becomes harder to determine the authenticity of a piece of work. Photos that seem to capture events or people in the real world may be synthesized by DeepFake AI. The power of generative AI could lead to large-scale manipulations of images and videos, worsening the problem of the spread of fake information or news on social media platforms (Gragnaniello et al., 2022). In the field of arts, an artistic portrait or music could be the direct output of an algorithm. Critics have raised the issue that AI-generated artwork lacks authenticity since algorithms tend to generate generic and repetitive results (McCormack et al., 2019).

# Prompt engineering

With the wide application of generative AI, the ability to interact with AI efficiently and effectively has become one of the most important media literacies. Hence, it is imperative for generative AI users to learn and apply the principles of prompt engineering, which refers to a systematic process of carefully designing prompts or inputs to generative AI models to elicit valuable outputs. Due to the ambiguity of human languages, the interaction between humans and machines through prompts may lead to errors or misunderstandings. Hence, the quality of prompts is important. Another challenge is to debug the prompts and improve the ability to communicate with generative AI (V. Liu & Chilton, 2022). Hence, it is necessary to provide training about prompt engineering, especially for those who are most frequently engaged in interaction with generative AI.

# Challenges associated with regulations and policies

Given that generative AI, including ChatGPT, is still evolving, relevant regulations and policies are far from mature. With generative AI creating different forms of content, the copyright of these contents becomes a significant yet complicated issue. Table 3 presents the challenges associated with regulations and policies, which are copyright and governance issues.

# Copyright

According to the U.S. Copyright Office (n.d..), copyright is “a type of intellectual property that protects original works of authorship as soon as an author fixes the work in a tangible form of expression” (U.S. Copyright Office, n.d..). Generative AI is designed to generate content based on the input given to it. Some of the contents generated by AI may be others’ original works that are protected by copyright laws and regulations. Therefore, users need to be careful and ensure that generative AI has been used in a legal manner such that the content that it generates does not violate copyright (Pavlik, 2023). Another relevant issue is whether generative AI should be given authorship (Sallam, 2023). Murray (2023) discussed generative art linked to non-fungible tokens (NFTs) and indicated that according to current U.S. copyright laws, generative art lacks copyrightability because it is generated by a non-human. The issue of AI authorship affects copyright law’s underlying assumptions about creativity (Bridy, 2012). It is imperative to consider the design and implementation of guidelines, regulations, and laws pertaining to the proper utilization of generative AI.

# Governance

Generative AI can create new risks as well as unintended consequences. Different entities such as corporations (Mäntymäki et al., 2022), universities, and governments (Taeihagh, 2021) are facing the challenge of creating and deploying AI governance. To ensure that generative AI functions in a way that benefits society, appropriate governance is crucial. However, AI governance is challenging to implement. First, machine learning systems have opaque algorithms and unpredictable outcomes, which can impede human controllability over AI behavior and create difficulties in assigning liability and accountability for AI defects. Second, data fragmentation and the lack of interoperability between systems challenge data governance within and across organizations (Taeihagh, 2021). Third, information asymmetries between technology giants and regulators create challenges to the legislation process, as the government lacks information resources for regulating AI (Taeihagh et al., 2021). For the same reasons, lawmakers are not able to design specific rules and duties for programmers (Kroll, 2015). Transparency and explainability of AI systems as well as collaboration between technology giants and the government can assist in improving the design and deployment of AI governance (Shneiderman, 2020a, 2020b).

Table 3. Challenges associated with regulations and policies.   

<html><body><table><tr><td>Challenges</td><td>Issues</td><td>References</td></tr><tr><td rowspan="2">Copyright</td><td>Using Al-generated content may directly or indirectly violate (Pavlik, 2023) copyright</td><td></td></tr><tr><td>Controversies exist over Al authorship</td><td>(Bridy, 2012; Murray, 2023; Sallam, 2023)</td></tr><tr><td rowspan="3">Governance</td><td> Lack of human controllability over Al behavior</td><td>(Taeihagh, 2021)</td></tr><tr><td>Data fragmentation and lack of interoperability between systems</td><td>(Taeihagh, 2021)</td></tr><tr><td>Information asymmetries between technology giants and regulators</td><td>(Kroll, 2015; Taeihagh et al., 2021)</td></tr></table></body></html>

# Challenges associated with the economy

A profound influence on the economy could be observed when industries apply generative AI. Generative AI can impact the economy in various aspects, from labor market and industry disruption to income inequality and monopolies. Table 4 presents the challenges associated with the economy, which include labor market issues, disruption of industries, and income inequality and monopolies.

# Labor market

The labor market can face challenges from generative AI. As mentioned earlier, generative AI could be applied in a wide range of applications in many industries, such as education, healthcare, and advertising. In addition to increasing productivity, generative AI can create job displacement in the labor market (Zarifhonarvar, 2023). A new division of labor between humans and algorithms is likely to reshape the labor market in the coming years. Some jobs that are originally carried out by humans may become redundant, and hence, workers may lose their jobs and be replaced by algorithms (Pavlik, 2023). On the other hand, applying generative AI can create new jobs in various industries (Dwivedi et al., 2023). To stay competitive in the labor market, reskilling is needed to work with and collaborate with AI and develop irreplaceable advantages (Zarifhonarvar, 2023).

Table 4. Challenges associated with the economy.   

<html><body><table><tr><td>Challenges</td><td>Issues</td><td>References</td></tr><tr><td>Labor market</td><td>Job displacement and unemployment caused by generative Al Reskilling is needed for workers who want to collaborate with generative Al and maintain their competitiveness</td><td>(Zarifhonarvar, 2023) (Dwivedi et al., 2023; Zarifhonarvar, 2023)</td></tr><tr><td rowspan="3">Disruption of Industries Income inequality and monopolies</td><td>Certain industries may be impacted or even replaced by generative Al</td><td>(Dwivedi et al., 2023)</td></tr><tr><td>The widening income gap between those who master generative (Berg et al., 2016; Al and those who do not</td><td>Zarifhonarvar, 2023)</td></tr><tr><td>Significant advances in the deployment of generative Al in large (Cheng &amp; Liu, 2023) companies can lead to uneven concentration of resources and power</td><td></td></tr></table></body></html>

# Disruption of industries

Industries that require less creativity, critical thinking, and personal or affective interaction, such as translation, proofreading, responding to straightforward inquiries, and data processing and analysis, could be significantly impacted or even replaced by generative AI (Dwivedi et al., 2023). This disruption caused by generative AI could lead to economic turbulence and job volatility, while generative AI can facilitate and enable new business models because of its ability to personalize content, carry out human-like conversational service, and serve as intelligent assistants. Industries need to carefully consider how to leverage generative AI, retrain employees to adapt to the evolving environment, and develop competitive advantages.

# Income inequality and monopolies

Generative AI can create not only income inequality at the societal level but also monopolies at the market level. Individuals who are engaged in low-skilled work may be replaced by generative AI, causing them to lose their jobs (Zarifhonarvar, 2023). The increase in unemployment would widen income inequality in society (Berg et al., 2016). With the penetration of generative AI, the income gap will widen between those who can upgrade their skills to utilize AI and those who cannot. At the market level, large companies will make significant advances in the utilization of generative AI, since the deployment of generative AI requires huge investment and abundant resources such as large-scale computational infrastructure and training data. This trend will lead to more uneven concentration of resources and power, which may further contribute to monopolies in some industries (Cheng & Liu, 2023). To overcome those issues, efforts should be made to provide broader access to generative AI education and training, which can help bridge the skill gap and provide equal opportunity for each individual. Governments and regulatory bodies should implement policies to avoid monopolies in the application of generative AI, such as by monitoring its market concentration and promoting interoperability standards.

# Human-centered AI collaboration

Human-centered AI (HCAI) emphasizes the design of AI with the awareness that it is part of a larger system consisting of human stakeholders (Riedl, 2019). HCAI should be able to not only understand humans from a sociocultural perspective but also help humans understand it (Riedl, 2019). Previous waves of AI were not able to adequately satisfy human demands, which partly accounted for their failure. Recent AI development started to emphasize empathy and alignment with human needs, AI transparency and explainability (i.e., to address AI’s interpretability and comprehensibility), AI ethics and governance, as well as digital transformation through AI literacy and intelligence augmentation, all of which address the human aspects of collaboration with AI (Xu, 2019).

The source of many of the challenges of generative AI can be attributed to the neglect of sociotechnical issues and human needs and values. Therefore, we put forward HCAI collaboration to guide the design and application of generative AI. First, HCAI emphasizes that the design of intelligent systems should be guided by human values and needs. During the development of generative AI algorithms and products, social ethics and regulations should be embedded into the systems, which could mitigate ethical and regulatory challenges. Second, the objective of HCAI is to augment human ability and provide guidance for the development of human-AI collaboration systems. “Human-in-the-loop” can mitigate some of the concerns over AI, such as singularity (i.e., AI advancing beyond human control), robot-driven unemployment, and threats to privacy and security (Shneiderman, 2020a; Xu et al., 2023). Humans and generative AI should collaborate harmoniously, with humans at the center and AI functioning as assistants (i.e., human-in-theloop). Having “human-in-the-loop” has the following advantages: reduce bias, create employment, augment rare data, maintain human-level precision, incorporate subject-level experts, ensure consistency and accuracy, make work easier, improve efficiency, provide accountability and transparency, and increase safety (Monarch, 2021). To achieve “human-centered AI collaboration,” the following issues warrant attention and consideration.

# Empathy and human needs

Human needs are a key basis for HCAI. For example, a human-centered design approach called the Need-Design Response approach (McKim, 1980) can identify human needs and overcome ethical implications in the early design stages (Auernhammer, 2020). The digital divide is one challenge that can be triggered by misalignment with human needs or demands. As ChatGPT attracts an overwhelming amount of attention worldwide, there are also high demands for its usage in both daily life and the workplace. Currently, the digital divide of ChatGPT involves both access and ease of use. On the one hand, OpenAI sets access restrictions and prohibits users from specific locations from using ChatGPT. On the other hand, some users complain that it is challenging to use the proper prompts to satisfy their demands effectively. These two aspects jointly exacerbate the issue of the digital divide. Besides them, human needs and expectations of generative AI also include but are not limited to efficiency, sustainability, safety, and creativity. For the future design of generative AI, we argue that considerations should be taken to satisfy human needs to improve human-AI collaboration.

# Transparency and explainability

AI transparency and explainability refer to understanding the reasoning for the output of AI systems (Flyverbom, 2016). With enhanced algorithm transparency, users can understand the “black boxes” of AI systems that are typically hidden. AI transparency and explainability can increase the interpretability and comprehensibility of the AI system and enable users to make informed decisions based on the algorithms rather than blindly trusting the system (Rader et al., 2018). In the field of human-centered explainable AI, the information users need to understand about the system is the core guidance for developing an explainable system (Sun et al., 2022). Following this principle, key information needed for a transparent and explainable generative AI system includes (1) a user-friendly introduction to the general architecture and mechanism of the model; (2) the model’s capabilities (e.g., functions and areas of expertise) (Sun et al., 2022); (3) the model’s limitations (e.g., uncertainty and risks) (Bhatt et al., 2021), (4) post-hoc explanations about how the model reaches its decision or conclusion; (5) contextual explanations that are applicable for specific contexts; and (6) other information to meet users’ needs that emerge in a usage context (Sun et al., 2022).

# Ethics and governance

Human-centered AI collaboration should adhere to ethical standards and align with human values. The ethical considerations should cover a range of challenges, including bias, privacy, and harmful content. As mentioned earlier, HCAI systems need to understand humans from a sociocultural perspective (Riedl, 2019). As generative AI such as ChatGPT operates under different sociocultural contexts across the world, ethical factors that will need to be considered become more complicated. An HCAI should be fair and accountable by avoiding discrimination and respecting people of different religions, races, nationalities, cultures, and genders. It should not present any offensive, violent, or erotic content or information that could harm people. Privacy is another important ethical concern in human-centered AI collaboration. As generative AI continues to aid in different facets of our work and lives, an increasing amount of personal and organizational data is collected by these systems. The security and confidentiality of the data should be ensured to enhance the trustworthiness of generative AI and foster human-AI collaboration.

Regulations and policies are essential to harness nascent technologies and achieve desired results (Qadir et al., 2022). Human-centered regulations and policies can deal with some of the aforementioned challenges through a two-step process. In the first step, the design of regulations and policies should be human-centered to be effective (Qadir et al., 2022). One example is the challenge with copyright issues. Generative AI can make use of any information on the Internet to generate an answer or response. The information used may involve someone’s copyright or other forms of intellectual property. Hence, the formulation of regulations and policies will need to take into account the interest and legal rights of human stakeholders. A human-centered approach is also effective in combating discrimination by eliminating inappropriate content. In this aspect, OpenAI has set up a content policy to avoid discriminative information in its generated answers. In the second step, human-centered regulations and policies should be effectively incorporated into AI algorithms. One counter-example is that although OpenAI has set up a content policy for ChatGPT, harmful content still exists from time to time due to jailbreaking or algorithmic limitations. Hence, AI algorithms need to be improved and enhanced to avoid such problems.

# Transformation through AI literacy and intelligence augmentation

In the HCAI approach, the intent of AI is to augment rather than replace humans. The key to intelligence augmentation (IA) is collaboration, where the AI literacy of users is crucial. AI literacy is defined as a set of competencies that enable individuals to know and evaluate AI technologies, communicate and collaborate with AI, and use AI effectively and ethically (Long & Magerko, 2020; $\mathrm { N g }$ et al., 2021). AI literacy enables users to effectively use and collaborate with AI (Fast & Horvitz, 2017) to digitally improve and transform work. Hence, users should be educated and trained to work with generative AI to become proficient (e.g., in prompt engineering) and ethical collaborators of HCAI.

Although there is fear that AI may lead to singularity, IA is the main focus in the application of AI and has turned the tension between humans and artificial intelligence into a symbiotic one (Zhou et al., 2021, 2023). Such a shift can lead to a more understandable and manageable future of AI development (Shneiderman, 2020b). Xu et al. (2023) put forward an HCAI framework that contains three interdependent aspects comprising technologies, human factors, and ethics. One objective of the framework is to augment human capabilities. Zhou et al. (2023) introduced four guiding principles for the design of IA, which include simplification, interpretability, human-centeredness, and ethics. Paul et al. (2022) stressed a humancentered approach to AI, which considers human factors and ethical concerns in human-machine systems. The intricacies of human-machine interaction serve as the foundation of AI. In HCAI collaboration, the problem-solving abilities and creativity of humans can be augmented. One way to achieve IA is through co-creation by humans and AI, where better performance is achieved than when humans work on their own (Fugener et al., 2021). Hence, AI can enhance human decision-making by analyzing data or providing insights and predictions. In the medical context, generative AI can help doctors with diagnosis. In the business context, it can help managers with market forecasting. Although generative AI poses numerous threats to the economy, it can benefit the economy and help overcome some of the challenges by augmenting human intelligence.

# Conclusions and implications

Generative AI models such as ChatGPT, Midjourney, and DeepBrain are among the most disruptive technology breakthroughs in recent years (Dwivedi et al., 2023). With the ability to produce new content such as text, images, and videos, generative AI models are regarded as the next milestone of artificial general intelligence (Luo et al., 2023). Generative AI holds immense potential for a wide range of applications in the business, education, healthcare, and content creation industries. However, generative AI also presents challenges, which we categorized into ethics, technology, regulations and policy, and economy. Many of these challenges arise due to the lack of HCAI. For generative AI to be successful, it needs to be human-centered by taking into account empathy and human needs, transparency and explainability, ethics and governance, and transformation through AI literacy and intelligence argumentation.

Generative AI is here to stay. Advancements in generative AI are accelerating and its disruption to business and industries will intensify. Generative AI is making a major impact on our work and lives to the point that working and collaborating with generative AI will soon become a norm, if not already a norm. Education will need to be transformed to teach the necessary hard and soft skill sets to enable students to collaborate and partner with generative AI in educational and workplace settings. Continuous learning and adaptation are necessary to upskill, reskill, and retool the workforce as AI continues to advance and redefine our workplace and our lives. We are living in an interesting and challenging time where adapting to the era of generative AI is necessary and unavoidable. Resistance is futile!

# Disclosure statement

No potential conflict of interest was reported by the authors.

# Notes on contributors

Fiona Fui-Hoon Nah is a Professor at the City University of Hong Kong. She received her Ph.D. in Management Information Systems from the University of British Columbia. She is the editor-in-chief of the Association for Information Systems (AIS) Transactions on HumanComputer Interaction, and a co-founder and former chair of the AIS Special Interest Group on Human-Computer Interaction (SIGHCI). Her research interests include human-computer interaction, virtual worlds, neuroIS, and applications of technology to education. Her publications have appeared in journals such as MIS Quarterly, Journal of the Association for Information Systems, Communication of the Association for Information Systems, Information & Management, International Journal of Human-Computer Studies, and International Journal of Human-Computer Interaction.

Ruilin Zheng is a Ph.D. student in the Department of Media and Communication at the City University of Hong Kong. He received his Master’s degree from The University of Hong Kong and his Bachelor’s degree from Jinan University, China. His research interests include humancomputer interaction, generative AI, and the metaverse.

Jingyuan Cai is a Master’s student at the Renmin University of China and an incoming Ph.D. student in the Department of Information Systems at the City University of Hong Kong. She received her Bachelor’s degree from the Renmin University of China. Her research interests include human-computer interaction, user experience, and social media analytics.

Keng Siau is a Chair Professor of Information Systems and the Head of the Department of Information Systems at the City University of Hong Kong. He received his Ph.D. in Management Information Systems from the University of British Columbia. He is the editorin-chief of the Journal of Database Management. His research interests include humancentered artificial intelligence, generative AI, and the metaverse. His publications have appeared in journals such as MIS Quarterly, Journal of the Association for Information Systems, Information Systems Journal, Journal of the American Society for Information Science and Technology, Communication of the Association for Information Systems, and International Journal of Human-Computer Interaction.

Langtao Chen is an Associate Professor of Business and Information Technology at the Missouri University of Science and Technology, formerly University of Missouri-Rolla. Starting August 2023, he will be an Associate Professor at the University of Tulsa. He received his Ph.D. in Computer Information Systems from Georgia State University. Dr. Chen’s research focuses on machine learning, online communities, user-generated content, health information technology, and user experience. He has published in journals such as Journal of Management Information Systems, Journal of the Association for Information Systems, Decision Support Systems, and Journal of Computer Information Systems.

# References

Alkaissi, H., & McFarlane, S. I. (2023). Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus, 15(2), 1–4. https://doi.org/10.7759/cureus.35179   
Arango, L., Singaraju, S. P., & Niininen, O. (2023). Consumer responses to AI-Generated charitable giving ads. Journal of Advertising, 1–18. https://doi.org/10.1080/00913367.2023.2183285   
Auernhammer, J. (2020). Human-centered AI: The role of human-centered design research in the development of AI. In S. Boess, M. Cheung, & R. Cain (Eds.), Synergy - DRS International Conference 2020, online (pp. 1315–1333). https://doi.org/10.21606/drs.2020.282   
Azamfirei, R., Kudchadkar, S. R., & Fackler, J. (2023). Large language models and the perils of their hallucinations. Critical Care, 27(1), 1–2. https://doi.org/10.1186/s13054-023-04393-x   
Berg, A., Buffie, E. F., & Zanna, L. F. (2016). Robots, growth, and inequality. Finance & Development, 53(3), 10–13.   
Bhatt, U., Antorán, J., Zhang, Y., Liao, Q. V., Sattigeri, P., Fogliato, R., Melançon, G., Krishnan, R., Stanley, J., Tickoo, O., Nachman, L., Chunara, R., Srikumar, M., Weller, A., & Xiang, A. (2021, July). Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, online (pp. 401–413).   
Biswas, S. S. (2023). Role of ChatGPT in public health. Annals of Biomedical Engineering, 51(5), 868–869. https://doi.org/10.1007/s10439-023-03172-7   
Boßelmann, C. M., Leu, C., & Lal, D. (2023). Are AI language models such as ChatGPT ready to improve the care of individuals with epilepsy? Epilepsia, 64(5), 1195–1199. https://doi.org/ 10.1111/epi.17570   
Bozkurt, A., & Sharma, R. C. (2023). Challenging the status quo and exploring the new boundaries in the age of algorithms: Reimagining the role of generative AI in distance education and online learning. Asian Journal of Distance Education, 18(1), 1–8. https://doi. org/10.5281/zenodo.7755273   
Bridy, A. (2012). Coding creativity: Copyright and the artificially intelligent author. Stanford Technology Law Review, 5, 1–28. https://ssrn.com/abstract=1888622   
Brundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G., Khlaaf, H., Yang, J., Toner, H., Fong, R., Maharaj, T., Koh, P., Hooker, S., Leung, J., Trask, A., Bluemke, E., Lebensold, J., O'Keefe, C., Koren, M., . . . Anderljung, M. (2020). Toward trustworthy AI development: Mechanisms for supporting verifiable claims. https://doi.org/10.48550/arXiv.2004.07213   
Bullock, J., & Luengo-Oroz, M. (2019). Automated speech generation from UN general assembly statements: Mapping risks in AI generated texts. https://doi.org/10.48550/arXiv.1906.01946   
Burrell, J. (2016). How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data & Society, 3(1), 1–12. https://doi.org/10.1177/2053951715622512   
Campbell, C., Plangger, K., Sands, S., & Kietzmann, J. (2022). Preparing for an era of deepfakes and AI-generated ads: A framework for understanding responses to manipulated advertising. Journal of Advertising, 51(1), 22–38. https://doi.org/10.1080/00913367.2021.1909515   
Carter, L., Liu, D., & Cantrell, C. (2020). Exploring the intersection of the digital divide and artificial intelligence: A hermeneutic literature review. AIS Transactions on HumanComputer Interaction, 12(4), 253–275. https://doi.org/10.17705/1thci.00138   
Cascella, M., Montomoli, J., Bellini, V., & Bignami, E. (2023). Evaluating the feasibility of ChatGPT in healthcare: An analysis of multiple clinical and research scenarios. Journal of Medical Systems, 47(1), 1–5. https://doi.org/10.1007/s10916-023-01925-4   
Chan, G. K. (2022). AI employment decision-making: Integrating the equal opportunity merit principle and explainable AI. AI & Society, 1–12. https://doi.org/10.1007/s00146-022-01532-w   
Cheng, L., & Liu, X. (2023). From principles to practices: The intertextual interaction between AI ethical and legal discourses. International Journal of Legal Discourse. https://doi.org/10. 1515/ijld-2023-2001   
Chen, R. J., Lu, M. Y., Chen, T. Y., Williamson, D. F. K., & Mahmood, F. (2021). Synthetic data in machine learning for medicine and healthcare. Nature Biomedical Engineering, 5(6), 493–497. https://doi.org/10.1038/s41551-021-00751-8   
Chui, M., Roberts, R., & Yee, L. (2022, December 20). Generative AI is Here: How Tools Like ChatGpt Could Change Your Business. Quantum Black AI by McKinsey. https://www. mckinsey.com/capabilities/quantumblack/our-insights/generative-ai-is-here-how-tools-like -chatgpt-could-change-your-business#/   
Cooper, G. (2023). Examining science education in ChatGPT: An exploratory study of generative artificial intelligence. Journal of Science Education and Technology, 32(3), 444–452. https://doi.org/10.1007/s10956-023-10039-y   
Cotton, D. R., Cotton, P. A., & Shipway, J. R. (2023). Chatting and cheating: Ensuring academic integrity in the era of ChatGPT. Innovations in Education and Teaching International, 1–12. https://doi.org/10.1080/14703297.2023.2190148   
Dasborough, M. T. (2023). Awe‐inspiring advancements in AI: The impact of ChatGPT on the field of organizational behavior. Journal of Organizational Behavior, 44(2), 177–179. https:// doi.org/10.1002/job.2695   
Deeks, A. (2019). The judicial demand for explainable artificial intelligence. Columbia Law Review, 119(7), 1829–1850. https://www.jstor.org/stable/26810851   
Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., Baabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., Albanna, H., Albashrawi, M. A., Al-Busaidi, A. S., Balakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks, L., Buhalis, D., . . . Wright, R. (2023). Opinion paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71, 1–63. https://doi.org/10.1016/ j.ijinfomgt.2023.102642   
Fang, W., Wen, X. Z., Zheng, Y., & Zhou, M. (2017). A survey of big data security and privacy preserving. IETE Technical Review, 34(5), 544–560. https://doi.org/10.1080/02564602.2016. 1215269   
Fast, E., & Horvitz, E. (2017, February). Long-term trends in the public perception of artificial intelligence. In Proceedings of the AAAI conference on artificial intelligence, San Francisco, California, USA, 31(1), 963–969.   
Fieser, J. (n.d.). Ethics. Internet Encyclopedia of Philosophy. Retrieved May 17, 2023, from https://web.archive.org/web/20180119084940/http://www.iep.utm.edu/ethics   
Firat, M. (2023). How ChatGPT can transform autodidactic experiences and open education? OSF Preprints, 1–5. https://doi.org/10.31219/osf.io/9ge8m   
Flyverbom, M. (2016). Transparency: Mediation and the management of visibilities. International Journal of Communication, 10(1), 110–122. http://ijoc.org/index.php/ijoc/arti cle/download/4490/1531   
Fugener, A., Grahl, J., Gupta, A., & Ketter, W. (2021). Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI. MIS Quarterly, 45(3b), 1527–1556. https://doi.org/10. 25300/MISQ/2021/16553   
Gefen, D., & Arinze, O. (2023). ChatGPT and usurping academic authority. Journal of Information Technology Case & Application Research, 25(1), 3–9. https://doi.org/10.1080/ 15228053.2023.2186629   
Gillotte, J. L. (2019). Copyright infringement in AI-generated artworks. UC Davis L Rev, 53, 2655. https://ssrn.com/abstract=3657423   
Gonzalez, W. (2023, April 25). How businesses can help reduce bias in AI. Forbes. Available at https://www.forbes.com/sites/forbesbusinesscouncil/2023/04/25/how-businesses-can-helpreduce-bias-in-ai/ .   
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2020). Generative adversarial networks. Communications of the ACM, 63(11), 139–144. https://doi.org/10.1145/3422622   
Gozalo-Brizuela, R., & Garrido-Merchan, E. C. (2023). ChatGPT is not all you need. A state of the art review of large generative AI models. https://doi.org/10.48550/arXiv.2301.04655   
Gragnaniello, D., Marra, F., & Verdoliva, L. (2022). Detection of AI-Generated synthetic faces. In Handbook of digital face manipulation and detection: From deepfakes to morphing attacks (pp. 191–212). Springer International Publishing.   
Gunawan, J. (2023). Exploring the future of nursing: Insights from the ChatGPT model. Belitung Nursing Journal, 9(1), 1–5. https://doi.org/10.33546/bnj.2551   
Guo, C., Lu, Y., Dou, Y., & Wang, F. Y. (2023). Can ChatGPT boost artistic creation: The need of imaginative intelligence for parallel art. IEEE/CAA Journal of Automatica Sinica, 10(4), 835–838. https://doi.org/10.1109/JAS.2023.123555   
Halloran, L. J., Mhanna, S., & Brunner, P. (2023). AI tools such as ChatGPT will disrupt hydrology, too. Hydrological Processes, 37(3), 1–3. https://doi.org/10.1002/hyp.14843   
Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18(7), 1527–1554. https://doi.org/10.1162/neco.2006.18.7.1527   
Hu, G. (2023). Challenges for enforcing editorial policies on AI-generated papers. Accountability in Research, 1–4. https://doi.org/10.1080/08989621.2023.2184262   
Hyder, Z., Siau, K., & Nah, F. F. H. (2019). Artificial intelligence, machine learning, and autonomous technologies in mining industry. Journal of Database Management, 30(2), 67–79. https://doi.org/10.4018/JDM.2019040104   
Iskender, A. (2023). Holy or unholy? Interview with open AI’s ChatGPT. European Journal of Tourism Research, 34(3414), 1–11. https://doi.org/10.54055/ejtr.v34i.3169   
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Dai, W., Madotto, A., & Fung, P. (2023). Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12), 1–38. https://doi.org/10.1145/3571730   
Kaplan, A., & Haenlein, M. (2019). Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence. Business Horizons, 62 (1), 15–25. https://doi.org/10.1016/j.bushor.2018.08.004   
Kasneci, E., Seßler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Günnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., . . . Kasneci, G. (2023). ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 103, 1–9. https://doi.org/10.1016/j.lindif.2023.102274   
Köbis, N., & Mossink, L. D. (2021). Artificial intelligence versus Maya Angelou: Experimental evidence that people cannot differentiate AI-generated from human-written poetry. Computers in Human Behavior, 114, 1–13. https://doi.org/10.1016/j.chb.2020.106553   
Kroll, J. A. (2015). Accountable Algorithms (Doctoral dissertation, Princeton University).   
Kung, T. H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepaño, C., Madriaga, M., Aggabao, R., Diaz-Candido, G., Maningo, J., & Tseng, V. (2023). Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLoS Digital Health, 2(2), 1–12. https://doi.org/10.1371/journal.pdig.0000198   
Liu, V., & Chilton, L. B. (2022, April). Design guidelines for prompt engineering text-to-image generative models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, New Orleans, Louisana, USA (pp. 1–23).   
Liu, J., Wang, Y., Huang, X., Korsós, M. B., Jiang, Y., Wang, Y., & Erdélyi, R. (2021). Reliability of AI-generated magnetograms from only EUV images. Nature Astronomy, 5(2), 108–110. https://doi.org/10.1038/s41550-021-01310-6   
Long, D., & Magerko, B. (2020, April). What is AI literacy? Competencies and design considerations. In Proceedings of the 2020 CHI conference on human factors in computing systems, Honolulu, Hawaii, USA (pp. 1–16).   
Luo, G., Zhou, Y., Ren, T., Chen, S., Sun, X., & Ji, R. (2023). Cheap and quick: Efficient vision-language instruction tuning for large language models. https://doi.org/10.48550/ arXiv.2305.15023   
Mäntymäki, M., Minkkinen, M., Birkstedt, T., & Viljanen, M. (2022). Defining organizational AI governance. AI and Ethics, 2(4), 603–609. https://doi.org/10.1007/s43681-022-00143-x   
Martinez-Arellano, G., Cant, R., & Woods, D. (2016). Creating AI characters for fighting games using genetic programming. IEEE Transactions on Computational Intelligence and AI in Games, 9(4), 423–434. https://doi.org/10.1109/TCIAIG.2016.2642158   
McCormack, J., Gifford, T., & Hutchings, P. (2019). Autonomy, authenticity, authorship and intention in computer generated art. In 8th International Conference on Computational Intelligence in Music, Sound, Art and Design, Leipzig, Germany (pp. 35–50). Springer International Publishing.   
McKim, R. H. (1980). Experiences in visual thinking. California: Brooks/Cole Publishing Company.   
Monarch, R. (2021). Human-in-the-loop machine learning. Manning, Shelter Island.   
Murray, M. D. (2023). Generative and AI authored artworks and copyright law. Hastings Communications & Entertainment Law Journal, 45(1), 27–44. https://doi.org/10.2139/ssrn. 4152484   
Newell, A., Shaw, J. C., & Simon, H. A. (1959). Report on a general problem-solving program. In Proceedings of the International Conference on Information Processing, Paris, France (pp. 256–264).   
Ng, D. T. K., Leung, J. K. L., Chu, S. K. W., & Qiao, M. S. (2021). Conceptualizing AI literacy: An exploratory review. Computers and Education: Artificial Intelligence, 2, 1–11. https://doi. org/10.1016/j.caeai.2021.100041   
Ntoutsi, E., Fafalios, P., Gadiraju, U., Iosifidis, V., Nejdl, W., Vidal, M. E., Ruggieri, S., Turini, F., Papadopoulos, S., Krasanakis, E., Kompatsiaris, I., Kinder-Kurlanda, K., Wagner, C., Karimi, F., Fernandez, M., Alani, H., Berendt, B., Kruegel, T., . . . Staab, S. (2020). Bias in data‐driven artificial intelligence systems—An introductory survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 10(3), 1–14. https://doi.org/10.1002/widm.1356   
Ozalp, H., Ozcan, P., Dinckol, D., Zachariadis, M., & Gawer, A. (2022). “Digital Colonization” of highly regulated industries: An analysis of big tech platforms’ entry into health care and education. California Management Review, 64(4), 78–107. https://doi.org/10.1177/ 00081256221094307   
Pan, Z., Yu, W., Yi, X., Khan, A., Yuan, F., & Zheng, Y. (2019). Recent progress on generative adversarial networks (GANs): A survey. IEEE Access, 7, 36322–36333. https://doi.org/10. 1109/ACCESS.2019.2905015   
Paul, S., Yuan, L., Jain, H. K., Robert, L. P., Jr., Spohrer, J., & Lifshitz-Assaf, H. (2022). Intelligence augmentation: Human factors in AI and future of work. AIS Transactions on Human-Computer Interaction, 14(3), 426–445. https://doi.org/10.17705/1thci.00174   
Pavlik, J. V. (2023). Collaborating with ChatGPT: Considering the implications of generative artificial intelligence for journalism and media education. Journalism & Mass Communication Educator, 78(1), 84–93. https://doi.org/10.1177/10776958221149577   
Porter, J. (2023, March 21). ChatGpt Bug Temporarily Exposes AI Chat Histories to Other Users. The Verge. https://www.theverge.com/2023/3/21/23649806/chatgpt-chat-histories-bugexposed-disabled-outage   
Qadir, J., Islam, M. Q., & Al-Fuqaha, A. (2022). Toward accountable human-centered AI: Rationale and promising directions. Journal of Information, Communication and Ethics in Society, 20(2), 329–342. https://doi.org/10.1108/JICES-06-2021-0059   
Qiao, H., Liu, V., & Chilton, L. (2022,). Initial images: Using image prompts to improve subject representation in multimodal AI generated art. In Proceedings of the 14th Conference on Creativity and Cognition, Venice, Italy, (pp. 15–28). https://doi.org/10.1145/3527927.3532792   
Rader, E., Cotter, K., & Cho, J. (2018, April). Explanations as mechanisms for supporting algorithmic transparency. In Proceedings of the 2018 CHI conference on human factors in computing systems, Montreal, Quebec, Canada (pp. 1–13).   
Ray, S. (2023, February 22). JPMorgan Chase restricts staffers’ use of ChatGPT. Forbes. https:// www.forbes.com/sites/siladityaray/2023/02/22/jpmorgan-chase-restricts-staffers-use-ofchatgpt/?sh $\boldsymbol { \mathbf { \rho } } _ { \cdot } =$ 714e937d6bc7   
Rieder, G., & Simon, J. (2017). Big data: A new empiricism and its epistemic and socio-political consequences. Berechenbarkeit der Welt? Philosophie und Wissenschaft im Zeitalter von Big Data, 85–105. https://doi.org/10.1007/978-3-658-12153-2_4   
Riedl, M. O. (2019). Human‐centered artificial intelligence and machine learning. Human Behavior and Emerging Technologies, 1(1), 33–36. https://doi.org/10.1002/hbe2.117   
Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5), 206–215. https:// doi.org/10.1038/s42256-019-0048-x   
Salakhutdinov, R., & Larochelle, H. (2010, March). Efficient learning of deep Boltzmann machines. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, Sardinia, Italy (pp. 693–700).   
Sallam, M. (2023). ChatGPT utility in healthcare education, research, and practice: Systematic review on the promising perspectives and valid concerns. Healthcare, 11(6), 1–20. https:// doi.org/10.3390/healthcare11060887   
Scheerder, A., Van Deursen, A., & Van Dijk, J. (2017). Determinants of Internet skills, uses and outcomes. A systematic review of the second-and third-level digital divide. Telematics and Informatics, 34(8), 1607–1624. https://doi.org/10.1016/j.tele.2017.07.007   
Shneiderman, B. (2020a). Human-centered artificial intelligence: Reliable, safe & trustworthy. International Journal of Human-Computer Interaction, 36(6), 495–504. https://doi.org/10. 1080/10447318.2020.1741118   
Shneiderman, B. (2020b). Human-centered artificial intelligence: Three fresh ideas. AIS Transactions on Human-Computer Interaction, 12(3), 109–124. https://doi.org/10.17705/1thci. 00131   
Shubhendu, S., & Vijay, J. (2013). Applicability of artificial intelligence in different fields of life. International Journal of Scientific Engineering and Research, 1(1), 28–35.   
Siau, K. (2018). Education in the age of artificial intelligence: How will technology shape learning? The Global Analyst, 7(3), 22–24.   
Siau, K., & Wang, W. (2020). Artificial intelligence (AI) ethics: Ethics of AI and ethical AI. Journal of Database Management, 31(2), 74–87. https://doi.org/10.4018/JDM.2020040105   
Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumelhart & J. L. McClelland (Eds.), Parallel distributed processing (pp. 194–281). MIT Press.   
Stancati, M., & Schechner, S. (2023, March 31). ChatGPT banned in Italy over data-privacy concerns. The Wall Street Journal. Available at https://www.wsj.com/articles/chatgptbanned-in-italy-over-data-privacy-concerns-4b984e75 .   
Sun, J., Liao, Q. V., Muller, M., Agarwal, M., Houde, S., Talamadupula, K., & Weisz, J. D. (2022, March). Investigating explainability of generative AI for code through scenario-based design. In 27th International Conference on Intelligent User Interfaces, Helsinki, Finland (pp. 212–228).   
Susarla, A., Gopal, R., Thatcher, J. B., & Sarker, S. (2023). The Janus effect of generative AI: Charting the path for responsible conduct of scholarly activities in information systems. Information Systems Research, 34(2), 1–10. https://doi.org/10.1287/isre.2023.ed.v34.n2   
Susnjak, T. (2022). ChatGPT: The end of online exam integrity? https://doi.org/10.48550/ arXiv.2212.09292   
Su, J., & Yang, W. (2023). Unlocking the power of ChatGPT: A framework for applying generative AI in education. ECNU Review of Education, 1–12. https://doi.org/10.1177/ 20965311231168423   
Taeihagh, A. (2021). Governance of artificial intelligence. Policy and Society, 40(2), 137–157. https://doi.org/10.1080/14494035.2021.1928377   
Taeihagh, A., Ramesh, M., & Howlett, M. (2021). Assessing the regulatory challenges of emerging disruptive technologies. Regulation & Governance, 15(4), 1009–1019. https://doi. org/10.1111/rego.12392   
Thorp, H. H. (2023). ChatGPT is fun, but not an author. Science: Advanced Materials and Devices, 379(6630), 313. https://doi.org/10.1126/science.adg7879   
Turnitin. (n.d.) Turnitin’s AI Writing Detection Capabilities. Retrieved April 10, 2023, from https://www.turnitin.com/products/features/ai-writing-detection   
University of Cambridge. (n.d.). Plagiarism and Academic Misconduct. Retrieved April 10, 2023 at https://www.plagiarism.admin.cam.ac.uk/what-academic-misconduct/artificial-intelligence   
University of Oxford. (2023, January 30). Four Lessons from ChatGpt: Challenges and Opportunities for Educators. https://www.ctl.ox.ac.uk/article/four-lessons-from-chatgptchallenges-and-opportunities-for-educators#Lesson3   
U.S. Copyright Office. (n.d.). What is Copyright? Retrieved May 17, 2023, from https://www. copyright.gov/what-is-copyright/   
Van Dijk, J. A. (2006). Digital divide research, achievements and shortcomings. Poetics, 34(4– 5), 221–235. https://doi.org/10.1016/j.poetic.2006.05.004   
Van Dis, E. A., Bollen, J., Zuidema, W., van Rooij, R., & Bockting, C. L. (2023). ChatGPT: Five priorities for research. Nature, 614(7947), 224–226. https://doi.org/10.1038/d41586-023-00288-7   
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems 30, 5998–6008.   
Wang, S., Li, L., Ding, Y., & Yu, X. (2022, June). One-shot talking face generation from single-speaker audio-visual correlation learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 36(3), (pp. 2531–2539). virtually   
Wang, W., & Siau, K. (2019). Artificial intelligence, machine learning, automation, robotics, future of work, and future of humanity – a review and research agenda. Journal of Database Management, 30(1), 61–79. https://doi.org/10.4018/JDM.2019010104   
Wan, W., Tsimplis, M., Siau, K., Yue, W., Nah, F., & Yu, G. (2022). Legal and regulatory issues on artificial intelligence, machine learning, data science, and big data. In J. Y. C. Chen, G. Fragomeni, H. Degen, and S. Ntoa (Eds.), Lecture notes in computer science 13518 (pp. 558–567). Springer.   
Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., &Gabriel, I. (2021). Ethical and social risks of harm from language models. https://doi.org/10.48550/arXiv.2112.04359   
Weizenbaum, J. (1966). ELIZA—a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9(1), 36–45. https://doi.org/10.1145/365153.365168   
Whittaker, L., Kietzmann, T. C., Kietzmann, J., & Dabirian, A. (2020). “All around me are synthetic faces”: The mad world of AI-generated media. IT Professional, 22(5), 90–99. https://doi.org/10.1109/MITP.2020.2985492   
Wong, Y., Fan, S., Guo, Y., Xu, Z., Stephen, K., Sheoran, R., & Kankanhalli, M. (2022, October). Compute to tell the tale: Goal-driven narrative generation. In Proceedings of the 30th ACM International Conference on Multimedia, Lisboa, Portugal (pp. 6875–6882).   
Woolf, M. (2022, December 8). ChatGPT passes the turing test. Metaverse Post. https://mpost. io/chatgpt-passes-the-turing-test/   
Xu, W. (2019). Toward human-centered AI: A perspective from human-computer interaction. Interactions, 26(4), 42–46. https://doi.org/10.1145/3328485   
Xu, W., Dainoff, M. J., Ge, L., & Gao, Z. (2023). Transitioning to human interaction with AI systems: New challenges and opportunities for HCI professionals to enable human-centered AI. International Journal of Human-Computer Interaction, 39(3), 494–518. https://doi.org/ 10.1080/10447318.2022.2041900   
Yin, Y., Siau, K., Wen, X., & Yan, S. (2022). Smart health: Intelligent healthcare systems in the metaverse, artificial intelligence, and data science era. Journal of Organizational and End User Computing, 34(1), 1–14. https://doi.org/10.4018/JOEUC.308814   
Zarifhonarvar, A. (2023). Economics of ChatGPT: A labor market view on the occupational impact of artificial intelligence. Available at https://papers.ssrn.com/sol3/papers.cfm? abstract_id=4350925 .   
Zhou, L., Paul, S., Demirkan, H., Yuan, L., Spohrer, J., Zhou, M., & Basu, J. (2021). Intelligence augmentation: Towards building human-machine symbiotic relationship. AIS Transactions on Human-Computer Interaction, 13(2), 243–264. https://doi.org/10.17705/1thci.00149   
Zhou, L., Rudin, C., Gombolay, M., Spohrer, J., Zhou, M., & Paul, S. (2023). From artificial intelligence (AI) to intelligence augmentation (IA): Design principles, potential risks, and emerging issues. AIS Transactions on Human-Computer Interaction, 15(1), 111–135. https:// doi.org/10.17705/1thci.00085   
Zhuo, T. Y., Huang, Y., Chen, C., & Xing, Z. (2023). Exploring AI ethics of ChatGPT: A diagnostic analysis. https://doi.org/10.48550/arXiv.2301.12867

Ruilin Zheng, Jingyuan Cai and Keng Siau City University of Hong Kong, Kowloon, Hong Kong SAR