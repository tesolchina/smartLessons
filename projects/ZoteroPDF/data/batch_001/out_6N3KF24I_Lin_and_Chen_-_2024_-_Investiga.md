# Investigating the capability of ChatGPT for generating multiple-choice reading comprehension items

Zhiqing Lin a, Huilin Chenb,'

a School of English Studies, Shanghai International Studies University, Shanghai, China b School of Education, Shanghai International Studies University, Shanghai, China

# ARTICLEINFO

# ABSTRACT

Keywords:   
Reading comprehension   
Automatic item generation   
ChatGPT   
Potential pitfalls

The development of multiple-choice reading comprehension items based on specific reading subskills is crucial in teaching, learning, and testing reading. But it remains a challenging task because this process is time-consuming and costly. This study aims to investigate the capability of ChatGPT (Chat Generative Pre-trained Transformer) for generating multiple-choice reading comprehension items. Psychometric models and human review were adopted to evaluate the item quality based on the benchmark of human-authored items. The results showed that ChatGPTauthored multiple-choice items were acceptable and comparable to human-authored items in terms of psychometric properties, and human review by questionnaire, expert judgment, and interview found that ChatGPT had the potential capability to serve as a test developer and assistant for teaching and learning reading. However, some shortcomings and potential pitfalls were also identified and room for improvement was discussed when ChatGPT is applied to generate items for educational purposes.

# 1. Introduction

Technology-mediated language assessments (TMLA) have been attracting more and more attention during the past few years (Barrot, 2023; Ockey & Neiriz, 2021). Automatic item generation (AIG), one of the most important areas in TMLA, seeks to generate the test item automatically given the reading materials via technology. The sinificance of AIG is t last threefold. First, i can reduce costs and enhance the eficiency of generating items for reading tasks (Chinkina et al., 2019). Second, it can alleviate the burden of language teachers when it comes to preparing the reading task or quiz for students, which can rescue language teachers from the "emotion labor' (Benesch & Prior, 2023). Third, it can provide personalized assessment and feedback for language learners.

However, AIG has not been given due atention in language testing. To date, ittl is known about whether the machine of AIG can generate multiple-choice items based on the given reading subskills and materials. Chat Generative Pre-trained Transformer (ChatGPT) differs from previous technology in its potential to fulfil thistask, which can provide not only the answer but also explanations to scaffold teaching and learning reading. As far as we know, there is no empirical reearch on applyig ChatGPT to the AIG task. Therefore, this study flls in this gap by applying ChatGPT to automatically generate multiple-choice items based on the given reading materials and subskils and evaluating its capabilit in AI from the perspectives of psychometric models and human review based on the benchmark of human-authored items.

# 2. Literature review

# 2.1. Empirical research on automatic item generation

The history of AIG can be traced back to 1969, which was proposed by Bormuth (1969). AIG pertains to the application of natural language procesing technology to generate test items. IG is arelatively new topic in language testing. The majorit of studies on AIG have been taken in the field of medical education and psychometrics. Regarding the research in medical education, various studies have been conducted to investigate the feasibility of applying AIG in medical asessment (see Falcao et al., 2022; Gierl e l., 2012; Gierl & Lai, 2017; Pugh et a., 2020; Westacot et al., 2023). Asystematic review on how to evaluate the items generated by machines for medical education was also conducted by Falcao et al. (2023). Besides, AIG has also been investigated in psychological measurement. Scholars have investigated the possbilit of automatically generating items to measure psychological construct e., Gotz et al., 2023; Hommel et al., 2021; Lee et al., 2023). Aside from these two important areas, AIG has also been applied in developing reasoning tests (Arendasy & Sommer, 2007; Sun et al., 2019), cognitive tests (Ry0o et al., 2022), and mathematical exams (Embretson & Kingston, 2018).

When it comes to the IG in language testing, broadly speaking applied linguistics, there are a few empirical studies that attempted to apply the AIG technology to generate wh-questions, illin-the-blank items, short answer questions, and multiple-choice items for students. Here are the quintessential empirical studies. For wh-questions, Chinkina et al. (2019) applied the technology in computational lingustics to generate wh-questions for tudents, and they reported that the perceived quality of human-authored items and machine-authored items were similar. For fillin-the-blank items, Shin and Gierl (2022) found that it was feasible to generate fill-in-the-blank items for reading comprehension using a series of text processing techniques. For short answer questions, Huang and He (2016) stated that Lexical Functional Grammar can serve as a method to generate short answer questions for students, and using paraphrasing techniques can improve the item complexity. Another similar research was conducted by Shin et al. (2022), who used the machine to furnish informative answers to reading comprehension questions. They reported that it was viable to scaffld students in learning and teaching reading using machines. For word fluency, Arendasy et al. (2011) applied the AIG to the German and English word fluency tests. They found that the items generated for both these two languages had high psychometric quality. For multiple-choice items, Attali et al. (2022) stated that machine-authore items were quipped withaccetable quality when they were evaluated by experts and psychometric models.

These studies have made important contributions to the investigation of AIG. But few of them can generate the item acording to specific language abilit (e.g., reading subskills. Besides, previous studies can only provide a single correct answer but not an explanation of the answer which plays an important role in scaffolding reading comprehension (Smit et al., 2017). Currently, it i not clear whether it ispossible to use machines to generate multiple-choice items based on specific reading subskills and explanations of the items. One plausible reason might be that the AIG for reading comprehension is more challenging than other areas. The feasibility of AIG is constrained by at lest two important factors. One is the extent to which the machine understands the theoretical definition of the reading subskill crrectly. The other is the machine's operational abilit in generating the desired item for the specified reading subskill and its corresponding explanations (Chinkina et al., 2019).

# 2.2. Language models for automatic item generation

Previous studies have endeavored to apply AI language models for AIG, including the recurrent neural network (RNN), the Generative Pre-trained Transformer 2 (GPT-2), the Generative Pre-trained Transformer 3 (GPT-3) during the past few years. Con cerning RNN, von Davier (2018) applied the RNN to generate personality test items. It was found that RNN held the potential for AIG, but there was sillsome rom for improvement. oncerning GPT-2, cases were done by Du et al. (2021) and Gotz e al. (2023). And the typical example of AIG using GPT-3 was conducted by Lee e al. (2023). As for the performance of these ifferent language models for AIG, it sems that the stronger the language model is the higher item quality is observed. For example, scholars found that GPT-2 outperformed RNN in AIG (Du et al., 2021; Li & Xing, 2021). One plausible reason might be that the GPT-2 is superior to RNN in understanding human language after the pre-training on more data. However, these language models aso have some limitations when it comes to AIG. As noted by Gotzet al. (2023), the machine-authored item was out of work sometimes, which suggests that a more intelligent language model is needed.

Aside from the aforementioned technology, ChatGPT (also known as GPT-3.5), an enhanced version of GPT-3, holds greater po. tential in automatically generating items for reading comprehension (Kasneci et al., 2023). It dffers from previous language models in its large-scale parameters and superior inelligence in many anguage-related tasks, which has aracted a wide range of attention once it was launched by OpenAI company. Most importantly, it might have the potential to generate items based on specific reading subskils and the explanation for the answer. However, to the best knowledge of the authors, there is scant empirical reearch on AIG using ChatGPT in language testing with very few exceptions (e.g., Shin & Le, 2023). This suggests that more research is needed to examine to what extent ChatGPT can generate test items for educational purposes.

One critical issue of applying the ChatGPT for AIG is prompt engineering which refers to how to write an effective prompt for ChatGPT. As far as we know, there is no universally accepted framework for prompt writing, but some important principle should be kept in mind when it comes to prompt engineering for specific tasks. The first principle is to be clear and specific (Lo, 2023). This principle is quite intuitie ince the language model will generate more precise content if more detailed instructions are provided. The second principle isto provide enough time for ChatGPT to think. This principle istpically aplied in inference-relaed tasks. The third principle is iteative improvement, which means that the process of writing the prompt for a task entails iterations of trial and error to find the best. Moreover, scholars (e.g., Giray, 2023) also stated that the prompt for ChatGPT should include the following elements 1) the instruction, 2) context, (3) input data, and (4) output indicator. Some of the elements are required and some of them are optional. It depends on the task situation and the capabilit of the AI language model in understanding the prompt. But in general, the more concrete the prompt is the more the generated results meet our expectations. These guidelines cast important light on how to write the prompt for AIG.

# 2.3. Reading subskills for test development and validation frameworks

The first stage in test development i to determine the language construct (Bachman & Palmer, 2010). Usuall, the language construct should be based on a wellaccepted theoretical framework. In language testing, the commonly used framework is the communicative language ability proposed by Bachman (1990). Reading ability is one of the most important parts of communicative language ability (Alderon, 2000. And reading can be see as a fundamental aspect of thinking (Chen et al., 2023; Pearson & Raphael. 1990 which is hierarchical in nature. The Taxonomy of ducational Objectives (Bloom et al., 1956) is a widely recognized hierarchy of thinking sils in education. Anderson and Krathwohl (2001) proposed a revised version of Bloom's taxonomy, which includes the levels of remembering, understanding, applying, analyzing, evaluating, and creating. The first three levels are considered to be lower-order thinking sills, while the last three lels are classfied as higher-order thinking skill Moore & Stanley, 2013). According to defintions of the thinking skils based on the revised version of Bloom's taxonomy (Anderson & Krathwohl, 2001), \*remembering pertains to recalling facts, terms, basic concepts, and answers; "understanding" covers organizing, comparing, translating, inter preting, and giving descriptions; "applying" emphasizes solving problems to new situations; \*analyzing" involves making inferences and generalizations; and "evaluating" presents opinions by making judgments.

As a rule of thumb, the selection of reading subskill for test development should be based on needs analysis and test pecifications (Alderson, 2000). This study aims to generate multiple-choice items for college students. Therefore, we follow the reading subskils specified in the test specification of Collee English Test band 4 (CET-4), which is a widely known and well-cceted tes in China for the measurement of college students English language proficiency (Zheng & Cheng, 2008). The reading subskill are shown in Table 1, which were translated from the Chinese test specification (National ollege English Testing Committe, 2016). These reading subskills are also the most important reading subskills investigated by previous scholars in the past few years (Aryadoust, 2020) and can be reflected in the herarchy of thinking skill. "Understanding explicitl stated details" is mainly concerned with making interpretations based on explicit information, and therefore reflects the thinking skils \*understanding" most. "Determining the meaning of words (phrases) from the context" can be regarded as solving word problems based on context, and therefore reflects the thinking skill "applying" most. Making nferences" can be reflected in the defintion ofthe thinking skill analyzing". Summarizing the main idea", also reflcting the thinking skill analyzing, is defied here as generalizing mplicitly stated main deas (Hughes, 2003), and therefore is higher than \*making inferences" acording to the level of cognitive processing (Grabe, 2009). "Recognizing atitudes and emotions) refers to understanding the atitude and emotions of the writers and the viewpoints of the passage and therfore reflects the thinking skill 'evaluating" most By linking the cognitive levels of thinking sills and the definition of the reading subskills in thistudy, a hierarchy of reding subskills can alo be established, whichi displayd in able1. However, lite is knw about how wll machines can generate reading items according to the subskill requirement.

Aside from the reading subskills for test development, it s also necesary to review the validity theory framework for the validation of test qualit. In language testing, the most frequently used validit frameworks are the socio-cognitive framework proposed by Weir (2005), the \*interpretation/use argument" by Kane (2013), and the "assessment use argument" by Bachman and Palmer (2010) Although these three important validty frameworks use ifferent terminologies, the essence of these frameworks commonly contains construct validity, content validt, fanessvaldity, critrion-related valdity, and consequentil valdity. onstruct valdity pertains to the extent to which the test items measure the intended construct, which can be evaluated using psychometric models such as item response theory and clasical test theory (McNamara, 1990). Content validity pertains to the extent to which the test is representative of targeted constructs and it can be evaluated by expert judgment (Haynes et al., 1995). Faness validity focuses on whether the item xhibits any bias for a certain group. The critrion-elatd validty perts to the extent towhich ths core is i ine with thr test scores measuring similar onstructs. Consequential validity refers to how the language test impacts language learning and teaching as well as the potential influence that the decision-making based on the test score has.

# 2.4. The current study

ased on the aforementioned review, this study attempts to investigate the feasibilit of applying ChatGPT to generate multiple

Table 1 Reading comprehension subskills in CET-4.   

<html><body><table><tr><td>Subskills abbreviations</td><td>Subskills</td></tr><tr><td>Understanding</td><td>Understanding explicitly stated details</td></tr><tr><td>Word (phrase) meaning</td><td>Determining the meaning of words (phrases) from the context</td></tr><tr><td>Inference</td><td>Making inferences based on the content.</td></tr><tr><td>Main idea</td><td>Summarizing the main idea</td></tr><tr><td>Sentiment</td><td>Recognizing attitudes and emotions</td></tr></table></body></html>

Table 2 Item fit of the item by both ChatGPT and human.   

<html><body><table><tr><td>Items</td><td>S-x2</td><td>df.s-x2</td><td>RMSEA.S-X2</td><td>p.S-x2</td></tr><tr><td>M1</td><td>15.39</td><td>9.00</td><td>0.06</td><td>0.08</td></tr><tr><td>M2</td><td>8.22</td><td>15.00</td><td>0.00</td><td>0.91</td></tr><tr><td>M3</td><td>14.45</td><td>15.00</td><td>0.00</td><td>0.49</td></tr><tr><td>M4</td><td>10.83</td><td>15.00</td><td>0.00</td><td>0.76</td></tr><tr><td>M5</td><td>9.96</td><td>11.00</td><td>0.00</td><td>0.53</td></tr><tr><td>M6</td><td>15.32</td><td>13.00</td><td>0.03</td><td>0.29</td></tr><tr><td>m7</td><td>17.15</td><td>12.00</td><td>0.04</td><td>0.14</td></tr><tr><td>M8</td><td>8.52</td><td>11.00</td><td>0.00</td><td>0.67</td></tr><tr><td>M9</td><td>7.12</td><td>13.00</td><td>0.00</td><td>0.90</td></tr><tr><td>H10</td><td>21.12</td><td>15.00</td><td>0.04</td><td>0.13</td></tr><tr><td>H11</td><td>20.83</td><td>14.00</td><td>0.05</td><td>0.11</td></tr><tr><td>H12</td><td>7.81</td><td>16.00</td><td>0.00</td><td>0.95</td></tr><tr><td>H13</td><td>18.30</td><td>15.00</td><td>0.03</td><td>0.25</td></tr><tr><td>H14</td><td>13.29</td><td>14.00</td><td>0.00</td><td>0.50</td></tr><tr><td>G1</td><td>11.31</td><td>13.00</td><td>0.00</td><td>0.58</td></tr><tr><td>G2</td><td>26.27</td><td>15.00</td><td>0.06</td><td>0.04</td></tr><tr><td>G3</td><td>20.08</td><td>11.00</td><td>0.06</td><td>0.04</td></tr><tr><td>G4</td><td>22.38</td><td>15.00</td><td>0.05</td><td>0.10</td></tr><tr><td>G5</td><td>11.93</td><td>12.00</td><td>0.00</td><td>0.45</td></tr><tr><td>G6</td><td>15.44</td><td>14.00</td><td>0.02</td><td>0.35</td></tr><tr><td>G7</td><td>21.02</td><td>12.00</td><td>0.06</td><td>0.05</td></tr><tr><td>G8</td><td>6.34</td><td>13.00</td><td>0.00</td><td>0.93</td></tr><tr><td>G9</td><td>7.21</td><td>15.00</td><td>0.00</td><td>0.95</td></tr><tr><td>E10</td><td>13.61</td><td>15.00</td><td>0.00</td><td>0.56</td></tr><tr><td>E11</td><td>11.32</td><td>14.00</td><td>0.00</td><td>0.66</td></tr><tr><td>E12</td><td>12.87</td><td>13.00</td><td>0.00</td><td>0.46</td></tr><tr><td>E13 E14</td><td>9.53 15.97</td><td>15.00 15.00</td><td>0.00 0.02</td><td>0.85 0.38</td></tr></table></body></html>

hoice items based on the given CET-4 reading materials and reading subskils and evaluate the item quality by psychometric models nd human review based on the baseline of human-authored items. This study is guided by the following research questions.

RQ 1: To what extent the multiple-choice items generated by ChatGPT are acceptable based on psychometric models? RQ 2: What is ChatGPT's capability for generating multiple-choice items according to the perception of English language teachers and experts?

# 3. Methodology

# 3.1. Generating items using ChatGPT

Writing an effective prompt for AlG not only needs the principles of prompt engineering but also continuous trials and adjustments (Ray, 2023). The proces of revising the prompt for large language models i called iteratie promptig (Heston & Khun, 2023), which is very important in prompt engineering. The procesof revising and applying the prompt for the AIG task in this study can be summarized as the following five procedures, including (1) initial prompt formulation and analysis (2) the adition of reading materials and reding subskill, (3) restricting the output formats, (4) overall testing and evaluation, and (5) aplication of the prompt to different passages:

(1) Initial prompt formulation and analysis. The irst procedure was about writing an initial prompt for ChatGPT for the AIG task and making analyses. At the beginning, we informed ChatGPT of the persona it was expected to play i.e. English test devel. oper). Then we asked ChatGPT to develop an English multiple-choice reading comprehension test. Although ChatGPT can generate the item and reading materials automatically, we found that the reading material was too simple for our targeted participants, and the item type was too homogeneous and could not measure different reading subskills.   
(2) Addition of reading materials and reading subkill. To overcome the problem we found in the frst procedure, what we did in the second procedure was to select some reading materials that should be appropriate for our participants. Besides, we also added some requirements on the item type (i.e. the reading subskil the item measures) in the prompt, asking ChatGPT to generate different items to measure different reading subskills. Then we input one reading material we selected from CET-4 together with the new item writing requirement into ChatGPT. We tested the prompt several times to see the item ChatGPT could generate t seemed that ChatGPT could generate different item types for the measurement of different reading subskils and item difficulty was possibly reasonable according to our judgement. However, we found that some of the items in the output had only two options.   
(3) Restricting the output formats. To iteratively overcome the shortcoming we found in the second procedure, what we did in this procedure was restricting the output formats in the prompt. To be specific, we revised the prompt and added more requirements on the output format of the expected item by requesting that the output should have ABCD four options. Also, we added some requirements on the output, asking ChatGPT to mark the reading subskills of each item. Moreover, we merged these re. quirements in the output format together into one single sentence.   
(4) Overall testing and evaluation. In this procedure, we concentrated on testing the prompt to check whether it was stable. More importantly, we also focused on evaluating the quality and relevance of the multiple-choice item ChatGPT can generate. After testing and revising more than 10 times, we ultimately chose the following prompt for AIG, which can help to generate better items based on our judgments about the face validity of CET-4 reading. The input into ChatGPT included the reading material and the prompt containing the requirements of desired items. The prompt for ChatGPT was originally written in Chinese and its English version is listed as follows.

"Based on the given reading passage, please generate 9 multiple-choice items. One for determining the meaning of words (phrases) from the context, two for understanding explicitly stated details, two for making inferences, two for summarizing the main idea, and two for recognizing attitudes and emotions. Each item should have ABCD four options and the reading skill tested in each question should be marked."

The first sentence of the prompt is for the overal instruction of the task ChatGTP has to do. The second sentence is on the specific task requirement for ChatGPT, which indicates the reading subskill and number of items. The final sentence is for the output specification. All these elements follow the basi principles and guidelines suggestd by previous scholars (., Giray, 2023; Lo, 2023) as is mentioned in the literature review.

(5) Application of the prompt to different passages. After finding the desired prompt for AIG, the final step was to generate all the items using the prompt we designed in the previous procedure. We generated the multiple-choice item for each of the two passages separately since it might be better for ChatGPT to think step by step (Zhou et al., 2022). We input the first reading material we selected and the final prompt we designed in the previous procedure into ChatGPT and thus we obtained the items for passage one as well as the reading subskill each item measures. We also asked ChatGPT to provide the answers and explain the item after it finished generating items. Finally, we did the same thing again for passage two, afer which we obtained ll the information which can be found in Appendix A.

The reading materials used in this study were adopted from CET-4, which had been validated by previous research (Yang & Weir, 1998; Zheng & Cheng, 2008). Two reading pasages were randomly selected from the test each accompanied by five original human-authored items in the CET-4 test. Reading material one was selected from passage one of the CET-4 multiple-choice reading comprehension section and reading material two was selected from passage two from the same CET-4 reading section. Beides, the two passages we selected for automatic item generation are about cultural differences and company management respectively. We think these topics are suitable for the participants as these two topics arealso in line with the test specification of CET-4. The two passages were used as the basis of item generatio through ChatGPr. We controlled the task difficulty mainly by choosing these two reading passages selected by the CET-4 test commitee, which should be at appropriatereadability for college students. The reason is that text readabilit, to large extent, determines the item diffculty (Choi & Moon, 2019; Ozuru et l., 2008). Another method we used is by setting the specification on the reading subskill the item should measure in the prompt, which can also control the item difficulty (Spencer et al., 2019).

We asked the ChatGPT to generate one item for \*word meaning in each passage since some relatively low-frequency words in the reading passage had been labeled in the text by the CET-4 test developer o indicate their Chinese meaning in the CET-4 test,such as tactile and pandemic, which leads to a few suitable test points for \*word meaning" suskill. We asked ChatGPT togenerate two items for these reading subskils besides \*word meaning" mainl for the sake of increasing the diversity and representativeness of the items authored by ChatGPT which generates contents with a certain degree of randomness.

# 3.2. Participants and data collection

To make it easier to refer to these items, they were named as follows. M1-M9 were ChatGPT-authored items in passage one; H10-H14 were human-authored items in passage one; G1-G9 were ChatGPT-authored items in passage two; E10-E14 were humanauthored items in passage two. These items formed a reading comprehension exam of 28 items. 213 participants took part in the exam. Their language proficiency was around B1 acording to the benchmark of CEFR (Council of Europe, 2001; Jin e al., 2022). The major of the participants was software engineering. The items were administered using Wenjuanxing (a web-based platform). Participants were informed to finish the exam independently within $6 0 ~ \mathrm { { m i n } }$ in class. The test data was used to answer RQ 1 based on psychometric models.

To answer RQ 2, the Turing test, expert judgment, and semi-structured interviews were conducted to understand the perception of English language teachers and experts. The rationale for inviting both English teachers and experts is that they can provide a more comprehensive understanding of th item quality from different perspectives. As for the Turing test 10 ChatGPT-authored items (M1, M3, M5, M8, M9, G1, G4, G5, G6, G9) were randomly selected and then they were randomly mixed with 10 human-authored items to form 20 items. Five English teachers (Care, Alex, Celia, Caroline, and Mark) were invited to make judgments on whether th item was generated by humans or machines. On average, these invited teachers have around 3-5 years of English teaching experience in university. They were not told any cues on which items were generated by humans or machines.

Besides, five experts (Luis, William, Linda, Wu, Wang) in applied linguistics were invited to make the judgment on the qualit of the aforementioned 10 randomly selected ChatGPT-authored items and 10 human-authored items. They were not told any clue about which item was generated by humans or machines as well The criterion included the following six dimensions adopted from Gierl and Lai (2013) and Pugh et a. (2020). These rules had been proven to be alid for assesing the quality of multiple-choice iems (Haladyna et al., 2002). The questionnaire was a five-point Likert scale, which ranges from "strongly disagree' to "strongly agree'

1. The question is carefully edited, formatted, and presented using correct grammar, punctuation, capitalization, and spelling.   
2. The distractors are homogeneous in terms of content, structure, and length.   
3. The distractors are independent and do not overlap.   
4. There is only one correct option.   
5. The distractors do not betray the correct answer.   
6. This is a high-quality item.

To investigate ChatGPT's capability to generate test items for the specified reading subskil, the five experts were also invited to asses the subskillexamined i each item according to Table 1. When there were some disagreements on some items, adiscussion was conducted among the five experts to arrive at aconsensus. In addition, subskils outside the table were also considered, if necessary. The rationale was that the ChatGPT-authored items may alo measure reading subskils other than those specified in Table 1. The semi structured interview (see Appendix B for the outline) was also conducted to understand English teachers and experts' perceptions to obtain a more detailed understanding of the quality of the items. It can also be used to investigate their atitude towards the alication of ChatGPT to generate test items.

# 3.3. Data analysis

Regarding the data analysis in RQ 1, the item fit item parameters including item difficulty and item discrimination), item-person map, information function, and distractor analysis were conducted to analyze to what extent the items generated by ChatGPT were acceptable. The two-parameter logistic model in item response theory (2 PL IRT model for short) (Birnbaum, 1968) was chosen to estimate the item fit, item parameters, and information function using the Mirt R package (Chalmers, 2012). Item-person map was conducted by the WrightMap package (Torres Iribarra& Freund, 2014) and the TAM package (Robitzsch et al., 2022). The Shinyl. temAnalysis package (Martinkova & Drabinova, 2018) was chosen to conduct distractor analysis. Based on these statistics, a comparison was made between ChatGPT-authored items and human-authore items. As for RQ 2, the number of experts choosing "agree" or "strongly agee" was counted up, which was followed by the calculation of Lawshe's content valit ratio (Lawshe, 1975) with the Psychometric R package. We used the five-point scale as the four-point scale tends to dichotomize the scale which may force raters to give a certain tendency (Almanasreh et al., 2019). The Mann-Whitney $U$ Test was conducted to see whether there were significant differences between ChatGPT-authored items and Human-authored items in terms of these six criteri. The audio recordings were also transcribed and then analyzed using thematic analysis. ll R packages were implemented by Rstudio (version 4.2.2) and the Mann-Whitney $U$ Test was by SPSs (version 25).

Table 3 Item difficulty and item discrimination parameter.   

<html><body><table><tr><td colspan="3">Machine-authored items</td><td colspan="3">Human-authored items</td></tr><tr><td>Items</td><td>a</td><td>b</td><td>Items</td><td>a</td><td>b</td></tr><tr><td>M1</td><td>a-0.48</td><td>a5.30</td><td>H10</td><td>0.14</td><td>2.03</td></tr><tr><td>M2</td><td>0.32</td><td>2.19</td><td>H11</td><td>0.48</td><td>0.19</td></tr><tr><td>M3</td><td>0.33</td><td>1.09</td><td>H12</td><td>a-0.19</td><td>a.1.97</td></tr><tr><td>M4</td><td>0.43</td><td>1.18</td><td>H13</td><td>0.16</td><td>4.12</td></tr><tr><td>M5</td><td>1.11</td><td>1.23</td><td>H14</td><td>0.22</td><td>5.06</td></tr><tr><td>M6</td><td>0.90</td><td>0.91</td><td>E10</td><td>0.32</td><td>0.70</td></tr><tr><td>M7</td><td>0.90</td><td>1.37</td><td>E11</td><td>0.57</td><td>1.08</td></tr><tr><td>M8</td><td>1.22</td><td>0.76</td><td>E12</td><td>1.11</td><td>0.56</td></tr><tr><td>M9</td><td>0.97</td><td>0.23</td><td>E13</td><td>0.21</td><td>2.08</td></tr><tr><td>G1</td><td>1.08</td><td>0.48</td><td>E14</td><td>0.21</td><td>2.60</td></tr><tr><td>G2</td><td>0.41</td><td>1.13</td><td></td><td></td><td></td></tr><tr><td>G3</td><td>1.51</td><td>0.29</td><td></td><td></td><td></td></tr><tr><td>G4</td><td>0.13</td><td>3.80</td><td></td><td></td><td></td></tr><tr><td>G5</td><td>1.04</td><td>0.11</td><td></td><td></td><td></td></tr><tr><td>G6</td><td>0.64</td><td>1.17</td><td></td><td></td><td></td></tr><tr><td>G7</td><td>1.13</td><td>0.21</td><td></td><td></td><td></td></tr><tr><td>G8</td><td>1.04</td><td>0.31</td><td></td><td></td><td></td></tr><tr><td>G9</td><td>0.25</td><td>0.34</td><td></td><td></td><td></td></tr><tr><td>Mean</td><td>0.79</td><td>0.01</td><td></td><td>0.38</td><td>0.27</td></tr></table></body></html>

Note. a Stands for the values not to be analyzed in this table.

# 4. Results

4.1. Results of RQ1: psychometric properties

# 4.1.1. Item fit

The test data of the 18 machine-authored items and the 10 human-authored items were fited to the 2 PL IRT model using the Mirt package. RMSEA (root mean square error of approximation) of $S { - } X ^ { 2 }$ can indicate the magnitude of item misfit (Chalmers, 2012). The item that is significant in the $\mathsf { p } . S \mathsf { - } X ^ { 2 }$ index might be regarded as lacking fit to the 2 PL IRT model (Orlando & Thissen, 2000, 2003). Regarding RMSEA, four items (namely, M1, G2, G3, and G7) were above 0.05 with the value f 0.06, sugesting that these items were marginally misfit Regarding the significance level, only G2 and G3 were significant at the 0.05 level. But no item was significant a the $p { = } 0 . 0 1$ level, suggesting all the items can be regarded as itted to the 2 PL IRT model though there existed negligible misfit for G2 and G3. It can also be seen that the machine-authored items and the human-authored items fit the model almost equally well.

# 4.1.2. Item parameters

The 2 PL IRT model can also provide two item parameters item difficulty (b) and item discrimination (a). The typical value of the item difficulty ranges from $^ { - 3 }$ to 3 and the item discrimination parameter ranges between 0.0 and 2.0 in practical use (Baker & Kim, 2017). The following table shows the values of these two parameters.

Table 3 shows that the item discriminating power of M1 and H12 were negative, indicating that these two items might be prob lematic. This suggests that both ChatGPT and humans may write the item which is abnormal in discrimination, but the proportion was small. It was found that nearly $9 1 \%$ of the participants answered M1 correctly after a closer examination of the test data, suggesting that M1 was the simplest item. When the means of item discrimination were calculate separately for the machine-authored items and the human-authored items (excluding M1 and H12), the reading items generated by ChatGPT were found to have higher discrimination than those created by humans.

As the M1 and H12 were problematic in terms of item parameters, they were removed from further investigation in the item-person map. Fig. 1 is the item-person map, which relates the item difficulty and students reding ability togther. verall speaking, the item difficultie of both human-authored and ChatGPT-authored items were geared to the reading abilit of the participants. But some items were too difficult for students eg., H13, H14, G4) and some were to easy (eg, M2, 13, 14). The average diffiulty of the machineauthored items was lower than that of the human-authored items. Interestingly, the items generated by ChatGPT in passage one were generally less difficult than those in passage two.

# 4.1.3. Information function

In item response theory, the information function indicates the measurement precision of the test item at ifferent level f abilit. The area under the information function represents the total information and the highest point of the information plot signifies the ability level at which the item provides the greatest amount of information (Baker & Kim, 2017). As is shown in Fig. 2, concerning passage one, five ChatGPT-authored items in pasage one measured students' reading subskills relatively well including M5, M6, M7,

![](img/13f712c652d5b815273f915df3907aa05373d3c9d02932eac81a10eaf73b4334.jpg)  
Fig. 1. Item-person map.

M8, and M9 whose total information was the largest. But some ChatGPT-authored items had relatively weak performance (e.g., M1, M2, M3) in that the information functions of these items were quite flat and low. Item H11 was the best among the five human-authored items in passage one. Regarding passage two, five ChatGPT-authored items in passage two showed relatively good measurement precision, namely G1, G3, G5, G7, and G8. For human-authored items, item E12 was the best item in terms of infor. mation function followed by E11. Taken as a whole, ChatGPT-authored items had comparable or even better performance than human-authored ones in terms of item information function.

# 4.1.4. Distractor analysis

The item qualit of multiple-choice items can also be reflected in the quality of options or distractors. Thus, distractor analysis was conducted to investigate whether there were qualit differences between the machine-authored options and the human-authored ones (see Appendix C for all results). Based on the total reading scores, the whole sample was divided into three groups roughly equal in number: high performers, middle performers, and low performers. Atrace lot was adopted toshow the examinee proportions in each group choosing the options of an item. Problematic options were detected when they violated the fllowing principles adopted from Martinkova and Drabinova (2018).

1. The number of the high performers choosing the correct option should be larger than that of the high performers choosing any of the distractors of that item.   
2. The solid line of correct answers should be monotonically increasing.   
3. The dotted line of distractors should be monotonically decreasing.

Four machine-authored items (M1, M3, G4, G6) and two human-authored items (H12, H14) were found to have problematic options (See Fig. 3). M1 violated principle 2 and principle 3 and the proportion of test takers selecting the distractors was also les than $5 \%$ indicating that this item was problematic. M3 and G4 violated principles 1, 2, and 3. H12 violated principle 3 since option C of H12 was monotonically increasing. H14 principles 1 and 3. G6 violated principle 3. To sum up, four out of 18 ChatGPT-authored items were problematic, and two out of 10 human-authored items were troublesome. It sems that the qualit of the machine-authored distractors was similar to that of the human-authored ones in terms of ratio.

# 4.2. Results of RQ 2: Turing test, expert judgment, and interview

# 4.2.1. Turing test

Table 4 is the result of the Turing test. Overallspeaking, the five invited English language teachers cannot consistently discriminate ChatGPT-authored items from human-authored items. Their classfication results of H-H, M-M, H-M, and M-H were different. Alex achieved the highest score in $\mathrm { H - H }$ , and Caroline ranked top in M-M. Celia and Mark had the highest number of H-M. In terms of M-H,

![](img/204ba4aab227aeb57647d85dd178a2d221aff7b805e96dfa693b494025facc3f.jpg)  
Fig. 2. Information functions.

![](img/4d93b20f0e456ee774df3016f0249bfbc04d169b69dfbaf6ecc2bc62b3a3b83a.jpg)  
Fig. 3. Selected trace plots of distractor analysis.

Table 4 Result of the Turing test.   

<html><body><table><tr><td></td><td>Carrie</td><td>Alex</td><td>Celia</td><td>Caroline</td><td>Mark</td><td>Means</td></tr><tr><td>H-H</td><td>7 </td><td>9</td><td>5</td><td>8</td><td>5</td><td>6.8</td></tr><tr><td>m-m</td><td>7</td><td>6</td><td>5</td><td>9</td><td>4</td><td>6.2</td></tr><tr><td>H-M</td><td>3</td><td>1</td><td>5</td><td>2</td><td>5</td><td>3.2</td></tr><tr><td>M-H</td><td>3</td><td>4</td><td>5</td><td>1</td><td>6</td><td>3.8</td></tr></table></body></html>

Note: H-H: The human-authored item is judged to be human-generated. M-M: The ChatGPT-authored item is judged to be machine-generated. H-M: The human-authored item is judged to be machine-generated. M-H: The ChatGPT-authored item is judged to be human-generated.

the highest number was 6 (Mark) followed by 5 (Celia). None of the participants could classify all these items correctly.

# 4.2.2. Expert judgment

The result of the expert judgment on the aforementioned six dimensions (see section 3.2) showed that most of the human-authored items and the ChatGPT-authored items were acceptable in terms of the six dimensions. The Mann-Whitney $U$ Test showed that there was no significant diference between the human-authored items and the ChatGPT-authore items at 0.01 level (see Appendix D). This suggests that the perceptions of English experts were similar in terms of these six criteria.

The experts were also invited to make judgments on what reading subskilleach item measures. As shown in Table 5, basically the reading subskils judged by experts matched the subskillsrported by ChatGPT, especially in the item for \*main idea' subskill. But differences were observed in some items (e.g., M1, M7, G3, and G6) for \*understanding" subskill. Expert reported that these items mainly involve locating exact information because the stem and options were merely copied from the reading materials, which might only measure surface recognition.

# 4.2.3.  Interview

The open-ended interview was also conducted to understand English teachers' perceptions after they finished taking the ques. tionnaire. Although Carrie could well distinguish ChatGT-authored items from human-authored items, she said \*Actuall I cannt el which item is generated by machines and which is by humans when I am making the judgment'. A similar response was also received from Caroline, who was mostly correct on this clssfication task. Interestingly, Caroline and Carrie both said I just follow the rule of classifying general items as the machine-authored category, and the detailed items as the human-authored category"

English teachers were also invited to expresstheir perceptions on the quality of both ChatGPT-authored and human-authored items. On the one hand, most of the teachers held a positive attitude toward ChatGPT-authored items. For example, Caroline said, "ChatGPT is an interesting and powerful tool which can provide acceptable multiple items and explanations."

On the other hand, in the interview, experts and language teachers also raised some potential pitalls and concerns which are listed as follows:

1. ChatGPT may fail o understand the prompt sometimes. The reason was that the prompt asked ChatGPT to generate two items for "understanding", inference", "main idea", and "sentiment" and one item for " word meaning" respectively. But ChatGPT generated four items for "understanding" in passage two, and only one item for "inference" and "sentiment' respectively.   
2. The answer provided by ChatGPT was inconsistent and might be wrong sometimes (see Appendix A for more information).   
3. ChatGPT tended to generate the items that were just copied from the reading materials to the option (e.g., M1, G2) while humanauthored items involved deeper processing.   
4. The test areas of human-authored items can cover the whole reading material while ChatGPT-authored items might not.   
5. The ethical problem (e.g., potential bias) should also be taken into account when ChatGPT is used to generate test items for educational purposes.

Table 5 Results of skill judgment.   

<html><body><table><tr><td></td><td>ChatGPT reported subskills</td><td>Human judgment subskills</td></tr><tr><td>M1</td><td>Understanding</td><td>Locating exact information</td></tr><tr><td>M2</td><td>Inference</td><td>Inference</td></tr><tr><td>M3</td><td>Sentiment</td><td>Sentiment</td></tr><tr><td>M4</td><td>Word meaning</td><td>Word meaning</td></tr><tr><td>M5</td><td>Inference</td><td>Inference</td></tr><tr><td>M6</td><td>Sentiment</td><td>Sentiment</td></tr><tr><td>M7</td><td>Understanding</td><td>Locating exact information</td></tr><tr><td>M8</td><td>Main idea</td><td>Main idea</td></tr><tr><td>M9</td><td>Main idea</td><td>Main idea</td></tr><tr><td>G1</td><td>Understanding</td><td>Understanding</td></tr><tr><td>G2</td><td>Word meaning</td><td>Locating exact information</td></tr><tr><td>G3</td><td>Understanding</td><td>Locating exact information</td></tr><tr><td>G4</td><td>Sentiment</td><td>Sentiment</td></tr><tr><td>G5</td><td>Main idea</td><td>Main idea</td></tr><tr><td>G6</td><td>Understanding</td><td>Locating exact information</td></tr><tr><td>G7</td><td>Inference</td><td>Inference</td></tr><tr><td>G8</td><td>Understanding</td><td>Understanding</td></tr><tr><td>G9</td><td>Main idea</td><td>Main idea</td></tr></table></body></html>

Note: Locating exact information" was used when the item just measures the surface information, whose stem and options were copied from the reading materials directly.

# 5. Discussion

# 5.1. Discussion of psychometric properties

The first question investigated the extent to which the items generated by ChatGPT were aceptable based on psychometric models. Five kinds of psychometric properties were reported in the result section, including item fit, item parameters, item-person map, information functions, and distractor analysis.

In terms of item fit, if the significance level was set at 0.05, G2 and G3 might be slightly problematic. But no item was significant at the 0.01 level, ugting that l the itms were aetable in tmsf item fit. Thi s in line wth th finding by Holling et al. (2009, who reported that machine-authored items showed a good fit to the Rasch model. It i also consistent with the finding by Doebler and Holling (2016), who found that the items generated by rule-based techniques itted the Rasch Poisson Counts model. This suggests that ChatGPT holds the potential to generate test items that can fit the psychometric model.

Regarding item difficult, some of the machine-generated items (e.g., M1) were obviously les dificult than the human-authored items. The reason probably lies in that ChatGPT generated some items only concerned with locating exact information while they were originall intended to ases more comple suskills It sms that hatGPT tends to interpret the subskill requirements in the prompts from a simpler perspective and thus generates easier items. This inding is in line with the findings by Rodriguez-Torralba et al. (2022), who claimed that the items generated by the transformer-based AIG system tended to measure retention rather than comprehension.

Besides, based on the item-person map, the dfficultiesof the item authored by both ChatGPT and humans were parallel t students abilities with few exceptions. This is in agreement with the finding by Sun et al. (2019), who reported that the item generator can produce items that can cover a range of dfficultie. Most importantl, this means that it would be posible to provide students with multiple-choice items whose dificulties are commensurate with students' reading ability. Moreover, the reason why the items generated by ChatGPT in passage one were les difficult than those in passage two might relat to the text's readability. We also calculated the text radability of these two passages using Coh-Metrix (McNamara et al., 2014) totest our hypothesis. It was found that the Coh-Metrix 2 readability of passage one was 14.92 and that of assage two was 12.08, suggestig that pasage one was easier than passage two in terms of text readability. This i consistent with our expectations since we know that the firstreading paag i usually easier than the second one. This can also explain why items in passage one were moderately easier than that in passage two as is shown in Table 3 numericall or in Fig. 1 vsually. This i in accord with the findings by Freedle and Kostin (199) who reported that the content and structure of reading materials explained at least $3 3 \%$ variance of the item difficulty of multiple-choice items in reading. When the item difficulty was compared within the same passage in passage one, it sems that the higher the lel of reading subskills (see Table 5) measured by the item, the more difficult the question was. This is i line with the findings by Basaraba et al. (2012) and Alonzo et al. (2009), who reported that item type for higher-lel reding suskills were more difficult than item types for lower-level reading subskills.

However, exceptional cases are also possible since the reading subskil is not the only factor that can contribute to the item dif ficulty. For example itm G8 (th item for \*understanding detals" subskills was more difficult than item G9 (the item for \*main idea") One plausible reason might related to the discrimination of options of these two items were ifferent. Another possible reason might be attrbuted to the diffrence in the overlap between the question and reading materials (Freedle & Kostin, 1993). In other words, whether explicit clues can be found in the reading materials directly also matters (Pearson & Johnson, 1978; Xu et al., 2022). The explicit clues for the correct answer to item G9 can be found in the firs paragraph of pasage two aftr our post hoc analys. This can also account for why item G9 was easier than item G8. These findings shed important light on how teachers can generate the items at the appropriate difficulty for students using ChatGPT and deepen our understanding of the factors impacting task difficulty.

Item discrimination and item information can represent the discriminating power and the measurement precision of test items respectively. According to the research results, the ChatGPT-authored items generally had higher discriminating powers than the human-authored items. The item information of these items was also relatively high since the calulation of the information function of the 2 PL IRT model is based on item discrimination (Baker & Kim, 2017). Furthermore, the reading subskill each item measures can explain why some items had higher item discriminating power and information function than others. As is shown in Table 5, these items with relatively lower discriminating power and information functions (e.g., M1) measured the relatively lower level of reading subskil. And the items with higher discriminating power and information function (e.g., M5, M6, M8, M9, G1, G3, and G7) measured the higher level of reading subskils, e.g, \*summarizing the main idea" and \*making inferences'. Therefore, it is necessary to set subskillrequirements when ChatGPT is used for AIG. Otherwise, more items simply involving locating exact information may be generated, which will in turn lower the overall difficulty and discrimination.

With respect to the result of distractor analysis generall speaking, the distractors generated by both humans and ChatGPT were acceptable with few exceptions. About 7 percent of ChatGPT-authored items (14 out of 18) were aceptable in distractor analysis. This indicated that ChatGPT had a moderate to high capacity to generate distractors for reading test items. This is in line with the findings by Shin et al. (2019), who found that machines can be used to generate the distractor in multiple-choice questions. It i aso in accord with the findings by Patra and Saha (2019), who reported that machine-authored distractors were as good as human-authored distractors. Similar findings were also reported by Das et al. (2019) who used the corpus-based method to generate distractors and Afzal and Mitkov (2014) who generated distractors based on dependency-based semantic relations. This finding is somewhat encouraging in that this suggests that ChatGPT can work as an asstant to geerat proper distractors. It has significant practical value for language test developers in generating high-quality distractors of multiple-choice items.

Furthermore, it is also worth pointing out that the finding of the study is limited by the small sample size i.e. 213 participants majoring in engineering), which sugges that empirical reearch with more samples is needed in the future. Besides, future studies can be taken to investigate the testperformance of partcipants from different backgrounds (e.g., humanities). Regarding sample size for the 2 PL model, several important factors should be taken into consideration, including the distribution of samples i. normal distribution or not), the length of th tet (i.. the number of items), and theestimation method. All these factors interact wth each ther, making it hard to arie a general rule for the minimal sample size (de Ayala, 2022 p.141). ccording to the simulation result of the previous study, Drasgow (1989, for example, reported that 200 samples with 5 items were sufficient for the unbiased parameter estimates for the 2 PL model using marginal maximum likelihood estimation (MMLE). Concerning sample distrbution, we calculated the total score of participants based on classical test theory and then conducted the normality test, finding that the ability of par. ticipants followed the normal distribution. This suggests that the sample distribution in our study is favorable for parameter estimation. Moreover, the item fit indexes (e.g., RMSEA) estimated by MMLE of the $2 ~ \mathrm { P L }$ model as shown in Table 2 also showed a relatively good fit as a whole, which can also lend supporting evidence to the parameter estimation of the 2 PL model. Given these rationales, we think that the results of the $2 \mathrm { P L }$ estimates in this study were acceptable and credible.

# 5.2. Discussion of teacher and expert perceptions

The second question dealt with the perception of English teachers and experts through the Turing test, expert judgment, and in. terviews. The results of the Turing test varied acros different English teachers. If the bet result was considered, such as the resut of Mark and Celia, i seems that ChatGPT-authored items could pass the Turing test, which indicates that ChatGPT-authored items can leave a similar impression for test takers as human-authored items do. This corroborates the findings by Gierl and Lai (2013), who reported that experts cannot consistently tell machine-authored items from human-authored items when they were asked to do the Turing test in the blind review. It is also consistent with the finding by Casal and Kessler (2023) who reported that lingusts and re viewers found it hard to distinguish ChatGPT-authored texts from human-authored texts. However, when the worst results were considered, for example, the resul b Carrie and Caroline, ChatGT-authored items faile in the Turing tes. This finding is i ine with the finding by Elkins and Chun (2020), who investigated whether the GPT 3 can pass the writer's Turing test and reported that GPT 3 sometimes excelled and sometimes failed.

Concerning the plausible reasons for the inconsistencies of the Turing test, the reasons might include the background and infor. mation literacy of the ivited English teachers. First of all although these five Enlish teacher all major in English-relateddisciplines, they diffr in their reearch interests. For example, the background of Caroline is language testing which might provide more professional knowledge for her to differentiate items. And threeof fie English teachers major in translation and one of them majors in language teaching. This can account for why Caroline performed best among them. The second important reason might related to their informatio iteracy related to the l language model, which ls t different critria for judgment, casing the inconsistencie across different English teachers in the Turing test fially. nd ost hoc interviews with these English teachers also confirm this For instance, Alex reported during the post hoc interview that he knew machines can generate some items related to sentiment analysis. Therefore he said that the item for sentiment (e.g., M6) was probably generated by machines. However, some English teachers, for example, Mark, do not know this and Mark thought that item (e.g, M6) was authored by humans. This also leads to the inconsistent phenomenon in the Turing test.

Regarding the expert judgment, the result showed that there were no significant differences between ChatGPT-authored items and human-authored items in the six dimensions as mentioned in section 3.2. This finding is consistent with the finding by Chinkina et al. (2019), who found that the perceived quality of machine-authored items and human-authored items were comparable in terms of format and anwerabilit. But it s also worth mentioning that th rubric used in his stdy did not cover all the citria  Hladyna & Downing, 1989 for item quality evaluation. For example, avoiding the use of "all f the above' in options is also an important rule for assessing the quality of multiple-choice questions, which is one of the areas ChatGPT-authored items (e.g., M7 and G3) might need to be improved. Besides, the result of expert judgment is also limited by the relatively small sample size of experts, which should be interpreted with caution. Future studies can overcome this limitation by recruiting more experts in the expert judgment section with more rules for item quality evaluation. In addition, in terms of the reading subskill reported by experts and ChatGPT, the majorit of them were consistent. This suggests that ChatGPT has the potential to generate the desired item for different reading subskill, which can solve one of the problems in diagnostic reading asessments. Previous diagnostic reading asessments mainly focused on diag. nosing reading subskills but paid lttle atention to remedial instruction (Sessoms & Henson, 2018). With this method, students may receive personalized remedial reading exercises and intructions more eficiently. However, there were also some inconsistencies. Experts believed that some items might not measure the reading skill it was intended to measure since ChatGPT might generate some items whose stems and options were directly copied from the assage (e.g., M1, M7, G2). One possile slution to tackle this problem is to add additional requirements in the prompt for ChatGPT (e.g., the requirement on the option). Altenatively, a sequence-to-sequence model can be embedded to paraphrase the items that were directly copied from the reading materials, which can enhance the cognitive complexity of the question (Huang & He, 2016).

During the interview, it was found that most of the teachers were positive about using ChatGPT to generate questions. But there were also some concerns and doubts about applying ChatGPT to generate multiple-choice items for educational purposes. On the one hand, teachers stated that ChatGPT-authored items were acceptable and helpful. This is in line with the standing of Kasneci et al. (2023), who claimed that ChatGPT can be used to generate reading comprehension items. On the other hand, some shortcomings, and potential pills ere aso identified by experts and language teachers. The problems relat to the intellignce, rliability, and ethical concerns of ChatGPT. Concerning intelligence, it was found that it generated more items for "understanding" reading subskills and fewer items for inferences" reading subskils in passage two. This is a discouraging result because an intelligent AIG system is supposed to generate the item based on the specific requirement. Unexpectedly, when we used the English version prompt to ask ChatGPT to generate items, it semed that it could produce the desired items for these five reading subskill. This suggests that ChatGPT might have diffrent operational definitions for the reading subskills when the name of reading subskills was written in different languages, eecially for the subskil of inference'. One possible rason might be that the majority of the pre-training data of ChatGPT was English rather than Chinese. Another posible reason is that ChatGPT is relatively weak at abstract reasoning and inference. s for the inconsistent answers provided b ChatG, it i unaccetable, eecially in high-stakes tets, which suggests that there is clear room for improvement in this aspect. The ethical concern of potentia ias in the use of ChatGPT for AIG also merits more attention, which reaes to the fairnessvalidity of language testing. To be more specifi, some bias items may be generated, especially when it comes to the ChatGPT-authored item for recognizing attitudes and emotions" subskill This problem should be properly handled if large anguage models ., ChatGT) are used for educational purposes. To mitigate this concern, technically speaking, the most commonly used method is reinforcement learning from human feedback. Another important method is to improve the data quality and enhance the diversity of the training data for ChatGPT (Ray, 2023). Regular evaluation and supervision by practitioners in language testing and experts in computer sciences are also essential (Kasneci et al., 2023).

# 6. Conclusion

This study aims to investigate the feasibility of applying ChatGPT to generate reading test items automatically and evaluate their item quality using psychometric models and human review. The results of the 2 PL IRT model, distractor analysis, the Turing test, the expert judgment, and the interview indicated that, when compared with human-authored items, ChatGPT had the potential to generate acceptable multiple-choice items based on the given reading materials.

This study has important theoretical and practical contributions. Theoreticall speaking, this study not only explored the relationship between the readability of reading materials and item diffculty but also discovered the links among reading subskill, item difficult, and item discriminatio. This contributes to a better understanding of the roles of readability and subskillrequirements in reading item generation using ChatGPT. It also has important pedagogical implications for teaching, learning, and asessing reading. Practicall speaking, thee findings suggest that ChatG has grt pplication propect for languag teachers, angage leaners, and language testing practitioners. For language teachers, ChatGPT can serve as a teaching asstant who can help language teachers produce multiple-choice reading test items or quizzes for students. This can reduce the burden on language teachers and rescue them from generating tes items laboriously. For language leners, ersonalized learning would be realized in a more fficient way. It might be possible for them to choose the topics and item types they are interested in with the aid of AI language models. For language testing practitioners, they can use ChatGPT to reduce the cost and increase the eficiency of generating multiple-choice items, which can also lay the foundation for the adaptive learning system.

Notwithstanding ChatGPT's capability in AIG, some shortcomings and pitall are also worth mentioning. The problems, such as failing to understand and generate the multiple-choice items acording to the prompt sometimes, inconsistency in providing answers, and potential ethical risks, are yet to be overcome in the future.

This research is not without limitations. First, due to space limitations, this paper has not yet investigated how the explanation generated by ChatGPT can scaffold reading, which deserves more research in the future. Scond, the samplesize of participants taking the exam, teachers in the Turing tes, and experts for item quality evaluation and the number of reding passages wre relatively small Which may limit the generalizability of the findings inthis research. Researchers can replicate this study with a larger sample size and more reading materials in the future. Third, this study does not take the reading proces of test takers into account, which is also a very important criterion for the ealuation of test items. Further studies can investigate the cognitive proces of test takers using the think aloud protocol. Fourth, this study only focuses on multiple-choice items. Whether ChatGPT has the same capability in other reading tasks remains known, which merits further study. Finally, AIG is an ever-developing research field and future studies need to keep up with the technological progress.

Despite these limitations, this study can cast important light on how language teachers can use ChatGPT to generate multiple. choice items. We recommend the following three tips summarized from this experiment for language teachers who are interested in applying ChatGPT to AIG in reading comprehension. (1) Selecting the appropriat reading material is essential, which might determine the item's difficulty to a large extent. (2) Specifying the reading subskill in the prompt i crucial for the content validit of the test item since ChatGPT might not take different item types (i.. item measuring different reading subskills) into account when the subskil requirement is not provided. (3) Prompt engineering entals understanding the basic principles and guidelines of writing a prompt and most importantly needs several times of iterative improvement. These practical tips might help language teachers better leverage large language models to develop more valid multiple-choice items in reading comprehension for students.

# Authors' statement

Zhiqing Lin and Huilin Chen wish to submit an original research article entitled Investigating the Capability of ChatGPT for Generating Multiple-choice Reading Comprehension Items" for consideration by System. We confirm that this work is original and has not been published elsewhere, nor is it currently under consideration for publication elsewhere.

Zhiqing Lin and HuilinChen do not have competing financial or non-financial interests that are directly or indirectly related to the work submitted for publication.

# CRediT authorship contribution statement

Zhiqing Lin: Writing - original draft, Resources, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Huilin Chen: Writing - review & editing, Supervision, Methodology, Funding acquisition.

# Acknowledgements

The work is supported by the major project of Shanghai International Studies University (2019114032), the major project of Key Laboratory of Artificial Intelligence in Multiligual Education of Shanghai International Studies Universty (A202201), and the major teaching reform project of Shanghai International Studies University (41004961).

# Appendix A. Supplementary data

Supplementary data to this article can be found online at https://doi.org/10.1016/j.system.2024.103344.

# References

Afzal, N. & to,  (2014). mtc tion f mile chce qtion usin ddd c etion. t ing 187), 16-1281 https://doi.org/10.1007/s00500-013-1141-4   
Alderson, J. (2000). Assessing reading. Cambridge: Cambridge University Press.   
Amanasreh,     he 019 f  u  eti  alt.    tie Phc, 152) 214-221. https:/doi.org/10.1016/j.sapharm.2018.03.066   
Alonzo,  a  ri. 00 d, t  e nri at  eag comprehension. Assessment for Efective Intervention, 35(1), 34-44. https://doi.org/10.1177/1534508408330082   
Ann01  t. n   
Aredasy,   er, 207). meric h  tio he cf aha- shi aphe utomatic generation of quantitativ rsonig ites. Leg and Individl Difences, 17(4), 366-383. htps://doi.rg/10.1016/j.lindif.2007.03.005   
Aredasy,  er,  M, 1).  c it i sly co n  ish s f  d  t. Journal of Cross-Cultural Psychology, 43(3), 464-479. https:/doi.org/10.1177/0022022110397360   
Arydst 0kr tt 8./1.10218   
Attli, ,  Lair,  e, , rk  n er,  2 intie g sr- mi tem generation. Frontiers in Artificial Intelligence, 5, Article 903077. https://doi.org/10.3389/frai.2022.903077   
Bachman, L. F. (1990). Fundamental considerations in language testing. Oxford: Oxford University Press.   
Bachman, L. F., & Palmer, A. S. (2010). Language assessment in practice. Oxford: Oxford University Press.   
Baker, F., & Kim, S.-H. (2017). The basics of item response theory using R. Springer International Publishing.   
Barro, J.. 2023). in hP for seod ana wing Ps and nial. Asin Wing 57, Aile 100745. hp/.10.1016/. asw.2023.100745   
aba  f,    02. th   il l, a ien truly exist? Reading and Writing, 26(3), 349-379. https://doi.org/10.1007/s11145-012-9372-9   
eh  ,    c 1 ..1016 j.system.2023.102995   
Biam (198e     n .   ck .  of s scores (pp. 392-479). Reading, MA: Addison-Wesley.   
Bloom, B, art,  st, , Hl,  Kl,  (1956).  f io ojcive, o : ive d onn.   
Bormuth, J. (1969). On a theory of achievement test items. Chicago, Illinois: University of Chicago Press.   
Casl  e,2.  h t  t f   a Methods in Applied Linguistics, 2(3). https://doi.org/10.1016/j.rmal.2023.100068   
amers . 012).  dsil itm se thy ckg fr t  m J o i e, 86), 19. h/.g 10.18637/jss.v048.i06   
Che   3 i Quarterly, 20(2), 166-189. https://doi.org/10.1080/15434303.2022.2140050 teaching. ReCALL, 32(2), 145-161. https://doi.org/10.1017/s0958344019000193   
Choi  0   t  n  , 17(1) 18-42. https:/doi.org/10.1080/15434303.2019.1674315   
Council f oe 2001). n n frmwrk f re fr l.  chg m. mbrgembridg Unst Pre.   
Das   e learning. Computer Applications in Engineering Education, 27(6), 1485-1495. https://doi.org/10.1002/cae.22163   
de Ayala, R. J. (2022). The theory and practice of item response theory. New York: The Guilford Press.   
Dobler,  lg,  (2016.  si d t  n le- t ti  asi t t h sos ml. ng and Individual Differences, 52, 121-128. https://doi.org/10.1016/j.lindif.2015.01.013   
Drasgow,  (199. e  ikt f th   d ed t 31,7-0 https://doi.org/10.1177/014662168901300108   
Du H        i Environments, 1-16. https://doi.org/10.1080/10494820.2021.1993932   
Elkins, K., & Chun, J. (2020). Can GPT-3 pas a writer's turing tet?Jornal of Cltal Analytcs, 5(2). htps://doi.org/10.22148/01c.17212   
Embretson, .   ston, . M. 2018.tic it gtion:  mre efcient pro for deloi mathemtic aheme itm? Jo of Educational Measurement, 55(1), 112-131. https://doi.org/10.1111/jedm.12166   
ala 27(2), 405-425. https://doi.org/10.1007/s10459-022-10092-z   
Falao     ti   qt i t automatic item generation. Advances in Health Sciences Education. https:/doi.org/10.1007/s10459-023-10225-y   
e    . 10.1177/026553229301000203   
Frede,  t 19  the  e n ilec s f e The  for th t vit f   ge Testing, 16(1), 2-32. https://doi.org/10.1177/026553229901600102   
Giel,  J,   (13ithe t    m c   o 3 doi.org/10.1111/medu.12202   
Girl, M. J ai,  (017 sing matic item gion o crt sotins ad atioae r mputerize fomativettig Alied Pchog Measurement, 42(1), 42-57. https://doi.org/10.1177/0146621617726788   
Gel, J i    0 i t t i io 8, 7576. /.g 10.1111/j.1365-2923.2012.04289.x   
Giray, L (2023. Pt eig h Ch: d fr a w.  of B nrg 12) 62963.h/o./01007 s10439-023-03272-4   
Gotz, F. M Martens,  Loomba ., & van der Linden, . (2023). Let the aithm spk How to use nral etworks for atmatic itm gration in psychological scale development. Psychological Methods. https://doi.org/10.1037/met0000540   
Grabe, W. (2009). Reading in a second language: Moving from theory to practice. New York: Cambridge University Press.   
Hldyna  ,  1.tf a  iech-tin   io 1), 51-78./. org/10.1207/s15324818ame0201_4   
Hldyna g,   , .00  lch-tinli o c   mn n Education, 15(3), 309-333. https:/doi.org/10.1207/s15324818ame1503_5   
Haynes, . d .  y, . 195. vay  h tioh ts amt. Assessment, 7(3), 238-247. https://doi.org/10.1037/1040-3590.7.3.238   
Heston,  & . (03). Prm engi in medl in. nti  ion, 2() 198-205. p/./.330/m2030019   
Holling n  0    .. org/10.1016/j.stueduc.2009.10.004   
Hmel, la   r k . 2021) om e a mdn r co- utomatic item generation. Psychometrika, 87(2), 749-772. https://doi.org/10.1007/s11336-021-09823-9   
Huang, ,   (2016). ic of ht aw qstion fr rg comion   rg 23, 457-489. https:/doi.org/10.1017/S1351324915000455   
Hughes, A. (2003). Testing for language teachers. Cambridge: Cambridge University Press.   
Jin, Y, Jie .  Wan, . (202. he stdy n th alme f  t t aag tanda.Foreign Language World. http://www.cnki.com.cn/Article/CJFDTOTAL-WYJY202202004.htm.   
e, M. 13 th s   f t s.   o, 1 73. /.11/j1200 language models for education. Learning and Individual Dferences, 103, Article 102274. https://doi.org/10.1016/j.lindif.2023.102274   
awhe,  1975. qtiveapch  vlt. P P 8 53-575. tp//.g/.11/17657097.01393.x   
e  Ff     in t   i state-of-the-rt natural language processng. Jonal of Busines and Psycholog, 38(1), 163-190. htps://doi.org/10.107/s10869-022-098646   
Li      i 32) 186-214. https://doi.org/10.1007/s40593-020-00235-   
Lo, L . (202.e th rk or oi i th mpt ig Th o f Ad iah, 94). s/ doi.org/10.1016/j.acalib.2023.102720   
artna  aa 018). htmas fr  eric ad   ys of s. hR l, 102, 503-515. https:/doi.org/10.32614/RJ-2018-074   
caara 190.   t d th vionf   t f  .  ig 1), 527. /./0.177 026553229000700105   
camara . Ger, ., ry,  ai,  014). mt iof tx ds wh e. mr r rsty Press.   
Moore, B., & Stanley, T. (2013). Critical thinking and formative asessments: Increasing the rigor in your clasroom. Routledge.   
atinalh t 01 in t  na Jiao Tong University Press.   
Ockey, ..,  eiz,  2021). Eing techmedate sond ge l ncaton aet dlivy md. e in i Principles, Policy & Practice, 28(4), 350-368. https://doi.org/10.1080/0969594x.2021.1976106   
Orando,  e  i- -i e o s itm e ty  d Pln, 41), 5064. https:/doi.org/10.1177/01466216000241003   
Orlando, M., & Thissen, D. (2003). Further investigation of the performance of $\mathsf { S } - \mathbf { X } ^ { 2 }$ : An item fit index for use with dichotomous item response theory models. Applied Psychological Measurement, 27(4), 289-298. https://doi.org/10.1177/0146621603027004004   
uru  em00e thi       th ieh Methods, 40(4), 1001-1015. https://doi.org/10.3758/BRM.40.4.1001   
Patra,   ah01 ri rchr  i f n tror r e che tio. ion n omation Technologies, 24(2), 973-993. https://doi.org/10.1007/s10639-018-9814-3   
Pearson, P. D., & Johnson, D. D. (1978). Teaching reading comprehension. New York: Holt, Rinehart & Winston.   
Person,   hael,  19 g ion  mif thkg,  .    .), ion f hg n oie instruction (pp. 209-240). Routledge.   
ugh, D  ,  i  he 200. itn e    qltyt ptin f knowledge? Research and Practice in Technology Enhanced Learning, 15(1), 12. htps://doi.org/10.1186/s41039-020-00134-8   
Ray . 0.: mi  d i  c i h o ad  o   hngs and Cyber-Physical Systems, 3, 121-154. https://doi.org/10.1016/j.iotcps.2023.04.003   
Robitzsch, A, Kiefer T., & Wu, M. (2022). Tam: Tst alysis modules. R package version, 4, 1-4. htps://CRAN.R-projec.org/package=TAM.   
Rdrig   - 02). di mci n tt e former models. Expert Systems with Applications, 208, Article 118258. https://doi.org/10.1016/j.eswa.2022.118258   
Ryo, . a .,     02   ie a c it  t ric properties. Sage Open, 12(2). https://doi.org/10.1177/21582440221095016   
Sesos,  1 pti  tcie d i ly Research and Perspectives, 16(1), 1-17. https://doi.org/10.1080/15366367.2018.143510 10.1080/15305058.2022.2070755   
Shi, .,  ,  J 019 ech   usi t  .e  , . h/. 10.3389/fpsyg.2019.00825   
Shi, D., , J  (2023). Can C make reg comion tetin tems n par wh man xrg, n n ec, 273), 27-40. https://hdl.handle.net/10125/73530.   
Shi,   J. . 2022. n epry t on th oti che eg mpeheion as a incti cfon dic n second language reading lessons. System, 109, Article 102863. https:/doi.org/10.1016/j.system.2022.102863 10.1016/j.system.2016.12.014   
pecer,  mr  er , mo   in,  019 dn h f t oty a qtion tye on reading outcomes. Reading and Writing, 32(3), 603-637. https://doi.org/10.1007/s11145-018-9883-0 fpsyg.2019.00884   
Torrs ribrra ., Fend,  (2014 Wiht map:  itm-n map with Qut intratio Avalableat hp/github.co/dd-t/ightmap.   
von Davie,. (2018. om itm gion wth rrn nl ks. Pcmerika 3(4) 847-57. p/.g/0.1007/1136-018908-   
Weir, C. J. (2005). Language testing and validation: An evidence-based approach. Basingstoke: Palgrave Macmillan.   
acot , Bar,   , l,  d   R S H 2023. d t t of ains  me and standard seting. BMC Medical Education, 23(1), 659. https://doi.org/10.1186/s12909-023-04457-0   
u      -   hr , 43(3-4), 211-231. https://doi.org/10.1080/02702711.2022.2094037   
Yang, H, & Weir, C. J.(1998). i.itb [The CET validation study). Shanghai hanghai Foreign Language Educatin Pres.   
heg,    (208. t revih t   h i53) 087. /g//52009243   
r   2a  9. https://doi.org/10.48550/arXiv.2211.01910