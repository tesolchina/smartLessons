# Evaluation of Cohen's kappa and other measures of inter-rater agreement for genre analysis and other nominal data

Gerald Raua\*, Yu-Shan Shih b

a Department of Electrical Engineering, National Chung Cheng University No.168, Sec. 1, Universty Rd., Minhsiung, Chiayi 621301, Taiwan b Department of Mathematics, National Chung Cheng University No.168, Sec. 1, University Rd., Minhsiung, Chiayi 621301, Taiwan

# ARTICLEINFO

# ABSTRACT

Keywords:   
Inter-rater reliability   
Cohen's kappa   
Move analysis   
Coding   
Statistical test   
Research methodology

Cohen's kappa () is often recommended for nominal data as a measure of inter-rater (intercoder) agreement or reliability. In this paper we ask which term is appropriate in genre analysis, what statistical measures are valid to measure it, and how much the choice of units affects the values obtained. We find that although both agreement and reliability may be of interest, only agreement can be measured with nominal data. Moreover, while kappa may be appropriate for macrostructure or corpus analysis, it is inappropriate for move or component analysis, due to the requirement of $\kappa$ that the units be predetermined, fixed, and independent. k further assumes that all disagreements in category assignment are equally likely, which may not be true. We also describe other measures, including correlation, chi square, and percent agreement, and demonstrate that despite its limitations, percent agreement is the only valid measure in many situations. Finally, we demonstrate why choice of unit has a large effect on the value calculated. These findings also apply to other studies in applied linguistics using nominal data. We conclude that the methodology used needs to be clearly explained to ensure that the requirements have been met, as in any other statistical testing.

# 1. Introduction

Tests of inter-rater reliability or agrement are important in many fields, both to ensure that the category descriptions are clear and that the final ratings are accurate, analogous to formative and summative assessment, respectively. Although often used inter. changeably, inter-rater agrement measures the extent to which the raters give identical ratings, while inter-rater reliaility measures the relative order and spred (variabilit) of the coding, and thus is only valid for numerical categories. Different tatistical tests are suitable depending on which is measured, the type of units coded, and the number of raters.

Genre analysis is oten equated with move analysis, in which a rater (coder) divides a text into a series of moves, each composed of one or more steps. Many move analysis studies have recommended the use of Cohen's kappa () to test agreement betwen raters Rather than comparing the percent agreement between two raters directly, $\kappa$ is an expression of the amount of agreement in excess of that which would be expected by random chance. However, ike any statistical test, Cohen's kappa has certain requirements that must be met for the test results to be meaningful. Although  has frequently been applied to move analysis coding, authors rarely explain in detail how they conducted the test, so it is not immediately apparent whether those requirements have indeed been met. In other studies, percent agreement, correlation, or chi-square have been used.

This study wil compare the suitabilit of various inter-rater analysis measures in genre analysis. We focus mainly on move analysis of reseach articles, where interrater analysis has en frequently used, but ao consider macrostructure analysis corpus analysis and component analysis, which code documents for categories at different levels, to demonstrate why the test chosen depends on the research methods.

The paper is organized as follows: In the following section we define terms, review the use of inter-rater analysi in genre analysis studies, and present the research goal. In section 3 we briefly present the methods of the study. In section 4 we first examine the requirements for different types f tests, then whether those are met in different types of genre analys and why, followed by an illustration of the importance of choice of unit Finall, we summarize the recommendations of the study and point to directions for future work to expand the conclusions to other areas of linguistics.

# 2. Literature review

In this section, afer defining terminology, we will discuss the purpose and identit of aditional raters, followed by a summary ot the inter-rater agreement measures that have been used in genre analysis, and finally the research questions.

# 2.1. Definition of terms

Since various terms have been used for the same concepts, sometimes incorrectly, before going further we will define importan! terms as they will be used in this paper.

# 2.1.1. Raters, units, categories

Raters: two or more individuals doing the rating, who decide how to assign every unit to a category. In genre analysis, these are trained coders. For most statistical tests, the raters must rate independently.

Units: items to be categorized by the raters. In genre analysis, portions of a text. For most statistical tests, units must be predetermined, fixed, and independent.

Categories goups to which the units are assigned by the rater. Categories used in genre analysis wil be discussed below. For most statistical tests, categories must be predetermined, fixed, independent, mutually exclusive, and exhaustive.

All these requirements are true for each measure of kappa and correlation discussed in this paper.

# 2.1.2. Nominal, ordinal, interval

In choosing the proper statistical test, i is important to consider the nature of the categories to which units are assgned (Tinsley & Weiss, 1975).

Nominal: Categories without order.   
Ordinal: Categories with a rank on a continuum, natural ordering.   
Interval: Ordinal categories with equal intervals.

# 2.1.3. Inter-rater agreement, inter-rater reliability

The distinction between inter-rater agrement and reliability was irst proposed by Tinsley and Weiss (1975), but the two terms are often used incorrectly as synonyms (Tinsley & Weiss, 2000).

Inter-rater Agreement (IRA): the extent to which raters make the same judgement. High agreement means raters assign the same units to the same category. Agreement can be tested for nominal, ordinal, and interval categories. Testing is based on percent agreement, with or without adjustment for chance agreement.

Inter-rater Reliabilit (IRR): the extent of variability and error in measurement, formally the ratio of true score variance to total ariance (Tinsley & Weiss 975). Highrliability means the ratings rein the same order, with simila variabilit, so that the items can be consistetly distingshd, readessof aolute value it al., 2013). hus, riabilit echnill ruir inval catgories, but is frequently used with ordinal (Tinsley & Weiss, 2000). Testing involves use of correlation or analysis of variance.

For ordinal or interval data, both agreement and reliability should be reported (Tinsley & Weiss 2000), just as both mean and variance should be reported in quantitative studies. For nominal data, only agrement makes sense, as there is no numerical meaning to the categories that would allow measurement of variance, even if the categories are assgned numerical designations.

Inter-rater Analys since some studies have used the two terms interchangeably, in some cases we wil use the term inter-rater analysis when the distinction is not clear or to indicate both types of measures.

# 2.1.4. Genre analysis methods

Genre, in the English for Specific Purposes (ESP) tradition, is defined a clas of communicative events with a shared communicative purpose and therefore similar structure (Swales, 1990, p. 58). Genre analysis seeks to elucidate that structure, often for pedagogical purposes.

Move analysis, often used synonymously with genre analysis, is a method of textual analysis in which a text is divided into moves and steps. Each move has a particular communicative purpose, whil steps within the move represent different rhetorical ways to accomplish that purpose (See Moreno & Swales, 2018, p. 40). The method has been used extensively to study the tructure of research articles, as well as other admic gres includng thes and disrtations, grnt proposals, aoratory rorts, tchnical reorts, and oral presentations.

In a broader sense, any genre can be analyzed at diffrent levels, and other analysis methods have been used, including macro structure, component, and corpus. The properties of each which are relevant to inter-rater analysis wil be presented in section 4.4.

# 2.2. The purpose and identity of additional raters in genre analysis

In genre analysis studies, the author is usuall the main rater (coder) of the data. Second raters may be used during the formative stage to ensure the validity and clrity of the categorie, or at the summative stage, to ensure the uniformity of the final coding. The second rater may be a subject-area specialist or, more commonly, another linguist.

A summary of the genre analysis literature that has reported inter-rater analysis is shown in Table 1.

Subject-area specialists have sometimes ben consulted to confirm and improve the categories and descriptions used for rating, to test progres toward a goal rather than whether the final product achieved the set goal. For example, Basturkmen (2012) asked authors of two articles in dentistry to comment on the moves and steps identified in articles they had written to ensure the categories were accurate. No inter-rater analysis was reported.

Once the categories are set, to ensure the accuracy of the ratings reported, frequently a second rater is asked to independently code the data. In many cases, the second rater chosen is another inguist. For example, in several studies the second rater had an MA in applied linguisics (Peacock, 2002, 2011) or TESL (Lim, 2010). In another it was a 'research associate f the author (Lin & Evans, 2012). Two additional raters with MAs in linguistics were used by Amirian, Kassian, and Tavakoki (2008) and a linguistics PhD candidate by Parkinson (2017). Occasionally an aditional rater or raters were chosen who were subject area specialist in the field under consideration (Kanoksilapatham 2005, 2012, 2015). In one study (Moreno & Swales, 2018), a comparison of research articles in English and Spanish, the second rater was a lecturer competent in both languages, not a specialist in either linguistics or the fields under study.

In some cases, there was no clear distinction between formative and summative evaluation, but discussions between raters continued until consensus is reached. For example, Shi and Wannaruk (2014) asked two subject area specialis, a professor and a PhD candidate, to code a portion of the corpus, then discussed differences until there was no disagreement. Chang and Kuo (2011), while developing teaching material for writing research articles, used 4 raters, the two authors and two graduate asistants, with the coding discussed until consensus was reached. Similaly, Cotos et al. (2015) each coded all the documents, checking inter-rater reliability regularly during the coding process.

# 2.3. Inter-rater measures used in genre analysis

In many gre anlys stdies, the isue f ratr agemet or reliabilit i nt addressed. The studs will not bediussed inthis paper. It is not our inten to question whether a second rater is needed, but what term and test are appropriae if inter-rater analysis is conducted.

As shown in Table 1, for macrostructure and corpus analysis studies, we were only able to find one paper each that used an additional rater. In both cases, the term used was inter-rater agreement, with percent agreement $( \% \mathrm { A } )$ used as a measure.

In move analysis, some studies report that they are assessing inter-rater agrement, while others use the term inter-rater reliabilit for te same type of et et thers rort tht they are sing a tt f inr-rar reliabilit to ass the agrement betwe raters.his indicates a lack of clarity about the difference between the two terms.

Among the articles in genre analysis and related studies that report inter-rater analysis, many have used Cohen's kappa (k) and several have argued it is the most appropriate measure. In early studie related to genre analysis, Hyland reported its use in studies of metadiscourse (1998) and stance markers (1999). Kanoksilapatham suggested it was a more appropriate measure than percent agreement given that moves are categorical variable (2005), and in order to compensate for chance agreement between raters (2007), although in each study she reported both measures. In more recent studies, Kanoksilapatham (2012, 2015) used only percent agreement, afer recognizing some problems of using kappa (personal communication). More recently, Parkinson (2017) used $\kappa$ in a study of lab reports. Cotos et al. (2015), with three raters, used $\kappa$ to test inter-rater reliability between pairs of raters. Most recently, Moreno and Swales strongly recommended kappa, stating, \*The Cohen's Kappa coefficient is considered more useful than the agreement percentage because it is achance-corrected measure, which takes into account the likelihood that the agreement between coders has occurred by chance." (2018, p. 55).

Table 1 Reporting of inter-rater agreement and reliability in genre analysis.   

<html><body><table><tr><td>Study</td><td>Analysis type</td><td>Rater purpose</td><td>Additional rater</td><td>Term(s) used</td><td>Test typea</td></tr><tr><td>Lin and Evans (2012)</td><td>Macro</td><td>Summative</td><td>Linguist</td><td>Agreement</td><td>%A</td></tr><tr><td>Cortes 2013</td><td>Corpus</td><td>Summative</td><td></td><td>Agreement</td><td>%A</td></tr><tr><td>Chang and Kuo (2011)</td><td>Move</td><td>Formative</td><td>Linguist</td><td>Reliability</td><td>Consensus</td></tr><tr><td>Shi and Wannaruk (2014)</td><td>Move</td><td>Summative</td><td>Subject</td><td>Reliability, Agreement</td><td>%A</td></tr><tr><td>Kanoksilapatham 2012, 2015</td><td>Move</td><td>Summative</td><td>Subject</td><td>Reliability, Agreement</td><td>%A</td></tr><tr><td>Kanoksilapatham (2007)</td><td>Move</td><td>Summative</td><td>Subject</td><td> Reliability, Agreement</td><td>K, %A</td></tr><tr><td>Kanoksilapatham (2005)</td><td>Move</td><td>Summative</td><td>Subject</td><td>Reliability</td><td>k, %A</td></tr><tr><td>Moreno and Swales (2018)</td><td>Move</td><td>Summative</td><td> Bilingual</td><td>Reliability</td><td>k, %A</td></tr><tr><td>Lim 2010</td><td>Move</td><td>Summative</td><td>Linguist</td><td>Agreement</td><td>K</td></tr><tr><td>Parkinson, 2017</td><td>Move</td><td>Summative</td><td>Linguist</td><td>Reliability</td><td>K</td></tr><tr><td>Cotos et al., 2015</td><td>Move</td><td>Formative, Summative</td><td>Linguist</td><td>Reliability, Agreement</td><td>k, ICC</td></tr><tr><td>Peacock, 2002, 2011</td><td>Move</td><td>Summative</td><td>Linguist</td><td>Agreement</td><td>Correlation</td></tr><tr><td>Amirian et al., 2008</td><td>Move</td><td>Summative</td><td>Linguist</td><td> Reliability</td><td>x?</td></tr></table></body></html>

$\% \mathbf { A } =$ percent agreement. ICC $=$ Intraclass Correlation Coefficient.

Different methods have been used to calculate k. Lim (2010) used the SP procedure for Interrater reliability analysis. Confusingly, this is abreviated IRA, an abbreviation used in this paper for intr-rater agrement as distinct from reliabilit (IR) (following Gisey et al., 2013). Parkinson (2017) used the online program Statstodo, while Moreno and Swales (2018) used another online program, N Vivo 10. As we will show in the ilustrative example in section 4.5, the choice of program affects the unit on which the calculation is done, which can make a difference in the results. In other cases, the method or program used has not been reported.

Correlation has been used by at last two authors for inter-rater analysis. Peacock (2002, 2011) used the correlation procedure in SPSS, which was then rported as  ercent. Cotos et al. (2015) used the Intraclass Correlation Coefficient (Ic) to test allthree raters together. We also found one study that used chi-square as a test of inter-rater reliability (Amirian et al., 2008).

# 2.4. Research questions

Much of the seminal work on inter-rater analysis was done in psychology (Tinsley & Weiss, 1975, 2000), so linguists may not be familiar with it. Moreover, since each field has diffrent types of units and categories, results from one field may not be directly applicable to another.

Although inter-rater analysis has been conducted in many genre analys studies, it is sometimes called agreement, sometimes reliabilit, and sometimes both terms are used interchangeably. Several different measures have been used, including percent agreement, Cohen's kappa, two different correlation measure, and chi-square. Several authors have suggested that kappa is a more approriate measure than percent agreement for genre analysis. Nevertheles, details of how the test has been conducted are rarely included, and no one has questioned whether genre analysis meets the required assumptions of various tests, or how the testing procedure influences the results.

Thus, the purpose of this study is to answer the following research questions:

1. Based on the goals of genre analysis, should we be interested in inter-rater agreement or reliabilit, or is each appropriate for different purposes?   
2. Which of the various statistical tests that have been used are valid, given the test assumptions and the methods used in genre analysis?   
3. How much does the choice of unit affect the results obtained?

Based on the answers to these questions, we will be able to determine an appropriate measure for inter-rater analysis in genre analysis. We also discuss briefly how the findings apply to other types of applied linguistics studies using nominal data.

# 3. Methodology

Whenever statistical tes are used, it is important for authors to clearly state the details of the experimental design or testing method, to ensure that the procedures ollowed meet the requirements f the test. Many statisticaltest depend on assumptions about the raters, units, and categories, as explained in section 2.1. Unfortunately, in genre analysis studies, these details have not been reported clearly.

Thus, in this study of research methodology, we begin by considering the goals of summative and formative inter-rater analysis, to determine whether reliability or agreement would be the appropriate measure for each.

Next, afer a brief introduction of the tests generall recommended for IRR and IRA, the units and categories used in four types of genre analysis wl be compared with the requirements of the tests. ince  has been recommended by several previous studie, we will examine its requirements in more detail.

Finall, si different tatistical pack ss units dfferntly, w will examine the effcts f using different ut, toillustrate importance of reporting all aspects of the experimental design, as in any other statistical testing.

# 4. Results and discussion

In this section we first determine whether we would like to measure inter-rater reliability (IRR) or inter-rater agreement (IRA) in genre analysis, then briefl describe recommended test for each. Next, we examine the properties of various genre analysis methods, in particular the nature of their raters, units, and categories, to determine which tests may be valid. The section concludes with an example to illustrate the importance of reporting details of the methods used.

# 4.1. Inter-rater reliability or agreement?

Whether it is appropriate to test IRR or IRA depends on the purpose of using a second rater, which differsfor formative and summative analysis. In fomatie tges, the purposeis to detmin the catgories, ensuring that they are clearly defined, distinct, and

sufficient to classfy all the units. Thus, IR, which ensures the categories can be distinguished, would be appropriate at this stage. Summative testing sees to ascertain whether raters give the same rating, that the units are assgned to the same categories. Thus IRA, which tests whether raters give identical ratings, would be appropriate at this stage.

# 4.2. Tests for inter-rater reliability

As described in section 2.1, IRR measures the extent of variability and error in measurement, so testing involves use of correlation which ultimately is based on analysis of variance.

# 4.2.1. Correlation

Correlation has been used by at least two authors for inter-rater analysis. Peacock (2002, 2011) used the correlation procedure in SPSS to test inter-rater agrement, which was then reported as a percent. Cotos et al. (2015) used Intraclass Corrlation Coefficient (ICC) to test all three raters together.

Correlation is basically a measure of association, not agreement. Consider a situation in which one rater's coding was consistently one number higher than the other. This would lead to perfect corrlation, although there was no agreement, making a standard Pearson's correlation misleading. To solve this problem, various versions of IC have been developed for different types of IRR (Gisev et al., 2013; McGraw & Wong, 1996).

Unfortunately, correlation-based tests are never suitable for nominal data (Tinsley & Weiss, 1975, 2000). Even if the data are coded numerically, so that calculation of a correlation is computationally possible, if they have no true numerical value, correlation has no meaning. In section 4.4 we will see that this is the case in all types of genre analysis.

# 4.2.2. Chi-square

We also found one study that used chi-quare as a test of inter-rater reliability (Amirian et al., 2008). Cohen (1960) notes that this is inappropriate, as chi-square, like correlation, is inflated by any difference from expectations, either agrement or disagreement. In particular, chi-square tets the hypothes that the proportion of units assgned to each category is equal, not agrement on individual ratings (Tinsley & Weiss, 1975) and therefore is never an appropriate test for either IRR or IRA in any field.

# 4.3. Tests for inter-rater agreement

Two main measures have ben used for IRA, percent agreement (also called percentage of agreement) and Cohen's kappa (). Since the latter has been recommended by several authors, more time will be spent describing in.

# 4.3.1. Kappa

Cohen's kappa () was irst proposed in 1960 as  measure of inter-rater reliabilit for psychological asssment or, more generall, nominal categorical data (Cohen, 1960). $\kappa$ is a statistical measure of the agreement in assignment of units to categories in excess of what would be expected by chance, based on each rater's tendency to assign units to each category.

To illstrate, consider a situation where two raters independently asigned 100 units to one of two categories. Table 2a indicates the number of units assigned to each category by each rater. 60 units were judged by both raters to belong to category 1, and 20 units to category 2, giving an observed total percent agreement of $8 0 \%$ However, the totals show that rater 2 assigned more units to category 1. As shown in Table 2b, based on their overall preference for each category, we would expect 49 units to be categorized by both in category 1 $( 0 . 6 5 ^ { * } 0 . 7 5 ^ { * } 1 0 0 )$ , and 9 in category 2, or an expected agreement of $5 8 \%$ by chance alone. For this simple case, kappa is calculated as $\frac { f _ { o } - f _ { e } } { N - f _ { e } } ;$ where $f _ { o }$ and $f _ { e }$ are the observed and expected frequencies of agreement, respectively, and N is the total number of units, or (60+20)-(49+9) $\begin{array} { r } { \frac { ( 6 0 + 2 0 ) - ( 4 9 + 9 ) } { 1 0 0 - ( 4 9 + 9 ) } = \frac { 8 0 - 5 8 } { 1 0 0 - 5 8 } = 0 . 5 2 . } \end{array}$ Kappa is often interpreted according to the scale proposed by Landis and Koch (1977), according to which this would be classified as moderate agreement.

Table 2 Example of calculation of Cohen's kappa.   

<html><body><table><tr><td colspan="6">2a. Observed agreement and (disagreement), based on the ratings assigned to each unit by each rater.</td></tr><tr><td colspan="6">Rater 1</td></tr><tr><td rowspan="5">Rater 2</td><td>Category</td><td></td><td></td><td>2</td><td>Total</td></tr><tr><td>1</td><td></td><td>1 60</td><td>(15)</td><td></td></tr><tr><td>2</td><td></td><td>(5)</td><td>20</td><td>75 25</td></tr><tr><td>Total</td><td></td><td>65</td><td>35</td><td>100</td></tr><tr><td colspan="5">2b. Expected agreement and (disagreement) by chance, based on the proportion of units ssigned to each category by each rater.</td></tr><tr><td colspan="6"></td></tr><tr><td rowspan="4">Rater 2</td><td></td><td>Rater 1</td><td></td><td></td><td></td></tr><tr><td>Category</td><td>1</td><td>2</td><td>Total</td><td></td></tr><tr><td></td><td>49</td><td>(26)</td><td>75</td><td></td></tr><tr><td>1 2 Total</td><td>(16) 65</td><td>9 35</td><td>25 100</td><td></td></tr></table></body></html>

k requires that there be only two raters, who both rate every unit. If there are more than two raters, but each unit has been rated by the same number of raters, Fleiss kappa may be appropriate (Fleisset al., 1969). This could include situations such as raters from different subject areas, each serving as a second rater for papers from their respective fields.

Cohen's kappa was also expanded to a weighted kappa (Cohen, 1968). This can be used for ordinal scales and to test accuracy as well a reliailit (precision), ut require  methd for a prior stting weights, suchas dstance n fixed rdinal scale. Weights must be decided and agreed on by specialists in each field, as there is no statistically rigorous way to assign them.

For all measures of $\mathbf { \kappa } _ { \kappa }$ the units must be predetermined, fixed, independent items, assigned to predetermined, fixed, independent, mutually exclusive, exhaustive nominal categories.

# 4.3.2. Percent agreement

Percent agreement, as the name implies, isa simple measure of the units for which there is agreement expressed as a percent of the total number of units. It i a decritie et with o predctie aility. That may not be  problem for IRA, sine grement i a measure of the particular raters, unit, and categories used, and therefore needs to be determined separately for each study (Gisev et al., 2013).

A more important question is whether the raters are likely to agree on a rating by chance, the issue addressed by kappa. Although this may be an issue in diagnosi of psychological disorders (Tinsley & Weiss 1975), i seems to us that random agreement should be less of a problem in move analysis As wil be iscussed in 4.4.3, distinctions between most categories are clear enough, at least at the level of move, that it seems unlikely that rater preference would greatly affect ratings.

Different methods of genre analysis involvecoding the document at diferent levels. A summary of the methods is shown in Table 3, for summative coding. For formative coding, most values would be identical, except that at this stage the categories are neither predetermined nor fixed for any analysis method.

For some methods, the units and categories would permit test of IRA using kappa, but i all cases the categories are nominal. Ever though they may be assgned numbers for ease of coding, the numbers have no numerical value. As mentioned above, this precludes al correlation-based tests of IRR.

Since inter-rater measures have most often been used in move analysis, we will irst look at that in detail before briefly examining how other methods differ. Since $\kappa$ has frequently been used and recommended, we will specifically consider whether the requirements for its use have been met.

# 4.4.1. Move analysis: raters

Usually in the genre analys literature there are two raters, whose role are clearly specified. Where there have been more than two raters, either the purpose has been to discuss until consensus was reached (Chang & Kuo, 2011; Cotos et al., 2015; Shi & Wannaruk. 2014), or they have rated different portions of the corus relat to their pcialties (Maswana t al., 2015). In case ike the later, if k is used, Fleiss' $\kappa$ would be the appropriate choice.

# 4.4.2. Move analysis: units

In sharp contrast to thestuation for raters, the units based on which an inter-rater measure was calculated have almost never been specified, and must be inferred from knowledge of the genre analysis procedure. In others, the computer software used may have influenced the results.

For many statistical procedures, including , the units must be predetermined, fixed, and independent. The three requirements are closely related, and antithetical to the way move analysis is normally carried out.

In genre analysis, standard procedure is for raters (coders) to determine the beginning and end of each move and step. For example. as Kanoksilapatham (2005, p. 272, emphasis added) stated, Inter-coder reliability was conducted to demonstrate that a unit of text can be defined in such a way that diferent individuals can demarcate the boundary of units at a sufficiently high level of agrement."

When raters did not agree on a boundary, different methods have been used to calculate $\kappa ,$ although these have not been specified in the articles. In one method, if the first ratr coded two phrases in a sentence as moves 1 and 2, but the second rater coded the whole sentence as move 2, this would be considered two moves with one diffrence between coders (Kanoksilapatham, personal communication). Altenatively, the sements coded by the fist rater can be used as a base, with the number of place t which ither the move assinment or break point differ from that considered disagreement (Parkinson, personal communication). However, in either method, neither the number nor identity of the units were predetermined, failing to meet the requirements for s.

Table 3 Different methods of genre analysis, and properties of their units and categories in summative coding   

<html><body><table><tr><td>Analysis method</td><td>Coding unit</td><td colspan="3">Unita</td><td colspan="6">Categoriesb</td><td></td></tr><tr><td></td><td></td><td></td><td>F</td><td></td><td>Categories</td><td>N</td><td>P</td><td>F</td><td></td><td>M</td><td>E</td></tr><tr><td>Macrostructure</td><td> Section headings</td><td>P p</td><td>F</td><td>1</td><td>Groups</td><td>N</td><td>P</td><td>F</td><td>1</td><td>m</td><td>E</td></tr><tr><td>Component</td><td>Paragraphs</td><td>p</td><td>F</td><td></td><td>Components</td><td>N</td><td>P</td><td>F</td><td>-</td><td>-</td><td>E</td></tr><tr><td>Move</td><td>Sentence, clause</td><td>-</td><td>-</td><td></td><td>Move, step</td><td>N</td><td>P</td><td>F</td><td></td><td>m</td><td>E</td></tr><tr><td>Corpus</td><td>Lexical bundle</td><td>p</td><td>F</td><td>1</td><td>Groups</td><td>N</td><td>P</td><td>F</td><td></td><td>M</td><td>E</td></tr></table></body></html>

a Unit: $\mathbf { P } =$ predetermined, $\mathbf { F } =$ fixed, $\mathrm { I } =$ independent. b Categories: $\Nu =$ nominal, $\mathbf { P } =$ predetermined, $\mathbf { F } =$ fixed, $\mathrm { I } =$ independent, $\mathbf { M } =$ mutually exclusive, $\mathrm { E = }$ exhaustive.

There are several ways the units could be predetermined and fixed before rating. One logical fixed unit would be the sentence. In earlier work, the sentence was often taken as the basic element for coding moves. Thus, Swales (1990, p. 143) and Holmes (1997, p. 326) assigned moves and steps to sentences in the examples shown. However, in other situations a single sentence was divided into two moves (Swales, 190, p. 159). Moreover, the sentences were not units, with each assgned to a move. Rather, the raters sought to identify boundary indicators for each move, of which sentences or clauses might be one indicator. According to Connor and Mauranen (199, p. 52), "Boundaries were marked with linguistic changes of various kinds, and, i particular, simultaneous changes in more than one indicator."

In more rcent studies, sentences have frequently been divided, at least at the level of step analysis. For example, Lim (2010, p. 283, emphasis added) stated, Hence, a step constituting a segment might consist of a main clause or even several sentences insofar as its occurrence was not interrupted by any other rhetoricl te. Thus, several sentences might be included in one segment (unit, but at other times the unit might be only a clause. This highlights another problem, that sequential sentences would not betruly independent even if sentences were used as the unit, as it is likely that in some cases subsequent sentences would be part of the same move. Similarly, since moves are listed in th order in which they tend toccur, if one unit is codd move 1, the probability is higher that the next unit will be coded move 2 than move 3. So again, the units are not independent, violating another requirement for k.

Another posible way of ensuring the units were predetermined and fixed would be for one rater to firs fix the unit boundaries before asking the second rater to assn each unit to a category. Whilethis oles the first two requirements it should be clear that ny such assignment of boundaries would ental a judgement about the move or ste contained within each, such that the second rating would not be independent of the first.

Another consideration is that the unit used for rating must be the same as the unit used in the tet. For example, if unit designation is by moves, which may include everal sentences, it i inappropriate to then calculate  based on sentences. In an extreme case, an online program, NVivo10 (QSR International), used by Moreno and Swales (2018), calculates  based on the number of characters in each section of et. However, categories are not assigned to characters, but o larger sements. An example of the result o lulation based on different units is shown in the illustrative example in section 4.5.

In summary, although it is potentially possble to designate units such that they meet the requirement by using fixed, pre. determined units (eg., sentences), this would require modification of the normal move analysis procedure. Nevertheles, it is not clear that the requirement for independence could be met for analysis of complete document, due to the sequential nature f the units and ordered move designations, both of which make certain categories more likely for subsequent units.

# 4.4.3. Move analysis: categories

In move analysis articles, the categories used (moves and steps) are always clearly stated, since this is the focus of the article, although they may not be specified in the context of the inter-rater tet. As discussed above, inter-rater analysis has been used during both formative and summative coding. During formative evaluation the categories are not necessaril predetermined nor fixed, either in number or description.

Of greater concern is whether the categories can truly be considered nominal and non-ordered. As Cohen notes in his description of k, An implication of the lack f order of the categories needs to be pointed up. Unlike stronger measurement situations, discrepancies between paired judgmens are treated as equal to each ther. (1960, . 38, emphasis added). Rcall that  tests the level f agrement in excess of what would be expected based on chance. While the descriptive categories of moves and steps initially appear tobear ittle or no relation to one another, at a deeper lel some categories are clearly more closely related. Rater disagrement between more similar categories would be more likely than between categories that are more distinct.

As an example, let us consider categories from the Introduction, using the moves and steps from Cortes (2013) adapted from two models presented by Swales (1990, 2004), as shown in Table 4.

Table 4 Moves and steps in research article introductions (from Cortes, 2013).   

<html><body><table><tr><td colspan="2">Move 1 Establishing a territory</td></tr><tr><td rowspan="8"></td><td>Step 1 Claiming centrality</td></tr><tr><td>Step 2 Making topic generalization/s</td></tr><tr><td>Step 3 Reviewing items of previous literature</td></tr><tr><td></td></tr><tr><td>Step 1A Indicating a gap or</td></tr><tr><td>Step 1B Adding to what is known</td></tr><tr><td>Step 2 Presenting positive justification</td></tr><tr><td colspan="2">Move 3 Presenting the present work</td></tr><tr><td rowspan="8"></td><td>Step 1 Announcing present research descriptively and/or purposively</td></tr><tr><td>Step 2 Presenting research questions or hypotheses.</td></tr><tr><td>Step 3 Definitional clarifications</td></tr><tr><td>Step 4 Summarizing methods</td></tr><tr><td>Step 5 Announcing principal outcomes</td></tr><tr><td>Step 6 Stating the value of the present research.</td></tr><tr><td>Step 7 Outlining the structure of the paper</td></tr><tr><td></td></tr></table></body></html>

Since the moves and stes frequetly occur in this order, or somerecursive version of it they arenot truly independent, as dcussed above with regard to units. Moreover, it should be intutively obvious that Moves 1 and 2, both of which discuss previous work, are more similar to one another than either is to Move 3, which presents the focus of the present work. While trained raters might possibly disagree n whether t call a ertain statement Move 1 or 2, it s far les likely that there would be a disagrement about Move 3. At the step level, it should also be obvious that Move 1, Step 3, citig previous work, and Move 3, Step 7, organzation, are quite distinct from the other steps in those moves, and highly unlikely to be coded as anything else, while some other steps in those moves are more similar. The same sitation exiss with regard to cateorie i ther dvisions. Thus, the catories have natural order, but are not part of a continuum. This i a distinct category from either nominal or ordinal, with logical but not numerical continuity. To the best of our knowledge, there is no separate name nor have statistical tests been developed specifically for logically sequential categories.

Even if a way were devised to overcome the problem of independence, it is also unlikely that the requirement of mutual exclusivity could be met. Designating units smaller than a sentence would be problematic, but one sentence frequently contains more than one move or step. For example, acording to otos et al., The unit of annotation was defined as  functional segment of text, which could be eiter a full sentence or a phrase within a sentence.. In other words, each sentence was given a move/step code indicative of the primary function of the sentence, and if a semen f the same sentence carried n adtonal functiol role it was coded with a secondary move/step tag." (2017, p. 95, emphasis added). Thus, if sentences were chosen as a predetermined unit, some units would be placed in more than one category, showing that the categories cannot be assigned to the units in a mutually exclusive manner. Furthermore, since it is common during coding to find items that do not fit any of the category designations, the categories may not be exhaustive.

It would be theoretically possible to resolve this problem using a weighted k. However, this require a priori assignment of weights to each cellin the matrix set prior to cllction of data (Cohen, 1968). While itis obvious that Moves 1 and 2 are more similar to one another than to Move 3, what numerical value should be ssigned to compensate for that? Weights can relatively easily beassigned to categories on an ordinal scale, based on proximity, but moves and steps are neither ordinal nor completely unordered, making the setting of weights subjective.

In summary, even if we could decide on predetermined, fixed, independent units, the categories are problematic, as they are semi ordered,differences in rating are not equall likely, and in practice it is hard to assgn any reasonable unit to a single category. This means that, again, the requirements for statistical testing may not be met.

# 4.4.4. Other types of genre analysis

Unlike move analysis in macrostructure analysis the section headings of a text, which are predetermined, fixed, and independent units, are assgned to predetermined, fixed groups such as conventional headings (e.g., Introduction, Methods, Results, Discussion, Conclusion), variants on those headings (e.g., Experimental methods), and content headings specific to the research (ang & Allison, 2004, p. 270). Sometimes other groups of functional headings common to afeld are added, like Background or Theoretical model Jin et al., 2020, p. 233). These categories are independent, and intended tobe mutually exclusive and exhaustive. Thus, the requirements for use of $\kappa$ can probably be met in macrostructure studies.

Corpus analysis also is likely to meet al the requirements for use of k. For example, Cortes (2013) extracted all lexical bundes meeting certain requirements and assigned them to moves. The units, selected by the computer for matching certain criteria, and the categories, adapted from previous research, were both predetermined, fixed, and independent, and the categories intended to be mutually exclusive and exhaustive. Similarl, Hyland (199 used a computer program to select stance markers, which were then rated by two raters.

Component analysis, a recently proposed method of analysis (Rau, 2020, 2021), codes at the paragraph level, so the units are predetermined and fixed. However, like move analysis, the sequential nature of both the paragraphs and components means they are not independent. Furthermore, since one paragraph can be asigned to more than one component, the categories are not mutually exclusive. Thus, $\kappa$ is again unsuitable.

In summary, if a research method mets all the requirements for units and categories listed in section 2.1.1, k can be used.

Table 5 Example of hypothetical ratings given by two raters.   

<html><body><table><tr><td rowspan="2">Sentence</td><td rowspan="2">Words</td><td rowspan="2">Characters</td><td colspan="2"> Move assigned</td></tr><tr><td>Rater 1</td><td>Rater 2</td></tr><tr><td>1</td><td>28</td><td>189</td><td>1</td><td>1</td></tr><tr><td>2</td><td>26</td><td>166</td><td>1</td><td>1</td></tr><tr><td>3</td><td>41</td><td>237</td><td>1</td><td>1</td></tr><tr><td>4</td><td>44</td><td>286</td><td>1</td><td>1</td></tr><tr><td>5</td><td>15</td><td>99</td><td>1</td><td>1</td></tr><tr><td>6</td><td>13</td><td>61</td><td>1</td><td>1</td></tr><tr><td>7</td><td>29</td><td>179</td><td>1</td><td>?</td></tr><tr><td>8</td><td>24</td><td>164</td><td>2</td><td>2</td></tr><tr><td>9</td><td>16</td><td>86</td><td>2</td><td>2</td></tr><tr><td>10</td><td>20</td><td>142</td><td>2</td><td>2</td></tr><tr><td>11</td><td>30</td><td>176</td><td>2</td><td>2</td></tr><tr><td>12</td><td>33</td><td>203</td><td>2</td><td>2</td></tr><tr><td>13</td><td>27</td><td>156</td><td>3</td><td>3</td></tr><tr><td>14</td><td>32</td><td>186</td><td>3</td><td>3</td></tr></table></body></html>

Otherwise, $\kappa$ is not a suitable measure. Correlation is only suitable for ordinal categories, and thus cannot be used for any of the genre analysis methods discussed here.

# 4.5. Illustrative example

In this section we provide a numerical example to show how choice of unit affects the calculation of both $\kappa$ and percent agreement. This example is based on a simple situation in which two raters do a move analysis of a research article Introduction, but one tran. sitional sentence is asiged to different moves by the two raters. The data on sentences, words, and character re take from an actual Introduction (Swales, 1990, p. 143). Hypothetical ratings are presented in Table 5, with a single disagreement highlighted in bold, although disagreement in rating would be highly unlikely for that clear example.

First let us examine what happens when $\kappa$ is calculated for the same data based on four different units: move, sentence, word, and character. For each unit, kappa was calculated as described in section 4.3.1.

As discussed in the results, in this situation $\kappa$ could not properly be calculated using moves as a unit, as the coding unit borders differ. However, for purposes of ilustration we here assume that there are four coding units, designated in a post-hoc manner based on the overlapping move boundaries. The first unit comprising sentences 1-6, is coded move 1 by both raters. The last two, sentences 8-12 and 13-14, are coded move 2 and 3, respectively, by both raters. Only the second unit, sentence 7, is coded diferently. For this situation, $\kappa = 0 . 6 4$ , indicating "substantial agreement" (Landis & Koch, 1977).

If sentences are taken as the coding unit, $\kappa$ increases substantially, to 0.88, "almost perfect agreement.' As discussed in section 4, this is the only unit of the four that could realisticall be used as a coding unit in genre analysis, lthough other problems remain with meeting the requirements for use of $\kappa _ { \mathrm { \Omega } }$ Doing the same calculation based on number of words or characters gives essentially the same values, 0.88 and 0.87, respectively.

However, with both words and characters the length of the sentence on which therei disagreement strongly afects the value of $\kappa$ For example, in Table 5 the sentence in question was cose to the mean for both words and characters. If we exchange the length of sentences 6 and 7, as shown in Table 6, and recalculate x, we find that it jumps to 0.94 and 0.96 for sentences and characters, respectively. Thus, a program like NVivo10, which calculates $\kappa$ based on the number of characters in a document segment, would tend to overestimate $\kappa$ if the segments of disagreement are short.

The vast difference in the calculated values shows the importance of clearly stating the unit used for calculating kappa. Since the units used have not bee clearly designated in genre analysis studies that have rported , i is impossible to know how o interpret the results. If moves were assigned to multiple sentences, but sentences were then used as the unit for calculation of $\kappa _ { i }$ the effect would probably be to overstate the actual agreement. This is of course, a secondary problem compared to not meeting the requirements for the test, but does point to the importance of clearly specifying units any time a statistical test of any sort is conducted.

With percent agreement we find a similar situation. In the example above, if the unit was moves, how many moves should the calculation be based on? Both raters ssigned three moves, ut the break point between 1 and 2 differed. Should we therefore say that since neither move 1 nor move 2 are identical, there was agreement only for move 3 $( 3 3 \% )$ ? Or should we say that since there was one disagreement in location of a boundary, there was agreement for 2/3 of the moves $( 6 7 \% ) ?$ Or, should we consider, as we did above, that there are four possible moves, and the raters agreed on three of those $( 7 5 \% ) \div$ If instead we use sentences as a fixed unit, the agreement jumps to 13/14, or $9 3 \%$ but if sentences were not really the unit on which the categories were assigned, this vastly overstates the agreement.

# 5. Conclusion

In formative stages of genre analysis, a test of IR would be desirable, but with nominal data all correlation-based measures are meaningless and invalid. Thus, only measures of IRA are possible.

For $\kappa$ to be valid, requirements must be met for the raters, units, and categories. However, in move analysis, raters determine the boundaries of moves and steps, violating the requirement for predetermined, fixed units. Moreover, as sentences in a document are sequential and sentences in a paragraph are related to one topic, the unit ae not independent Furthermore, since moves and steps are a semi-ordered list, they also fail to meet the requirement of independent categories. While weighted $\kappa$ would resolve some of these issues, there would be no way to objectively set the weights, negating the benefit. Many of these same issues would be true for component analysis.

Therefore, $\kappa$ is an invalid measure of IRA for move analysis or component analysis. Nevertheless, it could be appropriate for macrostructure and corpus analysis, where distinct units are identified prior to rating, but the units and categories would need to be clearly specified.

Table 6 Example of the effect of segment length on the value of k   

<html><body><table><tr><td>Sentence number</td><td>Words</td><td>Characters</td><td colspan="2"> Move assigned</td></tr><tr><td></td><td></td><td></td><td>Rater 1</td><td>Rater 2</td></tr><tr><td>6</td><td>29</td><td>179</td><td>1</td><td>1</td></tr><tr><td>7</td><td>13</td><td>61</td><td>1</td><td>2</td></tr></table></body></html>

Moreover, it sems unlikely that it would be possible to develop any more rigorous statistical test when the units are neither fixed predetermined, nor truly independent. Thus, for move analysis percnt agrement is the only potentiall valid measure of IRA. At least at the level of moves, i sems unlikely that rater preference for acategory would lead to random agreement, but more reearch would be necessary to confirm this.

No matter what statistical measure is used, it is imperative to state clearly what the units were and how the measure of agreement was calculated to correctly interpret the reults and compare betwee studies. Researchers must also nsure that calculations are based on the same units on which the ratings were done, which is not true for some online programs.

Further study would be required to extend these results to other areas of linguistics that use inter-rater reliability or agrement, but the principles described herein should be of assistance. For any studies measuring nominal data, the requirements listed herein for raters, units, and categories must be met for $\kappa$ to be valid.

# Funding

This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.

# Declarations of interest

None.

# References

Am (1), 39-63. Purposes, 11(2), 134-144.   
ag  1   r   2234   
Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37-46.   
Cohen, J. (198.ghe kap  gmt wh prsin for cet r partict Pl i 704, 21320   
Conor, U., &ranen,  (199). Lgstic analysis of gnt proals: ropn unon rrh gnts. ishfor ific Pes, 18(1), 47-62.   
Core . 213 t t  a   .   ore 1 (1), 33-43.   
Cotos  f   2015. g a o/iin  th o y Journal of English for Academic Purposes, 19, 52-72.   
Coos  f i. 2017 e/  r  tin ri an rt. gsh  i e 46 9010.   
Fleiss, J L, ohen, J, Everitt . S. (1969). Larg sample tandard errors of kappa and weighted kappa. Pychologicl Buletin, 72(5,323.   
Gis,     13). rar   rrat   ,  itio.  nd Administrative Pharmacy, 9(3), 330-338.   
Hmes   s  f  t Specific Purposes, 16(4), 321-337.   
Hyland, K. (1998). Persuasion and context: The pragmatics of academic metadiscourse. Jounal of Pragmatics, 30(4), 437-455.   
Hlad1         1) London: Longman.   
Ji, i   20  th  r  . ion  , 3 27-243.   
Kanoksilapatham, B. (2005). Rhetorical structure of biochemistry research articles. English for Specific Purposes, 24(3), 269-292.   
nksia . 007). rica me  r h tic. ir, .r,   .,  n  mve  ous analysis to describe discourse structure (Vol. 28, pp. 73-119). John Benjamins.   
ank12)  t  i  , 126 English for Specific Purposes, 37, 74-86.   
Landis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics, 33, 159-174.   
Li .010n  n  t  t   f  o Purposes, 9(4), 280-294.   
Lin, L, Eans, . (2012). ttrl aten in mprica ch artice: cossiiny sty. ngish for fic es 31(3), 150-160   
wana , 015a f  i  ht  , 1-11.   
McGraw, K. O., & Wong, S P. (1996). Forming inferences about some intraclas correlation cofcients. Psychological Metods, 1(1), 30.   
reno,  I,  we, J. (018. th moe ais  ods brig th fctiofo p. gis for iic e, 50, 4063.   
Parkinson, J. (2017). The student laboratory report genre: A genre analysis. English for Specific Purposes, 45, 1-13.   
Peacock, M. (2002). Communicative moves in the discussion section of research articles. System, 30(4), 479-497.   
Peacock, M. (2011). The structure of the methods section in research articles across eight isiplines. Aian ESP Jounal, 7(2), 99-124.   
SR Internatiol. Hw is th apa ficien callatd hps:/help-10.inttioa.com/kto/procre/un aoding omparion query. htm#MiniTOCBookMark5. Accessed 14 January 2021.   
Rau, G. (2020). Writing for engineering and science students: Staking your claim. London: Routledge.   
Ra 1    -   s   2, 46-57. https://doi.org/10.1016/j.esp.2020.12.001   
Shi, H, & Wannaruk, A. (2014). Rhetorica structure of research articles in Agricultural Science. English Language Teaching, 7(8), 1-13.   
Swales, J. M. (1990). Genre analysis: English in academic and research settings. Cambridge: Cambridge University Press.   
Swales, J. M. (2004). Research genres: Exploration and analysis. Cambridge University Press.   
Tinsley, H. E, & Weis, D. J. (1975). Inerrater reliability and agrement of subjective judgments. Joundl of Conseling Psychology, 224), 358.   
se, H , .J.0 rar reaty a    e,   ., o f  e istie n mathematical modeling (pp. 95-123). San Diego: Academic press.   
Yng  io (.h np st tti ih for i  3(), 264279

Gerld    g nginering rrh artice. His Ph.D. in pln rding disproved a sttistcl techique that had ben used for 40 yr to detine ee numer.

Yu-Shan Sh ar in tht oftic nahrit, , . h h stic romh rsit Wisconsin, Madison. His research interests lie in classification and regression trees and statistical computing.