# Data-driven learning with younger learners: exploring corpus-assisted development of the passive voice for science writing with female secondary school students

Peter Crosthwaite & Brett Steeples

To cite this article: Peter Crosthwaite & Brett Steeples (2024) Data-driven learning with younger learners: exploring corpus-assisted development of the passive voice for science writing with female secondary school students, Computer Assisted Language Learning, 37:5-6, 1166-1197, DOI: 10.1080/09588221.2022.2068615

To link to this article: https://doi.org/10.1080/09588221.2022.2068615

# Data-driven learning with younger learners: exploring corpus-assisted development of the passive voice for science writing with female secondary school students

Peter Crosthwaitea $\textcircled{1}$ and Brett Steeplesb

a School of Languages and Cultures, University of Queensland, St. Lucia, QLD, Australia; bIpswich Girls’ Grammar School, Ipswich, QLD, Australia

# ABSTRACT

Corpus-based approaches to language and literacy education, commonly known as data-driven learning (DDL), are increasing in prominence. However, the vast majority of DDL interventions involve adult tertiary level learners, leaving a dire need for comprehensive DDL studies for secondary education. The present study reports on a half-year DDL experiment conducted at an all-girls secondary school in Australia, focusing on the development of receptive and productive knowledge of passive voice constructions used when writing scientific research reports for a physical science class. Pre/post-tests were conducted testing learners’ recep tive knowledge and productive use of the passive, alongside data on learners’ autonomous use of corpora within a written research report. Learners’ perceptions of corpora and DDL were also collected through questionnaire survey and interview data taken both immediately post-training and three months after training. The results suggest learners’ corpus consultation was effective in improving use of the passive voice for science writing with pre-tertiary learners, although clear preferences for (and criticisms of ) certain corpus tools, functions and usage was apparent, and continued uptake post-training was relatively weak. Generally however, the implications of these findings paint a positive picture of what is possible regarding DDL with younger learners, and provide a model of how a DDL intervention with younger learners can be successfully managed and integrated in a context where secondary content teachers, rather than solely the applied linguist, can be the main stakeholders in a DDL intervention.

# KEYWORDS

Data-driven learning; DDL; secondary education; passive voice; corpus-based language teaching; English for specific purposes

# 1.  Introduction

The use of corpora for language teaching and learning (also known as data-driven learning, DDL) has been steadily increasing over the past decade, with a recent review of the field spanning over 400 representative studies (Boulton & Vyatkina, 2021). Meta-analyses of the field suggest the oft-purported – and now increasingly empirically proven  – benefits of corpus consultation include second language (L2) development of vocabulary (e.g. Lee et  al., 2019), English for academic purposes (e.g. Chen & Flowerdew, 2018) and L2 writing (e.g. Pérez-Paredes, 2019). These results are claimed to be achieved through the activation of individual inquiry-based and cognitive processes under a constructivist paradigm of learning (Cobb, 1999), in conjunction with socio-cultural learning opportunities during scaffolded student-teacher or peer-peer interactions (O’Keeffe, 2020) either in class or through interaction within blended/online learning environments (Crosthwaite, 2020; Mishan, 2013).

However, in the vast majority of cases, these results have been achieved with adult second language learners, learning (primarily academic) English, at university. While this is common in much research in computer-assisted language learning (CALL) (Gillespie, 2020), it is unfortunate that whole educational sectors including private education, learning in professional contexts, and pre-tertiary education have yet to receive much attention from DDL researchers. Numerous reasons for this situation have been suggested in the CALL and DDL literature, including ethical and bureaucratic complexities involved with gaining access to younger learners (Brown et  al., 2020); the need to convince gatekeeping teachers of the value of DDL, both for the teachers themselves as well as their learners (Tondeur et  al., 2019); (perceived) clashes with existing approaches to language and literacy education (Bednarek et  al., 2020); the workload demands placed upon pre-tertiary teachers leaving little room for professional development in or integration of CALL (Park & Son, 2020); and a general lack of funding for large-scale exploratory research into DDL to be carried out in pre-tertiary settings.

This leaves corpus linguistics and DDL researchers having to continue to ‘spread the word’ as ‘corpus missionaries’ (Römer, 2009), doing our best to promote the value of DDL to those outside of the academic bubble, which often results in the applied linguist being the main stakeholder rather than the teachers or institution involved (Pérez-Paredes, 2019). That said, we are now beginning to see the results of these endeavours, with a marked increase in studies on DDL for younger learners now entering the literature over the last few years (e.g. Fang et  al., 2021; Karras, 2016; Saeedakhtar et  al., 2020).

While an encouraging development, most of these studies still focus on the acquisition of general English by L2 learners in EFL contexts, leaving studies exploring the effectiveness of DDL by first language users of English or speakers of English as an additional language/dialect (EAL/D) in mainstream education currently lacking. Studies also presenting follow-up data on DDL use beyond initial DDL training are also sorely needed (Crosthwaite & Boulton, 2022; Boulton & Cobb, 2017).

In response, the present study reports on a half-year DDL experiment conducted in an all-girls secondary school in Australia, focusing on the development of receptive and productive knowledge of passive voice constructions with a mixed class of L1 monolinguals and EAL/D speakers. The DDL intervention was intended to help boost students’ knowledge and use of the passive voice for writing scientific research reports based on observed science experiments as part of an elective Physical Science class. We present data on pre/post-tests of learners’ receptive knowledge and productive use of the passive, data on learners’ autonomous use of corpora while drafting and revising a written research report, and their perceptions of corpora and DDL within questionnaire survey and interview data taken both immediately post-training and three months after training. The study’s research questions are as follows:

RQ1: Does DDL training result in improved receptive knowledge and productive use of the passive voice for secondary school age learners in pre-/post-test experimental conditions?

RQ2: How do secondary school students autonomously use corpus tools to make revisions to a written research report post DDL-training?

RQ3: What are the perceptions of secondary school students regarding DDL following DDL training and three months following training?

# 2.  DDL and younger learners: the story so far

Today’s primary and secondary school-age learners live in a rapidly evolving digital world, a situation exacerbated by COVID-19. This context results in increasing demand for the requisite abilities and skills needed to adapt to, utilise and become proficient in new technological innovations to stay ahead of the curve (Knobel & Lankshear, 2010). Following constructivist accounts of (language) learning, children have the opportunity to actively construct and enrich their own knowledge, skills and abilities through the use of technology, which (if selected and used appropriately) can provide learner-focused and learner-initiated language learning experiences that can have a significant impact on learners’ linguistic and cognitive development (Kewalramani et  al., 2020), as well as improve digital literacies and (language) data analysis skills.

However, there is a gap between the purported benefits of interacting with language-related applications and current practice (Chambers, 2019), with many learners and their teachers unaware of the myriad of CALL tools and applications available to them, and over-relying on passive rather than active use of popular existing applications including Google, (online) dictionaries, and translation websites to resolve language-related issues, with potentially fewer opportunities for meaningful and lasting learning gains (Pritchard, 2008).

In response, researchers working on data-driven learning (DDL) have sought to incorporate language corpora into classroom practice, either in the form of ‘hands-off ’ tasks involving the analysis of pre-selected corpus data, or (increasingly) ‘hands-on’ activities involving student-led corpus query, analysis and consolidation, with ‘every student a Sherlock Holmes’ (Johns, 1997, p. 101). DDL is claimed to enshrine active (rather than passive), constructivist, student-led learning, in that learners ‘learn best when they discover or can be led to discover themselves’ (Cobb, 1999, p. 15).

While actual empirical studies directly testing whether DDL leads to better learning in general are currently few in number (Crosthwaite & Boulton, 2022), a vast range of research has reported specific learning gains across a variety of target language forms and across a range of contexts and languages, with generally positive perceptions from students and teachers alike (Boulton & Cobb, 2017; Boulton & Vyatkina, 2021). When DDL is conducted in classroom settings, learners benefit from teacher-scaffolded corpus consultation activities together with the sociocultural benefits of peer-to-peer discussion in the form of increased language-related episodes during corpus consultation, analysis and interpretation (O’Keeffe, 2020). At least, the above claims have been supported in numerous studies on L2 learners in tertiary settings.

How might pre-tertiary learners be different to their adult counterparts in their ability to take part in DDL, and how might they go about it? We know that many children can be considered as ‘digital natives’ (Prensky, 2001), having grown up surrounded by internet-connected tablet devices and mobile phones, and may have already used a range of language-related applications including online games, searching for information via search engines, and communicating in real-time via chat or video-conferencing software. Therefore, they may already be used to ‘doing DDL’, in a sense, in terms of their ability to take in multiple sources of (digital) information and come to data-driven conclusions based on the task at hand (Boulton, 2015). However, to what extent that process leads to language acquisition is seldom tested directly, and, in almost all cases, a ‘corpus tool’ or concordances are nowhere to be seen. Younger learners are also in the process of developing L1 literacy, and so sudden exposure to concordances or extensive frequency/collocation lists may be overwhelming, even when the target language is the same as that of the learner’s primary language (Sripicharn, 2010). Instead, the go-to tools for language learning for young people are Google, Youtube, online dictionaries, or translation websites. Some studies have sought to harness these applications for DDL with younger learners with some success (Boulton, 2015; Frankenberg-Garcia, 2014; Gatto, 2019; Giampieri, 2019). Yet, such experiences arguably promote a more passive, ‘in and out’ approach to consulting language data (Thompson, 2013) – one lacking in opportunities for more linguistically-oriented, focus-on-form language learning experiences that corpora and DDL have been shown to provide.

An increasing body of research has now sought to empirically test the affordances of corpus consultation and DDL with younger learners with generally positive results. Focusing on studies over the last decade, Crosthwaite and Stell (2019) used the online SketchEngine for Language Learning corpus platform (SKELL, Baisa & Suchomel, 2014) with two primary age EAL/D speakers within a private home tutoring setting, with the researcher providing targeted written corrective feedback on the students’ writing for subsequent corpus-led revision. The students held positive perceptions of SKELL for resolving vocabulary errors, and regularly consulted the corpus autonomously outside of the feedback provided by the researcher. A similar finding is presented in Kim (2019) using paper-based DDL materials within the Korean primary EFL context, with students enjoying the process of deducing ‘rules’ from patterns in the concordances, and going against their teachers’ initially negative perceptions of DDL. At the secondary level, Liontou (2019) used DDL to teach idioms to Greek EFL learners using the BYU-COCA corpus platform (Davies, 2009) together with printed extension activities involving corpus data, with success on a post-test reported following just one hour of instruction on DDL.

A number of studies have investigated the acquisition of collocational patterns with younger learners, including Boontam and Phoocharoensil (2018) with $4 ^ { \mathrm { t h } }$ grade EFL learners in Thailand, using the Graded Readers Corpus within the Compleat Lexical Tutor (Cobb & Free, 2004), as well as Özbay and Olgun (2017)’s study which found significant differences between experimental and control groups’ knowledge of L2 preposition collocations following 15 weeks of instruction for Turkish EFL learners. Saeedakhtar et  al. (2020) used DDL for verb-noun collocations with Iranian EFL learners, with students using ‘hands-on’ DDL outperforming learners working under a ‘hands-off ’ condition in delayed post-tests. Fang et al. (2021) studied collocation learning through DDL with Chinese secondary school students, reporting improvements after just three training sessions.

Soruç and Tekin (2017) explored Ugandan EFL students’ vocabulary development, comparing a paper dictionary-only control group with a group using DDL, with increased learning gains for the DDL group over the control group. Karras (2016) reported similar gains for vocabulary development for EFL learners in the Vietnamese context. Tekin and Soruç (2016)’s investigation of DDL for Turkish EFL vocabulary development revealed five common themes arising from students’ perceptions, including ‘innovative’, ‘autonomous’, ‘easy and fun’ and ‘practical’ as positive aspects of corpus use.

In terms of improving grammar, Moon and Oh (2018, p. 48) used DDL with Korean middle school EFL learners, attempting to ‘unlearn’ overgeneration of the copula before thematic verbs (e.g. ‘he is dance very well’) using DDL activities exposing learners to negative evidence of erroneous forms via the construction and analysis of a learner corpus generated from learners’ own writing.

However, in almost each case, these studies were led by applied linguists rather than interested, DDL-capable teachers, and involved EFL learners studying general English as an L2. The former is problematic in terms of Chambers’ (2019) ‘research-practice gap’ where teachers are unable to replicate DDL interventions led by trained applied linguists, while the latter is problematic in terms of DDL gaining mainstream acceptance outside of L2/EFL contexts.

# 3.  Rationale for the present study

Pre-tertiary students must quickly develop knowledge of the language features required for disciplinary literacy in terms of comprehension (through reading) and production (through writing), with both frequently requiring interaction with technology. However, in Australia, only $1 7 \%$ of school leavers have attained the highest literacy levels, with students from non-English speaking backgrounds achieving even poorer results (OECD, 2013). In particular, the number of female high school leavers entering STEM fields at tertiary level is still low in Australia and globally (Australian Government, 2021), with stereotypes and cultural norms dampening girls’ interest in STEM, alongside limited learning resources and pedagogies for improving girls’ knowledge of science disciplines within secondary curricula (Australian Government, 2019).

Many girls feel frustrated when encountering STEM language, particularly those for whom English is an additional language (Jones & Seilhamer, 2020). Secondary school teachers also lack detailed understanding of scientific language within the curriculum, and their teacher education is inadequate in supporting struggling literacy learners (Merga et  al., 2020). Teachers also lack the technological content pedagogical knowledge (TPACK, Koehler & Mishra, 2009) to develop students’ abilities to discover the language of science through corpus technology. This is exacerbated under COVID-19-induced school lockdowns and in-class social distancing at a time when education systems worldwide are unprepared for digital learning and where $2 0 \%$ of teachers worldwide cite the need for improved ICT training (OECD, 2020). Providing targeted training and support in this area through the adoption of DDL is therefore a way for both teachers and students to fight back.

# 4.  Method

# 4.1.  Context

Following a 1 day teacher training session in DDL conducted at the research site, an all-girls’ secondary school in Queensland, Australia, the researcher was approached by two teachers who were interested in pursuing a DDL-led approach to improving students’ writing of science research reports required as part of the students’ Year 12 summative assessment. The teachers had concerns that the girls suffered from a general lack of exposure to science writing, as well as lacking the means to learn about science writing via their go-to language reference resources of Google and online dictionaries.

Following negotiations about what to target, the teachers suggested developing a suite of activities around the use of the passive voice, which is very frequent in the reporting of the methods undertaken for experimental procedures that the girls would first observe in-class and subsequently write up as a research report. Teachers reported that while the students might ‘know’ the passive voice construction, they still struggled to use it on occasion, and failed to recognise its importance within key sections of the research report genre. Crucially, while the researcher was responsible for developing the materials and selecting the corpora and corpus tools to use, the teachers were at the forefront of the delivery of the DDL activities, performing many of them in-class together with the students and without the researcher present. In this way, the main stakeholders of the procedure were considered to be the subject content teachers – a rarity in DDL research (Pérez-Paredes, 2019).

# 4.2.  Participants

Thirty-one female Year 9–10 students aged between 14 and 15 years old from an all-girls secondary school agreed to join the study, taken from two separate classes taught by two separate teachers using the same curriculum and materials. Of these, 24 were monolingual English speakers and 7 were EAL/D speakers with upper intermediate to advanced levels of English. Seven out of 31 students reported previously hearing about a corpus prior to the study but not using them for DDL, with the others never having heard of a corpus before.

# 4.3.  Materials

# 4.3.1.  Tests

Three tests of the passive voice were employed in this study, each conducted and hosted on Office365 Forms online, with each taken from Spada et  al. (2015) as available on the IRIS repository.

The first tested receptive knowledge of the grammaticality of passive voice constructions across 42 items (36 passive constructions and 6 distractors). For each question, students were asked whether they thought the given sentence was grammatically accurate, selecting ‘correct’, ‘incorrect’ or ‘unsure’.

The second tested students’ ability to identify and correct errors within passive voice constructions across 24 items (18 targeting the passive and 6 distractors). Students were told each sentence contained one error, and to then identify the error and re-write the sentence accurately. It was expected that given the mostly L1 English cohort, their metalinguistic knowledge of the passive voice construction would already be high, thus the purpose of these two tests was to confirm this knowledge empirically as well as to determine the level of knowledge of the EAL/D speakers.

The third tested students’ ability to produce passive voice constructions via an elicited picture sequence description task, again taken from Spada et  al. (2015). The sequence describes a package sent from a grandfather in Japan to a boy in Canada. Students were shown the whole 15-image sequence as one large image first, then scrolled down to a larger version of each individual image where they were asked ‘what happened to the package?’ in each case, with students writing a sentence requiring the passive voice (with the exception of only one of the pictures where the students are asked to explain what the boy did in active voice after receiving the ‘package’). Each submitted sentence for each prompt written appropriately in the passive voice (e.g. ‘The package was taken to the post office’) scored one point. Sentences written in active voice (e.g. ‘Kenji’s grandpa had to take the package to the Post Office’) or deemed erroneous due to lexical or syntactic inconsistency with the passive (e.g. ‘The package been sent from home to the post office) scored zero points. The researcher checked each individual sentence according to these criteria.

None of the tests were timed. Crucially, the learners were permitted to use any corpus tool on any of the post-tests, if they so wished. For each item on the three post-tests, students could check a box to indicate whether they had used a corpus tool to help formulate their answer. In this respect, the pre/post-test setup in this study is not strictly quasi-experimental, although it was not intended to be – rather, the purpose of the post-tests was to determine if students would actually use a corpus to help them answer the questions following DDL training, as well as to determine the accuracy of answers where corpus consultation had taken place. Not all students completed all of the tests due to drop out – exact numbers of participants taking each test are provided in the results.

# 4.3.2.  DDL training

DDL training was conducted in a blended mode, with students completing exercises developed by the researcher specifically for this study, hosted on the online $E D X ^ { 1 }$ platform within a pre-existing adult-focused short private online course for DDL (Crosthwaite, 2020). To create the exercises, the researcher developed an ad-hoc learner corpus of the students’ written report of their first observed experiment on how friction creates static electricity which can influence the directional flow of running water. The $1 0 \ \mathrm { m o s t }$ frequent verb forms appearing in passive constructions were selected as target items for subsequent corpus investigation, namely 1) is; 2) was; 3) were; 4) rubbed; 5) rubbing; 6) applied; 7) moved; 8) using; 9) measured; 10) charged. Most module activities then involved students observing a guided corpus query into one of the target items using either SKELL (Baisa & Suchomel, 2014), SketchEngine (Kilgarriff et  al., 2014) or Linggle (Lai & Chang, 2020), observing the concordance, frequency or collocation data (depending on the function of the tool being used in the exercises), replicating the guided query on their own for a different item, then consolidating this knowledge via multiple-choice question, peer discussion, or individual reflection. Students completed some activities in-class with teacher guidance and some out of class individually. This follows McEnery et  al. (2006) three I’s approach (illustration, interaction and induction) alongside Flowerdew’s (2009) ‘intervention’ stage through the assistance of the teacher.

Students completed five online modules taken 2 weeks apart, with each module taking between 30 minutes to 1 hour to complete. These include:

Module 1: How corpus tools do what your dictionary or translation website can’t (Introducing SKELL)   
Module 2: Corpora for the passive (continuing training in SKELL) Module 3: Exploring collocation through Word Sketch in SKELL and Linggle Module 4: Exploring a corpus of science writing (introducing SketchEngine and the British Academic Written English corpus, Alsop & Nesi, 2009)   
Module 5: Limiting queries to the physical science subcorpus of the BAWE

By the end of the training, students were expected to have basic knowledge of generating simple and wildcard corpus queries in SKELL, SketchEngine and Linggle; generating, understanding and extrapolating information from frequency and collocation statistics; the notion and value of querying different subcorpora; and how to use a corpus to learn more about passive voice constructions. By way of example, one unit in Module 3 explores the OR operator $( / )$ in Linggle for the construction X was charged vs. X was electrified as part of the friction experiment, which is a potential issue students may experience in their free report writing tasks (see section below). Students were directed to query Linggle using the syntax ‘was charged/electrified’, which results in 870,000 hits for the former and only 8,600 for the latter. Based on this information, students may ascertain that that $9 9 \%$ of the time, the expression ‘was charged’ seems to be used over ‘was electrified’ (although this does not mean that ‘was electrified’ should never be used). Students were then directed on how to study concordance lines for either option, before completing two multiple choice questions involving an alternative construction $^ \mathrm { { * } } \mathrm { { X } }$ was applied’. All such content is available for the reader at the course link.

# 4.3.3.  Students’ written production

Following DDL training, students produced a complete written research report based on an observed in-class experiment involving a comparison of the concentration of ethanoic acid in various vinegars. Students were advised that during writing, as well as proofreading/editing their work, they should label through coloured highlighting (in MS Word) anywhere in the text where they used an online tool to help produce or revise a given word or expression. The available options were Google, Google Scholar, Dictionary.com (or equivalent), Google Translate, SKELL, SketchEngine or Linggle. The reports were then analysed for the frequency of use of these tools, as well as the general accuracy/readability of the final produced expression where a tool was used ( $1 =$ accurate/ appropriate, $0 =$ error/inappropriate).

# 4.3.4.  Questionnaire surveys

Two questionnaire surveys were administered to all participants via Office365 forms. The first, based on a survey provided to teachers following DDL training in Crosthwaite et al. (2021), was administered directly following completion of the final post-training test, and was divided into four sections, including:

Preferred corpus software and functions   
Perceptions of corpus training   
Perceptions of DDL for improving knowledge/use of the passive voice   
What students would like to see in a hypothetical future tool

The second survey was administered 3 months following completion of the science elective, and asked only the following questions:

Are you still using at least one of the corpus tools for any reason?   
Which tool(s) are you still using?   
What are you using the tools to do?   
What you find most useful about the tools?   
(if not using the tools anymore) Why have you stopped using   
the tools?

Both surveys are available on the IRIS repository. Students could select more than one response on certain questions (e.g. ‘why have you stopped using the tools’).

# 4.3.5.  Interviews

Students took part in semi-structured focus group interviews of 5-6 students following completion of the DDL training, and were asked questions relating to:

Current online language learning tools Perceptions of corpus training Perceptions and use of online corpus tools Suggestions for improving tools/DDL

Each interview was audio recorded and transcribed by the researcher. They followed a semi-structured sequence of questions chosen by the researcher beforehand while retaining the interviewees’ freedom to elaborate and express their opinions on issues raised (Marshall & Rossman, 2010). In this manner, the interviewee has the freedom to go in unpredictable directions on topics, allowing the researcher further insight into the issues discussed. The interviews were transcribed and analysed into segments of meaning, with short phrases coded manually using in-vivo coding following procedures suggested in the Coding Manual for Qualitative Researchers (Saldaña, 2016). In-vivo coding uses the terms and concepts emerging from the participants’ words; thus, it is possible to capture their experiences and perceptions while preserving the meaning of their views and actions in the coding itself (Charmaz, 2014). One researcher carried out this coding process across multiple rounds of coding. Following the in-vivo coding, a broader categorization of concepts that express similar meaning were determined as emerging from the codes. These categories were grouped into themes, which merge the categories, codes and excerpts into one idea that can be explained and analysed. According to DeSantis and Ugarriza (2000, p. 362) ‘A theme is an abstract entity that brings meaning and identity to a recurrent experience and its variant manifestations. As such, a theme captures and unifies the nature or basis of the experience into a meaningful whole’. In this sense, the extracted themes represent broader concepts that unify participants’ answers. Finally, the themes, categories and code frequencies are visualised via mind maps to better understand the links between the perceptions and experiences of the interviewed participants.

# 5.  Results

# 5.1.  Tests of the passive

# 5.1.1.  Grammaticality judgement

26 students completed both pre and post-tests of grammaticality judgement of the passive voice. The median score on both pre- and post-test was 34 out of a possible 36, with means of 32.71 $\mathrm { ( S D = } 3 . 6 9 \mathrm { ) }$ for the pre-test and 32.88 $\langle \mathrm { S D } = 4 . 3 9 \rangle$ for the post-test. As the data was not normally distributed (Shapiro-Wilk $< . 0 0 1$ ), Wilcoxon’s Signed Rank Test showed there was no significant difference between pre- and post-test scores for grammaticality judgement, given the groups’ pre-test performance was already excellent $\left( Z = 0 . 8 3 7 \right.$ , $\textit { p } = \ . 4 0 3$ , $\mathrm { r } { = } . 1 6 4 $ ). Performance on the 6 distractor items in the pre-test was $9 6 . 2 \%$ and $9 4 . 8 \%$ on the post-test. Neither the L1 students $( p ~ = ~ . 0 5 9 )$ nor the EAL/D students $\left(  p = 1 . 0 0 \right)$ experienced significant improvement under individual Wilcoxon tests.

However, as mentioned in the method section, students were given the chance to use a corpus to check their intuitions on test items in the post-test, doing so a total of 158 times across the cohort. Of these, 139/158 $( 8 7 . 9 7 \% )$ of quiz attempts resulted in a correct/appropriate answer.

# 5.1.2.  Error correction

24 students completed both pre and post-tests of error correction of passive voice. The median score on both pre- and post-tests was 17 out of a possible 18, with means of 15.95 $\left( \mathrm { S D } = 2 . 8 6 \right)$ ) for the pre-test and 16.04 $\mathrm { \Delta } \mathrm { \langle } \mathrm { S D } ~ = ~ 2 . 9 5 ) $ ) on the post-test. As the data was not normally distributed (Shapiro-Wilk $< . 0 0 1$ ), Wilcoxon’s Signed Rank Test showed there was no significant difference between pre- and post-test scores for error correction, given the groups’ pre-test performance was already excellent $\scriptstyle ( Z = 0 . 5 5 9$ , $p = . 5 7 6$ , $\mathrm { r } { = } . 1 1 4$ ) Performance on the 6 distractor items in the pre-test was $9 5 . 1 \%$ and $9 2 \%$ on the post-test. Neither the L1 students $\left( p \ = \ . 5 0 7 \right)$ nor the EAL/D students $( p = 1 . 0 0 )$ experienced significant improvement under individual Wilcoxon tests.

In the post-test, where students were given the chance to use a corpus to check their intuitions, they did so 82 times. Of these, 71/82 $\left( 8 6 . 5 8 \% \right)$ of attempts resulted in a correct answer.

# 5.1.3.  Picture sequence retelling

23 students completed both pre and post-tests of the production of passive voice via the picture sequence retelling. The median score on the pre-test was 10 out of a possible 14, with a mean of 8.95 ( $\mathrm { S D ~ = }$ 2.93). The median score for the post-test was 11/14, with a mean of 9.78 $\mathrm { ( S D } = 2 . 4 4 )$ . As the data was not normally distributed (Shapiro-Wilk ${ < . 0 0 1 \} }$ ), Wilcoxon’s Signed Rank Test was used to determine the difference between pre- and post-test scores, with students scoring significantly higher on the post-test, with a large effect size ( $z = 2 . 5 0$ , $p = . 0 1 2$ , $\mathrm { ~ \bf ~ r ~ } =$ .521) (Tomczak & Tomczak, 2014). There was no difference between the L1 and EAL/D cohorts when comparing the differential between pre- and post-test results (Mann-Whitney $\mathrm { U } = 6 4 . 5 0$ , $\mathrm { t } = 1 . 0 5 0$ , $ { p } = 0 . 3 5 4 )$ .

Where students were given the chance to use a corpus to check their intuitions, they did so 50 times. Of these, 38/50 $7 6 . 0 0 \% )$ of attempts resulted in a correct/appropriate answer.

# 5.2.  Students’ free use of corpus tools for science report writing

Twenty-two students provided a written report based on an observed experiment comparing the concentration of ethanoic acid in various vinegars. Table 1 describes the frequency each tool was used for the production or revision of a given word/expression across the cohort, the proportion of accurate/appropriate expressions produced using each tool, and the count of production/revision of passive voice constructions in particular. Students highlighted any passage in their report where they used the applications in Table 1, using a unique colour to highlight an individual application as described in the method section.

The mean number of highlighted passages per student was 5.95 $\mathrm { \Delta S D = }$ 5.41). Three students did not highlight any passages in their reports.

In total, 196 passive constructions were located in the 22 texts (average of 8 per text), of which 26 $( 1 3 . 2 \% )$ were highlighted as revised using one of the tools in Table 1. Interestingly, students used a combination of corpus tools together with the more ‘traditional’ search engines including Google and Dictionary.com when drafting or revising their writing, often within the same document. Overall, the accuracy of students’ highlighted passages was very high for both corpus tools $( 9 5 . 5 \% )$ and non-corpus applications $( 9 5 . 2 \% )$ . However, a number of highlighted passages taken from Google, Scholar or Dictionary.com were lengthy i.e. copied and pasted at the whole sentence level or even the paragraph level, while highlighted passages involving corpus tools tended to focus on single words or 3-4-5-word expressions. This suggests that students used different kinds of tools for quite different purposes when allowed to do so freely. Encouragingly, the corpus tools (i.e. SKELL, SketchEngine and Linggle) were used twice as frequently overall $( \mathrm { n } = 8 9 )$ compared with the non-corpus applications $( \mathtt { n } = 4 2 )$ suggesting that the students had taken the DDL training onboard and were willing to use corpora to resolve language-related concerns with their writing during free production. This also applied to students’ highlighted passages involving passive voice constructions, with corpus tools used 20 times for this purpose compared with only 6 uses with other applications, 2 of which were eventually erroneous.

Table 1. Free use of corpus tools for science report writing.   

<html><body><table><tr><td>Tool</td><td>No. of times used</td><td>Proportion of appropriate/ total production</td><td>Times used for passive voice</td></tr><tr><td>Google.com</td><td>14</td><td>13/14 (92%)</td><td>2 (including 1 error)</td></tr><tr><td>Google Scholar</td><td>4</td><td>4/4 (100%)</td><td>0</td></tr><tr><td>Dictionary.com</td><td>18</td><td>17/18 (94%)</td><td>2 (including 1 error)</td></tr><tr><td>Google translate</td><td>6</td><td>6/6 (100%)</td><td>2.</td></tr><tr><td>SKELL</td><td>32</td><td>32/32 (100%)</td><td>13</td></tr><tr><td>SketchEngine</td><td>18</td><td>17/18 (94%)</td><td>1</td></tr><tr><td>Linggle</td><td>39</td><td>36/39 (92%)</td><td>6</td></tr></table></body></html>

# 5.3.  Post-training questionnaire survey and focus group interview data

# 5.3.1.  Current language learning tool use

In total, 27 students completed the post-training survey, and all students in both classes were present for the focus group interviews. Students were first asked in the interview data about their current use of online tools for computer-assisted language learning (Figure 1).

Regarding their current use of online tools for language learning, students reported using Google, online dictionaries and thesaurus, Grammarly and Beolingus (an online dictionary tool for L2 German). Students generally use these tools for ‘finding words’, with a smaller number of EAL/D students using them for direct translation, although they note issues with these tools’ basic functions and, importantly, a lack of context or a lack of disciplinary specificity for the sciences provided with any query results.

# 5.3.2.  Corpus tools used and their functions

Figure 2 describes students’ preferred corpus tools by order $\left( 1 ^ { \mathrm { s t } } / 2 ^ { \mathrm { n d } } / 3 ^ { \mathrm { r d } } \right)$ . Overall, students picked Linggle as their preferred tool, with over $7 5 \%$ picking this as their first choice. SketchEngine was ranked as most students’ $2 ^ { \mathrm { n d } }$ choice $( 5 0 \% )$ , with SKELL ranked as most students’ $3 ^ { \mathrm { r d } }$ choice $( 5 0 \% )$ . This is confirmed in the interview data (Figure 3).

Linggle is preferred primarily due to its ease of use and appealing user interface, while students also reported the versatility of SketchEngine despite the learning curve involved in its use. Regarding the specific corpus functions preferred by students, Figure 4 describes the following functions as ranked as their top three (of the 8 possible):

To summarise, students’ first and second preferences for corpus functions were both found in Linggle, namely the wildcard and OR $( ^ { \epsilon } / ^ { \gamma } )$ query options which allow students to enter their query then directly view how their query compares with the range of alternatives provided by Linggle. These accounted for over $5 0 \%$ of students’ $1 ^ { \mathrm { s t } }$ preferences for corpus function. While wildcards can be easily used in SketchEngine and SKELL, and with SketchEngine also allowing direct comparison of the frequency of a query term with other alternatives in the corpus, Linggle provides this data in a simple visual format without having to click through the frequency/collocation options in SketchEngine’s more complex user interface. That said, the ability to limit searches to physical science texts in SketchEngine was also favoured by students, as was the Word Sketch function in SKELL, which again allows users to see multiple options instantly following a given query. The GDEX function to simplify concordance results demonstrated during the DDL training was not seen as particularly useful as compared to the other functions, nor was searching for collocations in SketchEngine or viewing concordances in SKELL.

![](img/ea9f6bd4bf0183f53fc4fe70884dc392a35a467d80091cece71ca90c5a303217.jpg)  
Figure 1. S tudents’ current use of online language learning tools.

![](img/bc181637ae07e0c89929679c842e316c8ba24c79332595c58ad69f85bbd1b758.jpg)  
Figure 2. S tudents’ preferred corpus tools.

![](img/2e7d56dfaebc8fcd567f967af4d42270981c29bde12ffc41cb41f9cc2eaf78ed.jpg)  
Figure 3. S tudents’ perceptions of online corpus tools.

![](img/397cb2938550210e355d2d8e36340cc72813fcb18e15dc8b8cfb3ef0943764e2.jpg)  
Figure 4. S tudents’ preferred corpus functions.

# 5.3.3.  Students’ perceptions of corpus tools

Figure 5 describes students’ overall perceptions of the corpus tools they used during DDL training.

Overall, students were positive about their ability to understand frequency and collocation output via corpus tools, and also felt confident in their understanding of what they needed to search for. Easy access to the tools was also seen as positive, and students were encouraged by the overall speed of the tools. A more mixed picture was provided by students regarding the ease of using certain corpus tools, understanding concordance output, easily conducting corpus searches, and the learning curve involved in DDL. Learners were less enthusiastic about conducting complex searches, preferring a simpler-is-better approach. Individual quotes from the open-ended question on this topic included:

![](img/039fa326ca6b1776066fc4caa95782450e0f9b7fcd6f4df336d779c49a8e9813.jpg)  
Figure 5. S tudents’ survey perceptions of corpus tools.

It was difficult to understand at first where I was going in the corpus, specifically how to get to the frequency part of Sketch Engine.   
They were very confusing and I struggled to understand the steps for using them.   
Linggle is very easy to use and can see the results very clearly and users don’t need to spend much time to work out the different functions of the tool   
These tools are very good, and fantastic tools for writing research tasks, or even essays and reports. I found Linggle very easy to use, all the other tools like Skell and Sketchengine are amazing tools, but I found it hard to understand how to use them.

Regarding the interview data (Figure 6), students discussed the helpfulness of using corpus tools for both general and specific purposes, as well as how they planned to use corpus tools in the future.

In general, students felt they would use corpora for ‘finding words’, including consulting information on synonyms, frequency, and register/ genre differences. They also felt that exploring the different options corpora and corpus tools could provide was useful. Regarding using corpora for science writing, ‘finding words’ was again the main argument for corpora and DDL, including finding sentence starters, the ability to query within specific disciplines (in SketchEngine), and the advantages of corpora over Google. Regarding future use, students claimed they would use corpora for ‘finding words’ and ‘finding examples’, including some comments on using corpora in other disciplinary subjects including English and history.

![](img/a9de0ba1678d4a0100d9105304b5d4a91634b40e262ec0799d2cb384049d3bfe.jpg)  
Figure 6. P erceptions of corpus use (interview data).

# 5.3.4.  Perceptions of the DDL training

Figure 7 describes students’ perceptions regarding the online DDL training activities they completed.

Overall, students were positive about the online activities in general, although were less certain about the usefulness of embedded reflective discussion, and were quite mixed in their general enjoyment of the activities. Individual comments from the open-ended question in this part include:

The things they were asking me to do were complicated, so I had to ask others what I had to do. Some activities were not captivating enough. Maybe use some fun pictures. Maybe some memes would be helpful to make it more engaging.   
Some of the instructions for the SketchEngine and SKELL were confusing and a little difficult to read/understand. The reflection parts were really good though. I think the descriptions for the method could be more fun and concise.   
They were very good, but sometimes I was very confused of what I needed to do. Overall it was very useful!:)

![](img/a1f9897bf24bd9772fe1e36844f5382fb4e3de42de54c0e3c974c719286ea3ea.jpg)  
Figure 7. S tudents’ survey perceptions of DD L training.

The interview data (Figure 8) explored students’ perceptions of the usefulness of the training for understanding the passive voice and as well as for exploring the different functions of the corpus tools.

Students felt the DDL training was useful for learning about the passive voice in terms of providing plentiful examples of passive voice sentence structures. They felt this would be particularly useful for L2 English learners. Students also felt the training was useful for understanding how to find words, for understanding scientific writing in general, and for learning about the various tool settings.

# 5.3.5.  Perceptions of corpora for understanding and using the passive voice

Aside from the interview data in Figure 6 above, Figure 9 describes students’ perceptions regarding the usefulness of corpora for improving knowledge and use of passive voice.

Overall, students were generally positive about using corpora and DDL for this purpose. However, more mixed responses were provided regarding use of collocation/concordance output for understanding the

![](img/28fa3cd02914b65d4dbe0c949d753f87fbea0c651ce1dc1e50411d9a847b4d38.jpg)  
Figure 8. S tudents’ perceptions of online DD L training (interview data).

![](img/d706d0a90b0c9463aae4188d9b93cb820c091323ec674c785d3d313b639d6993.jpg)  
Figure 9. S tudents’ perceptions of DD L for the passive voice.

passive, although it is possible some students entered ‘disagree’ for one and ‘agree’ for the other. Only one student commented on this section:

They [corpora] really help understand passive voice, but since I wasn’t very good in understanding how SKELL and SketchEngine worked, it was hard in understanding what to put into [the] search.

# 5.3.6.  Issues and suggestions for improvement

The interview data explored students’ issues with corpora and DDL, and their suggestions for improving the DDL training, DDL tools and DDL as a pedagogical approach (Figure 10).

Regarding the issues faced during DDL, these all related to the user experience, rather than the nature of DDL per se. The complexity of the SketchEngine user interface was too steep a learning curve for many, despite its recent overhaul to be more user-friendly. In terms of suggestions for improving the experience, students also commented on the need to simplify the user experience, in particular favouring visual output over textual output. Students also felt the online DDL training materials could be more in line with the online training they currently experience for other subjects (although they did not comment specifically on what these were). This could be done by inserting how-to videos for each of the worked examples within the online training, as well as earning points/ avatars for successful completion of activities. Finally, in terms of future improvements to corpus tools, students suggested a range of functions and filters, including automatic citation/referencing of selected concordances, the ability to go from a concordance to the full paper or profile of the researcher involved, as well as filters for the subject/topic of corpus texts.

![](img/8d692591c28d0ef20173d4d7b90397436e993051c5b26acaea6406919c84ebf4.jpg)  
Figure 10. S tudents’ criticisms of DD L/Suggestions for the future.

# 5.4.  Follow up survey

Eighteen students responded to the follow-up survey sent three months following the DDL training. Of these, only four students $( 2 2 \% )$ reported still using corpus tools. For these students, Linggle was the preferred tool with 4 votes, and with SKELL and SketchEngine gaining 1 vote each. These students reported using the tools ‘to correct errors I am making’ $\left( \mathtt { n } = 3 \right)$ , ‘to learn which words go together’ $\left( \mathtt { n } = 3 \right)$ and ‘to find new academic expressions’ $\mathbf { \Psi } ( \mathbf { n } = 1 )$ .

The reasons why the other students had stopped using corpus tools are presented in Figure 11.

The main reason for students’ eventual rejection of DDL is the learning curve involved in using corpus tools, either in terms of the tools’ functions, or the act of using the tools for DDL itself (i.e. I don’t know how to use the tool properly for DDL). This has led to students reverting to

![](img/3264dc1d7eedf97828542ce286acece42ba60f675e2ede1bdab50a65681e0749.jpg)  
Figure 11. S tudents’ reasons for discontinuing corpus use.

Google, translation software and dictionaries for querying language-related issues in a number of cases (although it could be said they are still doing DDL here, even if they are not aware of this know). Difficulty understanding corpus output as well as feeling overwhelmed by said output are also listed as reasons, as well as issues with tool functions and speed. Responses to the open-ended question on this topic included:

I keep forgetting it’s there and sometimes it’s just easier to use a simpler tool.   
I just find it complicated and would much rather just Google Too many steps required to get the result I want   
It was too complicated to understand. Also the time it took was just not worth it

# 6.  Discussion

This study has shown the benefits and drawbacks of corpus consultation and data-driven learning for improving knowledge and use of the passive voice construction for science writing with pre-tertiary learners. The implications of these findings paint a positive picture of what is possible regarding DDL with younger learners, and provide a model of how a DDL intervention with younger learners can be successfully managed and integrated. We now discuss each research question in turn.

# 6.1.  RQ1

Regarding students’ performance on the three tests of the passive, students’ metalinguistic knowledge of the passive voice did not significantly improve, although it was very high to begin with. However, when faced with the need to check their intuitions about this metalinguistic knowledge during an explicit judgement or correction task, students frequently used corpora to do so, and were generally successful in generating appropriate judgements/corrections as a result.

Students’ productive knowledge of the passive voice (when prompted in the picture sequence description) does appear to have significantly improved, at least when we take students’ successful corpus queries into account. Therefore, the data suggest that student-led corpus consultation is useful for encouraging students to check and confirm their intuitions regarding the usage of the passive voice when formulating sentences requiring passive voice constructions, and adds to the body of research that using DDL with younger learners can result in gains for academic writing, and grammar in particular (e.g. Kim, 2019; Moon & Oh, 2018), although we remind the reader that these were not true quasi-experimental conditions as the learners were permitted to use corpora in the post-tests.

What DDL appears to be doing here is to give these learners the power to ‘make sense’ of their underlying metalinguistic knowledge of the passive, at least when unsure of their intuitions, by utilising the extra, more specific context available from corpus data, both at the corpus query stage (using the context of the target item as part of the query) and at the data retrieval and analysis stage (when reading concordances or consulting frequency information), which then aids them greatly when determining what to write at the level of actual production. While there is a strong link between metalinguistic knowledge and the productive knowledge of metalanguage of the type one would receive in ‘traditional’ grammar teaching contexts (Hu, 2011), accessing metalinguistic knowledge has been shown (in SLA research at least) to be controlled by factors including attention to form, processing automaticity, and linguistic prototypicality (Hu, 2002). While attention to form is stimulated in the first instance by the task at hand (in this case grammaticality judgement, error correction or free production), consulting corpus data could be said to promote additional attention to form, processing automaticity through the ‘input flood’ of concordance information (e.g. Meunier, 2019a; Sharwood Smith, 1993), and, with reference to frequency and collocation information, evidence of linguistic prototypicality (Ellis, 2017). Armed with this information, learners are then able to successfully mediate between their intuitions, the data, and their eventual production, even when their initial metalinguistic knowledge is at or near ceiling, as is the case with the learners in our study. Where metalinguistic knowledge is lacking, as with L2 learners, the learning gains can be, and frequently are, even stronger (Boulton & Cobb, 2017), although the number of EAL/D students in our cohort was too small to make a meaningful comparison, and their English proficiency was also considered to be very high.

# 6.2.  RQ2

Regarding students’ autonomous use of corpus tools and more ‘traditional’ search engines (e.g. Google), the findings suggest students are able to utilise both kinds of tools for ‘finding words’, are willing to use both kinds of tools even within the same document, and can use either kind of tool to make accurate revisions to their written work. However, close reading of students’ revisions found students tended to use non-corpus applications less to ‘find words’ but to find whole passages or definitions that could be copied directly into their reports, unlike their corpus use which focused more on smaller expressions. That said, we should not see students’ idiosyncratic usage here as problematic, or view this as DDL vs. non-DDL. The students’ use of Google, Scholar and Dictionary.com (and to a lesser extent even Google Translate) can still be (and should be) considered as ‘data-driven learning’, even if students are not consulting concordances from an ‘authentic’ corpus (Boulton, 2015; Szudarski, 2019). We are also encouraged by the finding that corpus tools were used to check intuitions or make revisions at a significantly higher frequency than that of Google, especially regarding the passive voice. However, this enthusiasm is tempered by our findings for the next RQ.

# 6.3.  RQ3

The results for RQ3 are a conundrum that DDL practitioners have to develop strategies to overcome in future research, namely that despite a high degree of reported satisfaction with corpora and DDL, and despite purpose-built dedicated DDL training materials using the latest DDL applications, continued use of corpora three months after the treatment was reported as very low. These findings are not unlike those for teachers doing DDL (e.g. Poole, 2020) or adult learners (e.g. Gilquin $\&$ Granger, 2010). We attribute this to three main reasons:

The focus on the passive voice was too narrow for students to get involved with DDL in a more meaningful way, despite them using corpora autonomously during writing on non-passive targets. A more contextualised approach was preferable.

The students saw the DDL intervention as an optional extra to the work they were doing, rather than being fully integrated within the curriculum The design of the corpus tools themselves are still not at a level where students feel comfortable with them as a long term accompaniment to/replacement of their go-to resources.

Regarding the first and second points, work is already underway to more meaningfully integrate corpus use into the curriculum with the next cohort, this time framing training as a ‘writing course’ on research reports, within which corpora are not the main focus of the training, and expanding the scope of the corpus activities to include academic phraseology involved in the key moves/steps of the research article genre. We acknowledge that the focus on the passive in this study may have been narrow, but this form was decided on as the target for DDL during consultation with the teachers involved. We also intend to track students’ corpus consultation more closely using search logs or screen capture (e.g. Pérez-Paredes et  al., 2011), so as to reveal insights into how our younger learners approach the task of corpus consultation and whether they go about this similarly to older learners. Regarding the third point, we emphasise a preference for ‘simpler is better’ for younger learners, given students’ reported preference for Linggle over more complex applications such as SketchEngine. It is apparent that for younger learners, the more ‘traditional’ DDL functions presented in the literature (e.g. frequency/collocation information) need to be framed and presented as they are in applications like Linggle to be seen as truly useful. Given the initially positive results, it does seem that after an initial learning curve, the benefit of the tools became clear. Not all of them get that far however, hence the need for better, more specific applications that streamline the corpus query and output process into just one to two clicks if we are to bring younger learners onboard. Gamification is also suggested a way for DDL to make inroads into pre-tertiary classrooms (e.g. Díez-Arcón & Martín-Monje, 2021), as well as doing DDL with tools not typically associated with corpora and concordances (e.g. Meunier, 2019b), or tools that are embedded into word processors resulting in less need to switch between these and the corpus tool (e.g. Collocaid, Frankenberg-Garcia et  al., 2019).

# 7.  Conclusion

Overall, the future looks bright for the use of DDL with younger learners. However, there is still more work to do on making sure that any intervention with younger learners leads to long-lasting DDL use. We suggest that the way to do this is to work even more with teachers on developing strategies to completely embed corpus use into classroom practice. The teachers in this study have gone as far as suggesting incorporating corpora as part of the suite of software students are exposed to upon entering school in Year 7, taking a ‘this is how we do things here’ approach which is predicted to lead to significant buy-in, not only from the students but the entire institution. We aim to report our success in this area in future studies.

In terms of the study’s limitations, while the number of students involved is rather small, this counts as a medium sample size in terms of Boulton and Cobb (2017) meta-analysis of DDL research. It would also have been useful to ask the students to note the corpus queries they produced for the pre- and post-tests as well as the free writing tasks because the three platforms used do not store users’ queries for analysis, but this option was not pursued so as to not overburden the participants. Finally, due to space, we aim to report teachers’ perceptions of the intervention in a future study. Nevertheless, the present study is one of the few empirical studies charting students’ engagement – and success – with corpus consultation, and one of fewer still where the teachers, rather than the applied linguist, were the main stakeholders of the intervention.

# Note

1. www.edx.org

# Disclosure statement

No potential conflict of interest was reported by the authors.

# Notes on contributors

Peter Crosthwaite is a senior lecturer in Applied Linguistics at the School of Languages and Cultures, University of Queensland, Australia. His research interests focus on English for General and Specific Academic Purposes, second language writing, corpus linguistics, and the direct use of corpora for education purposes. He has published widely on these issues in a number of leading journals. He is Associate Editor of the Journal of English for Academic Purposes, and serves on the editorial boards of Applied Corpus Linguistics, and Open Linguistics.

Brett Steeples is Head of Department for Science at Ipswich Girls’ Grammar School (IGGS), Queensland, Australia. With numerous years of professional experience, he is responsible for a range of science-related classes at IGGS from Years 7-12, as well as contributes assessment items and moderation for science assessment items from the Queensland Department of Education.

# ORCID

Peter Crosthwaite $\textcircled{1}$ http://orcid.org/0000-0002-1482-8381

# References

Alsop, S., & Nesi, H. (2009). Issues in the development of the British Academic Written English (BAWE) corpus. Corpora, 4(1), 71–83. https://doi.org/10.3366/ E1749503209000227   
Australian Government. (2019). Advancing women in STEM strategy. Retrieved December 9, 2021, from https://www.industry.gov.au/data-and-publications/advancing-women-in -stem-strategy/snapshot-of-disparity-in-stem/women-in-stem-at-a-glance   
Australian Government. (2021). Second national data report on girls and women in STEM. Retrieved December 9, 2021, from https://www.industry.gov.au/news/ second-national-data-report-on-girls-and-women-in-stem   
Baisa, V., & Suchomel, V. (2014 SkELL: Web interface for English language learning [Paper presentation]. In Proceedings of the Eighth Workshop on Recent Advances in Slavonic Natural Language Processing (pp. 63–70). Tribun EU.   
Bednarek, M., Crosthwaite, P., & García, A. I. (2020). Corpus linguistics and education in Australia. Australian Review of Applied Linguistics, 43(2), 105–116. https://doi. org/10.1075/aral.00029.edi   
Boontam, P., & Phoocharoensil, S. (2018). Effectiveness of English preposition learning through data-driven learning (DDL). The South-East Asian Journal of English Language Studies, 24(3), 125–141. https://doi.org/10.17576/3L-2018-2403-10   
Boulton, A. (2015). Applying data-driven learning to the web. In A. Leńko-Szymańska & A. Boulton (Eds.), Multiple affordances of language corpora for data-driven learning (pp. 267–295). John Benjamins.   
Boulton, A., & Cobb, T. (2017). Corpus use in language learning: A meta‐analysis. Language Learning, 67(2), 348–393. https://doi.org/10.1111/lang.12224   
Boulton, A., & Vyatkina, N. (2021). Thirty years of data-driven learning: Taking stock and charting new directions over time. Language Learning & Technology, 25(3), 66–89.   
Brown, C., Spiro, J., & Quinton, S. (2020). The role of research ethics committees: Friend or foe in educational research? An exploratory study. British Educational Research Journal, 46(4), 747–769. https://doi.org/10.1002/berj.3654   
Chambers, A. (2019). Towards the corpus revolution? Bridging the research–practice gap. Language Teaching, 52(4), 460–475. https://doi.org/10.1017/S0261444819000089   
Charmaz, K. (2014). Constructing grounded theory (2nd ed.) Sage.   
Chen, M., & Flowerdew, J. (2018). A critical review of research and practice in data-driven learning (DDL) in the academic writing classroom. International Journal of Corpus Linguistics, 23(3), 335–369. https://doi.org/10.1075/ijcl.16130.che   
Cobb, T. (1999). Applying constructivism: A test for the learner as scientist. Educational Technology Research & Development, 47(3), 15–31. https://doi.org/10.1007/BF02299631   
Cobb, T., & Free, P. (2004). The Compleat Lexical Tutor, v. 4. TESL-EJ, 8(3).   
Crosthwaite, P. (2020). Taking DDL online: Designing, implementing and evaluating a SP OC on data-driven learning for tertiary L2 writing. Australian Review of Applied Linguistics, 43(2), 169–195.   
Crosthwaite, P., & Boulton, A. (2022). DDL is dead? Long live DDL! Expanding the boundaries of data-driven learning. In H. Tyne (Ed), Discovering language: Learning and affordance, In press. Primary school students’ initial reactions to corpus use in a private tutoring setting. In P. Crosthwaite (Ed.), Data driven learning for the next generation: Corpora and DDL for pretertiary learners (pp. 150–170). Routledge.   
Crosthwaite, P., Luciana, & Wijaya, D. (2021). Exploring language teachers’ lesson planning for corpus-based language teaching: a focus on developing T PACK for corpora and DDL. Computer Assisted Language Learning, online ahead of print.   
Davies, M. (2009). Exploring English with online corpora: An introduction. Palgrave Macmillan.   
DeSantis, L., & Ugarriza, D. N. (2000). The concept of theme as used in qualitative nursing research. Western Journal of Nursing Research, 22(3), 351–372. https://doi. org/10.1177/019394590002200308   
Díez-Arcón, P., & Martín-Monje, E. (2021). G-Rubric: The use of open technologies to provide personalised feedback in languages for specific purposes [Paper presentation]. EDULEARN21 Proceedings (pp. 2635–2643). https://doi.org/10.21125/edulearn.2021.0574   
Ellis, N. C. (2017). Cognition, corpora, and computing: Triangulating research in usage-based language learning. Language Learning, 67(1), 40–65. https://doi. org/10.1111/lang.12215   
Fang, L., Ma, Q., & Yan, J. (2021). The effectiveness of corpus-based training on collocation use in L2 writing for Chinese senior secondary school students. Journal of China Computer-Assisted Language Learning, 1(1), 80–109. https://doi.org/10.1515/ jccall-2021-2004   
Flowerdew, L. (2009). Applying corpus linguistics to pedagogy: A critical evaluation. International Journal of Corpus Linguistics, 14(3), 393–417. https://doi.org/10.1075/ ijcl.14.3.05flo   
Frankenberg-Garcia, A. (2014). The use of corpus examples for language comprehension and production. ReCALL, 26(2), 128–146. https://doi.org/10.1017/S0958344014000093   
Frankenberg-Garcia, A., Lew, R., Roberts, J. C., Rees, G. P., & Sharma, N. (2019). Developing a writing assistant to help EAP writers with collocations in real time. ReCALL, 31(1), 23–39. https://doi.org/10.1017/S0958344018000150   
Gatto, M. (2019). Query complexity and query refinement: Using web search from a corpus perspective with digital natives. In P. Crosthwaite (Ed.) Data-driven learning for the next generation: Corpora and DDL for pre-tertiary learners (pp.106–130). Routledge. https://doi.org/10.4324/9780429425899-7   
Giampieri, P. (2019). The web as corpus in ESL classes: A case study. International Journal of Language Studies, 13(2), 91–108.   
Gillespie, J. (2020). CALL research: Where are we now? ReCALL, 32(2), 127–144. https://doi.org/10.1017/S0958344020000051   
Gilquin, G., & Granger, S. (2010). How can data-driven learning be used in language teaching? In A. O’Keeffe & M. McCarthy (Eds.), The Routledge handbook of corpus linguistics (pp. 359–370). Routledge.   
Hu, G. (2002). Psychological constraints on the utility of metalinguistic knowledge in second language production. Studies in Second Language Acquisition, 24(3), 347–386. https://doi.org/10.1017/S0272263102003017   
Hu, G. (2011). Metalinguistic knowledge, metalanguage, and their relationship in L2 learners. System, 39(1), 63–77. https://doi.org/10.1016/j.system.2011.01.011   
Johns, T. (1997). Contexts: The background, development and trialling of a concordance-based CALL program. In A. Wichmann, S. Fligelstone, T. McEnery, & G. Knowles (Eds.), Teaching and language corpora (pp. 100–115). Longman.   
Jones, S. A., & Seilhamer, M. F. (2020). Girls becoming mathematicians: Identity and agency in the figured world of the English-medium primary school. Journal of Language, Identity & Education, 1–17. https://doi.org/10.1080/15348458.2020.1795862   
Karras, J. N. (2016). The effects of data-driven learning upon vocabulary acquisition for secondary international school students in Vietnam. ReCALL, 28(2), 166–186. https://doi.org/10.1017/S0958344015000154   
Kewalramani, S., Arnott, L., & Dardanou, M. (2020). Technology-integrated pedagogical practices: A look into evidence-based teaching and coherent learning for young children. European Early Childhood Education Research Journal, 28(2), 163–166. https://doi.org/10.1080/1350293X.2020.1735739   
Kilgarriff, A., Baisa, V., Bušta, J., Jakubíček, M., Kovář, V., Michelfeit, J., & Suchomel, V. (2014). The Sketch Engine: Ten years on. Lexicography, 1(1), 7–36. https://doi. org/10.1007/s40607-014-0009-9   
Kim, H. (2019). The perception of teachers and learners towards an exploratory corpus-based grammar instruction in a Korean EFL primary school context. Primary English Education, 25(1), 123–152. https://doi.org/10.25231/pee.2019.25.1.123   
Koehler, M., & Mishra, P. (2009). What is technological pedagogical content knowledge (TPACK)? Contemporary Issues in Technology and Teacher Education, 9(1), 60–70.   
Knobel, M., & Lankshear, C. (2010). DIY media: Creating, sharing and learning with new technologies. Peter Lang.   
Lai, S. L., & Chang, J. S. (2020). Toward a pattern-based referencing tool: Learner interactions and perceptions. ReCALL, 32(3), 272–290. https://doi.org/10.1017/ S0958344020000105   
Lee, H., Warschauer, M., & Lee, J. H. (2019). The effects of corpus use on second language vocabulary learning: A multilevel meta-analysis. Applied Linguistics, 40(5), 721–753. https://doi.org/10.1093/applin/amy012   
Liontou, T. (2019). The effect of data-driven learning activities on young EFL learners’ processing of English idioms. In P. Crosthwaite (Ed.), Data-driven learning for the next generation: Corpora and DDL for pre-tertiary learners (pp. 208–227). Routledge. https://doi.org/10.4324/9780429425899-12   
Marshall, C., & Rossman, G. B. (2010). Designing qualitative research (5th ed.) Sage.   
McEnery, T., Xiao, R., & Tono, Y. (2006). Corpus-based language studies: An advanced resource book. Taylor & Francis.   
Merga, M. K., Mat Roni, S., & Mason, S. (2020). Teachers’ perceptions of their preparedness for supporting struggling literacy learners in secondary English classrooms. English in Education, 54(3), 265–284. https://doi.org/10.1080/04250494.2020.1775488   
Meunier, F. (2019a). Data-driven learning: From classroom scaffolding to sustainable practices. EL. LE, 8(2), 423–434.   
Meunier, F. (2019b). A case for constructive alignment in DDL: Rethinking outcomes, practices and assessment in (data-driven) language learning. In P. Crosthwaite (Ed.) Data-driven learning for the next generation: Corpora and DDL for pre-tertiary learners (pp. 13–31). Routledge/Taylor & Francis. https://doi.org/10.4324/ 9780429425899-2   
Mishan, F. (2013). Demystifying blended learning. In B. Tomlinson (Ed.), Developing materials for language teaching (pp.207–223). Bloomsbury.   
Moon, S., & Oh, S. Y. (2018). Unlearning overgenerated be through data-driven learning in the secondary EFL classroom. ReCALL, 30(1), 48–67. https://doi.org/10.1017/ S0958344017000246   
O’Keeffe, A. (2020). Data-driven learning–a call for a broader research gaze. Language Teaching, 54(2), 259–272. https://doi.org/10.1017/S0261444820000245   
OECD. (2013). Education Policy Outlook: Australia. Retrieved December 9, 2021, from http://www.oecd.org/education/EDUCATION%20POLICY%20OUTLOOK%20 AUSTRALIA_EN.pdf   
OECD. (2020). The impact of Covid-19 on education: Insights from education at a glance 2020. Retrieved December 9, 2021, from https://www.oecd.org/education/the-impac t-of-covid-19-on-education-insights-education-at-a-glance-2020.pdf   
Özbay, A. S., & Olgun, O. (2017). The application of DDL for teaching preposition collocations to Turkish EFL learners. The International Journal of Research in Teacher Education, 8(3), 1–10.   
Park, M., & Son, J. B. (2020). Pre-service EFL teachers’ readiness in computer-assisted language learning and teaching. Asia Pacific Journal of Education, 1–15. https://doi. org/10.1080/02188791.2020.1815649   
Pérez-Paredes, P. (2019). A systematic review of the uses and spread of corpora and data-driven learning in CALL research during 2011–2015. Computer Assisted Language Learning, 35(1–2),  36–61. https://doi.org/10.1080/09588221.2019.1667832   
Pérez-Paredes, P., Sánchez-Tornel, M., Alcaraz Calero, J. M., & Jiménez, P. A. (2011). Tracking learners’ actual uses of corpora: Guided vs non-guided corpus consultation. Computer Assisted Language Learning, 24(3), 233–253. https://doi.org/10.1080/09588 221.2010.539978   
Poole, R. (2020). “Corpus can be tricky”: Revisiting teacher attitudes towards corpus-aided language learning and teaching. Computer Assisted Language Learning, 1–22. https:// doi.org/10.1080/09588221.2020.1825095   
Prensky, M. (2001). Digital natives, digital immigrants. On the Horizon, 9(5), 1–6.   
Römer, U. (2009). Corpus research and practice: What help do teachers need and what can we offer? In K. Aijmer (Ed.), Corpora and language teaching (pp. 83–98). John Benjamins. https://doi.org/10.1075/scl.33.09rom   
Saeedakhtar, A., Bagerin, M., & Abdi, R. (2020). The effect of hands-on and hands-off data-driven learning on low-intermediate learners’ verb-preposition collocations. System, 91, 102268. https://doi.org/10.1016/j.system.2020.102268   
Saldaña, J. (2016). The coding manual for qualitative researchers (3rd ed.). SAGE.   
Sharwood Smith, M. (1993). Input enhancement in instructed SLA. Studies in Second Language Acquisition, 15(2), 165–179. https://doi.org/10.1017/S0272263100011943   
Sripicharn, P. (2010). How can we prepare learners for using language corpora? In A. O’Keefe & M. McCarthy (Eds.), The Routledge handbook of corpus linguistics (pp. 371–384). Routledge.   
Spada, N., Shiu, J. L. J., & Tomita, Y. (2015). Validating an elicited imitation task as a measure of implicit knowledge: Comparisons with other validation studies. Language Learning, 65(3), 723–751. https://doi.org/10.1111/lang.12129   
Soruç, A., & Tekin, B. (2017). Vocabulary learning through data-driven learning in an English as a second language setting. Educational Sciences, Theory and Practice, 17(6), 1811–1832. https://doi.org/10.12738/estp.2017.6.0305   
Szudarski, P. (2019). Effects of data-driven learning on enhancing the phraseological knowledge of secondary school learners of L2 English. In P. Crosthwaite (Ed.), Data-driven learning for the next generation (pp. 133–149). Routledge.   
Tekin, B., & Soruç, A. (2016). Using Corpus-assisted learning activities to assist vocabulary development in English. TOJET: The Turkish Online Journal of Educational Technology, Special Issue, 1270–1283.   
Thompson, P. (2013). The digital natives as learners: Technology use patterns and approaches to learning. Computers & Education, 65, 12–33. https://doi.org/10.1016/j. compedu.2012.12.022   
Tomczak, M., & Tomczak, E. (2014). The need to report effect size estimates revisited. An overview of some recommended measures of effect size. Trends in Sport Sciences, 1(21), 19–25.   
Tondeur, J., Scherer, R., Baran, E., Siddiq, F., Valtonen, T., & Sointu, E. (2019). Teacher educators as gatekeepers: Preparing the next generation of teachers for technology integration in education. British Journal of Educational Technology, 50(3), 1189–1209. https://doi.org/10.1111/bjet.12748