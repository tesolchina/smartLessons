# Topic and Accent Coverage in a Commercialized L2 Listening Test: Implications for Test-takers’ Identity

Vahid Aryadoust\*,

National Institute of Education, Nanyang Technological University, Singapore \* E-mail: Vahid.aryadoust@nie.edu.sg

I analyzed a corpus of the international English language testing system (IELTS) comprising 256 listening sections (1996–2021). The primary objective of the study was to gain insights into the assumptions made by test designers regarding the real-life contexts that test-takers will encounter. Overall, 15 superordinate topic areas and 300 subtopics were identifed in the corpus. There was relatively more diversity in topic coverage before 2000. However, the test did not incorporate texts that would address sociocultural matters related to local or international contexts. Additionally, English-as-L2 accents were virtually unrepresented with only three samples from 514 speakers, potentially suggesting a racialized perspective on the listening construct. I argue that it is possible that this way of testing promotes and normalizes test designers’ ideologies, while overlooking the importance of the diversity of domains that test-takers will encounter in daily life. I discuss the potential construction of test-takers’ identity through exposure to these topics and suggest that test designers may consider reevaluating topic and accent coverage in the test to improve fairness and equity in the test. Finally, I provide ideas on how the use of generative artifcial intelligence (AI) can enhance quality of language assessments.

Listening is typically seen as the act of understanding explicit and inferred meaning from audio texts (Vandergrift and Goh 2012). Consequently, listening assessments are often designed to measure these cognitive processes or related abilities (Aryadoust and Luo 2023). There is also a trend in research that focuses on the textual and topical aspects of listening tests (Aryadoust and Luo 2023). Particularly, there have been studies that examine the authenticity of listening texts (Rossi and Brunfaut 2021), vocabulary coverage (Ha 2021), effects of testing methods (O’Grady 2021), and the visual elements of listening assessment tasks (Ockey 2007; Suvorov 2015; Holzknecht et al. 2021). While these studies contribute to the feld of listening assessment, there has not been an examination of the topics covered in (commercialized) listening tests. This lack of research is concerning, as the topics covered in (listening) tests may impact the validity of the assessment and, notably, the construction of the identity of the test-taker (Barkhuizen and Strauss 2020), with identity referring to the sense of self, beliefs, and qualities that characterize people.

Topic refers to the specifc theme or subject that is being covered in a listening assessment such as food, daily life, travel, and academic and scientifc matters. The topic of a listening test is intended to provide relevance and context for the listening processes being assessed, as well as to make the test more authentic and engaging for the test-taker (Buck 2001). As a result, test designers endeavor to include topics that are deemed important in the target language use (TLU) domain or are interesting and familiar to the test-taker so as to motivate and engage them in the test (Peirce 1992; Qiu 2020). This can also create a sense of face validity and authenticity, as the test-taker is able to apply their language skills to a topic that is relevant to their own experiences or interests. On the other hand, the topics deemed to be disengaging or irrelevant to test-takers are not included in tests (Peirce 1992; Qiu 2020), as they may frustrate test-takers and impact their performance adversely.

Delineating appropriate topics for a language test is an essential part of the process of construct defnition and operationalization. A test that covers a wide range of relevant topics is more likely to be content valid,1 while a test with a limited range of topics may be prone to construct underrepresentation and produce invalid scores (Messick 1996). It is also believed that the test content should be specifcally targeted towards the topics and skills that are deemed relevant to the TLU domain. For example, when designing an academic listening test, it is common to focus on content that is exclusively relevant to a university setting, such as (simulated) conversations or lectures that may occur in academia (e.g. Vuković-Stamatović 2022).

It can be argued that topic coverage in test constructs, as a key component of high-stakes testing, is one of the contributing factors in the formation of identity and worldview among test-takers (Barkhuizen and Strauss 2020). However, the role of topic in shaping identity in language testing has received little research attention in the validation literature, with the majority of research focusing on psychometric validity (Aryadoust and Luo 2023). Test designers may actually utilize the way that they defne and operationalize constructs to promote and propagate certain ideologies, leading to some topics being excluded or emphasized in the test (Shohamy 2003, 2007). According to Shohamy (2003, 2007), this way of testing primarily favors authorities who desire to exert excessive control over societies, and can play a key role in manufacturing and naturalizing narrow and uncritical worldviews among test-takers. Given these considerations, it is important to examine the topic coverage of high-stakes L2 tests and consider how it may contribute to the dissemination of dominant ideologies.

The focus of this study is thus topic coverage in the listening section of the international English language testing system (IELTS). IELTS is a commercialized L2 test, which according to the British Council is ‘the world’s most popular English language profciency test for higher education and global migration with 3.5 million tests taken in the last year’ (British Council 2019, para. 10). Specifcally, IELTS is recognized ‘for entrance to universities and colleges across the Englishspeaking world, including $1 0 0 \%$ of universities in Australia and the United Kingdom, more than 3,400 institutions in the United States as well as hundreds of institutions in many other countries’ (British Council 2019, para. 2). This power to infuence stakeholders can also have signifcant consequences for the knowledge, interests, and even sense of identity of IELTS test-takers. Particularly, the topics covered in the test would be viewed as important by test-takers, and they are likely to focus a part of their language learning efforts around understanding and being able to communicate about these topics (see Hayes and Read 2004, for a relevant example). This is because, according to sociocultural theories (e.g. Foucault 1982; Bourdieu 1990), the symbolic power relations within the testing context can create a dynamic in which the test designer is granted symbolic power and agency. Consequently, the test designer’s ability to select topics for the test, as a key step in construct defnition, results in determining what domains are regarded as signifcant and what domains are impertinent. The topics covered in a test, whatsoever, would be deemed relevant and important to test-takers, while those not covered could be abandoned. Thus, high-stakes language tests may be understood as power tools capable of creating change and inculcating test designers’ ideologies in test-takers (Shohamy 2003, 2007). In this context, this is done by the selection of what to include and what to exclude from the test.

For example, the listening section of the TOEFL (test of English as a foreign language) test covers topics like ‘dining, entertainment, health, housing or travel’ or ‘academic-related announcement’ and talks. Similarly, the IELTS test includes certain topic areas in the test comprising of conversations and monologues about everyday social contexts such as ‘an accommodation agency’ and conversations and monologues in ‘an educational or training context’ such as ‘two university students discussion, perhaps guided by a tutor’ (IELTS.org, n.d.). The topics deemed as ‘sensitive’ by test-developers, such as religion, abortion, prejudice, and global warming are, however, removed from these high-stakes tests (Peirce 1992). This implies that language testing, particularly when commercialized, may not remain unbiased or impartial and can be infuenced by, and even reinforce, particular sociocultural standards and worldviews, regardless of whether such an outcome was intended by the test designers.

# Topic coverage in language learning and assessment

While topic coverage has rarely been problematized in the language assessment literature, there have been numerous language teaching and learning studies that examine the topic from the perspective of cultural sensitivity and taboo. Timina and Butler (2011) reported that Taiwanese students expressed reservation in discussing topics that were not discussed in the family such as ghosts, fnancial situation, signifcant other, sexuality, politics, mortality, and adoption. Similarly, teachers in United Arab Emirates expressed reservations about discussing several culturally sensitive topics (EL-Sakran and EL-Sakran 2021). By contrast, there are a number of studies that support the use of culturally sensitive topics in language learning. Haynes (2000) found that Japanese students were open to discussing topics which were considered taboo in the past such as AIDS. Other studies reported that L2 learners or teachers were open to engaging with culturally sensitive topics such as homosexuality and premarital sex (Tekin 2011) and religion and politics (Fahim and Amerian 2015) (see Gobert 2015, for a detailed review of possibly sensitive topics in L2 learning). Tunku Mokhtar (2010) extended the scope of culturally sensitive topics to, for example, politics, attitude, gender, ethnicity and race, food, cultures, and customs. Tunku Mokhtar reported that, in her multicultural setting, teachers advocated for teaching about these topics to prevent, for example, promoting ‘one religion at the expense of the others’ (p. 538). Teachers also believed that students should not be pushed to express support for certain political views, be embarrassed about their take on other sensitive issues, or offend their peers. In another study, Rahimi (2019) reported that L2 learners were not embarrassed about discussing taboo topics and actively explored whether studying taboo topics can actually help them develop their L2.

Language teaching scholars have also called for inculcating good values in students through addressing a diversity of topics to address and eliminate issues like cultural stereotypes. For example, in Malaysia, a multicultural country, Chandra (2010, cited in Tunku Mohtar 2010, p. 532) called for a pedagogical approach to ‘demolish ethnic stereotypes and communal generalisations which militate against mutual respect and harmonious relations between communities’. Chandra traces back some of the cultural stereotypes in Malaysia to the colonial period, maintaining that it is the responsibility of teachers to discuss the taboos and enlighten students about such harmful issues.

Overall, responsible practitioners and researchers are committed to offering a well-rounded and balanced language education which avoids restricting information and limiting students’ understanding of cultural differences and values. However, there is a scarcity of published research focusing on the topic coverage in commercialized language tests, with much of the available literature centering on psychometric or statistical modeling, resulting in a relative underrepresentation of test content in academic exploration (see, e.g. ETS reports). The above survey of topic sensitivity shows that what the designers of large-scale language tests may avoid including in their tests seem to constitute some of the major issues which many (if not all) L2 teachers and learners accommodate and are open about (Haynes 2000; Chandra 2010; Tunku Mokhtar 2010; Tekin 2011; Fahim and Amerian 2015; Rahimi 2019). By excluding these topics from language tests, test-takers are likely prevented from thinking, reading, writing, listening, and speaking about and engaging with these issues both in the L2 test and test preparation classes. This may pose a greater concern for testing-oriented cultures where language teaching and learning are heavily reliant on language tests. According to Sadker and Zittleman (2004, p.743), in public schools in the USA, there has been a trend of teachers shifting their focus away from ‘important but non-test topics’ towards preparing students for topics covered in high-stakes tests. This has led to the frustration of many teachers, who have to devote time and effort to doing something which they and their students do not identify with.

In all, it is important to examine the variety of topics covered in language tests, and consider how the test-developer’s choices may consciously or unconsciously promote specifc ideologies, agendas, and knowledge among test-takers and teachers (Shohamy 2007). By examining topic coverage in L2 listening tests, we may be able to enhance their content validity and fairness, and more effectively assess learners’ listening abilities in an increasingly globalized and complex world.

# Identity and topic coverage

The identity of a language learner/test-taker is infuenced by their ‘relationship to the world’ and how they perceive ‘possibilities for the future’ (Norton 2013, p. 45). The learner negotiates their identity and self through learning the new language across time and space (Barkhuizen and Strauss 2020). The learner–world relationship gradually forms their attitudes, qualities, worldviews, emotions, and expectations (Pavlenko and Norton 2007). Specifcally, social structures such as curricula and pedagogical practices or factors such as ‘assessment regimes’ and ‘education policies’ are integral to constructing the identity of the language learner (Barkhuizen and Strauss 2020, p. 8). From this perspective, the process of preparing for and sitting a high-stakes assessment can have an impact on learner identity, as these tests often demand a signifcant amount of investment, contributing to a learner’s sense of self and their relationship to the language they are learning (Barkhuizen and Strauss 2020).

Similarly, Stobart (2017) argued standardized and high-stakes assessments, as a social activity, can infuence the identity of test-takers. It is widely accepted that achieving a high score on a high-stakes test can lead to more access to power and a higher social standing (Stobart 2003, 2008, 2017). These exams can also infuence the content of curricula, which is a component of washback (although washback extends beyond the curriculum) (Tsagari 2012). This power status of assessments also affects how language learners view themselves and prepare for the test (Shohamy 2003). For example, test formats that are commonly applied in standardized testing such as multiple-choice items can encourage rote learning and shallow understanding, rather than deep intellectual engagement (Xie and Andrews 2013). This may occur particularly if the test specifcation restricts the measurement of certain subskills.

As Shohamy (2007, p. 117) argued, ‘[t]he introduction of language tests in certain languages delivers messages and ideologies about the prestige, priorities and hierarchies of certain language(s), and not others, leading to policies of suppression of diversity’. It is likewise possible to argue that when a test-developer frequently emphasizes certain topics in a high-stakes test, they create the perception that these topics have a higher status and signifcance. This emphasis can then shift the focus of learners and teachers towards prioritizing and limiting learning and teaching to these topics. Shohamy (2007) suggests that authorities are aware of the infuence that high-stakes assessments have and can use tests as a principal means of exerting control over society by ‘imposing specifc knowledge [on] students, teachers, principles, and educational systems alongside certain agendas and educational ideologies’ (p. 118). The corollary of this is that tests have the power to manipulate pedagogy, learner and teacher behavior, and even societal norms (Shohamy 2003, 2007), thus infuencing test-takers’ sense of self and identity (see Barkhuizen and Strauss 2020).

An example of the constraints arising from circumscribed topic coverage in listening tests is the use of the topics that are deemed culturally and politically correct and safe to test-developers (see Peirce 1992). For example, the TOEFL test designers have decided to limit topic coverage of the test by excluding topics like religion, abortion, and birth control, as they are deemed culturally sensitive and/or personal (Peirce 1992). Peirce (1992) discussed the challenges that test-developers at Educational Testing Service (ETS) face in identifying sensitive topics and those that may cause controversy or offense: ‘A potentially offensive passage (called sensitive at ETS) is diffcult to defne and, in fact, a subject of much debate amongst members of the TOEFL test development team. At ETS, topics on politics, religion, or sex were considered to be sensitive topics’ (pp. 668–669). While certain topics can elicit emotional responses from individuals, there is insuffcient evidence to establish clear guidelines for their exclusion from exams. As a result, testing experts tend to rely on their own beliefs and intuitions, which can be tainted by personal biases and sociocultural norms, to include or exclude a topic on a test. Despite concerns raised by some commercialized test designers, a recent study of a listen-to-speak assessment found that sentimentally charged content would not necessarily impact test performance (Chong and Aryadoust 2023). This suggests that while emotional responses are common and perhaps expected, they do not necessarily compromise the validity of the test (Chong and Aryadoust 2023).

# The present study

The main aim of the study is to examine the types and trends of topics covered in the IELTS listening test over time. Based on the research surveyed earlier, the study is based on the assumption that the topics covered in the IELTS listening test (i) may refect and reproduce the ideologies of test designers (Foucault 1982), and (ii) may potentially impact how language learners develop and express their identities (Bagiyan et al. 2021). As a secondary and related aim, the study will also examine the representation of English accents from regions outside of the UK, Australia, New Zealand, and North America in the IELTS listening test. The role of accent in L2 listening assessment and learning has been the subject of extensive research (e.g. Harding 2011; Abeywickrama 2013; Ockey and French 2016; Saito et al. 2017). As the IELTS test is promoted as an ‘international’ test, examining the diversity of accents represented within the test could be insightful, considering the signifcant reliance on this test in decision-making processes such as evaluating the language profciency of jobseekers, immigrants, and university applicants in many countries around the world. It is essential to consider the meaning of the term ‘international’ in the context of IELTS, as it is possible that test designers use this term to distance themselves from ideologies of native-speakerism and racializing the construct as ‘White’ (Piller 2001; Shuck 2006). However, it is also possible that the term ‘international’ is simply a refection of a neoliberal approach to expanding to a wider market (see Piller and Bodis 2022). The frst assumption can be addressed in this study by analyzing the accent coverage, while the second assumption would require taking a socio-economic perspective, which is beyond the scope of this study.

The study aims to address the following research question: What types of topics and English accents are covered in the IELTS listening tests published between 1996 and 2021? The IELTS listening test is an appropriate and relevant candidate for this study, since it is designed to replicate listening situations in both academic and general settings, where listening is a key component of communication (IELTS.idp. nd.). Specifcally, while other international language tests also aim to assess language profciency, the IELTS listening test purports to provide a relevant assessment of listening skills in both academic and general settings. By examining the topics and accents included in the test, we can thus gain insights into the assumptions made by the IELTS test designer about the sociocultural context that the test-taker will experience. As Fairclough (2010) suggested, discourse conventions of any group (here the test designer) can infuence not only the content of the texts produced by that group, but also what the group believes as the truth. Therefore, it is possible that the content of the IELTS listening test refects the beliefs of the test designer, and that this content may shape the perception of truth within the test content.

# Method

# Corpus

This study used listening tests taken from the Cambridge Practice Tests for IELTS, published by Cambridge University Press, which is a test designed to assess English language profciency in academic and non-academic settings. The materials consisted of previously administered IELTS tests administered to test-takers populations between 1996 and 2021. According to Read’s (2022) test review, the IELTS listening test includes four types of spoken genres: a conversation between two people, a monologue on everyday topics, a discussion between two and four people in an educational context, and an academic lecture. As a result, 16 IELTS test books containing 64 listening tests were used, each consisting of 4 sections, thus resulting in a total of 256 sections.

# Coding and analysis

Listening tests were collected from the Cambridge Practice Tests for IELTS published by the Cambridge University Press from 1996 to 2021. To address the aims of the study, a total of 256 sections were analyzed through a manual coding process to identify the topics covered in each. At the beginning of every section, a recorded voice provided the focus of the section. Based on this announcement and the content of the sections, three rounds of coding were conducted. First, a research assistant (RA), a PhD student in applied linguistics with main focus on language assessment, was trained to read the transcripts to perform two tasks: (i) identify the topic provided in the audio-recording, and (ii) establish specifc topic categories as an initial step towards consolidating the topics. For example, an announcer in Section 1 of the Cambridge IELTS 16 listening test 3 mentions ‘… you will hear a woman phoning to ask about a summer cycling camp for her young son, who is called Charlie …’. This particular scenario was labeled with the code ‘junior cycle camp (for son)’ for categorization purposes. As the principal investigator, I also listened to the audio recordings independently and held two online meetings with the RA to compare our coding. In several cases we used different wordings to identify the specifc subtopics of each section (e.g. health vs health benefts), while there were no signifcant discrepancies between our coded topic provided in the audio-recording, thus achieving $100 \%$ agreement. The high rate of agreement can be attributed to the straightforward nature of identifying the topics provided in the audio recordings.

Next, I categorized the subtopics into superordinate topics per section, using ChatGPT, Version 3.5, (chat Generative Pre-trained Transformer, a versatile generative artifcial intelligence [AI] system) (OpenAI 2023) as an aid in the categorization process. Large language models such as ChatGPT have been shown to effectively complete complex natural language processing tasks such as clustering topics into representative clusters based on shared themes. For example, Stammbach et al. (2023) showed that GPT-3.5 is able to summarize user reviews in a way that is both accurate and concise, as confrmed by human evaluation. In another study, Keskar et al. (2019) explored the performance of large language models for document clustering tasks. This study also showcased the models’ capacity to accurately group similar documents together. These fndings demonstrate the potential of generative AI systems like GPT to handle clustering tasks with high precision and accuracy.

In the present study, ChatGPT generated superordinate topics for each section. Specifcally, the following prompt was inputted in ChatGPT’s chat box:

The following list comprises the topics that are covered in a commercialized listening test. Please group them into thematically related superordinate topics.

To ensure the reliability of the grouping, I ran the prompt on two different occasions, and found consistent results. The process of synthesizing the topics also involved human interpretation and analysis where I carefully reviewed and evaluated the results generated by ChatGPT before drawing any conclusions. The subtopics and their superordinate topics were then visualized using RStudio, MATLAB, and the online Zingsoft application.

The RA also offered a qualitative analysis of the accents used in the test sections. To cross-validate this account, a second RA, a graduate student in applied linguistics with focus on language assessment and research methodology, listened to the auditory recordings of the IELTS listening test sections and categorized them as British English, American English, Australian/New Zealand, or other accents. The Australian and New Zealand accents were grouped together given their relatively limited presence in the corpus and the occasional diffculty that we encountered in confdently distinguishing between the two variants.

Prior to commencing the coding process, the second RA watched a number of videos on YouTube to practice differentiating between British, American, and Australian/New Zealand accents. This facilitated the RA’s understanding of possible distinctions in these accent variations. Overall, there were only 3 out of 514 $( 0 . 5 8 \% )$ instances of discrepancy between the two RAs. These disagreements occurred specifcally in Section 1, where the recordings were intended to emulate accents of three individuals who were from Holland, Japan, and Indonesia. In addition, to ensure the reliability of the coding outcomes, 24 audio recordings that posed challenges in identifying the respective accents were forwarded to me (the principal investigator), to verify the assigned codes. On top of these 24 audio recordings, I coded an additional 70 audio recordings to cross-validate the codes’ accuracy.

# Results

# Topic coverage

After identifying the topics in each test section, I used RStudio (Figure 1), MATLAB, the Zingsoft application, and a spreadsheet to visualize the fndings. Table 1 showcases the topics covered in each test section, which were coded according to the information provided in the audio-recording. For example, item 4 in Section 1 of test 1, published in 1996, was associated with ‘briefcase lost (lost property reporting to the police)’. This topic was then summarized as ‘reporting to the police’ in the specifc subtopic column, which provides a succinct overview of the audio recording. Another example from the lower portion of the table consists of items 36–39 in Section 4 of test 2 published in 1996, which was related to ‘Lecture about health issues’. This topic was then categorized under the health subtopic. Subsequently, the subtopics were submitted to ChatGPT for categorization and consolidation into broader categories, which are illustrated in Figures 1 and 2.

As demonstrated in Figure 1, tour-related topics are by far the most represented in Sections 1 and 2 of the tests $k = 1 3$ and 10, respectively). Section 1 dominantly represented tour booking $( k = 1 3$ ) followed by police reports and renting a house ( $k = 6$ each), job information and insurance $k = 5$ each), and so on. Among the list of topics, the most underrepresented $\mathrm { k } = 1$ each) included opening a bank account, camp application, making plans, and so on.

![](img/992cc4a3c7d3c903b0cbe44b97451dcf996fa967adedb41d1642e1ab4cf64322.jpg)  
Figure 1: Distribution and frequency of topics in the four sections of the IELTS listening test (1996–2021).

Table 1: Sample test coding for the superordinate and subordinate topics covered   

<html><body><table><tr><td>Date of publication</td><td>Test ID</td><td>Section number</td><td>Question number</td><td>Topic provided in the audio-recording.</td><td>Specific subtopic</td></tr><tr><td>1996</td><td>1</td><td>1</td><td>4</td><td>Briefcase lost (lost property. reporting to the police)</td><td>Reporting to the police</td></tr><tr><td>1996</td><td>1</td><td>1</td><td>5</td><td>Clock (lost property reporting to the police)</td><td>Reporting to the police</td></tr><tr><td>1996</td><td>1</td><td>1</td><td>6-10</td><td>Personal details form (lost property report to the police)</td><td>Reporting to the police</td></tr><tr><td>1996</td><td>2</td><td>1</td><td>1-10</td><td>Interview overseas students (overseas study experience) (survey)</td><td>Interview</td></tr><tr><td>1996</td><td>2</td><td>2</td><td>11-20</td><td>Bicycle shopping recommendation</td><td>Shopping</td></tr><tr><td>1996</td><td>4</td><td>2</td><td>13-21</td><td>Student banking</td><td>University services</td></tr><tr><td>1996</td><td>1</td><td>2</td><td>11-13</td><td>News headlines</td><td>News headlines</td></tr><tr><td>1996</td><td>1</td><td>2</td><td>14-21</td><td>News report</td><td>News. headlines</td></tr><tr><td>1996</td><td>3</td><td>2</td><td>13-23</td><td>Maritime museum introduction.</td><td>Guided tours in a museum</td></tr><tr><td>1996</td><td>2</td><td>3</td><td>25-30</td><td>Commercially grown banana plant (topic of tutorial paper assignment)</td><td>Study paper</td></tr><tr><td>1996</td><td>4</td><td>3</td><td>26-31</td><td>Aluminum can illustration (presentation preparation)</td><td>Course presentations</td></tr><tr><td>1996</td><td>2</td><td>3</td><td>31-32</td><td>Consumption of Australian bananas (topic of tutorial paper assignment)</td><td>Study paper</td></tr><tr><td>1996</td><td>4</td><td>3</td><td>22-25</td><td>Factsheet-aluminum cans (presentation preparation)</td><td>Course presentations</td></tr><tr><td>1996</td><td>3</td><td>3</td><td>28-32</td><td>Strategy for pricing (tutorial talk)</td><td>Course presentations</td></tr><tr><td>1996</td><td>2</td><td>4</td><td>40-41</td><td>Diet pyramid</td><td>Nutrition</td></tr><tr><td>1996</td><td>1</td><td>4</td><td>34-36</td><td>Course requirement form (introduction for freshers)</td><td>Education</td></tr><tr><td>1996</td><td>3</td><td>4</td><td>33-37</td><td>Space management in supermarket (business management)</td><td>Business</td></tr><tr><td>1996</td><td>3</td><td>4</td><td>38-42</td><td>Supermarket aisle (business management)</td><td>Business</td></tr><tr><td>1996</td><td>2</td><td>4</td><td>36-39</td><td>Lecture about health issues</td><td>Health</td></tr></table></body></html>

In Section 2, the second and third most frequent topics were job $\left( k = 7 \right)$ and university services and sports $\mathtt { k } = 5$ each). The topics that have been represented three times each in Section 2 are theater, shopping, parks, hotel room services, and guided tours in a park or museum. The least represented topics in this section $k = 1$ each) included, for example, animal protection and city suburbs. Section 3 shifted the focus to simulated conversations and brief lecturettes that supposedly occur in university settings. The most represented topics in this section included group assignments $( k = 1 5 )$ ), course presentation $\left( \lg = 8 \right)$ , supervisory advice on dissertations $\left( k = 6 \right)$ , and so on, while the least represented topics included, for example, campus facilities survey, course feedback and selection, and essay writing. Finally, Section 4 consisted of very short segments of simulated lectures, typically introductory sections. The topics covered in this section are general and span education $( k = 9 )$ , zoology $\left( \boldsymbol { k } = 8 \right)$ , business $\left( k = 7 \right)$ , history $( k = 6 )$ , architecture $\left( k = 5 \right)$ ), health $( k = 4 )$ , and so on. On the other hand, there are eight topics that have each only been represented once (e.g. archeology, biology, botany, energy, etc.).

In Figure 2, the topics that emerged in the analysis were categorized into several superordinate categories and subtopics, using ChatGPT. Overall, I identifed fve main superordinate topics in

![](img/a14dfd0662c5c38593ef91fb04aaffb2cc83d621bf74b9efa5988e97be811c8a.jpg)  
Figure 2: The radial tree diagram representing the main topics and sub-topics in the four test sections of the IELTS listening tests published between 1996 and 2021. The application used is available from https:// app.zingsoft.com.

Section 1: applications and registrations with 37 subordinate categories, planning and coordination $( k = 1 0 )$ , services $\left( \boldsymbol { k } = 8 \right)$ , transportation $\left( \lg = 8 \right)$ ), and travel and accommodation $( k = 2 4 )$ . Section 2 consisted of two superordinate topics: tours $\left( \boldsymbol { k } = 2 8 \right)$ ) and community and services $( k = 4 8 )$ , while Section 3 comprised four superordinate topics consisting of general academic topics $( k = 4 2 )$ , supervisory advice $( k = 1 3 )$ ), campus resources $\left( k = 7 \right)$ , and other $\left( \boldsymbol { k } = 3 \right)$ . Section 4 comprised four superordinate topics: sciences and technology $\left( \lg = 2 9 \right.$ ), social sciences $( k = 1 2 )$ , business and education $( k = 1 7 )$ ), and transportation and geography $( k = 1 4 )$ . The subordinate categories are also presented in Figure 2. Thus, overall, Sections 1 through 4 consisted of 87, 76, 65, and 72 subtopics $( k = 3 0 0 )$ ).

Finally, Figure 3 demonstrates longitudinal shifts in topic coverage in Sections 1, 2, 3, and 4. Overall, it appears that there was relatively more diversity in topic coverage in early years of test publication especially before 2000, while this coverage dwindled over time. Another notable trend is that the topics covered in each year do not fully repeat from one year to the next, although there are some partial overlaps between certain years.

# Accent coverage

The audio materials of the listening tests were examined and grouped according to the perceived accent of the speakers. Table 2 demonstrates the distribution of accents across the four test sections of the IELTS listening test. The data reveal a consistent dominance of British accents across all test sections (i.e. $7 8 . 2 \%$ , $8 5 . 9 \%$ , $7 8 . 4 \%$ , and $7 3 . 4 \%$ , respectively), followed by American accents $1 3 . 0 2 \%$ , $1 0 . 1 \%$ , $1 3 \%$ , and $2 0 . 3 \%$ , respectively). Notably, the representation of accents from individuals of Dutch, Japanese, and Indonesian backgrounds is extremely limited.

![](img/6a11f8f2274543352668434b688a911fc57ce7bd58b04e6cd1421e2bbc2e4a6d.jpg)  
Figure 3: Distribution and frequency of topics in the Sections 1, 2, 3, and 4 of the IELTS listening test over time.

Table 2: Distribution of speaker accents across four test sections in the IELTS listening test corpus   

<html><body><table><tr><td>Section</td><td>Accents</td><td># of speakers</td><td>% of Total</td><td>Cumulative %</td></tr><tr><td>1</td><td>British</td><td>136</td><td>78.2%</td><td>78.2%</td></tr><tr><td></td><td>Australian/New Zealand</td><td>12</td><td>6.9%</td><td>85.1%</td></tr><tr><td></td><td>American</td><td>23</td><td>13.2%</td><td>98.3%</td></tr><tr><td></td><td>Dutch</td><td>1</td><td>0.6%</td><td>98.9%</td></tr><tr><td></td><td>Japanese</td><td>1</td><td>0.6%</td><td>99.4%</td></tr><tr><td></td><td>Indonesian</td><td>1</td><td>0.6%</td><td>100.0%</td></tr><tr><td></td><td>Total</td><td>174</td><td></td><td></td></tr><tr><td>2</td><td>British</td><td>85</td><td>85.9%</td><td>85.9%</td></tr><tr><td></td><td>American</td><td>10</td><td>10.1%</td><td>96.0%</td></tr><tr><td></td><td>Australian/New Zealand</td><td>4</td><td>4.0%</td><td>100.0%</td></tr><tr><td></td><td>Total</td><td>99</td><td></td><td></td></tr><tr><td>3</td><td>British</td><td>127</td><td>78.4%</td><td>87.0%</td></tr><tr><td></td><td>American</td><td>21</td><td>13.0%</td><td>100.0%</td></tr><tr><td></td><td>Australian/New Zealand</td><td>14</td><td>8.6%</td><td>8.6%</td></tr><tr><td></td><td>Total</td><td>162</td><td></td><td></td></tr><tr><td>4</td><td>British</td><td>58</td><td>73.4%</td><td>73.4%</td></tr><tr><td></td><td>American</td><td>16</td><td>20.3%</td><td>93.7%</td></tr><tr><td></td><td>Australian/New Zealand</td><td>5</td><td>6.3%</td><td>100.0%</td></tr><tr><td></td><td>Total</td><td>79</td><td></td><td></td></tr></table></body></html>

As demonstrated in Figure 4, the British accent has been consistently dominant throughout the test’s history. The American accent, on the other hand, appears to have a downward trend after its peak in 2007. In addition, while the Australian/New Zealand accents were present in multiple test sections before 2005, they became mostly unrepresented in later years, with a few minor exceptions. Finally, the representation of English-as-L2 accents is only observed in three cases, which are prior to 2009.

# Discussion

This study set out to investigate the topic coverage in the IELTS listening test, a commercialized test that is taken by more than 3.5 million test-takers globally (British Council 2019). Despite its signifcance, the lack of transparency in the selection of topics for the IELTS listening test has not been addressed in language testing or applied linguistics research. As a secondary aim, the study also examined the representation of L2 speakers in the test. There are several major fndings in the study that will be discussed next.

# Topic coverage

From the perspective of topic coverage, the IELTS test designer seems to defne an L2 test-taker as an individual who should be able to comprehend information in short, simulated pieces of spoken discourse presented across the four test sections. Specifcally, there were 15 main topic areas covered across the four sections of the test, that is, Section 1) Applications and registrations, planning and coordination, services, transportation, travel and accommodation; Section 2) Tours, community, and services; Section 3) Academic topics, supervisory advice, and campus resources; and Section 4) Sciences and technology, social sciences, business and education, and transportation and geography. Thus, to the IELTS test designer, the world in which an IELTS candidate resides is composed of these dimensions. The avoidance of other topics pertinent to the life of university students, jobseekers, or immigrants could either indicate that the test designer is unaware of them (which is unlikely) or that they intentionally avoid them (which is more likely). Thus, the test designer appears to have overlooked the importance of many aspects of multiculturalism that have been addressed in language learning research (Haynes 2000; Chandra 2010; Tunku Mokhtar 2010; Tekin 2011; Fahim and Amerian 2015; Rahimi 2019). This oversight may promote a ‘standardized’ monoculture that is unique to this test and that can exclude other cultural perspectives. Specifcally, as the test is designed for both academic and general purposes, it should not underrepresent the daily experiences of a test-taker by confning language comprehension to, for example, simulated tour-booking conversations or academic talks invented by the test designer (see Figures 1 and 2). It is unclear how sampling from such a limited range of topics would represent a much broader universe of topics in TLU domains.

![](img/2220427baee10c2d1773eea314d68730a2d45ef9127f1f956d4dc2aa261a05cb.jpg)  
Figure 4: Longitudinal trend of accents in the IELTS listening test since 1996.

As a case in point, according to Read (2022), the main aim of Section 2 is to evaluate test-takers’ comprehension of ‘everyday topics’ and this study fnds that Section 2 mostly focuses on transactional conversations or conversation for ‘survival’ in daily life situations. I found that guided tours in different places such as gardens, parks, and museums were highly represented in this section, while a topic like animal protection was only represented once. Considering the scope of topics, it seems that this section has been developed almost exclusively for middle-class L2 test-takers, who desire to go on tours, visit museums, etc. This narrow defnition of the scope of the TLU domain in the test does not appear to be refective of the pluralistic nature of ‘everyday topics’ for different test-taking populations. Factors such as gender, ethnicity, religion, age, socioeconomic status, etc. do comprise a part of daily and learning experiences (Chandra 2010; Gobert 2015), which are overlooked in the test. In addition, a considerable amount of research in L2 learning has highlighted the role of topics that stimulate cognitive and affective engagement of L2 learners during learning and interaction (Bolitho et al. 2003), which is an area for further investigation in language assessment. From a validity perspective, this provides preliminary evidence for the underrepresentation of the construct of listening in the IELTS listening test (Messick 1996) and can attenuate the authenticity of the test.

In addition, a lack of coverage of numerous topics in the test creates a disconnect between testing, learning, and real world (see Sadker and Zittleman 2004; Shohamy 2007). In my view, language tests that do not provide learners with the opportunity to practice using the language to discuss sociopolitical and cultural issues may lead learners to see language learning as an abstract or disconnected activity, rather than as a tool for engaging with the world around them. Designers of commercialized tests, like IELTS, claim that language tests should be neutral and objective, and should not include topics that promote or advocate for any particular ideology (Pierce 1992). However, based on Bourdieu (1990) and Foucault (1982), including the topics that were identifed in this study is a clear indication of the test designer’s positionality and intentionality to introduce and reinforce a particular ideology among L2 test-takers. This ideology defnes an English-as-an-L2 user as an individual to understand some of the only 15 topic areas identifed in this study. It is suggested that the test designer acknowledge this subjectivity in the test design, rather than promoting the notion that the test is ‘recognised as being fair to all test takers, whatever their nationality, cultural background, gender or special needs’ (IELTS.org 2023).

It may be argued that the topics emphasized by test-developers may refect the topics that target language users would frequently encounter in the target language use domain. However, this argument does not consider the potential to emphasize topics that are easily measurable or even proftable, rather than those that are most pertinent to the TLU domains. In addition, a narrow focus on ‘frequently encountered’ topics (if this is true at all about the IELTS listening test) may neglect the importance of preparing test-takers for impromptu, but signifcant, situations they may experience in different TLU domains.

Some previous research has aimed to accurately defne the scope of construct defnition and operationalization of high-stakes L2 tests in general (e.g. Chapelle et al. 2008) and listening tests in particular (see Aryadoust and Luo 2023). While it is not deniable that the creators of commercialized L2 tests have generally good intentions, it is possible to argue that their biases and fnancial motivations may affect the content of the language tests they manufacture. As an example, in L2 testing, there is a potential for test designers to avoid particular topics that do not align with the beliefs or values of authorities, such as governments or educational institutions (Shohamy 2007); this is particularly likely in countries that would make a proftable market for selling tests. This postulation aligns with sociocultural theories presented by Bourdieu (1990) and Foucault (1982) suggesting that the belief system and worldviews of a social group shape what is considered truth and what is ignored in the discourse they produce. Thus, a litmus test of a test designer to determine the sensitivity and inclusion of a topic in a test can be whether it is congruent with the interests of political regimes. These interests can then be packaged as sociocultural values, while any ideology or truth in misalignment with these may be labeled controversial or sensitive cultural topics. It is important to investigate the extent to which the motivation to increase proft and expand internationally can lead commercial test designers to prioritize political and ideological considerations over more inclusive and scientifc approaches to defning the topics included in their tests. Unfortunately, this issue has not been widely investigated in L2 assessment research. This is likely due to the fact that this issue is incompatible with the dominant validity frameworks in the feld, among other factors. As a result, there is limited data available in the published literature, which makes it challenging to study this issue in depth.

Another fnding of the study was that there was relatively more diversity in topic coverage in early years of test publication especially before 2000. In addition, the topics covered in each year did not fully repeat from one year to the next, although there are some partial overlaps between certain years. Diversifying the topics covered in the test is a useful strategy to prevent construct underrepresentation and perhaps minimize the likelihood of cheating on the test. For this, the IELTS listening test designers should be commended. Topic diversifcation in the IELTS listening test, however, has only been implemented over a period of 26 years, and is not applicable to the group of people taking the test each year. There is a concern that the test may be unfair because some test-takers may receive a topic they are not familiar with or do not relate to in certain years. One potential solution to ensure that all test-takers are exposed to a diverse range of topics is to utilize digital technologies and generative AI to create a larger pool of topics and testlets that test-takers can select from. This would allow the test designer to better cater to the needs of different groups and ensure that the test is fair and representative of a wide range of topics (Aryadoust, forthcoming). This method would be more democratic and representative of the construct of L2 listening than the current assessment systems (see Shohamy 2007), in which each test-taker receives the same version of the test and has no choice over the topic. It is undeniable that there is a high cost involving the development and equating multiple versions of a listening assessment to cater to the needs of listeners. This is because that the feld of L2 assessment mostly relies on the traditional method of test development, which exclusively involves human experts and designers. In my upcoming work, I endeavor to demonstrate that utilizing AI systems may help test designers create parallel listening assessment tasks to accommodate the needs of listeners (Aryadoust forthcoming).

There is still an uncertainty about determining what topics should be considered sensitive or controversial in L2 (listening) assessments. A sensitive topic refers to any subject that may be treated as offensive, controversial, immoral, infammatory, or illegal (Charles and Dattalo 2018, p.587). These topics may include ‘racism, ableism, cheating, and stealing’, as well as religious and sociopolitical issues, which may still constitute some of the major issues that virtually all societies are facing on a daily basis. It is important to study sensitive topics and how they are perceived in various cultures, rather than avoid them because they may sound controversial or offensive to a test designer.

# Representation of English-as-L2 Accents

As previously discussed, English-as-L2 accents were signifcantly unrepresented in the conversations or lectures included in the tests. The reason behind this is that the test designer only recognizes ‘standard varieties of native-speaker English, including North American and British’ (IELTS. org 2023). However, this can be viewed as refecting the assumptions of test designers regarding the power dynamics that are embedded in the IELTS listening test as well as the power imbalance that the test designer is inculcating in test-takers (see Fairclough 2010). In other words, the test content including the accents represented seems to refect the ideologies and power dynamics that test designers want to reinforce under the guise of a ‘construct’. This approach may reinforce domination and power inequality in two ways: (i) by failing to recognize and represent speakers from regions outside of the UK, Australia, New Zealand, and North America and thus empowering these variants of English over other variants, and (ii) by representing the L2 testtaker as an individual presumably expected to interpret ‘optimal’ examples of English created by a somewhat exclusive group of English-as-L1-speakers. Thus, here, the positioning of L2 test-takers within the test raises concerns, as test designers seem to have represented the construct of English ‘as a white language’ and associated it with ‘ethnonational identities’ (Pillers and Bodis 2022, p. 6). It would provide evidence supporting Pillers and Bodis (2022, p. 2), who argued that the L2 English construct ‘is deeply racialized with white speakers in the inner circle and racialized others in the outer circle’. From this perspective, it is important to ask whether the claim laid on internationalization may refect a neoliberal approach to expanding to a wider market, rather than an attempt to deracialize the test (Piller 2001).

This fnding also brings attention to the notion of validity arguments, which have dominated the validity research in language assessment. A validity argument hinges on the premise that test-developers’ claim regarding the interpretations and uses of test scores ought to be substantiated by evidence (Chapelle et al. 2008). For example, Kane (2012, p. 4) asserts that ‘the basic questions [of validity] are straightforward. First, what is being claimed? Second, are these claims warranted, given all of the evidence?’ In continuation, Kane emphasizes that ‘it is appropriate to talk about [test-developers’] efforts “to validate” the claims being made’. Nevertheless, this approach raises an inevitable question: if the claims themselves offer a biased and ideologically driven understanding of the TLU domain, what would be the value of the evidence supporting them? In this paper, I demonstrated that the IELTS listening test may represent such a case, wherein, paradoxically, the evidence to support the claims made by the test designer (i.e. that only several ‘standard’ variants of English are represented in the test) would actually attenuate the representativeness of the test construct, as it leads to a test that presents a distorted view of language in the TLU domain. Given the problematic nature of such claims, it would be crucial to examine the test from alternative perspectives which emphasize scientifc accuracy in content representation rather than evidence accumulation in support of test-developers’ biased claims. This approach may also bring about more inclusivity and equity in assessments (see Kozleski et al. 2014).

# Recommendations for test development and research

Based on the discussion advanced above, I would suggest that test designers consider redesigning the test and reevaluating the construct defnition of the IELTS listening test. I further suggest several ways to address the issue of limited topic coverage and underrepresented English-as-L2 accents in listening tests.

First, including diverse viewpoints on controversial or delicate subjects may improve listening assessments. To achieve this, a diverse group of experts with knowledge on various subjects, including cultural, global, and societal issues, can be involved in the assessment development process. This level of involvement will allow for a more comprehensive understanding of listening, rather than just the ability to answer specifc questions, and will therefore better prepare individuals for the real-world challenges they may face (see Brownell 2018, for more ideas). Test designers who wish to improve the construct defnition and operationalization of large-scale listening tests may consider employing a team of interdisciplinary experts. For example, in addition to L2 experts, listening researchers in L1 domains should be consulted to ensure that the tests have a comprehensive coverage of diverse listening situations. Additionally, the perspectives of English-as-L2 language teachers, learners, and particularly test-takers should be sought to ensure that the listening tests are culturally appropriate.

Second, conducting thorough and systematic cross-cultural research can help shed more light on topic sensitivity. This means that topic coverage should not be exclusively determined by a group with the potential to be infuenced by, for example, fnancial gain or neoliberal perspectives on the market. In addition, to prevent issues of bias and unfairness, sensitivity reviews, and the cross-cultural research should not be limited to a single or dominant culture, but rather should represent a diverse range of cultures to ensure the fairness of the language profciency test.

Another suggestion would include the investigation of the actual impact of the topics that are deemed sensitive on test-takers’ performance. While it is obvious that commercialized language tests operate in a complex global context, where cultural and political sensitivities can arise, test designers should avoid speculating whether and to what extent a topic can affect test-takers and should conduct publicly available research to make informed decisions about topic (and accent) inclusion in listening tests.

An aspect to consider is accommodating test-takers with varying topic interests. In today’s digital age, it is necessary for language exams to offer a variety of topics for test-takers to choose from. Some authors, like Shohamy (2007), have rightly argued that preventing test-takers from selecting their preferred language (and I would like to add, topics) may even violate their human rights. Shohamy (2007) suggested that language tests that do not consider the diversity of the people taking them are disconnected from reality. A democratic listening assessment should, therefore, be inclusive and fexible in order to meet the needs and interests of a diverse group of test-takers, rather than impose rigid guidelines and expectations set by authorities and/or test designers.

Another implication of this study is that although test scores can offer useful insights into test-takers’ profciency, they should not be considered the defnitive and ‘objective’ measures of language (Shohamy 2007). Rather, the subjectivity of test designers can heavily infuence the content that is represented in tests (Jafarpur 2003). Specifcally, the IELTS listening test scores obtained by test-takers, while seemingly an objective measure, actually refect, inter alia, a complex interplay between test-takers’ characteristics and test designers’ knowledge and idiosyncrasies (see Jafarpur 2003, for an argument on test designer’s effect on test content).

Despite this subjectivity, certain unwarranted assertions have been made by the IELTS test designers, which could obscure some of the test’s limitations. This may potentially lead various stakeholders, including universities and governments, to utilize the tests, perhaps without a comprehensive grasp of their limitations. For example, the IELTS test designer stated that, ‘IELTS Academic test assesses your English-language profciency at an academic level to determine whether you are ready to study at an undergraduate or post graduate level, or work in a professional setting, such as doctor, nurse, teacher or lawyer’ (IELTS.idp. nd.). Claims of this nature, although seemingly compelling, can be misleading, as they may not fully consider the limitations of the test. Specifcally, to my knowledge, there does not seem to be any clear evidence to show that the IELTS listening test (or other sections of the test) offers a representative sample of the topics that certain professionals such as doctors, nurses, teachers, and lawyers would encounter in an English-medium environment.

# Limitations of the study

Several potential limitations of the study and avenues for future research could be posited. First, as the study primarily focuses on the IELTS listening test, the fndings may not be generalizable to other listening assessments used in different settings. I suggest that future studies should include other prominent language assessments to compare their topic and accent coverage.

Relatedly, the scope of the study was confned to listening tests from 16 IELTS preparation books published by Cambridge University Press to ensure the accuracy of the test samples. However, it should be noted that this decision could potentially circumscribe the representativeness of the corpus in capturing the full diversity of topics and accents featured in IELTS listening tests administered over the past 26 years. Therefore, conclusions concerning topic and accent diversity may not necessarily capture the entire spectrum of IELTS listening tests. This methodological choice calls for future research to examine more extensive corpuses.

Another potential limitation of the study is that it did not empirically investigate the increasing importance of accommodating a variety of English accents and topics due to, for example, global mobility. Future research could explore how much this underrepresentation of accents and topics in the IELTS listening test affects test outcomes or test-taker experiences. In particular, experimental studies that evaluate the impact of varying accents and topics on the performance and identity of test-takers could provide empirical evidence to support this limitation.

Finally, it is important to be aware of the potential risks of using generative AI like ChatGPT, which was used in this study. While generative AI can be a valuable tool, it is important to remember that it can sometimes generate biased or inaccurate texts. Researchers should therefore carefully evaluate the output of generative AI and be aware of the potential for ‘hallucination’ (the generation of false or inaccurate information). In the meantime, we should further note that bias is not limited to AI but can also be present in human-generated texts. Thus, regardless of the way in which texts are generated in language assessments, there should be systematic measures in place to identify and reduce bias. In my view, as transformer-based AI systems continue to evolve, they have the potential to surpass human judgement and eventually become more reliable.

# Conclusion

In sum, the IELTS listening test covers a range of accents and topics broadly relevant to academic settings and/or daily life. While this allows for a useful assessment of listening, it does not include a wide range of topics that students may encounter in academia or in an Englishspeaking environment, thereby resulting in underrepresenting the coverage and scope of the L2 listening construct. Thus, it would be helpful if the test-score users were conscious of these potential limitations and not rely on test scores as an exhaustive and defnitive depiction of a test candidate’s listening abilities. While certain topics can be challenging to incorporate in listening assessments, it is important for test designers to approach them responsibly, rather than completely avoid them, and ensure that test-takers are able to engage with a wide range of ideas and perspectives. Similarly, global mobility resulting from migration and higher education have reshaped the dynamics of TLU domains where English is spoken (e.g. Institute of International Education 2020). This has led to substantial English-as-L2 populations with various accents around the world, who should be represented in listening tests.

I did not intend to approach topic and accent coverage in listening assessment exclusively from the perspective of content validity or contemporary validity frameworks, because as I have argued in a recent paper, these frameworks may confate the science of assessment and policy matters (Aryadoust 2023). Restricting topic and accent coverage to content validity could lead to the underrepresentation of the characteristics of individuals. This is because, in my view, the mainstream language assessment research views content coverage within a limited framework built for restricted TLU domains. Specifcally, candidates taking profciency tests like the IELTS listening are not defned as citizens with diverse backgrounds concerned about socio-political issues which the world is struggling with, including but not limited to fake news, fundamentalism, censorship, the role of states and private sectors in global warming etc. A ‘typical’ IELTS listening test-taker is seen as an individual tasked with following instructions and listening to simulated transactional dialogues or speeches. How would this way of presenting information promote proactive and critical engagement with listening situations (see Floyd and Clements 2005; Brownell 2018), and would it not encourage a passive stance towards the test material and the simulated TLU situations?

Finally, it is important to clarify that I am not suggesting that the IELTS listening test has been designed with the intention of producing passive and uncritical test-takers—although this is suggested by the topics covered and the way the L2 listener is positioned in this test. The point I hope I have been able to convey is that the topic and accent coverage in this test does not entirely represent listening situations and could even foster a skewed perspective towards the English language. It might also potentially downplay considerations about societal issues and so-called ‘sensitive’ everyday life matters. Test designers should conduct thorough research on domain specifcations to understand what sort of situations test candidates engage with in actual TLU domains and how these can be represented in the construct defnitions of the test. L2 listening tests should not, inadvertently, limit the test-taker’s understanding of the world or possibly hamper their ability to communicate with a diverse range of language users with diverse accents.

# Notes on Contributor

Vahid Aryadoust is an Associate Professor at the National Institute of Education of Nanyang Technological University. His areas of interest are meta-analysis in L2 research and language assessment in general and listening assessment and pedagogy in particular. Address for correspondence: National Institute of Education, Nanyang Technological University, Singapore. <Vahid. aryadoust@nie.edu.sg>

# Acknowledgements

I would like to thank Azrifah Zakaria for her comments on an earlier draft of this paper; and Yuxin Liu, Wenxin Zhang, and Zhuohan Hou for their assistance in data processing. Any limitations in the study remain mine. ChatGPT, versions 3.5 and 4, and Google Bard were used to revise some of the sentences for clarity.

# Note

1 I am aware that the current approach towards validity is based on the validity of interpretations and uses of test scores, rather than the validity of the test. However, for clarity, I use the latter conceptualization of validity in this context (see Aryadoust, 2023, for an argument).

# References

Abeywickrama, P. 2013. ‘Why not non-native varieties of English as listening comprehension test input?,’ RELC Journal 44/1: 59–74. doi: 10.1177/0033688212473270.   
Aryadoust, V. forthcoming. Assessing Listening in the Age of Generative Artifcial Intelligence: A Researchbased Sourcebook. Routledge.   
Aryadoust, V. 2023. ‘The vexing problem of validity and the future of second language assessment,’ Language Testing 40/1: 8–14. doi: 10.1177/02655322221125204.   
Aryadoust, V. and L. Luo. 2023. ‘The typology of second language listening constructs: A systematic review,’ Language Testing 40/2: 375–409. doi: 10.1177/02655322221126604.   
Bagiyan, A. Y., et al. 2021. ‘The real value of words: How target language linguistic modelling of foreign language teaching content shapes students’ professional identity,’ Heliyon 7/3: e06581. doi: 10.1016/j.heliyon.2021.e06581.   
Barkhuizen, G. and P. Strauss. 2020. Communicating Identities. Routledge.   
Bolitho, R., et al. 2003. ‘Ten questions about language awareness,’ ELT Journal 57/3: 251–9. doi: 10.1093/ elt/57.3.251.   
Bourdieu, P. 1990. In Other Words: Essays Towards a Refexive Sociology. Translated by Matthew Adamson. Stanford University Press.   
British Council. 2019. IELTS grows to 3.5 million a year. https://takeielts.britishcouncil.org/about/ press/ielts-grows-three-half-million-year#:\~:text $=$ IELTS%20is%20the%20International%20 English,taken%20in%20the%20last%20year.   
Brownell, J. 2018. Listening: Attitudes, principles, and skills (6th ed.). Routledge.   
Buck, G. 2001. Assessing Listening. Cambridge University Press.   
Chandra, M. 2010. Values in the school system. Paper presented at the seminar on Multiculturalism: Lessons learned and the way forward on 29 September 2010. Universiti Pendidikan Sultan Idris, Malaysia.   
Chapelle C. A., M. E. Enright and J. Jamieson (eds). 2008. Building a Validity Argument for the Test of English as a Foreign Language. Routledge.   
Charles, J. K. L. and P. V. Dattalo. 2018. ‘Minimizing social desirability bias in measuring sensitive topics: The use of forgiving language in item development,’ Journal of Social Service Research 44/4: 587–99. doi: 10.1080/01488376.2018.1479335.   
Chong, J. J. Q. and V. Aryadoust. 2023. ‘Investigating the effect of multimodality and sentiments on speaking assessments: A facial emotional analysis,’ Education and Information Technologies 28: 7413–36. doi: 10.1007/s10639-022-11478-7.   
EL-Sakran, O. T. and T. M. EL-Sakran. 2021. ‘The case of culturally sensitive topics in the English Language Classrooms: Secondary school teachers’ perspective,’ Journal of English as an International Language 16/1: 37–59.   
Fahim, M. and M. Amerian. 2015. ‘Humanizing ELT curriculum: Teachers’ attitudes about the role of ‘divergent’ topics in developing critical students,’ Humanizing Language Teaching 17/1. https://old. hltmag.co.uk/feb15/mart04.htm   
Fairclough, N. 2010. Critical Discourse Analysis: The Critical Study of Language. Routledge.   
Floyd, J. J. and S. M. Clements. 2005. ‘The vital importance of critical listening: An extended example,’ International Journal of Listening 19/1: 39–47. doi: 10.1080/-10904018.2005.10499072.   
Foucault, M. 1982. ‘The subject and power,’ Critical Inquiry 8: 777–95.   
Gobert, M. 2015. ‘Taboo topics in the ESL/EFL classroom in the Gulf region’ in Intercultural Communication with Arabs. Springer, pp. 109–26   
Ha, T. H. 2021. ‘Exploring the relationships between various dimensions of receptive vocabulary knowledge and L2 listening and reading comprehension,’ Language Testing in Asia 11/20: 1–19. doi: 10.1186/s40468-021-00131-8.   
Harding, L. 2011. Accent and Listening Assessment: A Validation Study of the Use Of Speakers with L2 Accents on an Academic English listening Test. Peter Lang.   
Hayes, B., and J. Read. 2004. ‘IELTS Test preparation in New Zealand: Preparing students for the IELTS Academic Module’ in L. Cheng and Y. Watanabe (eds): Washback in Language Testing: Resource Contexts and Methods. Lawrence Erlbaum, pp. 97–111   
Haynes, L. 2000. ‘The taboo topic and teacher reluctance: An investigation into attitudes among university EFL teachers in Japan’ Unpublished Master’s thesis, Newport Asia   
Holzknecht, F., et al. 2021. ‘The effect of response order on candidate viewing behaviour and item diffculty in a multiple-choice listening test,’ Language Testing 38/1: 41–61. doi: 10.1177/0265532220917316.   
IDP Education Limited. 2023. Interim fnancial report for the half-year ended 31 December 2022. https://www.google.com/url?sa=t&rct=j&q=&esrc s&source $=$ web&cd=&ved=2ahUKEwj60--004_-AhXpR2wGHcToDmIQFnoECA8QAQ&url=https%3A%2F%2Finvestors.idp. com%2FDownloadFile.axd%3Ffile%3D%2FReport%2FComNews%2F20230223%2F02634858. pdf&usg=AOvVaw1V1VDff-5huooSFDXk-RHE   
IELTS.idp. nd. IELTS academic test. https://ielts.idp.com/singapore/about/which-test-do-i-take/ academic   
IELTS.org. 2023. Ensuring quality and fairness. https://www.ielts.org/about-ielts/ ensuring-quality-and-fairness   
IELTS.org. n.d. Listening test: What is the IELTS Listening test?. https://www.ielts.org/ how-to-use-ielts-results/four-skills/listening/format   
Institute of International Education (IIE). 2020. Open Doors: Report on international educational exchange. Washington, DC: IIE. https://opendoorsdata.org/   
Jafarpur, A. 2003. ‘Is the test constructor a facet?,’ Language Testing 20/1: 57–87. doi: 10.1191/0265532203lt244oa.   
Kane, M. 2012. ‘Validating score interpretations and uses,’ Language Testing 29/1: 3–17. https://doi.org. remotexs.ntu.edu.sg/10.1177/0265532211417210.   
Keskar, N. S., et al. 2019. ‘Unifying question answering, text classifcation, and regression via span extraction,’ arXiv preprint arXiv:2205.07557.   
Kozleski, E. B., A. J. Artiles and F. Waitoller. 2014. ‘Equity in inclusive education: A cultural historical comparative perspective’ in L. Florian (ed.): The Handbook of Special Education. Sage Publications, pp. 231–49   
Messick, S. 1996. ‘Validity and washback in language testing,’ Language Testing 13/4: 241–56. doi: 10.1177/026553229601300302.   
Norton, B. 2013. Identity and language learning: Extending the conversation (2nd ed.). Multilingual Matters.   
O’Grady, S. 2021. ‘Adapting multiple-choice comprehension question formats in a test of second language listening comprehension,’ Language Teaching Research. Advance online publication: 136216882098536. doi: 10.1177/1362168820985367.   
Ockey, G. 2007. ‘Construct implications of including still image or video in computer-based listening tests,’ Language Testing 24/4: 517–37. doi: 10.1177/0265532207080771.   
Ockey, G. and R. French. 2016. ‘From one to multiple accents on a test of L2 listening comprehension,’ Applied Linguistics 37/2: 693–715. doi: 10.1093/applin/amu060.   
OpenAI. 2023. ChatGPT (Version 3.5) [Computer software]. OpenAI. https://openai.com J. Cummins and C. Davison (eds): International handbook of English language teaching. Springer, pp. 669–80   
Peirce, B. 1992. ‘Demystifying the TOEFL reading test,’ TESOL Quarterly 26/4: 665–89. doi: 10.2307/3586868.   
Piller, I. 2001. ‘Naturalisation language testing and its basis in ideologies of national identity and citizenship,’ International Journal of Bilingualism 5/3: 259–77. doi: 10.1177/13670069010050030201.   
Piller, I. and A. A. Bodis. 2022. ‘Marking and unmarking the (non)native speaker through English language profciency requirements for university admission,’ Language in Society. Advance online publication. doi: 10.1017=S0047404522000689.   
Stammbach, D., M. Antoniak, and E. Ash 2023. ‘Heroes, villains, and victims, and GPT-3: Automated extraction of character roles without training data,’ arXiv preprint arXiv:2211.15914.   
Qiu, X. 2020. ‘Functions of oral monologic tasks: Effects of topic familiarity on L2 speaking performance,’ Language Teaching Research 24/6: 745–64. doi: 10.1177/1362168819829021.   
Rahimi, E. 2019. ‘A study of appropriacy of cultural conceptualizations of taboo topics in EFL classes,’ Konin Language Studies 7/4: 495–514. doi: 0000-0001-9944-7862.   
Read, J. 2022. ‘Test review: The International English Language Testing System (IELTS),’ Language Testing 39/4: 679–94. doi: 10.1177/02655322221086211.   
Rossi, O. and T. Brunfaut. 2021. ‘Text authenticity in listening assessment: Can item writers be trained to produce authentic-sounding texts?,’ Language Assessment Quarterly 18/4: 398–418. doi: 10.1080/15434303.2021.1895162.   
Sadker, D. and K. Zittleman. 2004. ‘Test Anxiety: Are students failing tests or are tests failing students?,’ Phi Delta Kappan 85/10: 740–51. doi: 10.1177/003172170408501007.   
Saito, K., P. Trofmovich, and T. Isaacs. 2017. ‘Using listener judgments to investigate linguistic infuences on L2 comprehensibility and accentedness: A validation and generalization study,’ Applied Linguistics 38/4: amv047–462. doi: 10.1093/applin/amv047.   
Shohamy, E. 2003. ‘Implications of language education policies for language study in schools and universities,’ The Modern Language Journal 87: 278–86.   
Shohamy, E. 2007. ‘Language tests as language policy tools,’ Assessment in Education: Principles, Policy & Practice 14/1: 117–30. doi: 10.1080/09695940701272948.   
Shuck, G. 2006. ‘Racializing the nonnative English speaker,’ Journal of Language, Identity & Education 5/4: 259–76. doi: 10.1207=s15327701jlie0504_1.   
Stobart, G. 2017. ‘Assessment and learner identity’ in M. A. Peters (eds): Encyclopedia of Educational Philosophy and Theory. Springer.   
Stobart, G. 2003. ‘The impact of assessment: Intended and unintended consequences,’ Assessment in Education: Principles, Policy and Practice 16: 139–40.   
Stobart, G. 2008. Testing Times: The Uses and Abuses of Assessment. Routledge.   
Suvorov, R. 2015. ‘The use of eye tracking in research on video-based second language (L2) listening assessment: A comparison of context videos and content videos,’ Language Testing 32/4: 463–83. doi: 10.1177/0265532214562099.   
Tekin, M. 2011. ‘Discussing the unspeakable: A study of the use of taboo topics in EFL speaking classes,’ Journal of Theory and Practice in Education 7/1: 79–110.   
Timina, S. A. and N. L. Butler. 2011. Uncomfortable Topics and Their Appropriateness in Asian EFL Classes (ED515120). Retrieved from ERIC database.   
Tsagari, D. 2012. The infuence of the Examination for the Certifcate of Profciency in English (ECPE) on Test Preparation materials. Internal report sponsored by the SPAAN fellowship for studies in Second or Foreign Language Assessment, Cambridge Michigan Language Assessments (CaMLA), Ann Arbor.   
Tunku Mokhtar, T. M. B. 2010. Raising teacher consciousness of cultural sensitivities in the classroom. Proceedings of the 4th International Conference on Teacher Education. Join Conference UPI & UPSI Bandung, Indonesia. http://fle.upi.edu/Direktori/PROCEEDING/UPI-UPSI/2010/Book_3/RAISING_TEACHER_ CONSCIOUSNESS_OF_CULTURAL_SENSITIVITIES_IN_THE_CLASSROOM.PDF   
Vandergrift, L. and C. C. Goh. 2012. Teaching and Learning Second Language Listening: Metacognition in Action. Routledge.   
Vuković-Stamatović, M. 2022. ‘Suitability of science & technology documentaries for EAP and EST listening,’ Journal of English for Academic Purposes 58: 101137. doi: 10.1016/j.jeap.2022.101137.   
Xie, Q. and S. Andrews. 2013. ‘Do test design and uses infuence test preparation: Testing a model of washback with Structural Equation Modeling,’ Language Testing 30/1: 49–70.