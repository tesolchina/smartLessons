# Exploring the potential of using ChatGPT for rhetorical move-step analysis: The impact of prompt refinement, few-shot learning, and fine-tuning

Minjin Kim\*, Xiaofei Lu

Department of Applied Linguisic, The Pennsylvania State University, University Park, PA, USA, 234 Sparks Bulding, PA, 16802, USA

# ARTICLEINFO

# ABSTRACT

Handling Editor: Dr Hilary Nesi

# Keywords:

Genre   
Move-step analysis ChatGPT   
Few-shot learning Prompt refinement Fine-tuning

Rhetorical move-step analysis has wielded considerable influence in the fields of English for Academic/Specific Purposes. To explore the potential of using ChatGPT for automated move-step analysis, this study examines the impact of few-shot learning, prompt refinement, and base model fine-tuning on its accuracy in move-step annotation. Our dataset consisted of the introduction sections of 100 research articles in the field of applied linguistics that have been manually annotated for move-steps based on a modified version of Swales' (1990) Create-a-Research-Space model, with 80 for training, 10 for validation, and 10 for testing. We formulated an initial prompt that instructed the base model to perform move-step annotation, evaluated it in a zeroshot setting on the validation set, and subsequently refined it with greater specificity. We also fine-tuned the base model on the training set. Evaluation results on the test set showed that fewshot learning and prompt refinement both led to significant albeit relatively small performance improvements, while fine-tuning the base model achieved substantially higher accuracies $( 9 2 . 3 \%$ for move and $8 0 . 2 \%$ for step annotation). Our results highlight the potential of using ChatGPT for discourse-level annotation tasks and have useful implications for EAP pedagogy. They also provide key recommendations for employing ChatGPT in research.

# 1. Introduction

Genre analysis has emerged as a crucial method for analyzing texts at the discourse level, pivotal in shaping pedagogical approaches aimed at providing novice writers with afoundational understanding of the rhetorical practices in new or unfamiliar genres (Moreno & Swales, 2018; Swales, 1990, 2004; Tardy, 2016). Particularly in the fed of English for Academic Purposes (EAP), theart of successful academic writing transcends the mere presentation of content; it critically hinges on adept rhetorical choices, which represent a daunting challenge for novice scholars (Kessler & Polio, 2023; Tardy, 2016). In response, genre analysts and EAP researchers have delved deep into exploring the rhetorical inricacies of prominent academic genres such as published research articles, doctoral diserations, and conference abstracts (e.g., Cotos e al., 2017; Lu et al, 2021a; Moreno & wales, 2018; Yoon & Casal, 2020. One of the most common analytical methods adopted in this feld is rhetorical move-step analysis, which involves disecting and describing therhetorical tructure of a gere through manual annotation of' moves, i. distinctive communicative units in atext, and steps, i.., smaller tx ments that build up move and hel ulfil the moe's urpose (Biber e al., 2007). Gre analysi research has offed valuable insights into the rhetorical structures of a range of academic (e.g., research artice, grant proposals, and lab reports) and non-academic genres (e.g., business emails) (e.g., Kesser, 2020; Park et al., 2021; Parkinson, 2017). These insights have been argued and shown to have potential for enabling learners to demystify and master the conventions of diferent types of academic discourse (e.g., Tardy, 2016).

Despite the widespread recognition of the usefulness of rhetorical move analysis, it scale is often limited by the time-consuming nature of manual coding (Casal & Kesslr, 2023). Although avariety of Natural Language Processig (NLP) tools have been developed to asses syntactic and lexical features of texts, such as the L2 Syntactic Complexity Analyzer (L2SCA; Lu, 2010) and the Tool for the Automatic Analysis of Lexical Sophistication (TAALES; Kyle, 2016), fewer automated tools for discourse-level analysi including genre analysis are available. This may not be surprising, as discourse-level annotation often requires a greater level of contextual under. standing of the text than lexical or syntactic annotation, making it more labor-intensive to render large amounts of manually laelled training data and more challenging for NLP models to achieve high accuracies in such annotation. Two pedagogically oriented automated genre analysis tols, AntMover (Anthony & Lashkia, 2003) and Research Writing Tutor (RWT; Cotos & Pendar, 2016) utilized traditional supervised leaning algorithms with lexical approaches such as bag of clusters and n-gram features, which may fal short in capturing the context-dependent complexity crucial for genre analysis. Models in the Generative Pre-trained Transformer (GPT) series ., ChatGPT), which utilize an extensive transformer-based neural network pre-trained on  ast corpus of text data, can potentially perform better on automated genre analysis, as they can process language with a deeper and more sophisticated understanding of context (Ray, 2023; Wu et al., 2023).

In order to examine the possbility of using ChatGPT for rhetorical move-step analysis, which involves a discourse-level annotation task, this study examines theffect of few-shot learning, prompt refinement, and base model fin-tuning on its accuracy in annotating move-steps. To this end, we sourced the introductio sections of 100 research articles in the field of applied linguisics that have been manually annotated for move-steps based on a modified version of Swales (1990) Creat-a-Research-Space model from the orpus of Social Science Research Article Introductions (Lu et al., 2021a). Our findings offer useful implications for employing ChatGPT and other large language models (LLMs) in discourse-level annotations and for EAP pedagogy. They also allow us to make important recommendations for using ChatGPT in research in general.

# 2. Literature review

# 2.1. Rhetorical move-step analysis and annotation tools

Over the past three decades, the concept of genre has been explored from diverse theoretical perspectives, including Systemic Functional Linguistics (Haliday, 1978), Rhetorical Genre Studies (Hyon, 1996; Paltridge, 1994), and Englis for Specific Purposes (ESP) (Hyland, 2007; Swales, 1990, 2004). Among these, the ESP approach stands out as particularly influential, which defines a genre as a clas of communicative events with purposes recognized and shared by the members of aspecific discourse community (Moreno & Swales, 2018; Swales, 1990, 2004). These events and their communicative practices are metaphoricall framed as rhetorical move. s-discoursal units serving coherent communicative functions and are further dissected into stes, more granular elements that help achieve the move's objective (Biber et al., 2007). Thus, genre analysis often referred to as rhetorical move (or move-step) analysis, seeks to unveil th recurring rhetorical tructures and linguistic features pivotal in accomplishing rhetorical purposes within a specific discourse community (Casal & Kessler, 2023; Swales, 1990, 2004).

This move-step analysis approach has profoundly impacted the field of EAP by providing insights into the rhetorical and linguistic features of diffrent types of academic writing, and such insights have formed a solid empiricl foundation for developing EAP curricula, resources, and teaching methods (e.., Cortes, 2013; otos et l, 2017; Kanoksilapatham, 2005; Lu et al., 2021a; essuto, 2015). One well-studied genre among this body of research is that of the research article (RA). For example, Kanoksilapatham (2005) analyzed 180 RAs from three engineering disciplines (civil, software, and biomedical) to identify the organizational structures of the texts with move-step analysis revealing how sub-disciplinary influences shape the construction of individual RA sections and offering engineering students and practitioners a versatile move-step framework to organize their ideas in alignment with the norms of ther disciplinary discourse. In addition to delineating the rhetorical structure of RAs and specific RA sections, some studies investigatedthe linguistic choices associated with diffrent rhetorical move-steps, which yielded valuable insights for EAP currculum and material development. Cortes (2013), for example, idntified leical bundes of various lengths in RA inroductions and linked those bundes to different moves and steps, resulting in a useful list for academic writers. Lu et al. (2021a) also presented a list of three types of phrase-frames from a corpus of 600 RA introductions, namely, those unique to a specific move-step, those primarily associated with one move-step but appearing in others, and those common across multiple move-steps without adirect link to any single one. The practical applications of move-step analysis have not ben imited to the RA genre, extending to various other genres such as con. ference abstracts, gant applicatios, business emails, and lab rorts ., asal & Ksler, 2023; Park et al., 2021; Parkinson, 2017; Yoon & Casal, 2020).

The move-step analysis typically involves a three-stage methodology as outlined by Casal and Kesser (2023): 1) development of a rhetorical move-ste model to capture paterns of rhetorical activity within the dataset, accommodating data variaility, 2) alication of the model to segment texts into rhetorical units, and 3) assessment and refinement of the move-step framework to enhance its reliabilit and validity through inter-coder agreement. Several recent studies have followed these steps in their move-step analysis, albeit with variations tailored to their particular research contexts (e.g., Cotos et al., 2017; Lu e al., 2021a; Yon & Casal, 2020). In studying established genres, researchers often begin with existing rhetorical move frameworks, such as the Create-a-Research-Space (CARS) model for RA introductions (Swales, 1990), adapting them to the specific rhetorical activities of the discourse community being studied. There is good consensus among researchers regarding the importance of coding procedure transparency and attention to inter-coder agrement and coding reliability (Casal & Kesser, 2023; Kim et al., 2024). Whereas the methodology is comprehensive and rigorous, the labor-intensive nature f the coding proces can ottill limit the scope of move-ste analysis studies. Specifically, this situation often compels researchers to work with smaller datasets, potentiall compromising the generalizability of their findings across more diverse datasets. Furthermore t also restrcts our abilit to conduct large-scale studies that explore the interaction among multiple text-related variables (e.g, discipline, part-genre), writer-related variables (e.g, L1 background, writing expertise), and linguistic features associated with move-steps.

Two tools desined to automate genre analysis for instructional purposes, ., AntMover (Anthony & Lashkia, 2003) and Research Writing Tutor (RWT; Cotos & Pendar, 2016), could be used to supplement manual analysis. AntMover was the first tool developed to automaticaly identify the tructure of RA abstracts acros various disciplines based on the CARS model. Trained using a Naive Bayes classifer, a type f supervised learning, on a dataset comprising 100 published astracts in the fieldf information technology, the tool achieved an average accuracy of $6 8 \%$ across its six-category classification system (Anthony & Lashkia, 2003), which could be considered relatively low for research purposes when compared to other NLP toos in our field. For instance, existing systems for word sense disambiguation (i.e, labeling instances of polysemous words with their specific meanings in context), atask that resembles the move-step annotation task to some extent, have achieved accuracies of around $9 0 \%$ (Lu & Hu, 2022). Although not extensively utilized for research, the tool has been adopted in a few genre-based intervention studies. For instance, Dong and Lu (2020) employed Ant Mover to facilitate guided enre analysis activities with 30 enginering master students, who were askedto use it to obtain a first-pass annotation of samples in aself-compiled specialized corpus of RA introductions in the students fields of study. Whil the activities enhanced the students genre knowledge and genre-based writing skill, the authors noted that AntMover's six categorie did not cover all move-steps in the corpus and asked the students to manually check the outut for missed or incorrctly taged sentences. his study highlights the potential of he automated gnre analysis ool for asting novice academic writers in mastering reearch article writing while also indicating areas for further enhancement. A more recent development in ths area is the RwT (Cotos & Pendar, 2016). Trained using a Support Vector Machine (SVM) algorithm on 650 RA introduction, this tool achieved average accuracies of $7 2 . 6 \%$ for move classification (3 classes) and $7 2 . 9 \%$ for step classification (17 classes). Cotos et al. (2020) explored 11 graduate-student writers' interactions with the tol, showing that the move-step tags it produced and its fdback and scaffolding features helped the students better understand the rhetorical structure of RA introductions, identify inconsistencies in their draft, and implement efective re. visions. However, this tool remains inaccessible to the public.

Although AntMover and RwT have exhibited substantial pedagogical value, there is much room for enhancement, particularly since they only rely on features based on bag of clusters and n-grams respectively in modeling. In genre analysis, understanding the context from surrounding sentences and the overall text fow i vitl. I other words, acurately identifying specificstructural steps often requires recognizing the context provided by adjacent steps (Biber e al., 2007). LLMs, known for their superior understanding of context (Ray, 2023), offer a complex architecture with a vastly larger number of parameters compared to simpler n-gram or bag of clusters approaches (Li et al., 2021; Ray, 2023), providing a promising avenue fr significant advancements in move-step annotation accuracy.

# 2.2.  ChatGPT (GPT-3.5) for classification tasks

Text classification, i.e, the procedure of assigning specific classlabels to texts, plays an important role in such NLP applications as sentiment analysis, topic labeling, and dialog act classfication (Li et al., 2021). Early research has tackled text classfication with traditional models such as Naive Baye, SVM, K-Nearest Neighbors (KN), and Random Forest paired with different types of linguistic features (e.g., bag of words, n-grams) (Li et al, 2021). While these models have advantages in stabilit, they require extensive feature engineering and ofte face performance limitations (Balkus & Yan, 2023; Li et al., 2021), partly due to their neglect of the inherent sequential structure and contextual detail in text, which hinders their abilit to grasp the meanings of words and other linguistic expressios in context (Brown et al., 2020; Li e l., 2021). I recogntio of these limitations, rent NLP rearch has pivoted towards deep learning models, which enable classifiers to capture complex word characteristics and contextual variation (Liu et al, 2023). OpenAI's GPT models and it consumer-facing service ChatGPT exemplify ths shift. With a high level of contextual understanding, these models can generate precise, relevant responses to user prompts, and their performance on specific tasks and/or in specific domains can be further improved through few-shot learning, prompt engineering, or fine-tuning (Brown et al., 2020; Kocon et al., 2023; Ray, 2023).

Few-shot learning operates by providing the model with $K$ examples of paired contexts and completions, followed by a single context example, from which the model is then tasked to predict the completion, whil zero-shot learning relies on a task's natural language description (i.e., prompt) only without any examples (Brown et al., 2020). Fine-tuning the base model with atraining dataset, essentially an extended form of few-shot learning, involves increasing the $K$ value to adjust the model's pre-trained parameters on a domain-specific dataset. Few-shot learning and fine-tuning are now both frequently employed to enhance the performance of the base model on targeted classfication tasks (Brown et al., 2020; Wei et al., 2022). For example, Loukas et al. (2023) reported that in classifying customer service querie into 77distinct intent categories, GPT-3.5 in a one-shot setting and GPT-4 in a three-shot setting achieved F1 scores of $7 4 . 3 \%$ and $8 2 . 7 \%$ respectively, outperforming several fine-tuned masked language models (e.g., P-MPNet-v2) in the same settings. Wachowiak and Gromann (2023) reported that in a 12-shot setting, GPT-3 achieved an accuracy rate of $6 5 . 1 5 \%$ in detecting the source domains of conceptual metaphors.

Prompt enginering refers to the process f carefully designing and refining the input prompts provided to language models to elicit specific or more accurate responses (Brown et al., 2020). This method could prove especially advantageous for genre analysis, because it could address the dfficult in automatically identifying cerain stes whose rhetorical meanings are not explicitly conveyd through functional language, a diffculty discussed in Cotos and Pendar (2016). While the difficulty makes the steps chllenging to detect with traditional supervised learning model, we have the potential,by utilizing ChatGPT, rfined via prompt enginering, to operationalize some of these challenging steps. Although rearch into prompt enginering is sill i it early stage, one notable study by Fatouros et al. (2023) demonstrated it effcacy. They used ChatGPT to perform sentiment analysis on financial news headines in a zero-shot setting. By providing questions or statements crafed in a way that guides the model to increasingly beter understand the task, they achieved a $3 5 \%$ performance improvement over traditional financial sentiment analysis models, such as FinBERT, showing the promising capabilite of ChatGPT when combined with carefully engineered prompts. Huang et al. (2023) alo examined ChatGPT's capabilit to detect and explain implicit hate speech in hateful tweets through careful prompt designs in a zero-shot setting. They reported that the model correctly identified $8 0 \%$ of implicit hateful tweets.

Within the fields of applied linguistics and language education, a few efforts have een made to explore the potential of ChatGPT in such tasks as automated esay coring and question generation. For example, Mizumoto and Eguchi (2023) employed the GPT-3 model to score 12,100essays sourced from the ETS Corpus of Non-Native Writen English (TOEFL11) and reported that the model alone could not predict the gold standard levels of the essays with an adequate level of accuracy and that a model combining the GPT-predicted scores with a set of lexical, yntactic, and cohesion features achieved beter performance. Lee et al. (2023) used ChatGPT to create an automatic question generation system for English reading comprehension and developed a step-by-step protocol for generating high-quality questions with ChatGPT through multiple validation rounds by experts and teachers. The protocol included the need to limit the passage length to 250 words and to clearly specify question types and formats i the prompt. They also noted some limitations of ChatGPT such as its dependence on the lexicon of the original texts and the restriction of question types to mainly WH-questions. The two studies above demonstrated the potential of using ChatGPT in language asessment and material development, while also pointing to areas for further exploration and improvement.

While previous studies have exhibited the potential of ChatGPT for classfication tasks, this potential and the challenges that may arise along with it have not yet been systematically exploited for discourse-level corpus annotation within the fields of applied linguistics and language education, and no study has explored the possbility of using ChatGPT for rhetorical move-step analysis. This analysis also differs from other clasification tass examined in existing studies in terms of the number and functional nature of the classes involved. To addres this gap, this study explores the potentia of using ChatGPT for rhetorical move-step annotation and examines the impact of prompt refinement, thee-shot learning, and fine-tuning on the model's annotation acuracy. The indings are anticipated to shed light on the feasibility of employing ChatGPT for other types of corpus annotation and analysis that can be framed as classfication tasks as well as the potential applications of ChatGPT in genre-based EAP pedagogy.

# 2.3. Research questions

This study aims to address the following two research questions:

Table 1 Move-step framework for research article introductions (Lu et al., 2021a).   

<html><body><table><tr><td>Move/Step</td><td>Description</td><td>Tag</td></tr><tr><td>Move 1</td><td>Establishing a research territory</td><td></td></tr><tr><td>Step 1</td><td>Claiming centrality or value of research area</td><td>[M1_S1a]</td></tr><tr><td>Step 1</td><td>Real-world contextualization</td><td>[M1_S1b]</td></tr><tr><td>Step 2</td><td>Making generalizations about research area</td><td>[M1_S2]</td></tr><tr><td>Step 3</td><td>Reviewing items of previous research</td><td>[M1_S3]</td></tr><tr><td>Move 2</td><td>Establishing a niche</td><td></td></tr><tr><td>Step 1</td><td>Counter-claiming</td><td>[M2_S1a]</td></tr><tr><td>Step 1</td><td>Indicating a gap</td><td>[M2_S1b]</td></tr><tr><td>Step 1</td><td>Question raising</td><td>[M2_S1c]</td></tr><tr><td>Step 1</td><td>Continuing a tradition</td><td>[M2_S1d]</td></tr><tr><td>Step 1</td><td>Pointing out limitations of previous research</td><td>[M2_S1e]</td></tr><tr><td>Step 2</td><td>Providing justification</td><td>[M2_S2]</td></tr><tr><td>Move 3</td><td>Presenting the present work via</td><td></td></tr><tr><td>Step 1</td><td>Announcing present research</td><td>[M3_S1]</td></tr><tr><td>Step 2</td><td>Presenting research questions or hypotheses</td><td>[M3_S2a]</td></tr><tr><td>Step 2</td><td>Advancing new theoretical claims</td><td>[M3_S2b]</td></tr><tr><td>Step 3</td><td>Definitional clarification</td><td>[M3_S3]</td></tr><tr><td>Step 4</td><td>Summarizing methods</td><td>[M3_S4a]</td></tr><tr><td>Step 4</td><td>Explaining a mathematical model</td><td>[M3_S4b]</td></tr><tr><td>Step 4</td><td>Describing analyzed scenario</td><td>[M3_S4c]</td></tr><tr><td>Step 5</td><td>Announcing and discussing results</td><td>[M3_S5]</td></tr><tr><td>Step 6</td><td>Stating the value of present research</td><td>[M3_S6]</td></tr><tr><td>Step 7</td><td>Outlining the structure of the paper</td><td>[M3_S7]</td></tr><tr><td>Step 8</td><td>Rationalizing research focus and design</td><td>[M3_S8]</td></tr><tr><td>Step 9</td><td>Presenting limitations of current study</td><td>[M3_S9]</td></tr></table></body></html>

1. To what extent can few-shot larning and prompt refinement improve the performance of the base model of ChatGPT (GPT-3.5) for annotating applied linguistics research article introductions with rhetorical move-steps?   
2. To what extent can fine-tuning improve the performance of the base model of ChatGPT (GPT-3.5) for annotating applied inguistics research article introductions with rhetorical move-steps?

# 3. Methodology

# 3.1. Data source

To addressthe two research questions, the present study employed the data from the Corpus of Social Science Research Article Introductions (CossRAl) (Lu et al., 2021a), which consists of the introduction sections of 600 published research articles from six social science disciplines: Anthropology, Applied Linguistics, Economics, Political Science, Psychology, and Sociology. For each discipline, the corpus compilers sampled the introduction sections of 100 RAs published in five high-impact journals endorsed by two disciplinary experts in 2012-2016, with one sample per issue and four samples per year. Each RA introduction was converted into a plain text file and manually cleaned to remove formulas, figures, table, parenthetical citation elements, and any textual oddities arising from the text conversion process. The resulting corpus consisted of 513,688 words across 600 introductions (mean $= 8 5 6$ $\mathbf { S D = }$ 476).

The corpus was manually annotated for rhetorical move-steps by a team of seven researchers based on a collaboratively refined version of wale's (1990) CARS model (see Table 1). The nnotation proceeded in three stages: framework reinement, inter-annotator agreement assessment, and independent coding. In the first stage, the seven researchers independently coded the same 30 introductions, reconciled discrepancies through extensive group discussion, and modified some steps in the CARS model based on the data. In the second stage, the team was divided into three pairs, with the seventh researcher working as a coordinator. Each pair was assigned a distinct st f 10texts. The two researchers in each pair frt independently annotatd the 10 texts, and these annotations were used to assess inter-annotator agreement. The average percent agreement was $9 1 . 7 \%$ for moves and $7 2 . 1 \%$ for steps. Each pair attempted to resolve discrepancies through discussion first, and all seven researchers then convened to resolve all remaining discrepancies from the thre pairs and made additional minor adjustments to the framework. As a result of the two stages, multiple substeps were added to the CARs model to account for the rhetorical functions that emerged from the data. In the final phase, all remaining texts were split into seven batches, each annotated by one researcher and reviewed by another. Discrepancies were resolved in a series of team meetings.

Throughout the annotation process, the team used the rhetorical chunk as the unit of analysis conducting a comprehensive examination of linguisic, structural, and content-based cues indicating shift in the authors' communicative goals and functions. Moves and steps were not treated as formal units. However, following Cotos and Pendar (2016) and Cotos et al. (2017), they employed the sentence as the unit of annotation, assigning rhetorical move-step tags (se the Tag column in Table 1) to the end of each sentence. While the unit of annotation (i.e, sentence) often does not align with the unit of analysis (i.e., rhetorical chunks), sentence-level annotation offrs practical advantages in maintaining consistency and improving accuracy in rhetorical chunk identification. In ther word se st aruniverally reoizd nd sil dntfiale units n wrting, thy offr r and consstt basis for marking textual features and communicative functions, whil chunk-level annotation may add an additional layer of subjectivity (i. e., potential inconsistencies in identifying rhetorical chunk boundaries) to the coding process. Moreover, detailed sentence-level annotations can highlight specific inguistic and rhetorical cues that signal the start and end of rhetorical chunks. For example, transitions, conjunctions, and topic sentences that may denote boundaries within the text can be more accurately identified when each sentence is meticulously analyzed. This precision at the sentence level makes i easier to aggregate sentences into meaningful chunks based on shared functions. This coding approach resulted in some sentences receiving multiple tags, with the first tag indicating the function realized by the main clause and other tags indicating functions realized by other parts of the sentence.

As the firs effort to test the possibility of using ChatGPT for rhetorical move-step annotation, we chose to focus on the field of Applied Linguistics. This decision was informed by the recognition that RA introductions have exhibited not only common charateristics across disciplines but also unique rhetorical and linguistic features pecific to each discipline Hyland, 2015; Lu et al., 2021b) The Applied Linguistics subcorpus contained 100 RA introductions with 63,333 words $( { \mathrm { m e a n } } = 6 3 3$ $S \mathrm { D } = 4 1 3 $ . We employed stratified random sampling to create two distinct subsets of 10 sample first, each with two introductions from five diferent journal. One was designated as the validation set (257 sentences) and the other as the testing set (182 sentences). The remaining 80 samples served as the training set (1556 sentences), with a subset of 0 samples (795 sentences) from this set employed toasses the impact of training sample size on the performance. The use of relatively smal training sample sizes (40 and 80 RA inroductions) in our ex. periments aligns with the capabilitie of ChatGPT (or LLMs in general) to lean from small training samples (Brown et al., 2020) and allows us to compare these capabilites against those of traditional machine learning models. For comparison, AntMover was trained on 80 abstracts and achieved an average accuracy of $6 8 \%$ on a test set of 20 abstracts. Contrastively, RwT was trained on a large training set of 650 introductions and achieved higher accuracies of $7 2 . 6 \%$ for moves and $7 2 . 9 \%$ for steps on a test set of 37 introductions. If ChatGT achieves higher acuracies with fewer training samples, it would confirm a mai benefit of LLMs, namely, ther efficiency in handling classfication tasks such as move-step annotation with smaler training sets compared to traditional machine learning algorithms (Brown et al., 2020). We adopted the standard practice of dividing data into training, validation, and test sets, typically at an 80/10/10ratio because this aproach has ben both theoretically and empiriclly found to ensure sfficient training data whil preventig overfitting, contributing to models that are both les error-prone and more generalizable (Gholamy et al, 2018; Pandey et al., 2022).

To further validate our results, we added two more test sets, one with 10 introductions (103 sentences) from Psychology and the other with 10 introductions (151 sentences) from Anthropology. For each set, we randomly sampled two introductions from each ofthe five journals in the respective field. These fields frequently intersect with applied linguistics and the additional testing can help evaluate the extent to which the model's ability could generalize across closely related fields.

# 3.2. Procedure

To examine the impact of few-shot learning, prompt refinement, and base model fine-tuning on the accuracy of ChatGPT for annotating rhetorical move-steps in RA introductions, we fllowed the procedure summarized in Fig. 1. We frst designed an initial prompt to guide the GPT-3.5 base model to annotate unlabeled RA introductions with rhetorical move-steps. This initial prompt included the definitions of moves and steps and detaled explanations of the goals of rhetorical move-step analysis, the unit of annotation (i.e., sentences), and the move-step framework and tags to be used in the annotation as outlined in Table 1. It further provided explanations for genre (.e, RAs), part-genre (i.., introductions), and discpline (i.., applied linguistics) of the texts to be annotated. The prompt then guided the model to segment paragraphs into sentences and annotate each sentence according to the specified framework. As some sentences in the corpus were annotated with multiple tags to account for their potential multifunctionality (Cotos et al., 2017; Lu et al., 2021a), the prompt alo included the possility of assiging multiple tas to a sentence with multiple rhetorical functions. Weevaluated the performance of the base model with the nitial prompt on the validation set, scrutinized the confusion matrix to identify potential areas of ambiguity for the model, and reined th itial prompt in an effrt t help the model address those areas. The full initial and refined prompts can be found in the Appendix.

The base model was subsequently evaluated on the test set in four setings: 1) with the initial prompt in a zero-shot setting, 2) with the initial prompt in a three-shot stting, 3) with the refined prompt i a zero-shot setting, and 4) with the refined prompt in a three. shot setting. In the three-shot setting, we provided the model with thre annotated samples (totaling 38 sentences) randomly selected from the training set. Finally, we proceeded to fine-tune the base model using the traning set (40 samples and 80 samples) with the refined prompt, aiming to enhance is annotation accuracy and adaptability to our specific domain. To this end, we utilized the gpt-3.5- turbo-1106 model recommended by OpenAI. We fine-tuned the model using the OpenAI API in Python and conducted evaluations within the Playground system. The OpenAI Playground is a web-based interface that simplifies the proces of constructing and testing predictive language models (OpenAl, 2024). This user-friendly platform allows users to select and load the desired model and enter system and user messages through a chat-like interface It offers a more interactive experience than coding in Python similar to chatting with ChatGPT, but with additional features such as the ability to adjust parameters, load specific models, maintain consistent prompts, and save results. Given an input text, the system outputs the text along with the assgned tags. We opted to use this platform for testing both the base and fine-tuned models, rather than batch-processing text via the PI in Python, because this approach may be more accessible to researchers and teachers who are not familiar with coding, making it esier for future researchers and teachers to replicate or further test our method.

Additionally, we conducted two further stages of validation. First, we tested the base model in a thre-shot setting with the refined prompt and also assessed the 80-introduction fine-tuned model (using the same refined prompt) with the same test set on three separate occasions to evaluate resut consistency. For each test we initiated a new session to ensure that the model did not retain any previous information. Subsequently, we asessed the performance of the 80-introduction fine-tuned model using additional test sets from Psychology and Anthropology, to further validate our resuls. OpenAl confirms that their models are not trained with data input through the API and Playground services (OpenAI, 2024).

# 3.3. Evaluation

To evaluate the performance of the base model i differen settings and of the fine-tuned models, we computed the precision, recall and F1 score for each move and step and the overall acuracy and weighted average precision, recall, and F1 score for all moves and steps. For each move or step, precision was the ratio of true positive (TP) predictions to the total number of true and false positive (FP) predictions, recall was the ratio of TP predictions to the total number of TP and false negative (FN) predictions, and the F1 score was the harmonic mean of precision and recall, i.e., ( $2 ^ { * }$ Precision \* Recall)/(Precision $^ +$ Recall). The harmonic mean is a type of average of rates or ratios that balances them in a way that does not disproportionally bias toward any of them. In the case of F1 score, it combines precision and recall into asingle measure that balances both, ensuring tha nither is disproportionately emphasized. For ll moves and steps, the overall accuracy was the ratio of correct redictions to the total number of predictions, and the weighted average precision, recall and F1 core were computed by firs weighting the value for each move or step by the number of actual instances of that move or step, then summing those weighted values, and finlly normalizing the summed weighted values by the total number of instances. For all metrics, the values range from 0 to 1, with a higher value indicating beter performance. These metrics, along with the confusion matix, were computed using the klearn.metrics package in Python. In the small proportion (10 out of 182) of cases where multiple labels were assigned to a singlesentence, the sentence was considered to have een corrctly annotated when at least one predicted tag matched the primary tag (i.e, the first tag) of the sentence to facilitate comparison with previous results. The significance of per formance differences between different experiment settings was tested with McNemar's test, a well-established statistical test for scenarios like ours based on the Chi-squared $( \chi ^ { 2 } )$ distribution (e.g., Dietterich, 1998; Kavzoglu, 2017). This test was performed using the statesmodels package in Python.

![](img/463d6cd54f8bc75db5a6cedd7d3cd7ebdf8c28fa9c6ead473c69d98097a01504.jpg)  
Fig. 1. Evaluation procedure of ChatGPT's accuracies in move-step annotation.

# 4. Results

# 4.1. Impact of prompt refinement and three-shot learning

In order to answer our first research question, we first evaluated the performance of the base model with the initial prompt in a zero-shot seting on the validation set. For move classification, the model achieved an accuracy of $2 8 . 5 \%$ and weighted average precision, recall, and F1 scores of $6 2 . 0 \%$ $2 8 . 5 \%$ , and $3 2 . 9 \%$ , respectively. For step classification, the model achieved an accuracy of $1 1 . 3 \%$ and weighted average precision, recall, and F1 scores of $3 9 . 2 \% $ $1 1 . 3 \%$ , and $1 3 . 4 \%$ respectively.

Following this evaluation, we examined the confusion matrice for both move and step classfications to refine the initial prompt. The recall rates for Moves 1 $( 2 0 . 5 \% )$ and 2 $( 2 8 . 5 \% )$ were both rather low, indicating that these moves were frequently misclassified as other moves. The analysi of the move-classification matrix and the misclassfied instances helped us identify potential sources of the confusion. For example, in Example 1, the model classfied an M1 S3 (Reviewing items of previous research) sentence as M3_S5 (Announcing and discussing result). Although the framework in the prompt indicated that Move 3 involves discussion of the present study by prefacing the Move description with Presenting the present work via, the model may have not associated that part of the description as constraining the subsequent descriptions of the steps. Thus, we added an explicit phrase in the step description to specify the boundary of the move (i., M3 5- Announcing and discussing the results/principal outcomes of the present reearch). In a similar fashion, we refined the descriptions of all moves and steps to clearly indicate the boundaries of each move and the scope of each step. At the move level, we revised "Establishing a research territory" as Establishing the broader research territory within which the present study is situated for Move 1, and Establishing a niche" as Identifying a niche in previous research" for Move 2. Examples of step description revisions included expanding indicating a gap' to indicating a gap in previous research'" and \*providing justifica tion' to \*providing justification of the present research area based on previous research".

Example 1

True Code: M1_S3 (Reviewing items of previous research) Predicted Code: M3_S5 (Announcing and discussing results)

Meara & Buxton argue that vocabulary recognition from various frequency bands is an eficient way of quantifying the number of words L2 learners actually know.

Misclassifications at the step level were most common between M1 S2 and M1 S3, as illustrated in Example 2. In the manual coding, M1 3 was designated for units focusing on a specific study while establishing the research teritory. To address this confusion, we refined the description of M1 S3 as "Reviewing a specific previous research study (reviewing one specific study, and human names typically indicate specific studes referenced within the article in an effort to clarify for the model that this step should be applied to units reviewing a particular previous research study and that the presence of author names could be a helpful indicator of this step. Overall, the prompt refinement proces resulted in step descriptions with increased specificity. With the refined prompt (see the

Appendix), the performance of the base model on the validation improved. For move classification, it achieved an accuracy of $3 7 . 9 \%$ and weighted average precision, recall, and F1 scores of $5 8 . 7 \%$ $3 7 . 9 \%$ , and $4 3 . 1 \%$ , respectively. For step classification, it achieved an accuracy of $1 4 . 5 \%$ and weighted average precision, recall, and F1 scores of $3 0 . 2 \%$ $1 4 . 5 \%$ and $1 6 . 7 \%$ , respectively. McNemar's tests indicated a significant improvement in move classification accuracy $( \chi ^ { 2 } = 7 . 3 7 4$ $\begin{array} { r } { p = 0 . 0 0 7 \mathrm { . } } \end{array}$ but not step classification accuracy $( \chi ^ { 2 } =$ 1.641, $p = 0 . 2 0 0 \}$

Example 2

True Code: M1_S2 (Making generalizations about research area) Predicted Code: M1_S3 (Reviewing items of previous research)

Research within the field of interlanguage pragmatics (ILP) has established that L2 pragmatics can be effectively taught.

The base model's performance was subsequently evaluated on the test set with the initial and refined prompts in zero-shot and three-shot settingsto assessthe impact of prompt refinement and few-shot learning, both searately and i tandem. In the three-shot settings, we provided the model with three manually annotated introduction sections (totaling 38 sentences) randomly sampled from the training set. The results of this evaluation are presented in Table 2 and visualized in Fig. 2 (for move classfication) and Fig.3 (for step classification).

With the initial prompt in a zero-shot setting, the base model achieved accuracy and F1 scores of $4 2 . 9 \%$ and $4 9 . 5 \%$ for move classification and of $1 7 . 0 \%$ and $2 0 . 1 \%$ for step classification. Integrating three-shot learning with the initial prompt led to some performance improvement, with accuracy and F1 scores increased to $4 6 . 2 \%$ and $5 1 . 4 \%$ for move classification and to $1 8 . 7 \%$ and $1 9 . 6 \%$ for step classification. However, the difference in accuracy was not statistically significant for either move $( \chi ^ { 2 } = 0 . 5 4 3$ $\begin{array} { r }  p = 0 . 4 6 1 \ \end{array}$ or step classification $( \chi ^ { 2 } = 0 . 1 2 1$ $p = 0 . 7 2 8 )$ . Replacing the initial prompt with the refined prompt also led to some performance improvement, with accuracy and F1 scores increased to $5 0 . 5 \%$ and $5 4 . 0 \%$ for move classification and to $1 9 . 8 \%$ and $2 2 . 8 \%$ for step classification, but the change in accuracy was again insignificant for either move $( \chi ^ { 2 } = 3 . 5 2 1$ $\begin{array} { r } { p = 0 . 0 6 1 \mathrm { . } } \end{array}$ or step classification $( \chi ^ { 2 } =$ 0.410, $\pmb { p = 0 . 5 2 2 } ,$ . Finally, combining the refined prompt with three-shot learning led to the greatest performance improvement, with accuracy and F1 scores increased to $5 2 . 7 \%$ and $5 8 . 3 \%$ for move classification and to $2 5 . 3 \%$ and $2 7 . 2 \%$ for step classification. Compared to the accuracy of base model with the intial prompt in a zero-shot setting, the improvement was statistically ignificant for both move $( \chi ^ { 2 } = 4 . 8 1 7$ $\pmb { p = 0 . 0 2 8 } )$ and step classification $( \chi ^ { 2 } = 5 . 6 0 0 , p = 0 . 0 1 8 )$ . Notably, the base model achieved a higher precision than recall inall four experimental settigs. Three-shot learning and prompt refinement led to recall improvement for both move and step classification. However, they reulte in lower prcision for move classfication and mixed changes in recal for step classfication.

# 4.2. Impact of fine-tuning GPT3.5

As shown in Table 2, the fine-tuned models demonstrated substantially improved performance for move (Fine-tuned Model 1: precision $= 9 2 . 5 \%$ $\mathrm { \ e c a l l } = 9 1 . 2 \%$ , F1 score $: = 9 1 . 8 \%$ , accuracy $= 9 1 . 2 \%$ ; Fine-tuned Model 2: precision $= 9 2 . 2 \%$ $\mathrm { r e c a l l } = 9 2 . 3 \%$ , F1 $\mathrm { ; c o r e } = 9 2 . 2 \%$ , accuracy $= 9 2 . 3 \%$ and step classification (Fine-tuned Model 1: precision $= 7 8 . 5 \%$ recall $1 = 7 5 . 3 \%$ F1 score $= 7 6 . 2 \%$ accuracy $= 7 5 . 3 \%$ ; Fine-tuned Model 2: precision $= 8 0 . 6 \%$ , recall $= 8 0 . 2 \%$ , F1 score $= 8 0 . 2 \%$ , accuracy $= 8 0 . 2 \%$ . The accuracies were significantl higher than that achieved by the base model with the refined prompt in a thre-shot setting for both move (Fine-tuned Model 1 $: \chi ^ { 2 } = 5 9 . 0 4 6 , p < 0 . 0 0 1$ ; Fine-tuned Model 2: $\chi ^ { 2 } = 5 9 . 5 6 8 , p < 0 . 0 0 1 )$ and step classification (Fine-tuned Model 1: $\chi ^ { 2 } = 7 6 . 6 7 6$ $p < 0 . 0 0 1$ ; Fine-tuned Model $2 \colon \chi ^ { 2 } = 9 1 . 0 0 0 , p < 0 . 0 0 1 \rangle$ . The two fine-tuned models' performance did not differ significantly for either move $( \chi ^ { 2 } = 0 . 0 5 0$ $\pmb { p } = 0 . 8 2 3 )$ or step $( \chi ^ { 2 } = 3 . 0 6 4$ $\begin{array} { r } { p = 0 . 0 8 0 \mathrm { , } } \end{array}$

Table 3 presents the performance of the best-performing model - Fine-tuned Model 2 (with 80 samples for training) on each move Notably, for Moves 1 and 3, the model achieved precision, recall, and F1 scores between $9 3 . 8 \%$ and $1 0 0 \%$ . However, its performance in classifying Move 2 was lower, with the three metrics hovering around $7 0 \%$ . Table 4 presents the performance of the fine-tuned model on each step. For seven steps, the model achieved precision, recall, and F1 score all over $8 0 \%$ . Meanwhile, it performed the worst on M1 S1a (Claiming centrality or value of research area), M2 S1c (Question raising), M2 S2 (Providing justification), and M3_S2a (Presenting research questions or hypotheses) with precision, recall, and F1 scores all at or below $5 0 \%$

To assess the stability and consistency of the base and fine-tuned models with the same inputs under identical conditions (using the same model parameters and prompt), we conducted thee additional tests with the reined prompt for both the base model (three-shot setting) and Fine-tuned Model 2, intiating each test in a new session to ensure no retention of previous data. The outcomes, as shown in Table 5, were relatively consistent. The base model achieved F1 scores ranging from $5 6 . 2 \%$ to $6 3 . 5 \%$ for move classification and from $2 3 . 2 \%$ to $2 9 . 7 \%$ for step classfication. Meanwhile, the fine-tuned model displayed greater consistency with F1 scores ranging from $9 1 . 9 \%$ to $9 3 . 0 \%$ for move classification and from $7 8 . 6 \%$ to $8 1 . 7 \%$ for step classification.

Table 2 Classification task evaluation results.   

<html><body><table><tr><td></td><td></td><td>Initial 0 shot</td><td>Initial 3 shot</td><td>Refined O shot</td><td>Refined 3 shot</td><td>Fine-tuned 1</td><td>Fine-tuned 2</td></tr><tr><td>Precision</td><td>Move</td><td>77.1%</td><td>73.6%</td><td>73.9%</td><td>72.7%</td><td>92.5%</td><td>92.2%</td></tr><tr><td></td><td>Step</td><td>48.4%</td><td>59.7%</td><td>54.0%</td><td>43.2%</td><td>78.5%</td><td>80.6%</td></tr><tr><td>Recall</td><td>Move</td><td>42.9%</td><td>46.1%</td><td>50.5%</td><td>52.7%</td><td>91.2%</td><td>92.3%</td></tr><tr><td></td><td>Step</td><td>17.0%</td><td>18.7%</td><td>19.8%</td><td>25.3%</td><td>75.3%</td><td>80.2%</td></tr><tr><td>F1 score</td><td>Move</td><td>49.5%</td><td>51.4%</td><td>54.0%</td><td>58.3%</td><td>91.8%</td><td>92.2%</td></tr><tr><td></td><td>Step</td><td>20.1%</td><td>19.6%</td><td>22.8%</td><td>27.2%</td><td>76.2%</td><td>80.2%</td></tr><tr><td>Accuracy</td><td>Move</td><td>42.9%</td><td>46.2%</td><td>50.5%</td><td>52.7%</td><td>91.2%</td><td>92.3%</td></tr><tr><td></td><td>Step</td><td>17.0%</td><td>18.7%</td><td>19.8%</td><td>25.3%</td><td>75.3%</td><td>80.2%</td></tr></table></body></html>

Note. Initial $=$ the initial prompt, Oshot $=$ zero-shot, Refined $=$ the refined prompt, 3shot $=$ three-shot. Fine-tuned $^ { 1 = }$ the fine-tuned model with 40 training samples, Fine-tuned $2 =$ the fine-tuned model with 80 training samples.

![](img/b7a6c990d9f5d5f28908165fcaac698d1c82aedc7acb4c80726f703e56da9701.jpg)  
Fig. 2. Move classification evaluation.

![](img/46fdfa6a4428daf5be9b060de0688a995fc701f7e56a6760361586abebabd5a8.jpg)  
Fig. 3. Step classification evaluation.

Table 3 Precision, recall, and F1 score of Fine-tuned Model 2 for move classification.   

<html><body><table><tr><td>Move</td><td> Precision</td><td> Recall</td><td>F1 score</td><td>Support</td></tr><tr><td>M1</td><td>95.6%</td><td>93.9%</td><td>94.7%</td><td>115</td></tr><tr><td>M2</td><td>71.4%</td><td>68.2%</td><td>69.8%</td><td>22</td></tr><tr><td>M3</td><td>93.8%</td><td>100.0%</td><td>96.8%</td><td>45</td></tr><tr><td>Weighted average</td><td>92.2%</td><td>92.3%</td><td>92.2%</td><td>182</td></tr></table></body></html>

Note. Support denotes the number of actual occurrences.

Subsequent validation was conducted using the two additional test sets. For the Anthropology test set, the model achieved a precision of $8 4 . 5 \%$ , recall of $8 6 . 7 \%$ F1 score of $8 5 . 0 \%$ , and accuracy of $8 7 . 7 \%$ for move classification, along with a precision of $7 2 . 6 \%$ recall of $7 5 . 3 \%$ , F1 score of $7 2 . 2 \%$ , and accuracy of $7 5 . 3 \%$ for step classification. For the Psychology test set, the model achieved a precision of $9 2 . 0 \%$ , recall of $9 1 . 2 \%$ F1 score of $9 1 . 1 \%$ and accuracy of $9 1 . 2 \%$ for move classification, along with a precision of $7 5 . 5 \%$ recall of $7 1 . 6 \%$ , F1 score of $7 1 . 7 \%$ , and accuracy of $7 1 . 6 \%$ for step classification.

Table 4 Precision, recall, and F1 score of Fine-tuned Model 2 for step classification.   

<html><body><table><tr><td>Step</td><td>Precision</td><td>Recall</td><td>F1 score</td><td>Support</td></tr><tr><td>M1_S1a</td><td>28.6%</td><td>33.3%</td><td>30.8%</td><td>6</td></tr><tr><td>M1_S1b</td><td>85.7%</td><td>100.0%</td><td>92.3%</td><td>6.</td></tr><tr><td>M1_S2</td><td>88.6%</td><td>84.9%</td><td>86.7%</td><td>73</td></tr><tr><td>M1_S3</td><td>82.8%</td><td>80.0%</td><td>81.4%</td><td>30</td></tr><tr><td>M2_S1b</td><td>61.5%</td><td>66.7%</td><td>64.0%</td><td>12</td></tr><tr><td>M2_S1c</td><td>0.0%</td><td>0.0%</td><td>0.0%</td><td>3</td></tr><tr><td>M2_S1e</td><td>66.7%</td><td>66.7%</td><td>66.7%</td><td>3</td></tr><tr><td>M2_S2</td><td>40.0%</td><td>50.0%</td><td>44.4%</td><td>4</td></tr><tr><td>M3_S1</td><td>94.1%</td><td>88.9%</td><td>91.4%</td><td>18</td></tr><tr><td>M3_S2a</td><td>50.0%</td><td>100.0%</td><td>66.7%</td><td>1</td></tr><tr><td>M3_S3</td><td>100.0%</td><td>90.9%</td><td>95.2%</td><td>11</td></tr><tr><td>M3_S4a</td><td>66.7%</td><td>66.7%</td><td>66.7%</td><td>6</td></tr><tr><td>M3_S5</td><td>100.0%</td><td>100.0%</td><td>100.0%</td><td>1</td></tr><tr><td>M3_S7</td><td>80.0%</td><td>100.0%</td><td>88.9%</td><td>4</td></tr><tr><td>M3_S8</td><td>66.7%</td><td>100.0%</td><td>80.0%</td><td>4</td></tr><tr><td>Weighted Average</td><td>80.6%</td><td>80.2%</td><td>80.2%</td><td>182</td></tr></table></body></html>

Note. Support denotes the number of actual occurrences; not all steps in the framework were present in the test set.

Table 5 Outcome consistency.   

<html><body><table><tr><td colspan="2"></td><td colspan="4">Move</td><td colspan="4">Step</td></tr><tr><td>Model</td><td>Attempts</td><td>1</td><td>2</td><td>3</td><td>4</td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td rowspan="4">Base</td><td>Precision</td><td>72.7%</td><td>76.5%</td><td>82.4%</td><td>74.5%</td><td>43.2%</td><td>60.9%</td><td>57.8%</td><td>53.1%</td></tr><tr><td>Recall</td><td>52.7%</td><td>51.6%</td><td>59.3%</td><td>52.2%</td><td>25.3%</td><td>19.2%</td><td>25.8%</td><td>23.1%</td></tr><tr><td>F1 score</td><td>58.3%</td><td>56.2%</td><td>63.5%</td><td>56.5%</td><td>27.2%</td><td>23.2%</td><td>29.7%</td><td>24.4%</td></tr><tr><td>Accuracy</td><td>52.7%</td><td>51.6%</td><td>59.3%</td><td>52.2%</td><td>25.3%</td><td>19.2%</td><td>25.8%</td><td>23.1%</td></tr><tr><td rowspan="4">Fine-tuned</td><td>Precision</td><td>92.2%</td><td>92.7%</td><td>93.1%</td><td>92.1%</td><td>80.6%</td><td>80.4%</td><td>83.6%</td><td>81.4%</td></tr><tr><td> Recall</td><td>92.3%</td><td>92.3%</td><td>93.4%</td><td>91.8%</td><td>80.2%</td><td>77.4%</td><td>81.9%</td><td>81.3%</td></tr><tr><td>F1 score</td><td>92.2%</td><td>92.5%</td><td>93.0%</td><td>91.9%</td><td>80.2%</td><td>78.6%</td><td>81.7%</td><td>80.9%</td></tr><tr><td>Accuracy</td><td>92.3%</td><td>92.3%</td><td>91.8%</td><td>93.4%</td><td>80.2%</td><td>77.5%</td><td>81.9%</td><td>81.3%</td></tr></table></body></html>

Note. The outcomes of the 1st attempt are the ones reported in Tables 2-4.

# 5. Discussion

The findings for the firt research question uncovered several intriguing aspects pertaining to the available domain (i.e., genre analysis here) adaptation methods for LLMs such as ChatGPT. First, our analysis of the confusion matrix of the clasifications on the validation se y the initil prompt in zro-shot stting showed that the lack of prompt specificit could hurt the model'scassification accuracy and an important goal of prompt refinement should be to improve prompt specificity. This analysis echoes Giray's (2023) point that when designing prompts for LLMs, academic writers and researchers should be aware that insufficient specificity could lead to ambiguity and result in diminished accuracy. With a higher level of specificty, our refined prompt helped improve the accuracy of the model's clasifications. Although the improvement turned out to be not statisticall significant, our analysis and results never. theless support Fatouros et al.'s (2023) conclusion from their experiments to exploit ChatGPT for sentiment analysis that adept prompt engineering and thorough prompt evaluation are critical before model deployment.

Second, our findings showed that three-shot learning alone did not significantly improve the performance of the initial prompt, but combining it with prompt refinement led to further, statisticall significant improvement in the model's performance for both move and step classification. The overall accuracy, however, was still low even for both move $( 5 2 . 7 \% )$ and step classification $( 2 5 . 3 \% )$ with both domain adaptation methods. Loukas et al. (2023) reported much higher F1 score for GPT-3.5 in a one-shot setting and GPT-4 in a three-shot setting in classifying customer service queries into 77 intention categories (over $7 0 \%$ and $8 0 \%$ respectively), and Lossio-Ventura et al. (2024) also reported a much higher accuracy $( 8 7 \% )$ for ChatGPT in a zero-shot setting in a three-level (positive, neutral, negative) sentiment analysis task. The much higher performance of ChatGPT on these tasks suggests that rhetorical move-step annotation may represent a more challenging and/or complex classfication task for ChatGPT, as it involves a deeper understanding of a larger context to determine the specific functions of sentences, while the intention classfication and sentiment analysis tasks both deal with short texts or individual sentences as standalone units.

The accuracy levels achieved by ChatGPT for move and step clasification also appeared lower than those reported for supervised learning models such as AntMover (Anthony & Lashkia, 2003) and RWT (Cotos & Pendar, 2016), although it is important to contextualize these differences. AntMover was trained on 554 sentences and achieved an average accuracy of $6 8 \%$ across six step-categories; RWT was trained on 650 RA introductions (15,460 sentences) and achieved overall accuracies of $7 2 . 6 \%$ for move classification and $7 2 . 9 \%$ for step classification. In contrast, the three-shot setting provided ChatGPT with three annotated RA introductions totaling 38 sentences only. Overall, these performance differences show that more rigorous domain adaptation than prompt refinement and few-shot learning is necessar to enable ChatGPT to successully handle the domain-specific task of rhetorical move-step annotation.

The findings from our second research question revealed that ine-tuning the base model led to substantially higher ccuracies, with $9 1 . 2 \%$ (Fine-tuned Model 1) and $9 2 . 3 \%$ (Fine-tuned Model 2) for move classification and $7 5 . 3 \%$ (Fine-tuned Model 1) and $8 0 . 2 \%$ (Fine-tuned Model 2) for step classfication. These accuracies surpassed those reported for AntMover, even with a much larger number of step categories. They also surpassed the accuracies reported for the RWT (Cotos & Pendar, 2016), even though the size of the training data (40 RA introductions with 795 sentences for Fine-tuned Model 1 and 80 RA introductions with 1556 sentences for Fine-tuned Model 2) was much smallr than that used to train the RWT (650 RA introductions with 15,460 sentences). These findings echo those of Brown et al. (2020), who observed that GPT models require considerably les training data to perform well i NLP tasks than traditional NLP models. The potential of GPT models to effctively tackle challenging domain-specific classfication tasks with smallscale domain-specific training data can dramaticallyalleviate researchers of laborious efforts in manual coding.

The higher accuracies of both Fine-tuned Model 1 and Fine-tuned Model 2than those achieved by existing systems highlight the efficiency f fine-tuned models. Whers there was n signficant efmance difference ween the two fine-tuned modls, acuracy did trend up from Model 1 to Model 2, suggesting that using larger training sets could potentially lead to further accuracy improvement. This observation sets the stage for future research to identify the optimal balance between maximizing accuracy and minimizing training sample size in the context of fine-tuning ChatGPT for move analysis similar to explorations of such a balance in other domains (e.g., Pecher et al., 2024).

Despite the substantial performance improvement, Fine-tuned Model 2 encountered some difficulties in accurately classfying Move 2 and the following four steps: M1 S1a (Claiming centrality or value of research area), M2 S1c (Question raising), M2 S2 (Providing justification), and M3 S2a (Presenting research questions or hypotheses). The model's lower performance on these moves and steps could be attibuted to two reasons. First, the training data for Move 2 and these four steps were relatively scarce. In our training dataset, Move 1 and Move 3 were represented about 4 and 2.25times more than Move 2, respectively, and M2 S1c, M2 S2, and M3 S2a were each represented with fewer than 50 instances. Cotos and Pendar (2016) similarly pointed out the diffculty in correctl classfying underrepresented stes in their training daaset. Second, some steps may have fewer explicit linguistic cues than Other steps, making it more difficult to automaticall identify them. Although not numerous, the training dataset contained 71 instances of M1_S1a (Claiming centrality or value of research area), yet the F1 Score for this step was only $3 0 . 8 \%$ . This step was similarly challenging to AntMover (accuracy $= 2 8 \%$ ) (Anthony & Lashkia, 2003), suggesting a lack of explicit linguistic cues for the model to accurately identify this step.

ChatGPT's capabilit to assign multiple codes to a single sentence addreses a limitation pointed out by Cotos and Pendar (2016) in existing tools such as AntMover and the RWT that can only assgn a single move or step category to each sentence. This is a desirable feature as some sentences in a text may serve multiple rhetorical functions. In the training set, 80 of the 1556 sentences $( 5 . 1 4 \% )$ were multifunctional. In the test set, 13 out of the 182 sentences (or $7 . 1 4 \%$ ) were multifunctional, among which 10 were assigned multiple tags by the fine-tuned model. Of the 10 sentences, the first predicted tag matched the primary tag in seven cases, and the second predicted tag machedthe rimary ta in three caes. Additional reearch is stil needed to further improe the model's eformance on multifunctional sentences and to refine the methodology for aggregating sentences as rhetorical chunks based on sentence-level annotations.

The results on outcome consistency demonstrate that with unchanged prompts and parameters, the performance of the models remained largely stable, particularly with Fine-tuned Model 2, which showed beter stability than the base model. Subtle changes in parameter setings or prompts can cause ChatGPT to produce diferent outputs a characteristic of its non-deterministic nature (eiss 2023). Therefore, maintaining consistent parameters and prompts isesential to minimize this randomness. Reiss(2023) highlighted inconsistencies in ChatGPT's zero-shot capabilities for text annotation in a task of classfying 234 websit texts into news or not news. noting variaility under different parameter settings or with slight alterations in prompts as well as some inconsistency even with repeated identical inputs. In contrast, our experiments showed that both the base and fine-tuned models maintained relatively stable accuracy levels, with the fine-tuned model demonstrating particular consistency. This difference may stem from our use of a 3-shot setting and fine-tuning, whereas Reisstested a zero-shot setting. Fine-tuning likely reduced output variabilit by focusing the model's \*knowledge" and response patterns on a narrower subset of styles. For example, we used highly consistent data for fine-tuning, which helps stabilize the model's behavior acros similar inputs. In contrast, the base ChatGPT model, trained on a diverse range of intenet texts, is optmized for generalization rather than speific task consistency. Additionally, the \*temperature" paameter impacts output randomness, where ower sttings yield more deterministi utputs, and higher setigs increase diverst and unpredictabilit. Our initial tests used a default temperature of 1 but further reductions could decrease randomness. The impact of temperature sttings on output reliability and validty requires further exploration. Therfore for reearchers using this approach for move analysis, several considerations are crucial. Firs, uniformity in parameters, model settings, and prompts, along with targeted fine-tuning, can significantly reduce inconsistency. Second, it i desirable for researchers to meticulously document ll parameters and prompts to ensure replicability. Third, validation of output consistency is strongly recommended to confirm model reliability. Despite the minor inconsistencies observed, the inificantly improed accracy, efficiency (requring les data to achieve comparable accuracy), and easy accessbilit to researchers and teachers highlight the considerable potentil of using ChatGPT for genre analysis and justif continued

exploration of this approach.

The results of the tests conducted with the two additional test sets from Anthropology and Psychology demonstrated that the model's capabilit for move annotation can reliably generalize to these two related disciplines, as it achieved acuracies exceeding $8 7 \%$ on both. For step classification, the model achieved accuracies exceeding $7 1 \%$ on both test sets. The slightly lower accuracy for steps may show that similr ommnictive fuctions are xtd diffeently in diffeent discline, with linguistic cue arying from one discipline to another to fulfll the same communicative objectives. These variations underscore the ned for further research to refine this tol, esuring it efectively caures the nuanced differences across aadmic fids.otably, the reuts on these two test sets surpassed that of AntMover and were comparable to that of the RWT with a much smallr training et, illstrating the model's efficiency.

The capabilities of the fine-tuned model have useful aplications in genre analysis research. Genre researchers can fine-tune the base model with sustantill smaller manually annotated training sets than previously required to automate rhetorical move-step annotation for different genres using appropriate move-step frameworks. This advancement has the potential to expedite the processes of genre analysis research and expand the scale of such research in terms of sample sizes. Building on reliable move-step frameworks established through manual analyses, genre researchers can capitalize on automated move-step annotations in largescale follow-up studies to more systematiclly explore such issues as disciplinary variation and form-function mappings, among others. Meanwhile, researchers concerned with the accuracy of the model's annotations could also use them as a useful first pass or second opinion in their in-depth move-step analysis of texts. Our results further demonstrate the potential of using ChatGPT for discourse-level annotations in general, and future studies could explore other types of discourse-level annotations, such as annotations of speech acts, pragmatic functions, and cohesive relations.

Methodologically, a significant advantage of this approach is the relative ease of creating domain-specific fine-tuned models compared to traditional machine learning models, which require extensive feature engineering and coding skils that may be less accesible to many researchers or teachers. Researchers can build and deploy their models using either the OpenAI API with a Python script or in Playground, a user-riendy platfor where they can uild and asily interact with their fine-tuned model and obtain tagged outputs for their input texts. his method not only simplife the proces but also nhances the accessilit of the tol to the field. Our findings suggest several important methodological recommendations for researchers looking to replicate or build upon this study. First, regarding training sample size, our results suggest that 3-shot learning may be insufficient for move analysis; while 40 training samples might be adequate for move classfication and some steps that are frequently present in the training set, categories lacking sufficien training data or those with subtle linguistic cues may require larger datasets. Meanwhile, our results showed that with 80 training samples, the performance of the model improved but only slightly. Further research is needed to explore the optimal balance between minimal training data and maxial accuracy. Moreover, when applied to different genres, the number of categorie (different levels of move and step) may influence the optimal training set size, as indicated by the different accuracies in our models move and step classifications. Therefore, researchers should determine the optimal sample size for each genre and may also consider exploring how the number of cateories interacts with traning si. Second, crting precise prompts through a meticulous refinement processis essential to ensure that steps or moves are clearly defined and any potentially overlapping definitions are distinctlydistinguished. Researchers can utilize confusion matrices to pinpoint areas of the prompts that require refinement and experiment with various prompts to identify those that yield the most accurate results. Future research should investigate the optimal level of specificity by testing different versions of prompts to prevent overfitting and enhanceaccuracy. This aproach will not only improve th framework but also enhance the clarity and understanding for human annotators. Third, using consistently formatted inputs can significantly enhance consistency in model outputs. Once an optimal prompt has been established, maintaining a uniform format for prompts, training dat, and user input is crucial, as this consistency diretly influences the preision f the outputs. Our experiments showed that such consistency was vital for reducing variabilit in the model's behavior and ensuring reliale results. Fourth, robust validation tests are crucial to ensure the reliaility of the model's outputs. It is important to repeatedly test the model with identical inputs, prompts, and parameter setigs, and to ccurately document the range of oserved accuracies to ases the models consistency. As mentioned earlier, rearchers should clearly report ll parameter used for training and testing the model and prompts so that future researchers can replicate and build upon their research. Additionall, using large test ets, when feasible, is recommended to enhance the validty of the findings and achieve greater rigor.

In terms of genre-based writing pedagogy, the model can serve as a valuable supplementary resource in guided corpus-based genre analysis activities designed to enhance larners genre competence. The automated annotations provided by the model can be auseful starting point for novice academic writers as they engage with genre analysis of academic texts. Directly working on genre analysis f such texts immediately following instructor explanations of the move-step framework can be rather overwhelming to learners new to the task. Intructors can ask learners to interact with the automated tool with authentic texts to se the framework in action, reducing initial intimidation. As they become adequately familiar with the framework, learners can collaboratively investigate the accuracy of the automated annotations. They can also be guided to identify move-step sequences, frequent move-steps, and linguistic features associated with different move-steps in automatically annotated expert text. Such hands-on experiences can help deepen learners understanding of the move-ste framework and genre-specific practices within their disciplines. Additionall, leaners can also use the model to analyze their own texts and assess how their own writing aligns with typical rhetorical structures of the target genre.

# 6. Conclusion

This study investigated the potential of utilizing ChatGPT for rhetorical move-step analysis and the impact of prompt refinement, few-shot learning, and fine-tuning on its accuracy for move and ste classfication. Our results showed that prompt refinement and fewshot learning helped improve the performance f the model but fine-tuning achieved more competitive and satisfactory accuracy with relatively smal training sets. The power to accurately automate move-step annotation has important implications for more comprehensive and larger-scal genre research. Fine-tuned models can also function as acessible and interactive ools to facilitate the design of corpus-based genre analysis activities in genre-based writing pedagogy. Our study also allowed us to offer methodological recommendations for using ChatGPT in research.

As an early attempt to adopt the GPT model for rhetorical move-step analysis, however, our study has several limitations, some of which can be addressed in future research First, we limited our investigation to a single discipline (with additional test sets from two related disciplines) to obtain an initial asessment of ChatGPT's ability to tackle this highly complex domain-specific task. Future studies can expand the scope of the evaluation using more data from diverse disciplines to understand the generalizability of the effectiveness f the model and methodology. Second, for prompt refinement, our exploration was confined to two prompts. Subsequent studies could adopt an iterative prompt refinement trategy with increasingly more specific prompts to ascertain the optimal balance between prompt specificity and generality to maximize accuracy whil preventing overfitting, as Giray (2023) advised. Third, we did not examine the effcts of parameter setting or training data size on the performance of the fine-tuned model. Future studies could experiment with different parameter settings,sample sizes, and numbers of move-step categories to shed light on the optimal parameter configuration and training sample size to maximize effciency and accuracy. Lastly, we assessed model consistency by repeating the tests thee times, but the consistency of both the base model and the fine-tuned model may warrant further exploration with varying temperature settings and more repetitions.

# Funding

This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.

# CRediT authorship contribution statement

Minjin Kim: Writing - original draft, Visualization, Software, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Xiaofei Lu: Writing - review & editing, Supervision, Resources, Methodology, Conceptualization.

# Declaration of competing interest

The authors have no conflict of interest to disclose.

# Appendix A. Supplementary data

Supplementary data to this article can be found online at https://doi.org/10.1016/j.jeap.2024.101422.

# References

Anthony   hi. 003.r  mh lg   s i th g d wtin f hl pr. cion on rsi Communications, 46(3), 185-193. https://doi.org/10.1109/TPC.2003.816789   
Bals .   0 t t ch  n 10. //10.1017 S1351324923000438   
Biber, D., Connor, U.,  Upton, T. A. (2007). Dicourse on the move: Using corpus analysis todescribe discouse structure. John Benjamins Publishing. Hghan ,     ,r 0  in Processng Systms, 33, 18771901. hp/rcegs.nrip.c/paper is/paper/2020/hash/1457c06bb4967418b8c14264a-Abstract.html.   
Cl     e   .   s 204. Routledge. https://doi.org/10.4324/9781003300847-7.   
Cor, . 203  t t a  h .   or 1 (1), 33-43. https://doi.org/10.1016/j.jeap.2012.11.002   
Cots E    . 01 e/t  r  i i an rt. gish r i  46 900. https://doi.org/10.1016/j.esp.2017.01.001   
Cotos  Hfn,  i . (2020. de gr wie ctio th a tft ritin r drig eis. Writing Research, 12(1), 187-232. https://doi.org/10.17239/jowr-2020.12.01.07   
Cotos  r  016.r cfiction n ricl ction fr  ckol, 3(1) 26. p/.og/.5/j v33i1.27047 org/10.1162/089976698300017197   
Dong .  00 Pni  h c-e as i.h for i e 8, 138154 https://doi.org/10.1016/j.esp.2020.01.005   
uros, , os, , ali,, k  s  (023). T t lyisin th inma th . che Learning with Applications, 14, Article 100508. https://doi.org/10.1016/j.mlwa.2023.100508   
hla, ,ch ha0180000 e  g d tn   i l o of Intelligent Technologies and Applied Statistics, 11(2), 105-111. https://doi.org/10.6148/IJITAs.201806_11(2).0003   
Gray . 2023) t rin th Ch: d r a wiers  of B ning 12) 62-633./i.g/0.1007 s10439-023-03272-4 HUIL   
Huan, , wak, , & n, J. (2023). 1s ChtG eter than hman atar?ottil and imittions of ChtGP in explang mplicit hate seech. Companion proceedings of the ACM web conference 2023 (pp. 294-297). https:/doi.org/10.1145/3543873.3587368   
Hyland, (200 g y  wi ini.  of d  n 63, 148164. /.106/j. jslw.2007.07.005 Hyland, K. (2015). Genre, discipline and identity. Journal of English for Academic Purposes, 19, 32-43.   
Hyon, s. (19).  in th tri mplicins for so  tcg l erl, 0 93-2. /sto.gtle/358930.   
anoksila . 205. hri s f br rh i. ish r i e, 2(, 29-2. h/.0.1016/. esp.2004.08.003   
avzoglu, . 2017). Chapter 3obctrinted rand Frest or high lution and coer mapping using quckbird- magry. InP. mui, . ekar, & V. E. Balas (Eds.), Handboo of neural computation (pp. 607-619). Academic Press https://doi.org/10.1016/B978-0-12-811318-9.00033-8.   
Kesser,  i aetithi  ri Purposes, 60, 182-192   
Kesser,  . 3   .,   . /. org/10.4324/9781003300847-1. Linguistics, 3(1), 100097. https://doi.org/10.1016/j.rmal.2024.100097   
o i wicki,  sy,  ki  ki  o 2023)  al , r none. Information Fusion, 99, Article 101861. https://doi.org/10.1016/j.inffus.2023.101861 doctoral dissertation]. Georgia State University. https://doi.org/10.57709/8501051.   
Lee, U, Jug, H, Jon, , hn,, Hwang, , on, J, ,  (2023). w-sht is ugh: Expring Ct rompt gnern mh for atomatic question generation in English education. Education and Information Technologies, 133. https://doi.org/10.1007/s10639-023-12249-8   
Li Q              l   0.0364 https://doi.org/10.48550/arXiv.2008.00364   
Li, ,      223).  r  h 202 neeon bioinformatics and biomedicine (BIBM) (pp. 4360-4367). https://doi.org/10.1109/BIBM58861.2023.10385482 transfomers (PT gt y u m as ts St asf CD-19 y d   11 Ae50150. / doi.org/10.2196/50150 P.r ,   .,  f t  i   p he secon multimodal AI for financial forecasting (pp. 74 80). https://aclanthology.org/2023.finnlp-1.7.   
Lu, X 010c f ic  n  wg     sic 54),496. /i. 10.1075/ijcl.15.4.02lu   
Lu, ,   2. - exl sohisticatin e ad hr retsh ond g wtin u. eor , 4(3) 1444-1460. https://doi.org/10.3758/s13428-021-01675-6   
Lu        or   6, 63-83. https://doi.org/10.1016/j.esp.2020.10.001   
u        na five social science disciplines. System, 100, 102543. https:/doi.org/10.1016/j.system.2021.102543   
h3)   usf  g  ic, 2) Article 100050. https:/doi.org/10.1016/j.rmal.2023.100050   
oreo,   w, J 018 tgthng m ais m d ring the ctiofo . gis fr ic P, 50, 4063 https://doi.org/10.1016/j.esp.2017.11.006   
OpenAI. (2024). OpenAI playground (May 4 version) [Large language model]. https://platform.openai.com/playground.   
Palridge,  (194e ais a iictif t r.dsic 5() 88299./.g/10109/i/5.3.288   
Pandey, K DahK, Pade, K,  Mndal  (2022. d de  m ssted pree trsiet alysis or aomatc reeror characterization. Petroleum Science and Technology, 40(6), 659-677. https://doi.org/10.1080/10916466.2021.2007122   
Par ,   021 st  n or     for  6 37./. org/10.1016/j.esp.2021.03.006   
Parkinson, . (017.  d  t   ay. gish r ic es, 45,1-13./.g/01016/j.02.03.006   
er 2402.12819. https://doi.org/10.48550/arXiv.2402.12819.   
Ray ..  i   ic, k c i  oad  o  o hngs and Cyber-Physical Systems, 3, 121-154. https://doi.org/10.1016/j.iotcps.2023.04.003   
Res, 3tth t      i0./i.g 10.48550/arXiv.2304.11085.   
Swales, J. M. (1990). Genre analysis: English in academic and research settings. Cambridge University Press. Swales, J. M. (2004). Research genres: Explorations and applications. Cambridge University Press. Tardy, C. M. (2016). Beyond convention: Genre innovation in academic writing. University of Michigan Press.   
eto 5e r      y iover. English for Specific Purposes, 37, 13-26. https:/doi.org/10.1016/j.esp.2014.06.002   
achwiak,L, &Goma  (023. -3 g mephrtiing mh mp wth tive lag . A Ro, J. -aber, Computational Linguistics. https://doi.org/10.18653/v1/2023.acl-long.58.   
ei        29.0652, 2022. https://doi.org/10.48550/arXiv.2109.01652   
u T          q A Journal of Automatica Sinica, 10(5), 1122-1136. https://doi.org/10.1109/JAS.2023.123618 Yoon J.   i ,  i-iiof  ti or. i Journal of Applied Linguistics, 30(3), 462-478. https://doi.org/10.1111/ijal.12300

fr Aca  w   st    it   i Acquisition: Perspectives, Issues, and Findings (2023, Routledge).