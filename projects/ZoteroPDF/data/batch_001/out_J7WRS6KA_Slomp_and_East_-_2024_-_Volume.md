We are pleased to present the 60th volume of Asessing Writing and the rich body of research and scholarship it contains. Before highlighting the contributions to this volume we wanted to provide a few updates regarding the journal.

We begin by congratulating Jessie S. Barrot, this year's recipient of the Liz Hamp Lyons' Best Paper Award for his artice, Using ChatGPT for second language writig: Pifals and potentials." At atime when the emergence of new AlI technologies generated hy. perbolic responses of Dickensian proportions it was the best of times, it was the worst of times--within the public and professional discourse, Dr Barrot instd took a sober look at this new technology drawing attention to its potntial and it limitatios. We hope that this ethos informs work on writing asessment and Al that will inevitably be published in Assesing Writing in the years to come.

The intersection of new technologies and writing is not new for the field of writing assessment, and so Dr Barrot's approach is very much in line with our field, highlighting its potential to lead sober conversations regarding the use and consequences of these new technologie. In thi ssue of the journal we se a number of papers that work within this tradition, applying a measured critical lens to technological interventions in writing assessment.

In our previous editorial we announced the creation of an early carer Editorial Board. That work has progressed substantially over the past few months. Our Editorial Board now includes55 members from acros 16 countries. Welcome to ll new B and Early Career EB members. We very much appreciate your work on behalf of the journal. With the addition of these EB members and the adition of two Associate ditors earlier this year, we expect procesing times for manuscripts submitted tothe journal to shorten over the months ahead.

This volume includes 5 book reviews. Over the coming volumes you willsee the number of book reviews published in the journal decline as we have decided to end this feature of the journal. This decision was not made lightly. However, data regarding the number of downloads per book review convinced us that there was very lttle interest in our published book reviews by our readers. We thank Maria Eugenia Guapacha Chamorro for her excellnt work as Book Review Editor over the past several years. We welcome her to the Editorial Board and look forward to her continued contributions to the journal and to the field.

This volume also includes two pecial issue editorials. Shulin Yu and Icy Lee's editorial, \*Writing asssment and feedback literacy: Where do we stand and where can we go?" and Beverly Baker and Atta Gebril's editorial The asessment of writing in languages other than English (LOTE). These editrials cap offtheir work of shepherding 29 artice through from proposal to publication, an enormous undertaking by these dedicated collagues. We sincerely appreciate the initiative, rigor and care both teams of special isue editors brought to this important work. These special issues make important contributions to our field, as they challenge us to continually expand our thinking about our discipline and ths journal. Baker and Gebril conclude their editorial with the hope that AseingWriting will expand its scope further to consider the publication of reearch on writing asessment of allanguages. Their special issue has certainly helped us to se a path toward this goal,and has introduced us to reviewers and authors who can asst us in reaching it.

This volume contains 16 research articles from scholars working in 13 countries across Asia, Europe, the Middle East and North America. It is heartening to see continued diversty of scholars who are working and publishing in our field. This volume also sees articles adressing writing asessment isues at allels f educational systems, from primary grades through to graduate studies.

Four studies in this volume examine isues related to automated scoring and feedack, or the corpus analytic tols and methods that support these processes.

Wilson and Huang examine the potential for bias in automated essay scoring of grade 3-5 students' writing. Their study compared the predictive validity of automated and human scoring for ELL and non-ELL student populations performances on both a state writing assessment and an English language proficiency test. They found that with respect to the stat writing asessment both scorin methods exhibited bias against ELL student populations. They conclude that human rater biases are likely baked directly into automated scoring processes,clling for greater transparency in automated scoring specifially with respect to the training of algorithms and to potetial biases present in the training data. This is a critically important finding, and thir callis timely. As we see a growth in automated scoring and feedback of student writing, i is important that we continually question the fairness inherent in such processes.

Khushik's study examining the efects of native language and pedagogical methods on syntactic complexity features of Norwegian and Pakistani secondary student writing takes up this challnge. Finding significant ifferences in syntactic complexity between the two groups, Khushik obseres that astudent's L1 isa critical factor in shaping their English language writing development. This study raises important questions about how measures of syntactic complexity might either perpetrate or expose cultural or linguistic bias when used as measures of L2 English language writing proficiency.

Martin and Dockrell report on a systematic review of research into writing productivity measures used to make inferences regarding elementary students' development as writers. The review identified two metrcs (Total Number of Words, and Correct Word Sequences) as potentiall effctive measures for identifying student writers who may be laging in their development. At the same time, they observed that much more research across a range of genres, utilizing a broader range of metrics, and that atend to both child- and task-related factors are needed to help us better understand methods for asessing the development of writing abilit in younger children.

Laudenbach, Brown, Guo, Ishizaki, Reinhart, and Weinberg's study reports on new developments of the Docuscope platform created by David Kaufer and Suguru Ishizaki at Carnegie Mllon Universty (Helberg etal., 2018; shizaki & Kaufer, 2012). The Write & Audit extension of Docuscope creates visualizations of key dimensions (invention, cohesion, clarity, and impressons) of a draft text, enabling writers to make informed choices as they problem-solve complex writing tasks. Their study highlights the value of automated feedback systems that can be tailored to the specific rhtrical context in which writers are perating so hat they can beter assist them in developing and enacting the complex rhetorical problem-solving skill that are critical to the development of writing ability.

Two additional papers also address asessment practice that shape and reflect students' understandings of writing as a rhetorical problem-solving activity. Wei and Zhao report on astudy that measures development of pragmatics in Chinese secondary students English writing in a task-based language teaching environment. Because task-based learning exposed students to a range of communicative outcomes and enabled them to draw on pragmatic knowledge drawn from their lives outside of the classroom, these tasks challenged students to develop problem-solving and critical thinking skill that supported their growth as writers. Cheong, Liu, and Mu examined the relationship between writing performance and task representation of secondary level students in Hong Kong. They identified three dimensions of task representation (source use, rhetorical purpose, and text format) as impacting writing performance, highlighting the importance of pedagogical methods that focus on rhetorical problem-solving and integrated writin skils as a means to promoting the development of writing ability.

Feedback continues to be an important topic of onsideration in this volume. Hanjani compares the behaviors and claims of Iranian undergraduate trained pr-reviewers when providing feedback, highlighting the complex process involved in peer feedback, and draws attntion to the importance of ongoing, targeted training of peer-reviewers as integral to their learning, and to their devel. opment as writers. Zhang and Gao examine the dynamics of learner engagement with peerfeedback among Chinese undergraduate students completing a TOEFL preparation course. Drawing on longitudinal data from multiple case studies, they demonstrate how student cognitive, behavioral, and affective engagement are shaped by individual and contextual factors. Bastola and Hu examined Nepalese graduate students' engagement with supervisory feedback on their master's thesis. They report significant differences in students and supervisors pereptions of students behavioral,affective, and conitive engagement with supervisor fdback, and they attribute the differenc to rang f instittional, supervisory, and student factors. They cation against supervisos taking a fcit view of student engagement focusing instead on fostering dialectic and dialogic spaces that foster deeper mutual understanding of the feedback provided and response to that feedback.

The remaining papers explore validity issues related to assessment tasks, scoring processes, and rating systems.

Three papers address validation questions rlated to scoring procedures and rating scale development Thwaites, Kollias, and Paquot examine the concurrent validity and rliabilit of comparative judgment as a technique for asessing long L2 texts based on a variety of prompts. They found evidence f satisfactory level of reliaility and concurrent validity with his scoring approach but also found that this work needs to be augmented by research into construct validity of what drives assessors' comparisons and decision making. Zou, Yan, and Fan present a validation study for an analytic rating scale developed for use with the CET-4 writing test that is used to asess writing proficiency of undergraduate students in China. Based on thir findings they argue that rating cales that are validated in advance of widespread use, based on actual samples of student work can minimize potential for halo effects that oo often plague analytic scoring This in turn enable assessors to develop more acurat profiles of writers strengths and weakneses acrossal the scoring criteria used. In the case of their study, they demonstrated that high, medium, and low performing groups of students all underperformed in the area of linguistic appropriacy regardlessof their level of overallperformance. Alves-Wold, Walgermo, and Foldnes highlight an often unexplored aspect of scale design and validation. They examine the influence of pictorial supports on Norwegian primary students' engagement with Likert scales used to asess writing interest, self-concept, spelling interest and self efficacy. Their study draws attention to how the visual design of rating scales can introduce diffring levels of construct irrelevant variance in aset data, which can impact the quality of ifrmation aessors are ale toollet, and th fereces that they are able to make through the use of such scales.

Yao, Zhu, and Zhan provide a cautionary tale regarding how meaningful a single measure of writing performance reall can be. Their study examines the relationship between writing performance, growth mindset and perceptions of the \*ideal writing self of secondary level students in China. They found that students reported low levels of enjoyment in L2 writing, attrbuting this to an instructional focus on preparation for a high stakes English writing exam. They also found that growth mindset and L2 writing enjoyment did not predict performance on exam-based writing tass. They suggest that to many factors might intervene in a single performance, and hypothesize that these factors may be more predictive of course based writing performance than of exam-based performance.

Three papers focus on the interactions between asessment design, language use and scoring. Arias-Hermoso, Agirre, and Larranaga examine interactions between source modality, language, argumentation and source use strategies by 9th grade trilingual Spanish students completing integrated writig tasks. Findings suggest that source use and argumentation sills are transferable across languages, highlighting that while there are undeniably linguistic and cultural dimensions to argumentation in writing, there are also stable underlying cognitive processes that seem to transcend language. Emmanouil, Zoe, Christos, and Markos examine the rela tionship between content, language, organization and cores Greek high school students receive on argumentative writing tasks. Their study highlights the effct of students' facility with language (vocabular, syntax and grammar) on raters asessment of content and organization, suggesting that essays exhibiting stronger use of these language features willbe scored more highly on content and organization than may be warranted. The paper highlights the interconnected nature of language use and the challenge of oper. ationalizing analytic rubrics. Barkaoui examined the efects of task dificulty and leaner variable n the inguistic characteristics of responses to writing tasks contained in the Duolingo English Test. This study which focused on examining generalization and extrapolation inferences, is part of a broader program of validation research into this new English language writing assessment. Barkaoui's nuanced findings demonstrate the value of rigorous and independent validation research into new forms of large-scale writing assessments.

We are grateful to the many contributors to this volume, the authors, the reviewers, the Editorial Board members, the associate editors, and the journal managers and Elsevier staff who handle manuscripts and internal processes. The continued growth and impact of Assessing Writing is a testament to ethic of excellence and care you each bring to this work.

# References

Helerg, , a, ski, ,fer,  er,  l,  018  x  h e  i os and reflction to support students' written decision-making. Asessing Writig, 38, 40-45. https://doi.org/10.1016/j.asw.2018.06.003 Global.