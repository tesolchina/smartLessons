# Linguistic factors affecting L1 language evaluation in argumentative essays of students aged 16 to 18 attending secondary education in Greece

Koskinas Emmanouil a,\*, Gavrilidou Zoea, Andras Christosb, Angelos Markos

a Department of Greek Philology, Democritus University of Thrace, Greece b Department of Mechanical Engineer and Management, International Hellenic University, Greece c Department of Primary Education, Democritus University of Thrace, Greece

# ARTICLEINFO

# ABSTRACT

Keywords: Language assessment Linguistic parameters Assessment rubric Parametric assessment tool Evaluation criteria

The purpose of this paper is to investigate linguistic factors affecting the evaluation of the argumentative essays in written tests taken by junior and senior students, aged 16 to 18, attending high schools in Greece. To achieve this, we analyzed textual characteristics and scoring of 265 juniors and seniors, graded by 15 different raters. To examine the contribution of linguistic parameters to the assessment, we developed an automated tool to record and evaluate students' lexical and syntactic features in the Greek language. The results revealed that the extensive use of nominal groups including an adjective and a noun and the utilization of both impersonal and passive syntax, as well as adverbs to a lesser extent, contribute the most to positive grading in language tests. Furthermore, we identified a correlation between language and the other criteria of the evaluation rubric, namely content and organization. The paper contributes to the discussion about objectivity in writing evaluation in the Greek setting and to the creation of a rubric that ensures a more effective assessment of writing tasks.

# 1. Introduction

Recent research has questioned the specificity and clarity of the asessment criteria, namely content, language, and organization (Lumley, 2002; Wind, 2020) in students written essays. A correlation among these asessment criteri (Janssen et al, 2015) could significantl impact the efectiveness of the assessment process. Despite extensive research on this topic for English, both as a frst language (L1) and a second language (L2), there is a notable lack of research regarding the actual use f rubric scales for Greek as L1 in the Greek examination system.

The Greek language examination in high school includes summarizing a given text (20/100 points) commenting on a provided poem or prose (15/100point) completing various language exercises (35/100 points), and crafting an academic-style essy (30/100 points) in different genres (e.g., article, public speech, etc.).

The essay evaluation involves the use of a rubric consisting of three istinct evaluation criteria: content (12/100 points), organization (10/100 points), and language (8/100 points, II.. 46/2016). Content pertains to the ideas expressed in students'essays, language focuses on the efectivenessof the esay's linguistic form (i.e, compliance with academic genre guidelines for students essays in senior classs), and organization concerns the appropriatenessof paragraphs, logical flow of ideas, and clarit of transitions between and within paragraphs (Li & Huang, 2022). It's worth noting that, following the recent educational reform in Greece @EK 4678/2022), the scoring system has undergone changes, with language now being scored with 3/15 or 8/30, accounting for 20 to $2 7 \%$ of the overall scoring per task.

Despite the relative consensus on the utilit of rubric (Hudson, 2005; Lumley, 2002; Schoonen, 2005; Zhang, 2010), there has been criticism in various internal or external aspects of their use (Attli, 2016; Bachman, 200; Rezaei & Lovorn, 2010). External aspects pertain to the relationship betwen the three evaluation criteria: language, content, and organization, whil internal aspects refer to the impact of linguistic factors (textual characterisics, morphosyntactic phenomena). The purpose of this paper is to address some of these isses wth the aim to ehance th intrity and efectivenes of studnt writing asssment. The rearh contriute to a deeper understanding of the assessment process and pave the way for the development of a new more purpose-fit rubric.

This last point is considered important since final exams in Grece include a language examination for admission to Greek Universities. The clarification of the parameters affecting language rating contributes to the reliability of the examination system. Following the above, the paper delves into the methodology, presents the results and their discussion, and finally concludes by addressing the study's limitations.

# 2. External aspects of language evaluation in essay writing

The relationship between language use, content, and organization presents challenges related to authenticity, inseparability, and specificty in the sessment process For instance, Janssen et al. (2015) tested an ESL rubric with five categories (Content, Organ. zation, Vocabulary, Language Use, and Mechanics) and found minimal variation ${ ( \boldsymbol { \eta } ^ { 2 } = . 0 9 }$ $\mathbf { p } = . 0 0 $ , indicating inconsequential differences among the categories. Lumley (2002) observed that rater apply the evaluation criterion of content dfferently,affecting the reliability of rubric assessments.

Schaefer (2008) discovered that content and language use were closely aligned in terms of difficulty levels, with a $5 0 \%$ probability of receiving the same result at certain levels. However, Carr (2006) identified asignificant relationship between content, syntactic features, and sentence vocabulary. Despite these findings, there has been limited examination of the direct relationship and potential collinearity between the evaluation criteria f content, language, and organization, especially among native language (L1) users, particularly in the Greek educational context. Nevertheless an interconnection between these criteria (Wind, 2020) may undermine the validity of the asessment Furthermore, a potential orrelation of the language rating scale with content and organization raises a second issue concerning specificity and clarit. This pertains to the individual parameters that influence the language rating scale and overall writing quality.Apxn popacTeAoc popac.

# 2.1. Internal aspects of language evaluation in essay writing

The essay in question focuses on the argumentative (persuasive) genre. It's noteworthy that, in the Greek context, specific criteria for language assessment in essays are not outlined; instead, a general framework is provided (@EK 5162/2023). Despite the asence of detailed language criteria, the significance of language proficiency is underscored by its corrlation with academic success in highstakes exams, particularly in writing (Crossey, Salsbury, McNamara, & Jarvis, 2010; Cumming, Grant, Mulcahy-Ernt, & Powers, 2004; Johnson & Lim, 2009; Johnstone, 2000; Le & Anderson, 2007). Nevertheles, writing is a multifaceted domain influenced by various factors (Bailey, 2011; Hyland, 2002; Jo, 2022; Teng & Zhan, 2023), many of which remain unexplored within the context of the Greek secondary education examination system.

Research has identified various linguistic characteristics such as syntactic complexity (Barrot & Agdeppa, 2021; Casal & Lee, 2019) lexical diversity (Crossley et l., 2012; Jarvis, 2017), word length (Crossley, Salsury, McNamara, & Jarvis, 2010; Culligan, 2015), and morphology (Sarte & Gnevsheva, 202; Uzun, 2021) as prominent features of academic writing. However, a comprehensive study that incorporates these parameters into the Greek examination system is considered important.

In particular, for the linguistic asessment of argumentative esays, researchers acknowledge the importance of four distinctive domains: general features (e.g, essay length, type/token ratio), lexical features (e.g., conjunctions, word length), grammar (e.g, modals), and syntactic complexity such as nominal groups (NG), sentence length, subordinate clauses and others (Becker, 2010; Lu, 2011; Taguchi et al., 2013).

Regarding general features, the total Number of Words, in many cases, afects language scoring (Attali, 2016; Bae & Bachman, 2010; Enright & Quinlan, 2010; Kobrin et l., 2011; Xie, 2015). Also, type/token ratio and Lexical diversity (LD), also known as lexical variety, range, or richnes, is considered highly important (Cowie, Douglas-Cowie, & Witchmann, 2002; Jarvis, 2002; Laufer & Goldstein, 2004; Malvern & Richards, 2002; McCarthy & Jarvis, 2007; Qian, 1999; Read, 2000; Yu, 2009). It serves as a strong predictor of linguistic performance $( \eta \mathsf { p } ^ { 2 } = . 2 5 0$ Crossley et al., 2012; Jarvis, 2017) and higher scores (Maamuujav, 2021; Treffers-Dalle et al., 2018). Lexical diversity is believed to influence the quality of an essay across various studies.

In regard to lexical features, word length (Enright & Quinlan, 2010; Adxov et al., 2020) is an indicator of lexical sophistication or difficult. Several researchers have found that word length impact oth text and vocabulary quality, consequently affecting language scoring.

Concerning syntactic complexity, it i emphasized by many researchers (Chapell & Chung, 2010; Derwing et al., 2002; Rimmer, 2006; Tyndall 991; Vermeer, 200) as an indicator of high linguistic performance production accuracy, and language competence. Morphosyntactic complexity, including elements such as determiners, adverbial use, noun modifiers, and tenses, has arelative impact on the final core and fewer errors as the score improves. Syntactic complexity is measured through many criteria, one of which is Sentence Length, measured in various forms such as T-units (TU) or complex T-units (cTU) and subordinate clauses. Those factors can

affect general scoring and language scoring. However, both the importance of TU length and subordinate clauses in language per.   
formance has been questioned (Taguchi et al., 2013).

Of course, we should mention that there are at least 200 linguistic indices for language asssment measured automatically by NLP programs such as Coh-Metrix for English, but not for the Greek Language, as far as we are concerned. In fact, there are numerous other linguistic factors that have been researched, including lexical frequency, cohesive markers, use of synonyms, and communication circumstances.

This lack f research tools limits our research scope considerably. As result, we confine ourselves to particular aspects of language assesment, but specialized for the linguisic asssment of the argumentative essa. In particular, regarding general features, this study examines the total number of words and LD. In respect to lexical features, word length, and for syntactic complexity, the total use of nominal goups and speficll adjective+noun (Lu, 2011), since i i an open catgory in Grk nd lays major rol in specificity in general, adverbs (Yilmaz & Dikilitas, 2017), sentence length, passive voice, and impersonal syntax. These indicators are considered indicative of sophistication in written essays in Greek academic education, both empirically and from research (Adxo, 2020), and they are indicative linguistic traits of the argumentative genre essay (Jo, 2022).

# 3. Research questions

To addressgaps in the existing literature concerning the correlation between assessment criteria for written assgnments produced by Greek students, this paper ams to investigate linguistc fctors inluencing theealuation of languag in writen assments among junior and senior students, ged 16 to 18, who attend high schoos in Greece, and the importance of language in the overall writing assessment.

Given these considerations, the research questions we aim to address are as follows:

(1) What is the role of language in students' writing evaluation? Previous literature sugges a correlation between content and language, primarily concerning the topic. However, it does not indicate a significant correlation between language and organization, with the exception of cohesion markers. We anticipate a correlation between content and language (Chan & Yamashita, 2022) but no significant correlation between language and organization, as the latter is generally considered more independent (Lumley, 2002).   
(2) What linguistic factors are associated with the evaluation criterion of language? We expect to find correlations between language scores and the following linguistic and morphosyntactic indices: lexical diversty and word length, TU length, the number of subordinate clauses per TU, passive voice, impersonal syntax, text length, nominal groups including an adjective and a noun (AdjNNG) and adverbs. Our expectations are grounded in existing research, which sugges correlations between language scores and these linguistic characteristics (Golparvar & Abolhasani, 2022; Lyashevskaya et al., 2021).

# 4. Methodology

# 4.1. Sample

The sample for this study comprised written texts in Greek, generated by native speakers. Specifically, the participants were students in the 2nd and 3rd grades of senior high school, aged 16 and 17 years, respectively. They attended the Hellenic College of Thessaloniki,a privat chool situated in Drosia Thermi, Thessaloniki, Grece t is worth noting that this particular school primarily caters to middecass families as inferred from its tution fees. All the students resided in the broader urban area of Thessaloniki.

All participants shared a common educational background in language, which included six years of Greek elementary school where they learned fundamental language structures, fllowed by four years of high school during which they honed thir kill in writing in the Greek language, encompasing various genres, including academic writing in their final year of attendance. While there were variations in individual writing proficiency, their language grades sugested that the sample was representative of a typical Greek high school in an urban setting.

The students were istributed across ten different school classes within the same school unit As the sample was not randomly selected but originated from the same school unit it can be categorized as a convenience sample. In terms of gender distribution, the sample consisted of 157 boys and 108 girls, constituting $5 9 . 2 \%$ and $4 0 . 8 \%$ Of the total, respectively.

Table 1 Raters' characteristics $( \mathrm { N } = 1 5 $   

<html><body><table><tr><td>Characteristic</td><td>Total</td></tr><tr><td>Males</td><td>6</td></tr><tr><td>Females</td><td>9</td></tr><tr><td>Master&#x27;s Degree</td><td>13/15</td></tr><tr><td>PhD Degree</td><td>2/15</td></tr><tr><td>With SATs Scoring Experience</td><td>15/15</td></tr><tr><td>Average Teaching Experience (years)</td><td>11.00</td></tr><tr><td>Students&#x27; Essays per Rater</td><td>18</td></tr><tr><td>From Private School</td><td>7</td></tr><tr><td>From Public Schoole</td><td>8</td></tr></table></body></html>

The school teachers responsible for supervising and assigning topics for the students essays numbered six, comprising four men and two women. The raters, who assessed the written texts, totaled 15 individuals, including nine women and six men. These rater. were drawn from both the Hellenic College of Thessaloniki (7 out of 15) and the 4th Public General Senior High School of Rhodes (8 out of 15). This selection aimed to enhance the study's integrity and objectivity by encompassing both private and public school environments (see Table 2 for details).

The topics assgned for the essays covered various themes, including depoliticization of youth (1), the importance of language and dialog (2), environmental poution (3), laconicism (4), and the utilization of new technologies in education (5). The study received approval from both the schools and parents for student participation. Importantly, none of the participants had been diagnosed with linguistic or cognitive delays.

# 4.2. Data collection

A total of 265 written samples were collected. On average, students wrote essays containing 494.73 words $( \mathrm { S D } = 1 4 3 . 3 9 )$ amounting to an overall word count of 131,104 words. These essays were based on four distinct topics, adhering to the tutors' sug. gested word range of 400to 500 words. For statistical accuracy, we excluded outliers by considering a text length within a range of $\pm$ 50 words from the mean. To mimic a exam setig (referencing Crossle et al., 2012), thessays were written in the classroom under supervision, spanning two schoo periods of 45 min each, without any breaks. These were timed, impromptu essays, and the primary academic genre was argumentative.

The collection of written texts spanned thre (3) school years, from 2016 to 2019. The selection process was carefully conducted to ensure a relative diversity within the sample. This diversty encompassed variations in topics, textual characteristics, and potential changes in scoring performance over time. However, its important to emphasize that the teaching and examination system for writing production remained consistent throughout this period. Notably, this system was similar for both the 2nd and 3rd grades of Greek High School, a the textual types (mainly argumentative) adhered to the guidelines established by the Greek Ministry of Education (I. 46/ 2016). Therfore, the topics,assessment criteria, and objectives for producing the texts were deliberately ket similar to facilitate the generation of comparable variations in the results, as suggested by Carr (2006).

It's worth noting that these essays should be categorized as Independent Writing (IW) rather than Integrated Writing (IW). While there was some preparation time of approximately two hours allocated to each topic to develop arguments and ideas, Independent Writing presupposes the ability to compose essays without direct source materials, as explained by Chan and Yamashita (2022). However, it's important to underscore that Independent Writing provides a valid representation of the underlying writing ability Guo et al., 2013).

To uphold confidentiality and anonymity, all collected data were stored without any personal identifiers

# 4.3. Materials and procedure

Students' essays were organized by class and distributed to the raters for grading, ensuring the exclusion of students' personal information details (gender, full name, school performance, etc.). This approach aimed to focus on text characteristics while eliminating background factors (Vogelin et l., 2019). The instructions provided directed ratersto adhere to the general written production assessment guidelines of the Greek Ministry of Education within the context of the Greek SATs aligning with their rgular professional routines. Consequently, the grading adhered to the specified rubric of 30/100 points (content: 12, organization: 10, language: 8), as mentioned earlier. Given that al raters had undergone formal training and practice for the Greek SATs at least twice in the past, detailed instructions or additional training were minimally necessary. The uniform rating experience also ensured a heightened focus on research goals, considering studies suggesting the impact of scoring experience in the assessment proces albeit with mixed results (Guo et al., 2013).

The students' essays were transcribed into electronic format using the Microsoft Word 2007 program for electronic processing. To exclude spelling from the assesment proces, a it was not within our original scope, we additionally correcte all spelling erors to facilitate the electronic processing of the writings.

Table 2 Pearson correlations between linguistic characteristics and evaluation criteria $( \mathrm { N } = 2 6 5 )$   

<html><body><table><tr><td>Characteristics</td><td>Content</td><td>Language</td><td>Organization</td></tr><tr><td>Number of Words</td><td>.411 *</td><td>.358 *</td><td>.432 *</td></tr><tr><td>Lexical Diversity</td><td>.389 *</td><td>.358 *</td><td>.475 *</td></tr><tr><td>Average Sentence Length</td><td>.023</td><td>.019</td><td>.029</td></tr><tr><td>Number of Subordinate Clauses</td><td>.372 *</td><td>.332 *</td><td>.394 *</td></tr><tr><td>Number of Polysyllable Words (&gt;6 Letters)</td><td>.438 *</td><td>.394 *</td><td>.479 *</td></tr><tr><td>Number of AdjNNG</td><td>.492 *</td><td>.444 *</td><td>.574 *</td></tr><tr><td>Number of Adverbs</td><td>.279 *</td><td>.247 *</td><td>.346 *</td></tr><tr><td>Use of Passive Voice</td><td>.260 *</td><td>.300 *</td><td>.408 *</td></tr><tr><td>Use of Impersonal Syntax</td><td>.349 *</td><td>.302 *</td><td>.405 *</td></tr></table></body></html>

Note: $^ { \ast } p < . 0 0 1$

# 4.4. Data analysis

To addres the irst research question, we conducted a Pearson correlation analysis to assess the relationships between language and content, language and organization, as well as organization and content. Lastly, we explored potential corlations among language, content, and organization to evaluate the transparency of the individual criteria within the rubric. Analyses were conducted using IBM SPSS Statistics Version 26.

To addres the second reearch question, which focuses on the rubric criterion of language, we began by using Pearson's correlation to evaluate the relationship between individual linguistic variables and language scores. The first linguistic variable we examined was Lexical Diversity (LD) defined as the range of unique words used in the tex. Although the Type/Token Ratio (TTR) iscommonly used in various formulas suggested by prior research (Jarvis, 2002; Lyashevskaya et al., 2021), we chose Carolls formula (1964) for our study. This choice was informed by the narrow range of lexical diversty observed in the analyzed text, making further adjustments unnecessary.

Next, we looked at word length, specificall focusing on polysyllabic words--defined in Greek as words with three or more syl. lables. We set a minimum threshold of six letters per word for our study. It's important to note that Grek allows syllables without an onset, making this measure more about letter count than syllable count.

In terms of syntax, we first examined Text Unit (TU) length and the number of subordinate clauses per TU. These metrics were automaticall calculated using our specialized tool. We included these variables because the literature review highlighted ongoing debates about the impact of TU length on language scoring. We also analyzed the use of passive voice, a feature recognized in written register but whose specificrlationship to language scoring is not well-studied. Additionall, we investgated the use of third-person singular present verbs, considered an impersonal usage in Greek and identified as a contributing factor to language scoring in some studies (Guo et al., 2013). We also considered text length as asyntactic variable. While one could argue that text length serves as an independent critrion related to content, language, and organization, existing reearch strongly suggests it inluence n scoring (Bae & Bachman, 2010; Chan & Yamashita, 2022; Guo et al., 2013; Kobrin et al., 2011; Shin & Gierl, 2021; Xie, 2015).

Finally, we explored the use of AdjNNG and adverbs. AdjNNG have been identified as significant predictors of language perfor. mance (Sarte & Gnevsheva, 2022; Uzun, 2021), especially in the context of noun phrase complexity. They also enhance descriptive accuracy, a quality highly valued in academic writing (Bailey, 2011). Adverbs, on the ther hand, have not bn as extesively studie, particularly in the context of the Greek language.

Next, to evaluate the effect of various textual characteristcs on language scoring, we utilized multiple regression analysis, with content, organization, and language scoring as the dependent variables and textual characteristics as the predictor variables. This enables us to make robust predictions about the overall quality of writing while reducing the overlap of contributing factors It also allows fra thorough examinatio of how linguistc complexity influences overall language scoring, as supported by previous research (Guo et al., 2013). Prior to running regressin analysis, we checked for the assumptions of linearit, independence, homoscedasticity, normality of residuals and absence of multicollinearity.

For data collection, we developed a prototype tool called "Gr-rater $^ \mathrm { \textregistered }$ " designed to interact with users and offer the flexibility to modify assessment parameters and export data in various formats. The too is also compatible with OCR software, enabling the conversion f manuscrits to electronic tex for further processig This ses the stage for it future roleas an automated rater based on human assessments. Notably, there was no existing automated processing tool for the Greek language, such as Coh-Metri $^ \mathrm { \textregistered }$ (Latifi & Gierl, 2021), necessitating that we build a reliable foundation from scratch.

# 5. Results

In addressing the first research question regarding the relationships among the three evaluation criteria - content, language, and organization - our analysis revealed strong correlations. Specifically, a statistcally significant, strong positive correlation was observed between content and language scores $( \mathrm { r } ( 2 6 5 ) = . 7 3 0$ $\mathbf { p } < . 0 0 1 \mathrm { \check { \ } }$ . This suggests that students who scored higher in language also tended to score higher i content. Similarl, a strong positive corrlation was found between language and organization scores (r $( 2 6 5 ) = . 7 2 3$ $\mathbf { p } < . 0 0 1 \mathrm { \check { \ } }$ . Students with higher language scores generally also had higher scores in organization. A significant, strong positive correlation was also noted between content and organization scores $( \mathbf { r } ( 2 6 5 ) = . 8 3 7$ $\mathbf { p } < . 0 0 1 \mathrm { \check { \ } }$ . Students who scored higher in content typically also scored higher in organization.

In relation to the second research question, which focuses on the corrlations between students' textual characterisics in their written assignments and their performance in the evaluation criteri of content, language, and organization, the findings are summarized in Table 2. We investigated nine specific linguistic variables: number of words, lexical diversity, average sentence length, 'number of subordinate clauses, 'polysyllable words, 'AdjNNG, 'adverbs, 'use of passive voice, and 'impersonal syntax.' The analysis reeale statisticall ignficant positive correlations between nearly al of these inguistic variables and the three evaluation criteria. The only exception was 'average sentence length, which did not show a significant correlation with scores in any of the categories.

Next, we report the outcomes of three separate multiple regression analyses, each with a different dependent variable: content scoring, organization scoring, and language scoring. Linguistic characteristics served as the predictor variables in these models. Given the strong correlations observed among certain variables - specificall, the number of words, lexical diverst, and the number of polysyllabic words, ll of which had Pearson correlation coefficients exceeding.80 - we opted to include only lexical diversity in the regression models. This choice was made to mitigate the risk of multicollinearit, which could otherwise jeopardize the rliabilit of our results.

The first regression model, which focused on content scoring, explained approximately $2 5 . 0 5 ~ \%$ of the variance in the dependent variable, as evidenced by an adjusted $\mathbb { R } ^ { 2 }$ value of.2505. A visual examination of scatter plots and residual plots supported the linearity assumption. The Durbin-Watson statistic of 2.1 indicated that the assumption of eror independence was satisfied. Both residual plots and the Breusch-Pagan test $( \mathtt { p } = . 2 5 )$ confirmed the assumption of homoscedasticity. The Variance Inflation Factor (VIF) values were close to 1 for most predictors, suggesting no severe isues with multicollinearit. Notably, AdjNNG and impersonal syntax emerged as statisticallysignificant predictors, with p-values of les than.001 and.033, respectively. These variables accounted for a moderate portion of the variance in content scores. Other variables did not reach statistical significance, suggesting they do not substantially contribute to the variance in content scoring based on this model. The findings are summarized in Table 3.

In the second regessin model, which focused on organization scoring, all rgression assumptions were met. The model acounted for $2 0 . 3 1 \%$ of the variance in the dependent variable, as indicated by the adjusted $\mathbb { R } ^ { 2 }$ value. In this model, only AdjNNG emerged as a statisticall igficnt predictor, with ap-value f les thn.001.ll thr vriables did not rech sttistical sificance as dtailed in Table 4.

In the third regression model, which accounted for $3 7 . 1 4 \%$ of the variance in language scoring, several predictor variables emerged as statistically significant. All regression assumptions were met. The model showed that AdjNNG $( \mathbf { p } < . 0 0 1 )$ , adverbs $\left( \mathtt { p } = . 0 3 5 \right)$ impersonal syntax $\left( \mathfrak { p } = . 0 0 4 \right)$ , and passive voice $( \mathtt { p } = . 0 1 0 )$ were all significant predictors. On the other hand, subordinate clauses and lexical diversity did not reach statistical significance. The results are summarized in Table 5.

# 6. Discussion

In examining potential correlations among the scoring categories, we discovered astrong positive relationship between content and anguage, aligning with previous studies that identified correlations between tex quality and analytic scores Golparvar & Abolhasani, 2022; Vogelin et a., 2019). This corrlation may sugest that employing elevated vocabulary and inricat grammar and syntax may lead evaluators to assign higher grades to content during essay evaluation, even when the actual content i lacking in quality. Furthermore, we found a significant positive relationship between language and organization. Similarly, this result may suggest that elevated linguistic complexity may lead evaluators ssign higher grades to sometimes not so well-structured and logical organized essays. Additionally, the strong correlation between content and organization, highlights that students with higher scores in content typically receive high grades in organization as well, aresult in accordance with some studies, particularly from a systemic functional linguistics (SFL) perspective (Thomas, 2022).

In the investgation of the relationship between language and overall score, we observed a significant positive correlation. This suggests that students with higher language performance also tend to achieve higher overall score, but the subjectivity of evaluation may contribute to this effect (Cumming, 2001; Elder et al., 2007; Hill & Cho, 2020; Kane, 2010; Rezaei & Lovorn, 2010; Schaefer, 2008).

Moving on to textual characteristics related to the evaluation criterion "language," we noted that text length (number of words) played a statistically significant role. This finding is consistent with previous research, indicating that text length interacts with other text features (Chan & amashita, 202; Guo t al., 2013; MacArthur et al., 2018; Lyashevskaya et al., 2021;), though text length alone may not necessarily connect directly to writing quality (Chodorow & Brustein, 2004).

Concerning lexical diversity (LD), our study confirmed its importance in language asessment, consistent with previous findings (Crossley et al., 2010; Crossey & McNamara, 2012; Jo, 2022; MacArthur et al., 2018; McNamara et l., 2010; Read, 2000). However, lexical diverst failed to predict scores i our multiple regresion analysis, ligning with some prior research (Golparvar & Abolhasan, 2022).

Regarding Mean Length of Sentence (MLS), we did not find statistically significant results, in contrast to subordinate clauses, which showed significance. The role of MLS and MLC in language assessment appears to vary across studies (Campfield, 2017; Golparvar & Abolhasani, 2022; Lu, 2010), suggesting a need for further investigation.

Also, we did not find subordinate clauses to be statistically significant in language scoring. Our findings align with previou research (Taguchi et al., 2013), which questions the role of subordinate clauses as an academic characteristic.

Word length (polysylable words) was found to be statistically significant in language scoring, in acordance with some studie (Guo et al., 2013), though the exact nature f it contrbution to lexical complexity requires further exploration (Clign, 2015; Ert) & Mumford, 2017).

Examining syntactic complexity, both AdjN and adverbs were found to be statistically significant in language scoring. Our findings align with previous research, which found that AdjN play a significant role i esay quality (Biber e al., 2016 Lu, 2011; Taguchi t al. 2013). Also, adverbs are linked to the proficiency level and are characteristic f the argumentative essay (Yilmaz & Dikilita, 2017). In fact, these descriptive elements contribute to linguistic complexity and overall language quality (Jo, 2022; Lu, 2011; Riemenschneider et al., 2021). However, the direct relationship between AdjN and adverb use and language assessment warrants further investigation. Finally, we identified a significant correlation between the use of 3rd person singular form verbs (impersonal syntax) and passive voice, both of which are connected to syntactic complexity and language proficiency (Golparvar & Abolhasani, 2022; Guo et al, 2013; Lu, 2010; Street & Dabrowska, 2014).

Table 3 Multiple regression of linguistic characteristics on content scoring.   

<html><body><table><tr><td>Predictor</td><td>B</td><td>SE</td><td> t-value</td><td>p-value</td><td>Beta</td></tr><tr><td>AdjNNG</td><td>.068 *</td><td>.015</td><td>4.55</td><td>&lt;.001</td><td>.394 *</td></tr><tr><td>Adverbs</td><td>.045</td><td>.041</td><td>1.09</td><td>.274</td><td>.066</td></tr><tr><td>Impersonal Syntax</td><td>.115 *</td><td>.054</td><td>2.14</td><td>.033</td><td>.135 *</td></tr><tr><td>Passive Voice</td><td>.004</td><td>.042</td><td>-.09</td><td>.923</td><td>-.006</td></tr><tr><td>Subordinate Clauses</td><td>.021</td><td>.027</td><td>.78</td><td>.436</td><td>.065</td></tr><tr><td>Lexical Diversity (%)</td><td>-.067</td><td>.189</td><td>-.354</td><td>.723</td><td>-.035</td></tr></table></body></html>

Note: \*statistically significant

Table 4 Multiple regression of linguistic characteristics on organization scoring.   

<html><body><table><tr><td>Predictor</td><td>B</td><td>SE</td><td>t-value</td><td> p-value</td><td>Beta</td></tr><tr><td>AdjNNG</td><td>.044 *</td><td>.012</td><td>3.43</td><td>&lt;.001</td><td>.307 *</td></tr><tr><td>Adverbs</td><td>.034</td><td>.036</td><td>.95</td><td>.339</td><td>.060</td></tr><tr><td>Impersonal Syntax</td><td>.072</td><td>.046</td><td>1.55</td><td>.122</td><td>.101</td></tr><tr><td>Passive Voice</td><td>.049</td><td>.036</td><td>1.34</td><td>.180</td><td>.087</td></tr><tr><td>Subordinate Clauses</td><td>.011</td><td>.023</td><td>.46</td><td>.641</td><td>.040</td></tr><tr><td>Lexical Diversity (%)</td><td>.002</td><td>.164</td><td>.01</td><td>.986</td><td>-.001</td></tr></table></body></html>

Note: \*statistically significant

Table 5 Multiple regression of linguistic characteristics on language scoring.   

<html><body><table><tr><td>Predictor</td><td>B</td><td>SE</td><td>t-value</td><td>p-value</td><td>Beta</td></tr><tr><td>AdjNNG</td><td>.036 *</td><td>.008</td><td>4.45</td><td>&lt;.001</td><td>.354 *</td></tr><tr><td>Adverbs</td><td>.048 *</td><td>.022</td><td>2.12</td><td>.035</td><td>.118 *</td></tr><tr><td> Impersonal Syntax</td><td>.085 *</td><td>.029</td><td>2.91</td><td>.004</td><td>.169 *</td></tr><tr><td>Passive Voice</td><td>.059 *</td><td>.023</td><td>2.58</td><td>.010</td><td>.148 *</td></tr><tr><td>Subordinate Clauses</td><td>.013</td><td>.014</td><td>-.92</td><td>.358</td><td>-.070</td></tr><tr><td>Lexical Diversity (%)</td><td>.100</td><td>.103</td><td>.96</td><td>.334</td><td>.083</td></tr></table></body></html>

Note: \*statistically significant

Overall this study highlights the multifaceted nature of language assessment and the influence of various linguistc factors on scoring, underscoring the need for ongoing research in this area.

In general, our study's implications focus on the practical use of the rubricas an asesment tool, especiall for Greek Language. In particular, the correlation of the individual rubric's categories highlights both the objectivity issues, the possble need for more elaborated categories, and the strong predictive role of language as a category in writing aessment. Furthermore, i clarifies the role of aforementioned linguistic factors in language assessment in Greek, serving as a basis for related comparative research to other languages as well.

# 7. Conclusions, research limitations and future research

This study focused on two central research questions:

1. The importance of language in overall writing assessment.   
2. The specific linguistic factors that have an impact.

Following the research findings, as far as the first research question is concerned, we conclude that language plays a pivotal role in the writing assment proes, afetin th rsiual catgories (i. content, ognzation and ovrall aement as wll arding the second research question we conclude tht certain lexical and syntactic features, as presented above, afect language assesment, but certainly, this is an open issue for further research, towards more predictable results.

Our findings corroborated the critical role of language in writing assessment while also shedding light on the complexities introduced by linguistic factors. One of the significant contributions of this study is it focus on asessing linguisic complexit, a practical issue in evaluating student writing.

The findings of the study have implications for both academic asessment and pedagogy. Understanding the relationship between Ianguage, content and organization challenges the current the rubric and provides the opportunity to create a more comprehensive one. Such an enhanced rubric could yield more reliable evaluations, considering factors highlighted in this study (e.g., number of words, lexical diversity, use of Adj NGN, etc.). The professonal development of teachers would further empower them in the adept application of the new rubric. It also provides valuable data for developing an automated tool for the automatic evaluation of the language category within the Greek examination system. Such a tol could complement human rating, ensuring objectivity and transparency in the evaluation process.

Finall, it informs educators and curriculum designers about the holistic natur of effective eay writing. It suggests that fostering linguistic skills alone may not be sufficient; attention to organizational aspects and content is equally vita for enhancing overall writing proficiency. In practical terms, this finding encourages educators to incorporate strategies that address both language use content and organizational structure in writing instruction, aiming for a more comprehensive and impactful approach to developing students' academic writing abilitie. Aditionally, the recognition of this corrlation contributes valuable insights to the broader discourse on the interconnected dimensions of language proficiency and effective communication in academic settings.

However, the study has limitations, primarily due to the imited research available on lexicon asessment in the Greek language. Our automated system lacked vocabulary frequency lis, and the software available for automatically recognizing various grammatical and syntactical phenomena was limited. This prevented us from including several grammatical categories, such as nouns, pronouns, verbs (tenses, etc.), and prepositions. Furthermore, we did not examine the influence of specific types of subordinate clauses on language scoring.

Additionally, it's important to note that we did not preserve the original orthographic form of the students' writings, despite recognizing the significance of speling and accuracy as linguistic factors in language asessment (Riemenschneider et al., 2021; Vogelin et a., 2019). This decision was made for two primary reasons: first, to minimize bias in human scoring, as our study did not focus on vocabulary accuracy; and second, to facilitate computer tagging of other linguistc features using the corrected text. Moreover, the absence of specialized software for asessing the Greek language restricted our abilit to investigate the role of cohesive features, which are undeniably important.

Lastly, while our study primarily employed quantitative methods, the psychological aspects of the rating proces aso warrant exploration through qualitative research methods, offering a potential avenue for future studies.

In conclusion, further elucidation of the language rating process- especially for the Greek language - is crucial for the developmen! of efficient Automated Writing Assessment Tools.

# CRediT authorship contribution statement

Angelos Markos: Methodology, Data curation. Andras Christos: Software. Manolis Konstantinos Koskinas: Writing - review & editing, Writing - original draft, Visualization, Data curation. Gavrilidou Zoe: Writing - review & editing, Writing - original draft, Supervision, Methodology.

# Declaration of Competing Interest

This paper i conducted as post-doctoral research at the Department of Greek Philology, University of Thrace. We have no knowr conflict of interest to disclose.

# Data availability

Data will be made available on request.

# References

Attli . (016.     a  d w e t 31) 115./i. 10.1177/0265532215582283   
Bachman, L F. (2000. Modern language testing at the turn of the century: Assuring that what we count counts. Language esig, 17(1), 1-42.   
Bae, J, ah . 2010 n itiion f f witing ts d tw s aros  laa. e etin 272) 21323. /oi.og 10.1177/0265532209349470   
Bailey, S. (2011). Academic writing. A handbook for international students. Routledge,   
Bat J t   .g 10.1016/j.asw.2020.100510   
Becker, A.(2010). Distinguishing linguistic and discoure fatures in ESL students' writen performance. Modem Jounal of Aplie Linguisics 2.   
Bibr  t tis 3 (5), 639-668. https://doi.org/10.1093/applin/amu059   
Cp07.ic   3 971.//1/52562350   
Car, . . (206. The factorstrre f tet k hactristcs and xaee orme. a eting 23), 269-289. hp/i.o/10.1191/ 0265532206lt328oa   
asal   0tql  t    1. /. 10.1016/j.jslw.2019.03.00   
Chan   ).ng s c -. 4 ./.01210062 org/10.1177/0265532210364405   
Chdrow,  & sten, J (2004). nd y gh ites cen    h Rt . 73). i, J Educational Testing Service,. Speech, 45(1), 47-82.   
Crosey, , 012. tg d  wi oi    ad sicictio.  f chn Reading, 35(2), 115-135. https://doi.org/10.1111/j.1467-9817.2010.01449.x   
Crosey,  ry, ,  Jis . 010 Ptg  n    g tl in Testing, 28(4), 561-580.   
01     -45-3201) LT202OA. 1esung, 21(2), 10/-145. nttps://a01.0rg/10.1191/U2655322u41t2/80a   
ewgier,  rw 202). T d  wol " j-ie aive grr.  s, 11(2), 84-99.   
er  ,    41, 37-64. https://doi.org/10.1177/0265532207071511   
nright,  . 0. ommn   f ewit  ish ag  that   ig 273, 317-334. https://doi.org/10.1177/0265532210363144 413-433. https:/doi.org/10.1177/026553221667339   
Golparvar, S., & Abolhasani, H. (2022). Assessing Writing, 53, 1-16. https:/doi.org/10.1016/j.asw.2022.100644   
Guo  oe  13).   jo qt   n dn   w ple A comparison study. Assessing Writing, 18, 218-238. https://doi.org/10.1016/j.asw.2013.05.002   
Hile,   0P ti e t  st,   m  a uii ing 33), 453-7/oi. 10.1177/0265532220912412   
Hudson, T. (205). Trends i asesment scaes ad criteron- refreced lnguage asemet. nul Rviw of Aplied Linguisics, 25, 208-220.   
Hyland, K. (2002). Options of identity in academic writing. ELT Journal, 56(4), 351-358.   
Janssen, G., Meier, V., & Trace, J. (2015). Building a beter rubric: Mixed methods rubric revision. Asssing Writing, 26, 51-66.   
Jaris  0 s -i  t ( 5-./1920a doi.org/10.1016/j.system.2021.102698   
Jn. 09  ra 655.. 10.1177/0265532209340186   
Jonstone, . (00.xt-tivea f md  i primar ) d rl ory tio cad nd he oan experience. Language Testing, 17(2), 123-143, 0265-5322(00) LT173OA.   
Kane, M. (2010). Validity and fairness. Language Testing, 27(2), 177-182. https:/doi.org/10.117/0265532209349467   
bri    1e an t christc e e, n  cog  154169. https://doi.org/10.1016/j.asw.2011.01.001   
Laf,   021 rnf jor  r g  si -i  o   a tie Testing, 38(1), 62-85. https://doi.org/10.1177/0265532220929918   
Laufer, B., & Goldstein, Z. (2004). Testing vocabulary knowledge: Sie strength, and computer adaptivenes. Language Lening, 54(3), 399-436.   
Lee, H K & don, . (200). Valt d topic lityf a wting omae tet. ge eting 24(3, 307-0. /oi.og/10.1177 0265532207077200 national writing raters. Assessing Writing, 51, 1-15. https://doi.org/10.1016/j.asw.2021.100604   
Lu, X. (2010). Automatic analysis of syntactic complexity in second language writing. Intenatinal Jounal of Corpus Linustics, 15, 474 496.   
Lu, . (201). os-d alin f ytic cxty me a inef e-e  rs  met.  rly, 451) 36-62.   
Lumley, T. (2002). Ament criteria i a lag-ale writing est: what do they rely mean to the rater Lagage esting 19(3), 246-276, .1191/ 0265532202lt230oa.   
Lysh 21)    g ./.106/. asw.2021.100529   
am 1   a     9,212 https://doi.org/10.1016/j.asw.2021.100540   
acrr  n  h018h si  p lf iwn   , athoe features change with instruction? Reading and Writing: An International Journal, 1-24, 10.1007/s11145-018-9853-6.   
ae0      tg 91, 85-104. https://doi.org/10.1191/0265532202lt221oa   
car,  Ji, 207.il anria e i 24, 459-8./i./.11/053207080767   
camara, .., Cosey, ., cary, . 010). isic fes of riting qualit. wie natio, 27 57-86. ps:/oi.g/10.1177 0741088309351547   
Qin 1th canadienne des langues vivantes, 56(2), 282-308.   
Read, J. (2000). Assessing Vocabulary. Cambridge University Press,.   
Raei,   201 iity a dt f uric for  t wt in 15, 89. /i.0.1016/. asw.2010.01.003 Writing, 50, 1-18. https://doi.org/10.1016/j.asw.2021.100561   
Rimmer, W. (2006). Meast natical complexity: The Gordian knot. Language Testing, 23(4), 497-519, 0.1191/0265532206lt339oa.   
Sart g Writing, 51, 1-16. https:/doi.org/10.1016/j.asw.2021.100595   
Scaefer,  (2008. Rter ia attems in an  wiing amt. g ting 25(4), 465 493. ps:/.rg/0.1177/02653208094273   
Schoonen  (205.iit f wng so: n apicatin f stl qtin mdig  ting 1, 10. /g/0.191/ 0265532205lt295o:   
Shi  ,1i  o  c r  g 82) 247-272. https://doi.org/10.1177/0265532220937830   
Street  014  c  a e n tie an f th  pled Psycholinguistics, 35, 97-118. https://doi.org/10.1017/S0142716412000367   
Tgchi,  ,  l13 si  ante win t  tive   ion program. TESOL Quarterly, 47, 420-430.   
t g Writing, 57(1), 1-15. https://doi.org/10.1016/j.asw.2023.100728   
Tas .w  y  d ctsti i. si  ion 72, 1-17. https://doi.org/10.1016/j.linged.2022.101120 39(3), 302-327. https://doi.org/10.1093/applin/amw009   
Tyndall . (1991). What influences raters judgment of student writing. Linguistics and Education, 3, 191-202.   
un,K 2021). e dtion t f n and b  i wtingrisof y d omplty vias ing 50, 1-13. https://doi.org/10.1016/j.asw.2021.100572 Vermeer, A. (200). Coming to grips with lexical rchnessin spontaneus sech data. Lnguage etig 171), 65-83, 0265-5322(00) 1680A. gei , J     01     r    tig Writing, 19, 50-63. https://doi.org/10.1016/j.asw.2018.12.003   
10.1016/j.asw.2019.100416 org/10.1016/j.asw.2015.05.001 Yilmaz, , & Dikilitas,  (2017).FL ers u of averbs in amtive eays. vL (ch n th d Lnage), 1), 69-87. Yu, G. (2009). Lexial irsit n wting and paking task pmnes. plied Linusics 31(2, 236-259.htp/oi.org/0.1093/aplin/amp024 Zhang, . 010 sh ay nd stcy f laci ion d ompin mm md.  ing 271,   
119-140. https:/doi.org/10.1177/0265532209347363 o     3)   
127-137.