# EAP practitioners' assessment behavior: Bringing the hidden-away to light

Parisa Karimpour, Farhad Mazlum

University of Maragheh, Maragheh, E. Azerbaijan, 55181-8311, Iran

# ARTICLEINFO

# ABSTRACT

Keywords:   
EAP assessment   
EAP practitioners   
EAP syllabi   
Achievement testing

Our information about assessment behavior of EAP teachers constructing and using their classroom tests is so scanty that Schmitt and Hamp-Lyons (2015) believe it is hidden away from language testing community. We investigated how two different groups of Iranian EAP practitioners (discipline-area teachers and English language specialists) perceive and practice EAP assessment. We interviewed thirty teachers and examined forty test samples. Encouraged by our interview and corpus data and since achievement tests are inherently syllabi dependent, we content analyzed all undergraduate program syllabi. We found important differences pertaining to test tasks, type of knowledge to be assessed and features of good EAP tests between content specialists and English teachers. Iranian EAP teachers' assessment behavior seems to be attributable to their perceptions of goals of EAP courses as well as course objectives enshrined in syllabi contents. We discuss our findings and conclude with suggestions for EAP teacher-testers/ assessment worldwide.

# 1. Introduction

Alderson (1988a) once remarked that, \*it is rather sobering and perhaps depressig to note the minimal attention paid to testing within ESP [English for Specific Purposes] (Alderson, 1998a, p. 87). Since then, reearch attempts to establish theoretical foundations of ESP/EAP asessment (Douglas, 2013) and delineate its pedagogical values (Basturkman & Elder, 2006) have helped with its professional maturity and sophistication. Much of the attempt, however, has en focused on large-scale international tests such as IELTS (International English Language Testing System) or TEEP (Test f English for Educational Purposes) serving proficiency purposes and EAP (English for Academic Purposes) tests serving other important functions (e. achievement, placement, and diagnostic) have been marginalized (see Schmitt & Hamp-Lyons, 2015).

In this paper, we make a case for the need to focus on EAP achievement tests constructed and used by classroom teachers in different EAP settings. We believe the cas is warranted because, first, while i is EAP practitioners worldwide who design and use EAP tests, EAP and ESP asssment theorization \*has been left in the hands of language testers" (Schmitt& Hamp-Lyons, 2015, p. 3). More specificll, AP practtionrs assmet bhavior has en hiddn away and, a  reult, has conributd lite to thery construction or wider understandings of their assesment practice in clas. The amount of published research assciated with classoom EAP tests is admittedly in much shorter supply leading to the fact that the construct classroom EAP teachers test is under-defined and under-theorized; what do EAP teachers constructing and using their tests think they test? Why? How do they do it? Answers to such questions should be systematiclly sought to see how EAP practitioners theorize and practice EAP assessment. This is important because despite similaritie between EAP assessment and language aessment in general there are featuresassociated with the former (Douglas, 2013, discussed below) making EAP construct a unique one in its own right. Thus, the study of EAP practitioners assesment perceptions and practices concerning this construct is important to see how what has ben theorized by assessment specialists is perceived and practiced by classroom teacher-testers; do they test what they should? Why? This justification (investigating EAP assesment in theory and practice i in tune withthe broader argument in educational research that studies addressing classroom realities, teachers, and students practices and beliefs are needed not only to examine any potential gaps between what isassumed (by educationalists, for example) and what i practiced but als to plan for improvement purposes in general (Vanderlinde & Braak, 2010). Second, and in relation to the first, given that EAP practitioners usually come from one of two backgrounds, i.e. humanities and specialized disciplines (Anthony, 2018), and that such backgrounds tend to afect EAP teachers' perceptions of what constitutes EAP (Llosa & Grapin, 2019; Ye, 2020), research studies are needed to examine how the two groups perceive and construct EAP tests and Why. We believe that such studies can further help understand classroom EAP assessment realities which have been neglected in EAP assesment in general (Chang, 2014; Schmitt & Hamp-Lyons, 2015). Finally, ollowing Savin-Baden and Major (2013) for whom the term positionality \*reflcts the position that the researcher has chosen to adopt within a given research study" (p. 71), we take the position that EAP assessment literature is markedy skewed it is primaril focused on international standardizd proficiency tests (e.g. IELTS and TOEFL) and, to alesser degree, national tests (se Lumley & Qian, 2003 for Hong Kong & Olaofe, 1994 for Nigeria) serving proficiency purposes and other EAP tests (achievement in our study) have been marginalized. We believe due attntion has not ben paid to how EAP practitioners worldwide asses their students and why; we, therefore, choose t focus on two groups f Iranian EAP teachers assessment behavior with the hope that more studies rflectig aessment realities of other EAP contexts are conducted. We believe such studies can contribute to a more inclusive EAP asessment research ecosystem. We believe that information on EAP assesment going on in Type Four contexts of EAP (Dudley-Evans & St.John, 1998) is quite scanty. In such contexts, \*l tertiay education is taught i the L1 with English recognized as an auxiliary language, benefiting some students who may need to read cutting edge studies found in English only (Carkin, 2005, p. 87). Little has been published about EAP testing in such contexts that cover Latin America, many countries in the Middle East, South East Asia, Eastern Europe and Scandinavia, and mainland Western Europe (Dudley-Evans & St.John, 1998). We, representing Type Four, feel our EAP assessment context has had littl reflection in EAP assessment literature.

The study intends to contribute to such reflection by answering the following research question: How is EAP asessment perceived and practiced by two groups of Iranian EAP practitioners: content specialists and EFL (English as a Foreign Language) lecturers? The question encompasses important EAP assessment ssues such as what EAP teachers believe they should test and why, which test tasks they design and why, f hey cooperate witheach ther and how, if they are aware of asic asessment principles, and whether they are provided with any professional scaffolding in EAP assessment.

# 2. English and EAP in Iran: A brief snapshot

English is clasically viewed a foreign, and not a second, language in Iran. Currently, Iran follows a two-tier mainstream education system: 6 years of primary and 6 year f secondary schooling. Iranian students with different L1 backgrounds formally start learning English in the first year of the second tier when they are 13 but an earlier start is growing popular in the fee-paying priate sector. Although ther foregn languages are listed as competing options in the National Curriculum, it is nglish that prevails. Also, leaning a foreign language iscompulsory for allIranian students. When in universties, they need to take a three-credit General English ourse aimed at further fostering their reading comprehension, general vocabulary and grammar. The courses are fllowed by either a two credit, three-credt or four-credit techncal nglish coures (g. Enlish for the students of law, Eglish for the students of bology, etc.) in BA/BS programs. More specifically, the courses arediscipline-based ESAP (English for Specific Aademic Purposes) courses the main goal of which is to bridge the gap between the learners general English reading competence and their ability to read discipline-based texts' (Atai & Shoja, 2011, p. 306). The emphasis on reading skill is justified on the grounds that Iranian university students need to acces the lat sciific and technogcal nomation in ther field f tudy by redig oriinl rech paprs, textbooks, Intenet pages, etc. in English. With regard to four ESP/EAP situations proposed by Dudley-Evans and St.John (1998), Iran's EAP programs belong to Type Four contexts explained above.

EAP teachers in Iran come from two different groups: content specialists from content departments and EFL lecturers who either come from English departments or are visiting English teachers. It might be surprising to the readers that the former group (a PhD holder f chemistry, for example) has no educational background in English studies but onsiderably outnumbers the later in offering such courses. EFL lecturers have either an MA or PhD in Teaching English to Speakers of Other Languages (TEsOL) or other English. related degrees (e.g. Translation Studies, Linguisics, etc.). It should be noted that EAP syllai development is highly centalized in Iran: the Ministry develops them and al teachers are given the corresponding syllabi. Iranian AP teachers are left on their own to interret the syllabi d follw thewit itl inrmedary inolement f universities faultie or eartments th tch at. Unlike some contexts, there are no faculty or department handbooks for Iranian AP teachers. They choose which materials to teach and what and how to test. Moreover, unlike the growing trend in many Asian universities, i.e. English Medium Instruction (EMI), and the fact that EAP courses are considered as useful bridge to EMI (Anthony, 2018), EAP programs have a marginal status in Iranian universties and suffer fundamental problems. No university in Iran follows a full EMI program since, in addition to other problems, a politico-ideologically driven English demotion policymaking is pursued at the state level since English is sometimes described as the language of the enemy (referrig to the US and the UK) that might threaen Iranian students' religious and national identity (Mazlum 2022).

A moot question in Iranian EAP context, and other contexts (Anthony, 2018; Flowerdew, 2013; Starfield, 2016) s: Who is the right person to teach EAP? Field pecialists or EFL teachers? Diffrent Iranian stakeholders (heads of discipline-specific departments, heads of English departments, EFL teachers, content instructors, EAP students) harbor opposing views. Iranian students in general tend to prefer EFL specialists more (Atai & Asadi, 2013). EFL teachers of EAP are argued to be more aware of some core ssues (e.g. needs analysis, EAP methodology, EAP syllabi designing, course objectives) in general. The positive side, nonetheless should be seen in tandem with challenges this group encounters: they often feelalien in the new territory and have problems coping with field specific knowledge (Soodmand-Afshar & Movassagh, 2016) and are not recognized by content specialists as the right individual to offr EAP. Iranmehr et al. (2018) report that Iranian syllabi set no qualifications for EAP teachers. This leaves heads of content departments, and not those f language departments since t i th former group making decisions, on their own to decide whether to shoulder the responsibility on an inside-the-department fild specialist or invit an EFL teacher to fr the course. The criteria for the first choice might sound even stranger for an outsider. Content specialis end up teaching EAP courses simply because their general English is assumed tobe better than the other academic staf. The assumption i yet more strangely supported on the grounds that the teachers have been to an English speaking nation for their PhD programs, they have more papers published in English, they speak English more fluently, etc. It should be mentioned that team-teaching is not a known practice in Iran (Soodmand-Afshar & Movassagh, 2016).

# 2.1. EAP assessment

One key ssue in EAP is asessment. Although once Hutchinson and Waters (1987) expressed their concern over the lack of "any sound theoretical or empirical basis for EAP testing" (p. 146), seminal research endeavors by Douglas (200, 2005, 2013), OSullivan (2012), OSullivan and Weir (2011), among others, have helped with theoretical maturity of EAP assessment leading Emery (2014), for example, to state that \*.. test developers of today are equipped with more robust theoretical frameworks' (p. 199). For Douglas (2013), EAP testing shares some common grounds with other areas of language asessment whil it is characterized with its own qualities. A far as similarities are concened, the acceptd principles of measurement including reliability, validit, and impact should be adhered to by all language testers including EAP practitioners. At the same time, Douglas (2013) argues that three features distinguish EAP asessment from asessment in other areas of language education: contextual variation, precision associated with specific purpose language, and the interaction between language and disciplinary background knowledge. The fact that \*physicians use English differently when talking with other medical practitioners than when talking with patients ibid: 368) is a testament of contextual variation although both contexts belong to Medical English. Specific purpose language precision means that members of a discourse community strive for maximum accuracy and precision in communication a concomitant of which might be the jargonish tone associated with diffrent fields what might seem to outsiders as unnecesaril obtuse is a mechanism to achieve clarity and diminish the likelihood of ambiguity.

Douglas's main contribution is probably the third feature he ascribes for EAP tets and his definition of EAP construct: a composite construct including both field specific content knowledge and specific purpose language knowledge. In other words, while in other non-specific purpose tests content and background knowledge is often viewed as a confounding variable resulting in construct irrelevant variance, in EAP assessment it is aconstituent element of the trait interacting with language knowledge (Douglas, 2013). It should be noted that whereas the inseparability of language and content knowledge in EAP testing is recognized nowadays by Douglas (2013), OSullivan (2012) and Airey (2016), earlier cholars (e.g. Criper, 1981) argued for distinguishing the two. For Airey (2016) although EAP courses pursue "... mainly language learning outcomes'" it i a ... allay to think that content and language can be separated' (p. 73). This takes us to the distinction between general and specific purpose testing. Douglas (2013) argues for the EAP construct and features assciated with EAP assessment yet he contends that the distinction between general and specific purpose testing is blurred and it makes more sense to view different tests on a continum of specificity \*with quite general purpose test, such as TOEFL, at the one end and quite narrowly defined tests, such as the test of Proficiency in English for Air Traffic Control, at the other" (Douglas, 2005, p. 858) while acknowledging the difficulty of agreeing on where on such a continum any given test can be placed. Similarly, Knoch and Macqueen (2019) locate language tests used for professional purposes on the same notion of continum of specificity.

Additionally, for Douglas (2005) EAP testing is ". special case of communicative language testing in that they hare a number of critical features, including the notions of language performance, specific contexts of use, and communicative capacity (p. 858, our emphasis). Additional, henotes that EAP testing methods and contents are fundamentally ffected by the analysi of Target Language Use (TLU) stuation. In a similar vein, Brunfaut (2014) refers to the notion of locality in EAP assessment and streses that contextual features of the discourse community a test-taker plans to join are important in such tests.

EAP assessment is undertaken for proficiency, placement and achievement purposes. This classification is found in classc (e.g. Hutchinson & Waters, 1987) and new publications (e.g. Sabieh, 2018; Weigle & Malone, 2016). Common to these classifications is the significance atributed to teacher constructed tests in EAP courses worldwide. By definition, achievement tests of EAP are syllabi dependent, classoom leveled, usually teacher constructed tes, the results of which are used either \*summatively to ases students achievement as part of a grade or formatively to give information about the successor falure of what has ben taught and leaned (Weigle & Malone, 2016, p. 608). In almost ll EAP programs, asessment f leaner achievement i inevitable because the information it yieds is nry for cour der, thrs, sts an rso dide, . whher stnts  hve, ae lite, h or are acquring the language inherent in whatever form of EAP they are learning (Sabieh, 2018, p. 3). In the past it was argued that achievement tests of EAP are les problematic since they are intenal to the course, do not have to conform to external influences, and are not prone to theoretical difficulties. That such tests are less problematic should not hide the intricacies involved since

..all EAP testing endeavors involves five steps: (1) to identify and operationally state the objectives or learning outcomes based on students' needs analysis, (2) to create the scenarios, (3) to provide the EAP case-sensitive information, (4) to develop the tasks, activities, or items, and (5) to create the assessment tools or rubrics (Sabieh, 2018, pp. 3-4, emphasis added).

Each step requires careful considerations. For instance,creating scenarios entails an elaborate acount of abilites to be tested, how detailed the results should be, how important the backwash is, and what constraints affect test development, administration and scoring. Provision of EAP case-sensitive information adds more to EAP test construction since test designers need to clearly specify the content to be tested, test structure, assment techniques, time alocation for diffeent test tasks, and channels or mediums of test tasks. How AP test takers peformances will be scored and what levels of performance will e used as achievement criteria are further necessary considerations that should be addressed in asessment rubrics. As a result, EAP teachers creating domain-specific tests e.g. summative achievement tests would face certain truggles. They wl be challenged by iues ertaning to the choice of test tasks to be included, the extent of releance of such tasks to students target sitation needs, the tension \*. between aseing discrete bits of anguage and asessing language in a specific context, which involves both language and disciplinary knowledge (Weigle & Malone, 2016, p. 617) thus creating a healthy balance between focus-on-language and focus-on-content, how students are . going to use English to communicate and complete the task they are being asked to do?" (Sabieh, 2018, p. 4), the consequences of their tests (Schmitt & Hamp-Lyons, 2015), the nature of collaboration with a language expert or a content specialist, etc.

Despite the emphasis placed on classroom-level EAP teachers assessment behavior, their assessment perceptions and practices are largely hidden away, to borrow from Schmitt and Hamp-Lyons (2015), from EAP specialis and language testing community. The current study is a step to address such a gap in EAP assessment literature.

# 3. Current study

The study investigates EAP asessment perceptions and practices of Iranian teachers constructing achievement tests. While the study focuses on Iran, the findings can be of relevance to the wider ssessment community since alarge number of EAP practitioners are teacher-testers who design and use their own tests.

# 3.1. Method

# 3.1.1. Data collection

To collect data, we used interviews and examined test samples as well as undergraduate program syllabi in Iran's Ministry ol Science, Research and Technology (MSRT).

# 3.1.2. Interviews

We conducted semi-structured interviews (Appendix I) with 30 EAP teachers from content $\left( \mathrm { N } = 2 0 \right)$ ) and English departments $( \mathbf { N } =$ 10) at six universties. The interview protocol was based on several pertinent issues of assessment i literature (Dudley-Evans & St John, 1998; Douglas, 2000, 2001, 2005, 2010, 2013; Sabieh, 2018). Each interview lasted $3 0 { - } 4 0 \mathrm { m i n }$ We ensured our participants of their anonymity and confidentiality of data. Interviews (either in Farsi or Azerbaijani Turkish) were recorded and transcribed verbatim. Table 1 gives more demographic information of the participants and the courses they offer.

Table 1 Interviewees' demographic information.   

<html><body><table><tr><td>Teaching experience</td><td>Degrees</td><td>Age range</td><td>Gender</td><td>EAP Courses</td></tr><tr><td>Below 5 years:</td><td>MA:</td><td>Under 30:</td><td>Male: 70%</td><td>Law, medical plants, information technology, urban planning, civil engineering, chemistry,</td></tr><tr><td>36.6%</td><td>76.6%</td><td>16.6%</td><td>Female:</td><td>hadith, philosophy, computer engineering, horticulture, industrial management, agronomy,</td></tr><tr><td> 5-10 years:</td><td>PhD:</td><td> 30-35:</td><td>30%</td><td>miniature and glass, Islamic jurisprudence, geology, biotechnology, physics, electrical</td></tr><tr><td>33.3% 10-15 years:</td><td>23.3%</td><td>33.3% 35-40:</td><td></td><td>engineering, sports management...</td></tr><tr><td>16.6%</td><td></td><td>33.3%</td><td></td><td></td></tr><tr><td>15-20: 6.6%</td><td></td><td>Over 40:</td><td></td><td></td></tr><tr><td>Over 20 years:</td><td></td><td></td><td></td><td></td></tr><tr><td>6.6%</td><td></td><td>16.6%</td><td></td><td></td></tr></table></body></html>

Table 2 EAP test samples of the study.   

<html><body><table><tr><td>Academic fields</td><td>Number of samples</td><td>Courses</td></tr><tr><td>Agriculture</td><td>7</td><td>Agricultural engineering, medical plants, agronomy, biotechnology, soil sciences, horticulture, agricultural machinery</td></tr><tr><td>Human Sciences</td><td>9</td><td>Geography and urban planning, law, psychology, philosophy, sports management, industrial management, Farsi language</td></tr><tr><td>Social Sciences</td><td>7</td><td>and literature, public administration, business management Accounting, economics, public relations, political sciences, sociology, history of Islam</td></tr><tr><td>Basic Sciences</td><td>5</td><td>Biology, physics, chemistry, geology, computer sciences</td></tr><tr><td>Hawza Studies</td><td>4</td><td>Hadith, Islamic jurisprudence, Quranic sciences, Islamic sciences and history</td></tr><tr><td>Engineering</td><td>6</td><td>Electrical engineering, civil engineering, computer engineering, information technology, mechanical engineering, material sciences and metallurgy</td></tr><tr><td>Art</td><td>2</td><td>Miniature and glass art, handicrafts studies</td></tr></table></body></html>

# 3.1.3. Test samples

We included forty teacher-designed EAP test samples (22 from the interviewees and 18 from universities where EAP courses are offered for undergraduate students in North-West of Iran in four provinces) in our corpus. They were typical samples f course-exit tests Iranian language and subject specialists develop in undergraduate programs. Since the seting was limited to six universities, the corpus was affcted by the existing programs at these universities. Also, some (five) EAP teachers did not provide us with thir test samples since they believed such samples were for teacher-students use and not for analysis purposes. The corpus comprised of EAP tests of Agriculture, Human Sciences, Social Sciences, Basic Sciences, Hawza Studies (a field of study teaching students traditional Islamic theories and disciplines as well as contemporary isues of the Islamic world), Engineering and Art with 7, 9, 7, 5, 4, 6 and 2 samples respectively. Table 2 gives a summary of the corpus.

# 3.1.4. EAP syllabi

Iran's MsRT website divides all undergraduate programs into 11 broad groups: Veterinary Medicine, Social Sciences, Human Sciences, Basic Sciences, Hawza Studies, Marine Studies, New Technologies, Engineering, Agriculture and Natural Resource, Militay Studies and Art. Thesyllabi fr each group are divided into two groups: dated and in use. All in-usesyllabi (429) were intilly included in the corpus. The number dwindled to 245 because some programs with in-use syllabi lacked EAP courses (60); some others had two or three in-use syllabi and the latest versions were included (82) some listed EAP courses in their lists of courses for a degree but provided no syllai (17) and finally, some programs did not ffer EAP courses (25). New Technologies was excluded from the corpus since there was just one undergraduate program with a dated syllabus. Table 3 summarizes the data.

# 3.2. Data analysis

Information on analyzing data from the three sources is given below. During data analyses, the analysis of different data types informed analysisof ther types. More specificlly, the syllabi content was checked when the interviewees talked about language skills they prioritied in EAP testing and this, in turn, was examined with tes samples. Similarly, Iranian EAP teachers test items and task types were compared with what they expressed during interviews and with their corresponding syllabi. Following Dornyei (2007), we employed member checking; we shared our findings on interview data with some interviewees to check for the accuracy and dependability of them. Such participant feedback and the citations we give help with interpretive validity of the analysis. We also asked an expert in qualitatie data analysis to do an independent analysis of some of the corpus in the study to help us further with the credibility of our findings and discussions.

# 3.2.1. Interview data

Interview transcriptions were subjected to qualitative and quantitative content analysis.First all interview transcriptions to eacl question were read several times to familiarize with datato make asense of the whole. Then, we went on with identification and noting pieces of data (i.., key words, phrases .) that were relevant to the research question. For example, for the questionWhat should EAP tests aim at? we extracted two recurrent themes (i.. field-specific technical terms and reading comprehension) and calculated the frequency of such key ides/themes across our data. Next, we organized the initil ideas across interview transcriptions fr each interview question into more generic, inclusive and descriptive labels. We then checked these general categories against our interview data to ensure that data pieces and our categories support each ther (raun et al., 2014). Finally, the researchers met to iscuss the results of data analyses and negotiate any potential discrepancies.

Table 3 Academic fields, in-use syllabi and the number of syllabi selected.   

<html><body><table><tr><td>Academic Fields</td><td>In-use syllabi</td><td>Selected for study</td></tr><tr><td>Agriculture &amp; Natural Resources</td><td>19</td><td>13</td></tr><tr><td>Engineering</td><td>71</td><td>55</td></tr><tr><td>Marine Studies</td><td>13</td><td>7</td></tr><tr><td>Hawza Studies</td><td>60</td><td>32</td></tr><tr><td>Basic Sciences</td><td>29</td><td>17</td></tr><tr><td>Human Sciences</td><td>145</td><td>76</td></tr><tr><td>Social Sciences</td><td>33</td><td>22</td></tr><tr><td>Veterinary Sciences</td><td>3</td><td>1</td></tr><tr><td>Military Sciences</td><td>15</td><td>8</td></tr><tr><td>Art</td><td>41</td><td>14</td></tr><tr><td>Total</td><td>429</td><td>245</td></tr></table></body></html>

# 3.2.2. Test samples

To analyze the content of EAP test samples, we relied on two leading inclusive questions: What language sills and components are assesed? What test tasks and assesment techniques are used? The questions, however, did not confine our analysis and we decided to keep 'open' with any other assessment behavior and keep a record of them too.

# 3.2.3. EAP syllabi

Our examination revealed that Iranian syllabi could be divided into two groups: those with a section addressing assessment named How to Asses Students (hereafter HAS) and those without it. Almost al HAS sections included a table in which syllai designers provided teachers with some information on asessment irst we independenly studied allthe tables to quantify the major categories (O'Leary, 2014).The process was straightorward since the only information in tables was limited to time of test adminstation, score allocation and, in quite few caes, item types. In other words, a typical syllabus asked teachers to hold mid-term and final examinations, follow a grading policy (e.g. $4 0 \%$ mid-term; $6 0 \%$ final exam) and, in quite few cases, use some assessment techniques or question types. High inter-researcher agrements asured us of the consistency f frequency analyses procedures. Second, since syllai as documents handed down by ministries or other regulatory bodies with statements of what is to or at least what should) be learnt provide a visible basis for testing and because a good number of Iranian EAP syllabi lacked HAS sections, we decided to content analyze the Course Objectives sections of the syllbi. The second data analysis procedure could make comparisons between teachers' perceptions of EAP courses, crseectives, A asment, et.. and ylab contents possib. Wed and r-red ll ors ectie sections f l syllabi indendently first. Next, we developed a coding scheme based on the identified and agreed-upon recurrent key words/phrases in Course Objectives sections (i.. reading, writing, spaking, listening, technical field-specific vocabulary, traslation, grammar and others) followed by another round of frequency analyses. This led to frequencies of the course objectives (Table 6 below).

# 4. Findings

Findings pertaining to interview data and EAP tests analyses come first followed by those of syllabi analysis.

4.1. EAP assessment in Iran: Teachers' perceptions and practices

During interviews, Iranian teachers expressed their views on several issues of assessment. Their views are presented accompanied when applicable, with examples coming from their practices.

# 4.1.1. EAP construct: what should be assessed and why?

According to the interviewees, the general purpose of EAP assessment is first, asssig technical vocabulary knowledge' and, second, 'comprehension of discipline-specific reading texts'. More specifically, sixteen content teachers $( 8 0 \% )$ and six language teachers $( 6 0 \% )$ held such a view. One of the interviewee, for instance, argued that, \*.. EAP tests should provide us eedback on whether std e d on fc th gi.. nd f ty  cal f r n phc ts content teacher no. 4) " focus n tchncal ts firt . they students] should know what they are and mean... his is important in this cours. .. her  k.. ie t  t..  ch he   .. he nt.. or  ther materials" (content teacher no. 12). Four EAP teachers with backgrounds in English language teaching $( 4 0 \% )$ believed that, in addition to such purposes, EAP tests should aim at asessing students academic writing abilities' and oral skills too. For example, one EAP teacher noted that, \*it is not technical scinific words only . they [studens] should be asesse f they can write a short report of the experiment they ave recently conducted in lab or the one depictd visuly (anguage teacher no. 1). Another teacher tated that, \*making oral presentations on simple technical topics should be assessed too' (language teacher no. 3).

Interestingly, viewing 'technical vocabulary knowledge' and 'comprehension of discipline-specific reading text' as the main purposes of EAP assessment supports Table 6 in which a similar prioritization is evident in ranian syllabi. The way to pursue such purposes, however, is different for subject specialists versus language experts, as detailed further below.

For eighteen content teachers $( 9 0 \% )$ and 8 of language teachers $( 8 0 \% )$ , EAP assessment is fundamentally different from General English asesment because the former asesses content and discipline-specific knowledge whereas the latter deals with whater is nontechnical English. Otherwise sated, the interviewee almost unanimously believed that the key distinguishing feature of the two is 'subject-specific knowledge' This view needs to be examined with regard to the interviewees ideas concerning what knowledge EAP tests should assess. For sixteen of content teachers $( 8 0 \% )$ it should be discipline-specific content knowledge' only. Such a strict adherencetocontent knowledge assessment only has led some of them to view English language knowledge as aconstruct irrelevant factor that might be excluded from tests. Following such a belief, they sometimes tend to develop test items with instructions in Farsi (the official medium of intruction) so that the English language knowledge barrir is circumvented. For instance, one such test item (given to EAP students of biotechnology) reads:

sLs ojLs 4 4?gj L.ow1 5LyiSLJgLw S:50+JSs uyiS ojj sLouig#: j1 S:o1S jiLLj ojLi ju (jL+Sgi Jxo) oLw s\$1i5 S   endo s peri  epi meso (The following oval is abacterial cell. Which prefixes (meso, epi, peri and endo) does each number within arrows represent?)

![](img/7503b6ee3ce7d97920e13c16e6ccdac2445cce7c15114cf69c88f32d10025d9d.jpg)

A similar assessment behavior is present in a multiple-choice EAP test of law designed by a content specialist:

(Which one is the English equivalent for \* $w _ { 1 } = 0 g ( x _ { 1 } , 3 ^ { s } )$

a. Assessment b. Custodial c. Existing d. Sentencing.

We conducted fllow-up telephone interviews with teachers using Farsi in their items. Their practice is rooted in their perceptions of what EAP tests should ases and thir students' low General English knowledge that might negatively affect their performance. For instance, a content teacher argued that \*i is technical knowledge I am tesing .. whatever stands in the way to this aim .. such as poor General Engish el.. should be somehw adrese.. nd  use Frsi to hand it (connt techr no. 6), and anther tachr nted that "well... ad on y xrice . any dnts face problems undertndn what som questions ask . ht' w iitte th job by using Farsi ... but I know it looks clumsy' (content teacher no. 19).

Another consequence of the content knowledge only' belief is designing test items that are primarily knowledge rather than language tests. The following item comes from a content teacher's test of geology:

. forms when basaltic magma erupts under water.

a. Vent   
b. Crater   
c. Pillow lava   
d. Stratovolcano.

The attempt to separate language knowledge and content knowledge is not popular with EFL lecturers. For most EFL teachers $( 8 0 \% )$ , content' and language' knowledge are inseparable. Tes items should ".. engage both simultaneously . you cannot think of an item depending on either language or content knowledge solely (language teacher no. 7) and \*both elements are naturally there when they [students] answer items .. whe I give a reding omprehension test .. nd it is about mdwifery ..then... t's natral tha both English knowledge and midwifery knowledge get related' (language teacher no. 6). Although language teachers generally believed that EAP and EGP tes are diffeet and tht the aer ns t addres,  a mixtre f al skils and sub-skill whch ght be sed indifferent contexts like shopping, asking for direction, discussing a social isue" (language teacher no. 5), they noted that EAP test items should involve students' both language and content knowledge; \*. items that can be answered using one component only are ll-formed (language teacher no. 10).

# 4.1.2. EAP test tasks

As with test tasks, content teachers and language teachers differed in their views. Content teachers voted for translation tasks $( 6 0 \% )$ followed by reading comprehension tasks $( 3 5 \% )$ and definition tasks $( 5 \% )$ whereas language teachers laid more emphasis on reading comprehension tasks $( 7 0 \% )$ . For them, translation tasks came last $( 1 0 \% )$ ; oral and writing tasks $( 2 0 \% )$ were perceived to be better candidates.

Language teachers preferred reading comprehension tes tasks more since ...diffrent reading comprehension questions involve different le f comprehnsin angage cher no.3), . comprehension f txs they test-takrs] will nount and nd more ineal wol  fe aage o. ) ad, in rn its rth ce ir.  fe his course, th [ ile redg  of tc t. dy nmpetho t.. hts w,  think rn cprhtss ar e mrtant and relevant' (language teacher no. 6). Examination of reading comprehension test tasks designed by both groups revealed that typically multiple-choice questions were given. True/false and short-response techniques were also used, though les frequently. For language teachers, oral and writing tass as yet other altenaties f asessment were, .. ncesary sine students prouctive abilities should be asessed too (language teacher no.4). Oral tass require students to present on a technical topic and writing tas primarily mean asking students to write a short 'report' on a subject-specific topic.

Translation tasks, according to the content teachers, might be of several forms. Students might be required to provide Farsi translations of field-specific terminologie, sentences or longer texts; i.e. production tasks. For example, in an EAP tet for students of civil engineering, they were required to write down Farsi equivalents of baffled apron, over chute and hydraulic jump. Students of philosophy, medical plants and Islamic jurisprudence were given long technical' texts to be translated to Farsi. Alternatively, translation tasks might be of recognition type: students were expected to 'recognize the right translation of a term. For instance, students of miniature and glass art were given the following:

Generally, the water color is translucent, so that much of the white paper show [sic] throw. Translucent means:

1.ouiis sL   
2. ouiis olw   
3.Jw JL   
4.Hi oLj1

Likewise, students of Hadith (a field of study in which students are taught the collection of traditions containing sayings of the Prophet Muhammad) were asked to choose the correct translation of the underlined part: Because he died les than three months after completing it, it is known as Farewell Pilgrimage.

a.&ia b.oyoc t C. 'g t d. g1sgl d7

Finall, to asses students translation competence, they might be required to back translate. EAP students of computer engineering, for example, were asked to provide the English equivalent of 'je', $\therefore t ^ { \prime } \in ( 0 , 1 )$ (components and assemble respectively).

As for definition tasks', students were either required to provide a short English definition of field-related terms (e.g. tacticel planning, situational anals ec. in an EAP test for the students of industrial management) or were given definitions and asked to give the intended technical terminology (e.g. removal of small molecules from a macromolecule prearation by allowing them to pass across a semipermeable membrane in an EAP test for the students of horticulture). Table 4 compares test tasks designed by each group.

Table 4 Comparison of test tasks designed by EAP teachers.   

<html><body><table><tr><td>EAP teachers</td><td>Test tasks</td></tr><tr><td>EFL teachers</td><td>Reading comprehension (multiple-choice and true-false items), speaking/oral tas (short cass presentations), writing tasks, translation (English to Farsi)</td></tr><tr><td>Content teachers</td><td>Translation (English to Farsi; Farsi to English) rading comprehension (mainly multiple-choice questions), definition tasks (focusing on technical teminologie), trnsation projects (each stdent is expected totranslate a teacher-assgned field specific paper, book chapter or article independently and hand it in when the course ends)</td></tr></table></body></html>

# 4.1.3. Cooperation in EAP assessment

To know if language and content teachers cooperate in EAP ssessment in general, a question was put to the interviewees. Sixteen content teachers $( 8 0 \% )$ and six language teachers $( 6 0 \% )$ answered negatively; 4 subject teachers and 4 language teachers stated that they cooperated with each other. Further probing during the interviews, however, revealed that for the later group cooperation meant informal talks on an occasional basis. For example, one of the subject teachers noted that, \*..I ask my collagues in the language department about asssment techiques . I'v see thy ue various tas" (content teacer no.2). The lc f cooperation was aributed to diffe fatr.  inri  tht, . fe P ter  iffnt tbook, th tch e . p iews nd styles pral in techig d testng. cooperio under such cicste i rll difilt (connt ther no.14) and another one expre hat to e frank .I'm t sy for tht.. n thn thy angag tcher hae done th o inl nis .. their job is over and it's my tn now wth AP content teacher no. 8). Another teacher believed that, oopeation is useful for teaching not testin (connt techr no. 17. Finally, a lanag techr exprsd that, usuall ake the iative and xs y willines to wok tgter   .. .. ttg...  ptn..   th   lingas I am ... and even sometimes they prefer to avoid any collaboration' (language teacher no. 9).

# 4.1.4. Assessment knowledge of EAP practitioners

When asked about the key features of good EAP tests, interesting standpoints distinguishing language from content teachers were expressed. Almostall language specialists showed an awarenes of technical notions of reliability, validity and test impact and pro vided technical explanations of the terms. They, in general, held that their assessment behavior affcted their students' learning behavior and that test could be used to have positie washback efect. For instance, one of them aserted that, \*in my test, I design tasks focusing on meaning. This way my students lean they should read for meaning.." (language teacher no.4). Even four of them $( 4 0 \% )$ argued that test tass need to mirror students' workplace needs; \*the reason I focus on reading comprehension in my tes is that they [students] wll need it later... when they find a job.. theyll need to red' (language teacher no. 1). oreover, five of them $( 5 0 \% )$ argued for diversity in aset technques and sigficnce f itm charactristics when using multiple-choice tems i. item facility, item discrimination and choice distribution). One potential explanation for their technical knowledge of assessment is the compulsory testing courses they take in BA/MA/PhD programs. The only relevant feature highlighted by subject teachers, however, was content coverage': they believed that there should be a correspondence etween test and course content: \*fetures that  good tes should hve.. first and formost is tht I hod test what we hae covered togter in cl. stdts wll cmpin f ncude somhingtht we he nt covered' (content teacher no. 11). This was an indication of the fact that one dimension of content validity was of significance to them too. Other characteristics raised by the content teachers were minor format and style, appearance and managerial/administrative issues such as checking for typos, optimal timealloation, eas f scorig, legibility of test items and questios starting and ending the test punctuall, ec. One content teacher whose course syllabus suggested the use of portfolio asessment had no idea what the term meant.

# 4.1.5. Availability of EAP assessment guidelines/rubrics

Finally, eachers were asked if they were provided with any asessment guidelines. Ninety percent of the participants answered negatively. This was expected since large number of syllabi did not address asessment a all Syllabi with HAS were also rarely used. One of the interviewee noted that, . our syllabus does not clearly specify asesment procedures (content teacher no. 12). Another participt sta tht, . te is ne inle tm o met. th its e  ha idms d uz..is this a guideline?" (onnt the n.7).d, nthP cer vo er dsfacto syig . i cn e cd  delin.... the syabus just says that you'd beter give quzes . and final exams' (content teacher no. 13). An interesting fat was that content teachers, when left with no practical detailed guidelines, tend to follow the tradition': they test as they were tested. Five language teachers $( 5 0 \% )$ believed that what came in HAS was not an assessment guideline since it stated the obvious; i.e. mid-term and final examinations should be given. It should be noted that the other five language teachers had not ben provided with the syllabi. Generall speaking, the articipants agreed that the HA sections of EAP syllabi were limited', too brief and of ittl, and sometimes, no use'.

# 4.2. EAP test samples

Test tasks teachers reported as potential candidates in EAP tests were observed in the corpus. Several points, however, are worth mentioning. First, although reading comprehension is recognized by both groups as their testing policy, post-reading questions were, in some cases, different both in technique and nature. Compared to content specialis, EFL teachers tended to employ multile types of comprehension questions. For example, whereas field specialists mostly developed multiple-choice items, EFL teachers employed multiple-choice, true-false, open-ended short answer questions, inference questions, cloe tests, word formation tasks, etc. As noted above, speaking tasks, according to the interviewees, are used during the course. Such tasks, reported by 5 EFL teachers, are typically limited to one specific genre i.e.formal classpresentations). No extended writing tasks were observed in EFL teachers' samples.

As far as content specialists' test samples are concened, what they reported as their most popular test tasks (i.e. translation) was dominant. Almost $7 0 \%$ of what they included in their samples can be subsumed under the umbrella term of translation. First, translation of technical terminologies and field-specific texts make up $4 0 \%$ and $6 0 \%$ of translation tasks in the corpus respectively. Second, a closer scrutiny of what this group reported in interviews and used in their tsts as definition tasks revealed an interesting fact. Definition tasks targeting students content knowledge differed in response format. While in ates f plant biology or gronomy, for instance, tet takers are required to supply a one-word technical term e.. Any change in D i calld ..; Top part f pistil which receive olle i .) studnts of indstrial management and boechlo ed to do more; they ed o proide reponse emanding combination of their field related knowledge with their English writing abilit (e.g. describe tactical planning'; explain gel flration chromatography, semigamy' and Paster effect). Admittedly, the completion of such tass asks for not only a good amount of English writing abilit but also knowledge of language of description. Picture tasks were also observed, though les frequently (4 in total), to assess students knowledge of domain specific teminologies. In such tasks, pictures relating to students disciplines are given and they are asked to label the parts, components, etc. (e.g. picture of a flower asking for test takers' knowledge of stigma, anther, sepal and filament).

# 4.3. EAP assessment in Iranian syllabi

The first and the most important observation was that most Iranian EAP syllabi $( 6 1 . 2 2 \% )$ were basically silent' on assessment; they contained nothing whatsoever about assessment. The syllabi, after providing some official information (e.g. the prerequisite courses, the number of credits the course cares, number of hours necessary for cassroom meetings, if the course is theoretical, practcal, or both, etc.), briefly laid out general goals and objectives of the course and some went further suggesting reading materials. Only $3 8 . 7 7 \%$ of the syllabi from the Ministry included a section addressing assessment.

The most important point about syllabi containing something' about asessment was that it was when' and how often' of giving tests that is of prime significance. More specifically, in almost all such syllabi $( 9 0 . 5 2 \% )$ , the HAS sections required teachers to have 'regular, as the course runs', 'midterm' and final' examinations. Technically speaking, Iranian teachers were required to give formative and summative tests. In few cases $( 2 6 . 3 1 \% )$ , the assessment guidance provided a grading system. For instance, EAP teachers of agricultural engineering were required to follow a Final $\mathrm { E x a m } = 5 0 \%$ Midterm $\mathrm { E x a m } = 3 5 \%$ Quizzes $= 1 5 \% ^ { \prime }$ grading policy. Finally, few syllabi (only $1 8 . 9 4 \%$ with a HAS section referred very briefly to alternative assessment techniques. The EAP syllabus for Natural Sciences, for instance, required teachers to use portflio asesment. The potential causes of the marginalized status of assessment are discussed later.

Table 5 provides information on syllabi without and with HAS sections in each academic discipline. For the latter, numbers under test time core allocation and test task coluns indicat the number of imes ach is reerred to in syllabi contents. For instace, in EAP syllabi of Basic Sciences (17 in total), there are 11 syllabi with the section How to Assess Students; in them, time of test administration (mid-term and final tests), score alocation, and suggested test tasks appear 10, 8, and 7 times, respectively.

As mentioned above, an understanding of Iranian EAP course objectives is relevant since in EAP programs not only ".. the content included in a course and the ways of assessng mastery of the content should be aligned with the learning objectives" (Charles & Pecorari, 2016, p. 170) but als large number of Iranian syabi lack a HAS section; leaving teachers alone with course objectives to study, interpret and plan for assessment. Table 6 presents the frequency of objectives in our corpus. The Total row shows that for n         y.

Table 5 Iranian EAP syllabi with and without HAS sections.   

<html><body><table><tr><td>Academic fields</td><td>Without HAS</td><td>With HAS</td><td>Test time</td><td> Score allocation</td><td>Test task</td></tr><tr><td>Agriculture and natural resources</td><td>2</td><td>11</td><td>8</td><td>7</td><td>1</td></tr><tr><td>Engineering</td><td>29</td><td>26</td><td>26</td><td>8</td><td>1</td></tr><tr><td> Marine studies</td><td>4</td><td>3</td><td>3</td><td>2</td><td>1</td></tr><tr><td>Hawza studies</td><td>27</td><td>5</td><td>5</td><td>0</td><td>0</td></tr><tr><td>Basic sciences</td><td>6</td><td>11</td><td>10</td><td>8</td><td>7</td></tr><tr><td>Human sciences</td><td>59</td><td>17</td><td>13</td><td>2</td><td>5</td></tr><tr><td> Social sciences</td><td>12</td><td>10</td><td>9</td><td>2</td><td>2</td></tr><tr><td>Veterinary sciences</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td></tr><tr><td> Military sciences</td><td>3</td><td>5</td><td>5</td><td>1</td><td>0</td></tr><tr><td>Art</td><td>8</td><td>6</td><td>6</td><td>3</td><td>1</td></tr><tr><td>Total</td><td>150 (61.22%)</td><td>95 (38.77%)</td><td>86 (90.52%)</td><td>25 (26.31%)</td><td>18 (18.94%)</td></tr></table></body></html>

Note: HAS $\mathop { : = }$ How to Assess Students.

Table 6 Frequencies of course objectives in syllabi.   

<html><body><table><tr><td></td><td>Speaking</td><td>Writing</td><td>Reading</td><td>Listening</td><td>Technical vocabulary</td><td>Translation</td></tr><tr><td>Social sciences</td><td>11.11%</td><td>0.11%</td><td>33%</td><td>0.11%</td><td>27.77%</td><td>22.66%</td></tr><tr><td>Human sciences</td><td>6.38%</td><td>4.25%</td><td>76.59%</td><td>0%</td><td>61.70%</td><td>12.76%</td></tr><tr><td>Basic sciences</td><td>0%</td><td>6.25%</td><td>40.62%</td><td>0.11%</td><td>40.62%</td><td>9.37%</td></tr><tr><td>Agriculture</td><td>8.82%</td><td>14.7%</td><td>17.64%</td><td>5.88%</td><td>23.52%</td><td>29.41%</td></tr><tr><td>Hawza studies</td><td>1.81%</td><td>0%</td><td>40%</td><td>0%</td><td>45.45%</td><td>12.72%</td></tr><tr><td>Engineering</td><td>12.38%</td><td>13.27%</td><td>19.46%</td><td>8.84%</td><td>35.39%</td><td>10.61%</td></tr><tr><td>Marine sciences</td><td>0%</td><td>13.33%</td><td>20%</td><td>6.66%</td><td>46.66%</td><td>13.33%</td></tr><tr><td>Art</td><td>2.85%</td><td>11.42%</td><td>31.42%</td><td>0%</td><td>37.14%</td><td>17.14%</td></tr><tr><td>Total</td><td>10.61%</td><td>12.65%</td><td>52.24%</td><td>6.12%</td><td>62.04%</td><td>22.85</td></tr></table></body></html>

Before discussing our findings, we should mention four points. First, and as far as learning objectives of EAP courses are concerned, Table 6 suggests that for Iranian syllabi designers three objectives make up the core of EAP programs: lerning' and getting familiar with field specific vocabulary knowledge (a bit over $6 2 \%$ , reading discipline-specific texts $( 5 2 \% )$ and translation competence (almost $2 3 \%$ . We believe teachers asessment practices and perceptions should be viewed with respect to such sequencing of learning objective. Second, the correspondence etween syllabi content and Iranian EAP teachers' asssment behavior is not a complete one. In other words, we found that some syllabi contents are not reflcted in EAP tests. For instance,communicative use of written and oral English (ESP Syllabus for the Undergraduate Students of Information Technology, p. 27) and listening to and writing academic reports and lectures (ESP syllabus for the Undergraduate Students of Materials Sciences, p. 151) have no reflections in EAP tets and teachers views. Sylas rs fr the stdnts feons xplicy stt that, . boe the fs ws onration f tch texts of economics'" while students of economics ".. need to learn the mechanisms of communication in their discourse community, get familiar with the key elements of academic papers, learn how to write paper abstracts, know how to participate in an international conference, .. (ESP Syllabus for the Undergraduate Students of Economics, p. 53). Such contents, though very rare in syllabi, are not reflected in teachers' standpoints and practices probably because their perceptions of the goals of EAP courses (i. getting familiar with content-specific terminologies and comprehension f field-specific texts) overshadow' syllabi contents. This is supported by the findings of Ghaedrahmat et al. (2017). In their study of Iranian EFL instructors and content specialist teachers' views on aviation English programs, they found that \*all [EAP] teachers believe teaching vocabulary is the basis of EAP (p. 234) and EAP tests should focus primarily on technical words and terms" (p. 235). As a result, even in few cases when communicative eficiency, academic writing abilit, oral use of English for aademic purpose, etc. reenshrined in syllai, such rhtoric finds no lace in test evelopment This is reminiscent of Borgs (2009) emphasis on the relationship between teacher cognition and classroom practice it is Iranian teachers cognition of what EAP is that filers into their assessment practice. Third, although it is usually comprehension' of subject-specific texts that is accented in Iranian sylabi, there is a tendency among subject specialist teachers, and to a lesser degree among EFL teachers, to associate comprehension' with translation' in testing. This is partially atributable to EAP students low English proficiency level. According to Mazdayasna and Tahririan (2008), most EAP practitioners, ".. were particularlydissatisfied with the low reading abilit of the students and indicated that they haveno other alternatives but to tranlate specialized English texts' (p. 280). Whether translation competence is what Iranian EAP students need is a controversial issue. No systematic needs analysis projects have been carried out by syllabi designers to support the inclusion of translation competence in Iranian sylai. For Anthony (2018), the use of translation in EAP classes (and, by implicatio, in item designing) is permitted if i serves the needs of students but the results of ranian researchers suggest that a set of newly evolving needs (e.g. academic writing and listening, paper publication, conference presentation, knowledge of cyber genres such as email and videoconferences, etc.) is more fundamental. Finally, some syllbi isted two or more skill or elements (.. oth reading and technical vocabulary as their course objectives. hat is why th total in Human Sciences, for example, does not add up to $1 0 0 \%$

# 5. Discussion

In responding to the research question, this study aimed at investigating Iranian EAP practitioners asessment perceptions and practices. We started with interviewing Iranian EAP teacher-testers coming from English and content departments followed by an examination of test samples. Our interview and corpus data analyses motivated us to examine all Iranian EAP syllabi designed by the MSRT.

We begin our discussion with the concern Schmit and Hamp-Lyons echoed in 2015: EAP teachers "'are often given tasks related to assesment that are beyond their sill set and there isacommon expectation in universities that an EAP teacher will know how to develop, administer and interpret anguage assessments (p. 6). We use our findings to argue that the concern is not only letimate but also multidimensional. First, we start with EAP construct dimension. For most discipline-area experts $( 8 0 \% )$ and a good number of language specialists $( 6 0 \% )$ EAP assessment is fundamentally a question of assessing field-specific technical jargons and comprehension of highly technical texts. Additionall, the primacy of carrier content over real content among teacher-testers becomes more evident when our participants' views about EAP vis- $\grave { \mathbf { a } }$ -vis EGP testing are considered: $9 0 \%$ of the first group and $8 0 \%$ of the second group believed thatit i disciline-scific knowledgethat distiushes the two. arrer content rr to field-cific ontnt knowledgelement of specificity in EAP whereas real content is the language used in the former. An EAP practitioner might use a table of statistics, for example, a the carrier content to teach the language of comparison, the rel content). Both issues stand against the current thoretical understandings in EAP in general. When distinguishing EAP, EMI and CLIL (Content and Language Integrated Learning), Airey (2016), for example, argues that EAP is primarily a language, and not a content, focused enterprise. Similarly, more than two decades ago, Dudley-Evans and St.John (1998) insisted that EAP teachers are not responsible to teach, and by implication to test, technical vo cabulary of the carrer content. We should mention that the primacy of field-specific content knowledge and knowledge of discipline-specific terminologies among EAP practitioners is not unique to our study; the same is reported by Salager-Meyer et al. (2016) in eight Latin American countries. Likewise, in Macianskiene and Bijeikiene's study (2018) of EAP in Lithuania, a similar emphasis on content is reported, which is likely to find its way to assessment Such EAP construct misinterpretations le some of our content teachers to design knowledge rather than language tests. The precedence of technical terms over other learning goals in Iranian syllabi indicates that the same misconception has been prevailing among syllabi designers too). We use our findings to argue that the construct dimension has a second layer: some content teachers' view and practice of separating language and content knowledge. This isalo inconsistent with the current EAP construct theorization emphasizing the indivisibilit of the two (Douglas, 2013). Although Iranian EAP students low general English proficiency should be considered as factor in shaping such an asesment perception and practice and despite the fact that \*EP is generally designed for intermediate or advanced students" (Dudley-Evans & St. John, 1998, p. 5), EAP can be used with beginners too. It remains to be seen and brought to light how EAP students' low English proficiencyreported by Evans and Gree (2007) in Hong Kong, for exampleaffects asessment i ther contexts. I should be noted that discrepancies between language and subject specialists over the same construct has been reported in content-focused class o. Llosa and Grapin (2019), for example, report that content teachers and ESL (English as a Second Language) teachers differ in their views and practices of asessing English learners' language proficiency in such contexts. This indicates that teachers' educational backgrounds exert a inluence over ther preptions of the construct (hat it is ad how it should e asesse In our study, one group used translation whereas the other employed reading comprehension tass to test the same trait. We believe how EAP teachers educational background relates to their asessment perception and practice is another hidden-away aspect of EAP testing that needs further investigations.

The second dimension of the concern, we argue, is the theoretical knowledge base of ssessment among the larger group of Iranian EAP practitioners (content specialists). They knew almost nothing about the basic principles and concepts of language asessment. Douglas (2013) arges that EAP asesment i held to the general measurement rinciples such as impact validity and reliabilit. Other scholars' (e.g. Sabieh, 2018; Weigle & Malone, 2016) elaboration of what skills and knowledge EAP test designing entails further affirms that EAP testing i a highly technical task for which professional training is necessary. This takes us to the need to address Language Assessment Literacy (LAL) of EAP practitioners i our context and beyond. The need to promote lnguage teachers LAL is a general educational oncern (Deygers & Malone, 2019) but we believe that EAP practitioners LAL deserves a special atention due to the extra professonal requirements shouldered on them when they teach and test EAP. Such requirements include an understanding of a new and different construct with thre features discussed above (Douglas, 2013), designing test tass representing TLU features (Knoch & Macqueen, 2019), adopting a needs-responsive assessment behavior, exploring the multiple literacies they are expected to teach (Ding & Campion, 2016), and by extension, to test, etc. Our emphasis on the need to address EAP practitioners' LAL is also supported by the disappointingly few studies in this arena: Canadian EAP teachers' low LAL (Huang, 2018),similar chllenges observed among Turkish EAP teachers (Celik, 2021) and those in the UK (Manning, 2014) leading some (e.g. Charles & Pecorari, 2016; Woodrow, 2018 to argue for the need to promote EAP teachers' LAL worldwide. We suggest in-service training programs, workshops or similar professional teacher development initiatives for such purposes. Of crucial significance is the need to raise teachers awareness about what EAP is and is not (Airey, 2016; Anthony, 2018), specific features ssociated with EAP language ability, and the latest arguments about the EAP construct (Douglas, 2013). Such programs can also help with their knowledge of asessment theory, technical kill neessary for itm constuctio, and principles and concets that arether important elements in crrent aroaches to LAL (Taylor, 2013). Aditionally, objective and iterature-informed professional criteria should be established to specify who qualifies as a teacher-tester.

For the second point in our discussion and reerring back to the justificationof this study, we use our interview data independent and detached assessment practices by subject and English teachers) to underscore the need for assessment collaboration between language and subject specialists ffering EAP. We endorsecollborative EAP assessment practice in our context due to the testinginformed technical consulation a good number of language teachers can provide subject teachers and the later group's role in helping with decisions over carrier content. We, of ourse, admit that not all AP teachers with backgrounds in Applied Linguistcs are necessarily asessment literate (Soodmand-Afshar & Ranjbar, 2021). We also believe that joint assessment projects between language testing specialists and local AP community are necessary and useful. To advocat our suggestion, we refer to the oint proect in which language assessment specialists from the Universt of Reading supported Bilkent Universty in Turkey to design and administer locally appropriate EAP tests (Schmitt & Hamp-Lyons, 2015). Similar cooperation programs can provide an opportunity for the cascading of professional expertise and knowledge to local EAP teacher-testers and, at the same time, create a channel by which localassessment features are processed for possible theorization. Of course, collaboration between subject and language teachers at the local level is feasible once more fundamenta isues are adresse: respect to each individual's disciplinary identit and expertise, espousing ateam approach regarding other components of EAP, and, probably more importantly, negotiating the ontological distances (the reality and nature of the phenomenon) between the two stakeholders (Barron, 2002).

Finall, as far as Iranian EAP syllabi are concerned, we believe that the syllabi as official documents need a thorough reappraisal. First, we found that EAP assessment is a fundamentall neglected domain in them while they are expected to specify the content and sequencing of language, genre, and skill for an individual course, in addition to information about materials and evaluation procedures"

(Anthony, 2018, p. 8, our emphasis). When EAP teachers are lft n their own devices and they are not provided with any asessment models, rubrics or guidelines, littl can be expected. Second, setting knowledge of tchnical teminologies, reading disciplin-specific technical texts and translation of such texs as the prime learning objectives for EAP programs indicates that Iranian EAP students voices have not been heard (Benesch, 2001) since, as noted before, students neds move beyond such a limited and linear equencing fashion and should be fairly incorporated in the yllabi, otherwise syllai designers accountability (Rea-Dickins & Germaine, 1992) to meet the end-users needs is questioned. Even though the common trend in different context is that EAP practitioners and educational centers design their own context-tailored syllabi, we believe that itis sill important to investigate how EAP assessment istreated in them.

# 6. Conclusion

Douglas (2013)argues for the recognition of EAP construct and ascribes three important features to it Accepting his argument adds to the professional responsibilities of EAP testers since they are expected to adhere to not only the basic principles of language assesment but also take into account the peculiaritie of the AP construct and its features. In our justification of the study, we noted that EAP practitioners' aessment behavior at classroom level is under-researched and the construct they test o belie they should test, nd how they do it and why, is under-defined. Drawing upon our findings and discussion of some other EAP setings, we conclude that EAP teacher-testers in some contexts (e.g.Iran and several Type Four EAP setings) tend to define EAP construct in terms of primarily content knowledge and asess their students accordingly. Nowadays, EAP scholars acet that it i difficult to distinguish language knowledge and background knowledge but emphasize that specific purpose testers are not in the businessof asessing \*how much of engineering .. people know" (Douglas, 2005, p. 850) and EAP is primarily a language-focused, and not a content-focused, undertaking (Airey, 2016; Anthony, 2018; Douglas, 2013). It is true that EAP language ability is inextricably intertwined with background knowledge and has jargonish tone, two (of three features of EAP language abilit discussed above, but neither content nor discipline-specificterminologies make up the main focus of EAP teaching and testing. Such EAP asessment behavior is also less likely to address  featre f sudets fieds, hee faig to achee ned-onsivnss which is the first aolute chractristic of EAP as defined by Dudley-Evans and St.John (1998). It remains to be seen whether such EAP construct (mis)interpretation by classroom teachers is prevailing in other contexts in which case we might think of two potential explanations: it i either because professional EAP construct theorization has not been eficientl filtered down to classroom teachers, the position we adopted and argued for EAP-wise LAL promotion of teacher-testers, or due to some EAP practitioners tendency in some contexts to develop their own EAP theories (in which case, we believe, they probably step into a non-EAP terriory). Our callfor more investigation is further justified on the ground that EAP courses are offeed by teachers coming from two backgrounds (Anthony, 2018). It is important to note that te focus onEAP teachers assssment pecetion and practices reflective of significant non-assessment isue . AP teaching, syllabi and curricula development). EAP teachers designing language tets of EAP are likely to behave similarly in their teaching activities while practitioners constructing primarily knowledge test are likely to focus on disciplinary content knowledge. Similarly, the prevalence of some asessment behavior (focus on translation and field-specific technical terms) might be reflective of learning objectives enshrined in EAP syllabi as learning objectives in EAP syllabi affect EAP practitioners teaching and testing (Charles & Pecorari, 2016).

In our justifications of the study, we also referred to the fact that not only literature on EAP asessment is essentill focused on proficiency tests with subsequent marginlization of other types of EAP tests but also is in need of information on all types of EAP situations. While literature on ESP/EAP assessment in Situation 1 (English-speaking countries where overseas students study in a foreign system) and Situation 2 (an ESL situation in which English is widely spoken and is the oficial language of education) is relatively rich, we know littleof the same issue in Situation 3 (where only certain subjects are taught in English) and Situation 4 in which subject courses are delivered in students L1 or the oficial/national language but English is an auxiliary language). Even if we focus on Situation 4 (our case) only, it covers Latin America, many countries in the Middle East, South East Asia, Easten Europe and Scandinavia, and mainland Western Europe (Dudley-Evans & St.John, 1998) where EAP practitioners design and use their own tests. Do they (both language and subjec pecialsts do EAP teting as reflected in professional literature or follow a different path? How do they do it and why? Studies from various settings are needed to see how EAP teacher-testers worldwide perceive and practice assesment which can eventually bring the hidden-away to light. Future studies focusing on different EAP students views of and expectations from EAP tests will acommodate the notion of students rights. Concerns over our ittleknowledge about EAP teaching have been echoed by the scholars. Our knowledge of classroom EAP testing, we believe, is even less.

# Author statement

Parisa Karimpour (first author): data collection (interviews, test tasks, EAP curricula and syllabi). Farhad Mazlum (second and corrsponding author): supervision, data interpretation, draft preparation, writing, reviewing, and editing.

# Funding

The researchers received no funding for this work.

# Acknowledgement

We express our sincere gratitude to Dr. Crosthwaite, the Associate Editor, and two anonymous reviewers of the Journal of English for Academic Purposes for their constructive comments and views on the manuscript. We also thank EAP teachers who accepted to take part in the study.

# Appendix A. Supplementary data

Supplementary data to this article can be found online at https:/doi.org/10.1016/j.jeap.2023.101321.

# References

Aire, . (2016). AP,  CL   Hyland &.  ., Th  hook f ngish r cc p . 71-83)  Yr e.   
Aldeon, J. (1)  s ation inP. e  .)   t  Pice n i Modern English Publications & The British Council.   
Anthony, L. (2018). Introducing English for specific purposes. New York: Routledge.   
Atai    n   iin. ESP Across Cultures, 10, 35-54. htp://edipuglia.it/wp-content/uploads/ESP%202013/Mahmood-Reza-Atai-Seyed-Asadollah-Asadi.pdf.   
Atai,   11      e   t 42(3), 305-323. https://doi.org/10.1177/0033688211419392   
Barron, C. (2002). Problem-solving and EAP: Themes and isues in a collaborative teaching venture. English for Specific Puposes 22, 297-314.   
Basturkman  r, . (206). he pctic f LSP. Avie . er s.) h hoo f le inusics . 672-694).fod: Blacel.   
Benesch, S. (2001). Critical English for academic purposes. Malwah, NJ: Lawrence Erlbaum Associates.   
Brg . (209    Rch  .),g    cr in . 1101). r Cambridge University Press.   
Braun  k  014c a. n er,  .) ti   i  95-17. London: Palgrave Macmillan. https://doi.org/10.1007/978-1-137-29105-9_7.   
Brfaut, . 014). g fr sic  r d e .   rel, 11(2) 216-225./.10.1080/ 15434303.2014.902060   
Cn. 5. r a   f  58 e Erlbaum Associates.   
Cei  2 n   i .   n i education (pp. 355-365). Ankara: Vizetek Publications.   
Chang, C.. 014) cr s a rcteat E t hmh sty. The  of h ish o ic cadic Purposes, 2(4), 625-645.   
Chare,  ri,  206    P.   . t .), nis    . 169-182) London & New York: Routledge. https://doi.org/10.4324/9781315682129-13.   
Criper, . (181.ithe  pa (2)  Jo  .)   ti . 120  e ih .   
Deygrs,   019 t t i  3 3-38/ doi.org/10.1177/0265532219826390   
Ding  amon, . 016) AP tcer met n P h  ylnd (d.), The  hdook of Eish fo c pe (p. 547-599) (Abingdon: Routledge).   
Dornyei, Z. (2007). Research methods in applied linguistics. Oxford: Oxford University Press.   
Dougls, . (200sg ges fr spi e. mbridge, K: mbrige ersit re. hp/i.g/10.1017/0978051173291   
Douglas, D.2001. a r si r criti  thy e r ting 182, 171-185. /.0.1177 026553220101800204   
oglas,  25. o  .  . f    78 NJ: Lawrence Erlbaum Associates.   
Douglas, D. (2010). This won't hurt a bit: Assessing English for nursing. Taiwan International ESP Journal, 2(2), 1-16.   
Dglas, .2013)P a    .)  ok   for   . 367-84.str ey Blackwell. https://doi.org/10.1002/9781118339855.ch19.   
Dudley-Evans, A., & St John, M. J. (1998). Developments in ESP: A multidisciplinary approach. Cambridge: Cambridge University Pres.   
Emery, H. (2014). Developments in LSP testing 30 years on? The case of aviation English. Language Asessment Quarterl, 11(2, 198-215.   
ESP Sylbus for the Undergraduate Students of Economics, Iranian Ministry of Science, Research, and Technology, Tehran, Iran.   
ESP Syllabus for the Undergraduate Students of Infomation Technology, Iranian Minstry of Sience, Reearch, and Technology. Tehran, Iran.   
ESP syllaus for the Undergraduate Students of Materials Sciences. Iranian Ministry of Science, Research, and Technology. Tehran, Iran.   
Evans, . & Green, C. (2007). Why EAP i neessry: A survey of Hong Kong tetiary student. Jounal of English for Acadeic Purposes 6, 317.   
Flowerdw  013).  as ad ic men  P.  B  t .), Th hdok f ish o i es p. 325-346). Oxford: Blackwell. https://doi.org/10.1002/9781118339855.ch17.   
ha 7 t of Foreign Language Research, 8(1), 207-246.   
Huang, . 018 lorcrtiae P rthe ionr tie i d  f s r c e 35,7084   
Hth ,  (. for    m rt P.//10.1017 CBO9780511733031 Research on English Language, 7(2), 171-194.   
Knoch, U., & Macqueen, S. (2019). Assessing English for professional purposes. London & New York: Routledge.   
Lsa     e language proficiency in the content clsroom. Atlanta, GA: LTRC). Paper presented at the Language Testing Research Collquium. series (pp. 135-147). Alexandria, VA: TESOL.   
Manning, A. (2014). Investigating EAP assessment literacy. Paper presented at the ALTE conference, Paris.   
ay  t   .  i for Academic Purposes, 7, 277-289. https://doi.org/10.1016/j.jeap.2008.10.008   
Mazm,  (202 ish the wd ia   th g f the d g r n  aak  . ng Policy, 21(2), 261-290.   
Olaofe, I. (1994). Testig English for acadmic purposes (EAP) in higher education. Assesment & Evaluation in Higher Edcation, 191), 37-48.   
O'Leary, Z. (2014). The essential guide to doing your research project. London: Sage.   
OSullivan, . (2012. smt ie in ang fr sific puro. Th Md ag Jo, 96, 71-8. s/do.org/10.111/j.1540 4781.2012.01298.x   
OSulia ,  Wir,  . (01 t   io niv .  t r ti . 32. ne, Palgrave Macmillan.   
Rea-Dickins, P., & Germaine, K. (1992). Evaluation. Oxford: Oxford University Press.   
bh .  r    1   . https://doi.org/10.1002/9781118784235.eelt0378.   
Salagr-Meyr,  e ra, L , . 2016). AP nrica d, . h .), Th  ho o gs r ad purposes (pp. 109-124). London & New York: Routledge.   
Savin-Baden, M., & Major, C. H. (2013). Qualitative research: The essentidl guide to theory and practice. London: Routledge.   
Schit,  m-  015Th o P c     Es oAc e, 18./o./10.1016 j.jeap.2015.04.003   
Sooman-sr  gh  2016.  i   t po      of is for e, 22, 132-151. https://doi.org/10.1016/j.jeap.2016.04.002   
Sodma  , 1)       io 7 474.. org/10.1016/j.stueduc.2021.101042 org/10.4324/9781315676203-14.   
Taylr  013.tin th thy, rticed prc of  tig to s stak oe rectio.  estg 0, 403-412. https://doi.org/10.1177/0265532213480338   
nd     is Educational Research Journal, 36(2), 299-316. https://doi.org/10.1080/01411920902919257   
Wegle, . e 201sis r a  .),   of h fo  e (pp. 608-620). London, UK: Routledge.   
Woodrow, L. (2018). Introducing course design in English for specific purposes. Abingdon, New York: Routledge.   
Ye, Y. (2020 P f  cie d gg s n an  c t sud w d, 7 111/./0.06/. amper.2020.100065