# The relative significance of syntactic knowledge and vocabulary breadth in the prediction of reading comprehension test performance

Toshihiko Shiotsu Kurume University Japan and Cyril J. Weir University of Bedfordshire UK

In the componential approach to modelling reading ability, a number of contributory factors have been empirically validated. However, research on their relative contribution to explaining performance on second language reading tests is limited. Furthermore, the contribution of knowledge of syntax has been largely ignored in comparison with the attention focused on vocabulary. This study examines the relative contribution of knowledge of syntax and knowledge of vocabulary to L2 reading in two pilot studies in different contexts – a heterogeneous population studying at the tertiary level in the UK and a homogenous undergraduate group in Japan – followed by a larger main study, again involving a homogeneous Japanese undergraduate population. In contrast with previous findings in the literature, all three studies offer support for the relative superiority of syntactic knowledge over vocabulary knowledge in predicting performance on a text reading comprehension test. A case is made for the robustness of structural equation modelling compared to conventional regression in accounting for the differential reliabilities of scores on the measures employed.

# I Background

Among the different approaches to research in reading, componential approaches are concerned with identifying possible explanatory skill factors or components involved in the reading process, as opposed to explaining how those components operate in the process. Hoover and Tunmer (1993) view this approach essentially as an attempt to model reading ability rather than the reading process itself by taking a view of reading as a “set of theoretically distinct and empirically isolable constituents (4)”. Carr and Levy (1990: xi) see value in such an approach: “Many investigators believe that the kind of full characterization that results from component skills analysis is the only way to get an accurate picture of reading ability, how it changes developmentally, and what creates individual differences among readers. . ..”

The literature on componential analyses of L1 reading suggests individual differences in reading ability may be accounted for by such factors as vocabulary knowledge, word recognition skills, phonological awareness, and working memory span (Baddeley et al., 1985; Cunningham et al., 1990; Jackson and McClelland, 1979; Palmer et al., 1985). Following this, L2 research has also begun to explore these factors (Koda, 1998; Harrington and Sawyer, 1992; Laufer, 1992; Segalowitz et al., 1991), although the simultaneous comparison of multiple factors adopted by L1 research is rarely utilised in the L2 context (see, however, Haynes and Carr, 1990; Shiotsu, 2003; van Gelderen et al., 2004). Further L1 and L2 research adopting such multi-skills analyses is necessary.

In addition, in L2 research, consideration needs to be given to the neglected skill factor of syntactic knowledge in such comparisons in adult L2 reading. Early support for this is found in such publications as Berman (1984), and later in Urquhart and Weir (1998: 269), where they note: “Grammar is a component of reading that has been almost ignored in the research. It seems to us that this is an interesting and potentially valuable research area which L2 teachers and applied linguists are in a good position to investigate.” More recently, in his book on reading assessment, Alderson (2000: 37) refers to “the importance of knowledge of particular syntactic structures, or the ability to process them, to some aspects of second language reading” and states that “[t]he ability to parse sentences into their correct syntactic structure appears to be an important element in understanding text”.

The limited literature on the significance of grammatical or syntactic knowledge for L2 reading is inconclusive. Ulijn (1981; 1984) claims on the basis of his research that L2 reading requires little syntactic processing but much lexical-conceptual processing. On the other hand, in his work on the English Language Testing Service (ELTS) Revision Project, Alderson (1993) found high correlations between a grammar test and tests of academic reading. Similarly, qualitative research by Bernhardt (1991; 2000) suggests that syntax plays a significant role in L2 reading ability. This is in marked contrast to the findings of some quantitative studies in the area that we discuss below, and such incompatibility needs addressing.

Empirical work focusing on the relative significance of L1 reading ability and L2 knowledge/proficiency on L2 reading ability (Alderson, 1984) is relevant as it has generally been shown that L2 reading ability is better predicted by the learners’ L2 knowledge/ proficiency level than their L1 reading ability at least until a certain level of L2 proficiency is developed (see Bernhardt, 1999; 2000, Bernhardt and Kamil, 1995 and Yamashita, 1999 for reviews in this area). Given a consistently larger effect found for L2 knowledge/ proficiency variables than for L1 reading, a dedicated investigation of the former might prove illuminative. Furthermore, the large amount of L2 reading variance unaccounted for by the usual pairing of one L2 knowledge/proficiency independent variable and one L1 reading comprehension independent variable argues that closer definition of the independent variables in such investigations is worthwhile. As the contribution (albeit limited) of L1 reading is already well established and the nature of the relationship well covered in the L2 reading literature (see, e.g., Alderson, 2000; Grabe and Stoller, 2002; Lee and Schallert, 1997; Upton and Lee-Thompson, 2001; Urquhart and Weir, 1998; Yamashita, 1999), we confined ourselves in this particular study to investigating whether greater delicacy in operationalising L2 knowledge/proficiency variables would lead to an increase in the amount of L2 reading variance explained.

Alderson (1993) produced evidence of a strong connection between grammar and reading. During the development of the IELTS test, it was found that there were very high correlations between the grammar test and different tests of reading.1 In fact, Alderson found that grammar correlated more strongly with different reading measures than these different reading measures did among themselves. However, studies in this area are problematic. Alderson (1993) observed that “the results, then, appear to show that a (vaguely defined) generalised grammatical ability is an important component in reading in a foreign language” (p. 218). But, as he himself admits, he was unable to avoid a degree of “contamination” of the grammar variable, since his grammar measure involved the processing of sentence semantics, e.g. the referential and sense meanings of lexical items. Since reading involves the extraction and reconstruction of meaning (including the processing of appropriate forms in meaningful contexts), it is clear that any grammar test with a heavy emphasis on meaning is likely to overlap with reading tests. Thus, such studies, where the grammar items are based on functional or semantic grammars, are most likely confounded by the contamination pointed out by Alderson and open to the criticism that the degree of shared variance is likely to have been enhanced by correlating like with like.

Of more interest is the degree to which knowledge of formal syntax is of help in any particular type of reading. A test of grammar used in such research must as far as possible be just that. Urquhart and Weir (1998) describe how Alderson’s test consisted of six sub-sections: (i) vocabulary; (ii) morphology; (iii) prepositions, pronouns, etc. along with rather vaguely termed “lexical sets”; (iv) verb forms, etc.; (v) transformations; and (vi) “reference and cohesion”. In Urquhart and Weir’s more formal restricted definition of “grammar”, sub-section (i) must be eliminated; the “lexical sets” of section (iii) are doubtful. As far as section (vi) is concerned, “reference” is normally treated as a form of cohesion. If cohesion involves relationships outside the sentence, it will not be classed as syntactic, while identifying cohesive links between pronominal forms and their referents has more to do with pragmatics than syntax.

Alderson (1993) makes clear that in further research the grammar and reading tests should be as separate from each other as possible. Since most grammar tests involve the students in processing written language, this is not an easy task. Urquhart and Weir (1998) suggest a number of steps we can take to reduce the resemblance. Whereas most definitions of syntax treat it as being sentence-bound, comparatively few written texts used to test reading are similarly limited in scope. Therefore we can achieve some measure of difference by basing our grammar test on decontextualised sentences or phrases. This will tend to rule out using continuous text, as in the cloze procedure, or the gap-filling of continuous text recommended by Heaton (1988) and used by Alderson (1993).

In response to Bernhardt’s (1999) emphasis on the contribution of grammatical knowledge to reading performance, Alderson (1999) cautions against underestimating the role of lexical competence in reading. Accordingly we need to look for empirical evidence on the relative contributions of grammar and vocabulary, though it is at best rather limited.

Bernhardt (1999) and Alderson (1999) identify Brisbois (1995) as the only linguistic threshold study that separated grammar and vocabulary as independent predictor variables. Brisbois (ibid) has been reviewed as “the most sophisticated of the studies” (Bernhardt, ibid) on the linguistic threshold for L1-L2 reading ability transfer. After such a remark, one may be tempted to accept Brisbois’s claim based on her analysis that her vocabulary measure correlated more strongly with her reading comprehension measure than did her grammar one. On closer inspection, however, there are several elements in Brisbois’s work that complicate its interpretation, and even mislead its readers. Firstly, there is the issue of the timing of her data collection. Brisbois reported that her learners took the grammar test two months prior to the collection of their criterion reading comprehension data, which in itself is problematic. By contrast, they took the vocabulary test just two weeks before the reading comprehension measure. It is difficult to judge the effects of such a time difference in this type of cross-sectional research, but it certainly discourages us from accepting her claims at face value.

Another source of concern is the difference in the methods of testing grammar and vocabulary knowledge. For grammar knowledge, Brisbois adopted a test consisting of “multiple-choice and cloze items” (p. 572). It thus seems that she sought a general measure of breadth of grammar knowledge. Her vocabulary test, on the other hand, consisted of 50 words selected from the test passages for the reading comprehension measure but not yet studied in the curriculum. Thus, there was a lack of consistency here: her grammar test was a general measure, whereas the vocabulary test focused on specific linguistic elements of the target reading passage. These are some of the aspects of her methodology which may well have given some advantage to the vocabulary variable as a predictor of reading comprehension performance.

Additionally, a part of Brisbois’s (1995) commentary on her regression results is misleading. Since she always chose to enter L2 grammar subsequent to L1 reading and L2 vocabulary in her hierarchical regression models, and since the two L2 variables had a substantial intercorrelation ( $r = . 5 7 \AA$ ), even when grammar added significantly to the total reading variance accounted for, its associated $\mathbf { R } ^ { 2 }$ increment value was obviously smaller than that for vocabulary. Though Brisbois discusses this potentially confusing aspect of hierarchical regression analysis, she does not do so in the context of comparing grammar and vocabulary. She simply stated that “the variance of the L2 vocabulary scores contributed more to that of the L2 reading comprehension scores than did the variance in the L2 grammar scores” (pp. 576–7), which is not surprising given the correlation pattern. However, that grammar made a significant independent contribution to the amount of total reading variance above and beyond that explained by vocabulary was never underscored. Overall, the role of vocabulary appears somewhat overstated while that of grammar understated.

In short, Brisbois’s data and her description of them should not be accepted as clear evidence of the relative significance of vocabulary over syntax.

There are few other published studies involving separate measures of syntax, vocabulary, and reading skills in L2. Barnett’s (1986) data led her to conclude that both syntactic and vocabulary knowledge affect reading comprehension, as increases in the levels of syntactic and vocabulary knowledge of her students seemed almost symmetrical in their effects on reading recall performance. Haynes and Carr (1990) found their students’ reading comprehension performance correlating better with vocabulary than grammar but the students’ reading speed showed the reverse pattern. In his study of the reading performance of L2 learners of Dutch, Bossers’s data (1992) indicated that vocabulary and grammar were both significant predictors, with vocabulary achieving a slightly stronger prediction. On the other hand, Yamashita (1999) claimed on the basis of her regression results that the contribution of vocabulary was much larger than that of grammar.

Apart from Barnett’s study (1986), which employed ANOVA, the other studies providing indices of the contributions of grammar and vocabulary to reading are based on conventional multiple regression analysis using observed variables as both the predictors and the criterion. A recent study by van Gelderen et al. (2004) also included separate vocabulary and grammar measures as predictors of reading ability but adopted a more sophisticated approach, structural equation modelling (SEM; see Kunnan, 1998 for discussion of SEM in language testing research).

SEM, also called Covariance Structure Analysis, allows the researcher to evaluate regression models that take latent ability variables as the predictors and criterion, and one of its many advantages over conventional regression is that it takes into account and partials out the differences in measurement errors across the observed variables. It is well known that correlation coefficients are affected by the reliabilities of the measures correlated (Hatch and Lazaraton, 1991), and in conventional regression analysis based on observed scores, the independent variable with more items and/or greater spread in scores, for instance, will be advantaged as a predictor.

These extraneous factors were not taken into account in the studies above (Haynes and Carr, 1990; Bossers, 1992; Yamashita, 1999). It is notable that, despite Yamashita’s conclusion, all three of these studies showed neither predictor to be clearly superior. Given the similar regression and correlation coefficients obtained for grammar and vocabulary in these studies, there must be some concern that the discrepant reliabilities of the predictor measures could have unduly influenced the outcomes in favour of vocabulary. The use of SEM would give us more confidence in the interpretation of the results. It is of particular interest that the SEM results of van Gelderen et al. (2003; 2004) have shown that grammar is as important as, or even superior to, vocabulary in predicting reading performance.

The literature on the relative contribution of the grammar and vocabulary knowledge to reading performance is too limited to offer convincing evidence for supporting one or the other of the two predictors and would benefit significantly from further empirical research, preferably based on much larger samples than 50 (as in Bossers, 1992) or 60 (Haynes and Carr, 1990) and employing more sophisticated approaches such as SEM.

# Operationalising constructs

One of the central questions we address in this paper is the degree of overlap between performance on a test of formal syntax and on a text reading comprehension test. Urquhart and Weir (1998) argue that the more text-focused or “communicative” a grammar model is, the closer grammar tests based on this model are likely to be to tests of reading skills. There would be little point in correlating such tests with reading tests; little would be revealed from a significant overlap between the scores on a reading comprehension test and those on a syntax test that focused on the reading comprehension of the test sentences.

As Urquhart and Weir (1998) and Alderson (1993) point out, in testing grammar/syntax we should make our instruments reflect as closely as possible that construct alone. The overlap in the behaviours sampled in what is meant to be a test of syntax with those in the test of reading must be minimised. Such a requirement is not easy to meet, since most measures of L2 syntactic knowledge would involve the processing of visually presented text, which may in itself be judged as a kind of reading. Also, the more meaning extraction is integrated in the task and the more contextualised the task becomes, the more it seems to include the characteristics of reading. It appears then that to achieve independence from a test of reading, a test of syntactic knowledge should attempt to reduce the need for semantic processing as far as possible and keep contextualisation to a minimum.

Following Urquhart and Weir we shall use “grammar” in the traditional sense, to refer either to syntax, or to syntax and grammatical morphology, what Quirk et al. (1972) refer to as “morphosyntax”. Sampson (1975: 38) defines syntax as “. . . how words are put together to form sentences”. For Richards et al. (1992) grammar is “the way in which linguistic units such as words and phrases are combined to produce sentences in the language” (p. 161). Grammatical or syntactic knowledge would seem to cover largely the same ground according to these authors and is primarily concerned with the well-formedness (or ill-formedness) of a sentence or subparts of a sentence such as a clause or a phrase. In this paper, the terms grammar and syntax are used interchangeably.

Horrocks (1987) and Crystal (1997) seem to attach more importance to meaning than do the authors cited so far. According to Horrocks (ibid): “Syntax is concerned with the principles according to which words can be combined to form larger meaningful units, and by which larger units can be combined to form sentences” (p. 24). For Crystal (ibid) syntax is “the way in which words are arranged to show relationships of meaning within (and sometimes between) sentences” (p. 94). So, even in these brief definitions, there appear to be some significant divergences. In his recent work, Purpura (2004) goes even further in defining grammatical knowledge, so that it includes knowledge of phonological, lexical and cohesive forms, along with their meanings. Urquhart and Weir (1998: 259) remark: “We still are faced with the problem of deciding on the scope of syntax, i.e. what comes under ‘syntax’ and is therefore to be included in our research, and what is outside, . . . where to place the boundary between syntax and semantics . . .”. Their pragmatic advice for those researching the contribution of a knowledge of syntax to reading is to take a “formal”, “structuralist” model, with as little recourse to “meaning” or “communicative value” as possible; one which sets out to describe the permissible (grammatical) sequences of words or formatives in the sentences of whichever language we are concerned with.

In order for our measure of syntactic knowledge to consist as far as possible of items where the answers are decided fundamentally by formal as against semantic criteria, we conducted a preliminary item content analysis, to be detailed in the methods sections, and sought to operationally define our syntactic knowledge variable. Similarly, in the case of vocabulary, we need to focus on an aspect of lexical competence that can be manifested through tasks requiring minimal contextual text comprehension. While this is not always easy to achieve, our main study adopted a test format requiring only the matching of word and phrase-level meaning.

# II General method of analysis

To address the relative significance of syntax and vocabulary knowledge in the prediction of text reading comprehension performance, we conducted three successive studies of increasing rigor and scale, employing separate measures of syntactic knowledge, vocabulary knowledge, and text reading comprehension to elicit data from L2 readers of English. Of the three studies, we treat the first two as our preliminary studies and the third as the main one. All three studies adopted measures of syntactic knowledge that underwent rigorous content analysis by language experts, whose procedures will be presented in the description of each study.

As the primary method of analysis, all our studies employed structural equation modelling (SEM) using the software AMOS (Arbuckle and Wothke, 1999). AMOS takes the covariance matrix for analysis, and for the estimation of parameters, our studies were based on the Maximum Likelihood method. In SEM each latent variable must be based on at least two observed indicator variables, and since only one test was available for each of the three ability areas of interest, we adopted the practical solution of splitting each test into parallel halves for the purposes of the analysis (cf. Schoonen et al., 1998; van Gelderen et al., 2004). The distribution of the data for each of the three studies was examined on the basis of Mardia’s (1970) estimate of multivariate kurtosis and was found to be within the acceptable range of multivariate normality after removing a small number of outliers.2 The acceptability of the models was judged on the basis of the following indices of fit: the chi-square test $( \chi ^ { 2 } )$ , chisquare per degree of freedom $( \chi ^ { 2 } / \mathrm { d f } )$ , the goodness of fit index (GFI), the non-normed fit index (NNFI), the comparative fit index (CFI), and the root mean square error of approximation (RMSEA). For a model to be judged acceptable, $\chi ^ { 2 }$ should be non-significant, $\chi ^ { 2 } / \mathrm { d f }$ less than 2, GFI, NNFI and CFI in the upper .90’s in each case, and RMSEA less than .05 (Raykov and Marcoulides, 2000).

![](img/7543c97398816a44918cdb2c118eb0d9f4fef427d6546d0000051e746b6eac1b.jpg)  
Figure 1 Graphic representation of the model tested3

Our general model is graphically presented in Figure 1, where the latent text reading variable is regressed on the two latent predictor knowledge variables. We included for comparison two alternative models in which one of the two structural regression paths from the two predictors of syntax (ws) and vocabulary (wv) was constrained as carrying a zero regression weight. The $\chi ^ { 2 }$ difference test was employed to evaluate any difference in fit between each of these more constrained models and our basic model, in which both weights were freely estimated. If the difference in $\chi ^ { 2 }$ is found statistically significant, the model with poorer fit should be rejected in favour of the other one. If constraining that regression weight for a predictor at zero does not lead to deterioration of fit, that predictor is thought to add little to the prediction beyond what is already accounted for by the other predictor (see, e.g., Schoonen et al., 1998). Upon selection of the model that best described our data, we also obtained values on our freely estimated parameters (regression weight, correlation, and percentage of criterion variance explained) for that model.

In our main study, which was based on the largest number of students, we also conducted a post-hoc sub-group analysis to test the invariance of the chosen model across higher and lower achievers in terms of reading ability within the sample. Its details are discussed separately in section V.3 below.

# III Study 1

# 1 Participants

For Study 1, we obtained data from an administration of the reading component of an end-of-course test delivered on a pre-sessional EAP programme at a UK university, as well as a test of syntactic knowledge administered for the present study to a subset of this population ${ \bf \dot { \chi } } _ { n } = 1 0 7 { \bf \dot { \chi } } _ { \bf \dot { \chi } }$ ). The analyses were conducted by extracting the data relevant for our research purposes. The participants were from a wide variety of L1 and nationalities, and the majority aimed to commence a postgraduate or undergraduate degree course at the UK university by performing successfully on the exit test.

# 2 Instruments

Text Reading Comprehension $\mathbf { k } = 2 0 ,$ ): The criterion text reading comprehension measure was a composite score on the global comprehension parts of a reading test battery administered to the present sample as an end-of-course test (Green, 2000) but originally compiled for the university level EAP population in the People’s Republic of China (cf. The Advanced English Reading Test (AERT) Project, reported in Weir et al., 2000).

The measure consisted of six separately-timed subsections based on a total of four academic passages ranging in length from 600 to 1000 words, and it employed short answer, true-false with justification, and table/flowchart or sentence completion tasks $\bf \ddot { X } = 1 1 . 6 5$ , $\mathrm { S D } = 4 . 1 9$ , alpha $= . 7 9$ ).

Knowledge of Vocabulary $\mathbf { k } = 1 0 \mathbf { \Omega } _ { , }$ ): As an index of the participants’ vocabulary knowledge, this study drew on another part of the reading test battery above (Weir et al., 2000) that targeted understanding of contextualised meaning of academic words through a task requiring them to find words from a given word bank that appropriately filled blanks embedded in two different academic passages of approximately 500 words long each $\overline { { \mathbf { X } } } = 4 . 1 2$ , $\mathrm { S D } = 2 . 3 1$ , alpha $= . 6 4 \AA$ ).

Knowledge of Syntax $\mathbf { k } = 3 0 ,$ ): Each item in the syntactic knowledge test presented a sentence with one part replaced by a blank. There were four response options, which had similar semantic content but only one satisfied the syntactic constraints imposed by the structure of the rest of the sentence ( $\overline { { \mathbf { X } } } = 2 0 . 9 0$ , $\mathrm { S D } = 5 . 3 0 $ , alpha $= . 8 3$ ).

This measure had undergone a preliminary validation study. To operationalise the syntax variable for Study 1, we first considered the grammar paper included in the development of the Test of English for Educational Purposes (TEEP) by Weir (1983: 371–3), since it had been designed for, and empirically trialled (alpha $= . 9 2$ ) on, a large sample ${ \mathrm { ~  ~ n ~ } } = 4 6 6 { \mathrm { ~  ~ \Omega ~ } }$ ) of our intended population, and many of the 60 items seemed to meet the suggested requirements of being decontextualised and focusing more on awareness of acceptable sentence constructions and less on sentence semantics.

Given our desire to minimise the amount of reading for meaning in the syntax test and as far as possible to limit the focus to syntactic knowledge, we went through a number of stages in eliminating mainly semantically based items from the original TEEP grammar test. We first gave the test to a group of ten experts in applied linguistics at a UK university and asked them to screen out any items where the focus was not clearly on syntactic knowledge (i.e., not resolved by formal criteria, but semantically). The actual questions to the judges were as follows:

“Which of 1 and 2 below do you think is more relevant to answering each of the 60 questions? If you are not sure, please choose 3.”

1) Ability to identify the acceptable sentence structure in terms of syntax.   
2) Ability to grasp the semantic meaning of the sentence.   
3) Not sure.

We next gave a reduced set of items to a group of 20 teachers of ESOL and asked them to make similar judgements. After careful scrutiny of this feedback, the number of items which met our criteria for inclusion in a more formal test of syntax was reduced to 30.

# 3 Results

The analysis started by comparing the following three models, representing different hypotheses. In Model 1, the regression weight on neither syntax nor vocabulary knowledge was constrained, and so the two predictors were hypothesised to make a significant contribution to the prediction of text reading comprehension performance. Model 2 specified that the regression weight on the syntax variable was zero, hypothesising that this variable made no additional contribution to the prediction after vocabulary had accounted for a significant amount of text reading variance. In Model 3 vocabulary was constrained by a zero regression weight, and thus only syntax was hypothesised to make a significant unique contribution to the explanation of the text reading variance. In all three models, the latent syntax and vocabulary variables were assumed to correlate significantly with each other.

Table 1 lists the fit statistics, from which it is clear that Model 1 offers an acceptable fit and the best fit among the three, as the $\chi ^ { 2 }$ and all of the other fit indices support it over the other two (see Hu and Bentler, 1995 for details on the interpretation of fit indices). We thus proceeded to compare the parameter estimates of interest, regression weights on syntax and vocabulary, based on Model 1.

The beta values in Table 2 show that syntax accounts for text reading performance slightly better than does vocabulary (beta $= . 4 7 ^ { * }$ vs . $. 4 2 ^ { * }$ ). Correlations of the predictors with the criterion reading variable were moderately high $( r = . 6 2 ^ { * }$ and . $6 0 ^ { * } )$ . Syntax and vocabulary jointly accounted for $55 \%$ of the total reading variance, of which $38 \%$ could be explained by syntax alone, leaving $17 \%$ as the size of unique contribution made by vocabulary. Since vocabulary singly accounted for $36 \%$ of the reading variance, the size of unique contribution syntax made to the prediction was $19 \%$ . As shown in the far right column, the two latent predictor variables correlated moderately with each other ${ \mathit { r } } = { \mathit { 3 7 } } ^ { * }$ ).

Table 1 Estimates of model-to-data fit: Study 1 $( n = 1 0 7 )$   

<html><body><table><tr><td>Model (constraint on ws/wv)</td><td>x2</td><td>df</td><td>x2/df</td><td>GFI NNFI</td><td></td><td>CFI</td><td> RMSEA</td><td>difference (from Model 1)</td><td>df</td></tr><tr><td>1(None)</td><td>4.821</td><td>6</td><td>.803</td><td>.985 1.011</td><td></td><td>1.000</td><td>.000</td><td>-</td><td>-</td></tr><tr><td>2 (ws = 0)</td><td>25.065*</td><td>7</td><td>3.581</td><td>.925</td><td>.854</td><td>.932</td><td>.156</td><td>20.244*</td><td>1</td></tr><tr><td>3 (wv = 0)</td><td>19.024*</td><td>7</td><td>2.718</td><td></td><td>.903</td><td>.955</td><td>.127</td><td>14.203*</td><td>1</td></tr></table></body></html>

Note: $\chi ^ { 2 } =$ chi-square, df $=$ degree of freedom, GFI $=$ goodness of fit index, NNFI $=$ non-normed fit index, CFI $=$ comparative fit index, RMSEA $=$ root mean square error of approximation; $^ { * } p < . 0 5$ .

Table 2 Regression and correlation summary: Study 1 $( n = 1 0 7 )$   

<html><body><table><tr><td></td><td colspan="2">Reading x</td><td>Syntax x</td></tr><tr><td></td><td>Syntax</td><td>Vocabulary</td><td>Vocabularye</td></tr><tr><td>Beta</td><td>.47*</td><td>.42*</td><td>-</td></tr><tr><td>r</td><td>.62*</td><td>.60*</td><td>.37*</td></tr><tr><td>% explained</td><td>38%</td><td>36%</td><td>14%</td></tr><tr><td>% jointly explained</td><td colspan="2">55%</td><td>-</td></tr></table></body></html>

Note: $^ { * } p < . 0 5$

The results of our first preliminary study have thus provided us with empirical data to support both syntax and vocabulary knowledge as important predictors of text reading comprehension performance. Between the two, syntax appeared to contribute slightly more to the prediction of the text reading performance than did vocabulary.

# IV Study 2

# 1 Participants

A total of $1 8 2 \mathrm { L } 1$ -Japanese EFL learners at three different universities in Japan responded to items in the following measurement instruments.

# 2 Instruments

Text Reading Comprehension $\mathbf { k } = 2 0 ,$ ): The criterion text reading comprehension measure for Study 2 was a test consisting of four passages, each followed by five questions offering four answers to choose from. Two passages were taken from the College English Test (CET) reading test (Yang and Weir, 1998) developed for an EAP population, and the other two from the Lee and Schallert (1997) test, prepared for an EFL population, both in East Asia. The items following each passage were such that they required the synthesis of information across several sentences and a correct answer could not be identified by simply locating or understanding a particular word or phrase. The passages were approximately 80 to 260 words long. $\overbar { X } = 1 0 . 0 7$ , $\mathrm { S D } = 4 . 3 8 $ , alpha $= . 8 0$ ).

Knowledge of Vocabulary $k = 6 0 \AA$ ): As an index of the breadth of the students’ English vocabulary knowledge, Study 2 adopted a variant of the Vocabulary Levels Test (VLT; see Nation, 1990 and Schmitt et al., 2001). VLT is made up of a number of item clusters, each of which contains three brief target words/phrases along with six word choices, and the candidate has to match each target item with the corresponding word. Since it tests what may be termed “sight vocabulary”, in terms of knowledge of the form and common meaning of the word without contextual clues, it is assumed to involve different operations from those required for semantic processing in more contextualised sentence reading comprehension (cf. Study 1 above).

Schmitt et al. (2001) developed their version of a VLT based on extensive empirical research. Prior to the present study, their prototype version was pre-tested with a sample of Japanese EFL learners in order to obtain item data and create a smaller set of items that would form a vocabulary breadth test with sufficiently high reliability when administered to Japanese university students (see Shiotsu, 2003 for details). It was this pruned version of VLT that was adopted for the present study ${ \overline { { X } } } = { \bar { 2 } } 4 . 1 4$ , $\mathrm { S D } = 1 1 . 5 0$ , alpha $= . 9 4$ ).

Knowledge of Syntax $k = 3 2 ,$ : The test of syntactic knowledge for Study 2 had the same item format as the one employed in Study 1, although the actual item set was renewed and they underwent a separate content analysis to be described below. Bachman’s (1986) analysis of the Structure section of a past form of TOEFL had led him to conclude that the 15 items required “syntactic competence almost exclusively” (p. 78). Because of the probable difficulty of such TOEFL items for our sample of lower level Japanese EFL readers, a somewhat easier set of 20 items that also appeared to test syntactic knowledge were extracted from the grammar section of TEEP (Weir, 1983). The two sets of items were merged and subjected to a preliminary content analysis, as a result of which the total number of items was reduced to 32 $\bar { X } = 1 5 . 4 1$ , $\mathrm { S D } = 5 . 6 9$ , alpha $= . 8 2 \AA$ ).

The content analysis involved 11 L1-English ELT experts with at least a master’s degree in applied or theoretical linguistics or TEFL and three Japanese lecturers of English syntax with at least a master’s degree in linguistics. They were asked to give categorical responses on each of the initial 35 items as to whether each one primarily tested the candidate’s knowledge of syntax, lexical semantics, or sentence semantics. We specifically requested that they judge which of the following three areas each item seemed to be testing:

(1) The candidate’s “knowledge of the meanings of certain words and phrases” (Lexical-Semantic Knowledge).   
(2) The candidate’s “knowledge of sentence structures and that of acceptable sequences and forms of words in terms of syntax” (Syntactic Knowledge).   
(3) The candidate’s “understanding of the meaning of the overall sentence” (Sentence Reading Comprehension).

Although most of the items were considered by the majority of participants to be testing syntax more than the other two abilities, three items were judged to require either lexical-semantic knowledge or sentence comprehension ability rather than syntactic knowledge. Therefore, they were removed from the item set, and the total test length was set at 32 items (see Appendix for samples of both accepted and rejected items).

# 3 Results

As in Study 1, the SEM analysis initially compared a model in which no regression weight was constrained (Model 1) against two others in which either syntactic knowledge or vocabulary breadth was constrained to have a zero regression weight.

As can be seen in Table 3, both the $\bar { \chi } ^ { 2 }$ value and all the alternative fit estimates indicated that Model 1 approximated our data at an acceptable level and best of the three models, so we proceeded to compare the regression weights on syntactic knowledge and vocabulary breadth based on Model 1.

The individual parameter estimates based on Model 1 are in Table 4, in which syntax is shown to exceed vocabulary in standardised regression weight (. $. 6 1 ^ { * }$ vs . ${ } ^ { 3 4 ^ { * } }$ , percentage of reading variance explained $( 7 9 \%$ vs $72 \%$ ), and percentage of reading variance uniquely explained ( $11 \%$ vs $4 \%$ ). The two predictors correlated very strongly $r = . 8 4 ^ { * }$ ) with each other.

Table 3 Estimates of model-to-data fit: Study 2 $( n = 1 8 2 )$   

<html><body><table><tr><td>Model (constraint on ws/wv)</td><td>x?</td><td>df</td><td>x2/df</td><td>GFI</td><td>NNFI</td><td>CFI</td><td>RMSEA</td><td>x2 difference df from Model 1</td><td></td></tr><tr><td>1 (None)</td><td>7.521</td><td>6</td><td>1.254</td><td>.987</td><td>.996</td><td>.998</td><td>.037</td><td></td><td>-</td></tr><tr><td>2 (ws=0)</td><td>28.007*</td><td>7</td><td>4.001</td><td>.952</td><td>.948</td><td>.976</td><td>.129</td><td>20.486*</td><td>1</td></tr><tr><td>3(wv=0)</td><td>12.022</td><td>7</td><td>1.717</td><td>.978</td><td>.988</td><td>.994</td><td>.063</td><td>4.501*</td><td>1</td></tr></table></body></html>

Note: $\chi ^ { 2 } =$ chi-square, df $=$ degree of freedom, $\mathsf { G F l } = !$ goodness of fit index, NNFI $=$ non-normed fit index, ${ \mathsf { C F l } } =$ comparative fit index, RMSEA $=$ root mean square error of approximation; $^ { \ast } p < . 0 5$ .

Table 4 Regression and correlation summary: Study 2 $( n = 1 8 2 )$   

<html><body><table><tr><td></td><td colspan="2">Reading x</td><td>Syntax x</td></tr><tr><td></td><td>Syntax</td><td>Vocabulary</td><td>Vocabulary</td></tr><tr><td>Beta</td><td>.61*</td><td>.34*</td><td></td></tr><tr><td>r</td><td>.89*</td><td>.85*</td><td>.84*</td></tr><tr><td>% explained</td><td>79%</td><td>72%</td><td>71%</td></tr><tr><td>% jointly explained</td><td colspan="2">83%</td><td></td></tr></table></body></html>

Note: $^ { * } p < . 0 5$

The basic pattern of the relative significance of syntactic knowledge and vocabulary breadth thus confirmed what we found in Study 1. Data from this study again suggested that both syntax and vocabulary contribute to explaining the individual differences in text reading comprehension test scores but syntax was a somewhat stronger predictor.

# V Study 3

# 1 Participants and instruments

For Study 3, the sample comprised L1-Japanese EFL learners from five universities in Japan with an initial total of 624 participants. The instruments for the measurement of the three ability areas were identical to the ones in Study 2 above $\overline { { X } } \ = 1 0 . 4 9$ , $\mathrm { S D } = 4 . 1 1$ , alpha $= . 7 4$ for reading; $\overline { { X } } = \mathrm { 1 5 } . 8 9$ , $\mathrm { S D } = 5 . 6 0 $ , alpha $= . 7 9$ , for syntax; ${ \bar { X } } = 2 4 . 1 2$ , $\mathrm { S D } = 1 2 . 8 1 $ , alpha $= . 9 5$ for vocabulary). For this main study we adopted the conservative approach, which would be required in IRT (Henard, 2000), of excluding from the analysis those participants who received either a perfect or zero score on any of the ability measures. This process reduced the sample size to 599. With the six observed variables created from the split halves of the three ability measures, an initial test of multivariate kurtosis (Mardia, 1970) suggested that the data distribution was non-normal. However, use of an arcsin transformation (see Woods et al., 1986) and removal of eight outliers based on Mahalanobis distance (cf. Byrne, 2001) led to a drop in the coefficient to an acceptable level (1.52, with a critical ratio of 1.9). We thus proceeded to conduct our main analysis with the data from the remaining 591 participants.

# 2 Results of single group analysis

The first step in our SEM analysis of the full sample in Study 3 was to compare the same three models tested in Studies 1 and 2 to determine whether both the latent syntax and vocabulary variables were helpful for the prediction of latent text reading ability. Table 5 shows that, unlike Studies 1 and 2, all models in Study 3 produced significant $\chi ^ { 2 }$ values, which can be an indicator of lack of fit. However, as all the alternative fit indices suggested a fairly high level of fit for Models 1 and 3, this was probably a result of the increase in sample size rather than a poor model fit (cf. Hu and Bentler, 1995). Between Models 1 and 3, a significant difference in $\chi ^ { 2 }$ emerged, supporting the former. Other fit indices also indicated the relative adequacy of Model 1, in which both syntax and vocabulary are thought to contribute to the prediction of the reading performance. Thus, the estimates of regression weights and correlations were based on Model 1.

Table 5 Estimates of model-to-data fit: Study 3 single group analysis $( n = 5 9 1$ )   

<html><body><table><tr><td>Model (constraint on ws/wv)</td><td>x</td><td>df</td><td>x2/df</td><td>GFI</td><td>NNFI</td><td>CFI</td><td>RMSEA</td><td>difference (from Model 1)</td><td>df</td></tr><tr><td>1 (None)</td><td>16.887*</td><td>6</td><td>2.814</td><td>.990</td><td>.989</td><td>.999</td><td>.055</td><td>-</td><td>-</td></tr><tr><td>2 (ws = 0)</td><td>66.106*</td><td>7</td><td>9.444</td><td>.966</td><td>.949</td><td>.976</td><td>.120</td><td>49.219*</td><td>1</td></tr><tr><td>3(wv = 0)</td><td>22.115*</td><td>7</td><td>3.159</td><td>.987</td><td>.987</td><td>.994</td><td>.060</td><td>5.228*</td><td>1</td></tr></table></body></html>

Note: $\chi ^ { 2 } =$ chi-square, df $=$ degree of freedom, GFI $=$ goodness of fit index, NNFI $=$ non-normed fit index, ${ \mathsf { C F l } } =$ comparative fit index, RMSEA $=$ root mean square error of approximation; $^ { \ast } p < . 0 5$ .

Table 6 lists the standardised regression weights for the latent predictor variables as well as the percentages of the reading variance they singly and jointly accounted for. Syntax is shown to be a much stronger predictor than vocabulary in terms of the regression weight (. ${ . 6 4 ^ { * } }$ vs . $. 2 5 ^ { * } )$ , the percentage of reading variance it singly accounted for $72 \%$ vs $62 \%$ ), and the percentage of reading variance it uniquely accounted for ( $12 \%$ vs $2 \%$ ). The intercorrelation between the two predictors was very high $( r = . 8 4 ^ { * }$ ). In short, the results here clearly support the relative significance of syntax over vocabulary for this full sample.

# 3 Results of subgroup analysis

As a means of identifying possible discrepancies in the patterns of relationships for relatively higher and lower achievers within the full sample, a simultaneous multi-group SEM analysis was performed. Based on the typical patterns of enrolment at universities in Japan, we identified higher achievers as being English majors at private universities and non-English majors at public universities, whereas non-English majors at private universities were defined as a lower achieving group. This method avoided the problem of creating two non-normal sub-samples, which would inevitably have occurred if the sample had been split at the mean. Dividing the full sample and removing three outliers based on Mahalanobis distance, we obtained subgroups of higher ${ \bf \ddot { \phi } } n = 3 4 3$ ) and lower achievers ${ \bf \ddot { \Delta } n } = 2 4 5$ ), neither of which showed any indication of non-normal distribution.

Table 6 Regression and correlation summary: Study 3 – single group analysis $( n = 5 9 1$ )   

<html><body><table><tr><td></td><td colspan="2">Reading x</td><td>Syntax x</td></tr><tr><td></td><td>Syntax</td><td>Vocabulary</td><td>Vocabulary</td></tr><tr><td>Beta</td><td>.64*</td><td>.25*</td><td></td></tr><tr><td>r</td><td>.85*</td><td>.79*</td><td>.84*</td></tr><tr><td>% explained</td><td>72%</td><td>62%</td><td>70%</td></tr><tr><td>% jointly explained</td><td colspan="2">74%</td><td>-</td></tr></table></body></html>

Note: $^ { * } p < . 0 5$

Following the results of our single group analysis above, we tested a model in which the unstandardised coefficients on the paths among the three latent variables of reading, syntax, and vocabulary were freely estimated within each group but were fixed across the group division. Fit statistics were consulted to determine whether this model of cross-group invariance held or whether alternative models with group-dependent path coefficients should be explored.

Table 7 lists the fit statistics, and despite the significant $\chi ^ { 2 }$ statistic, its value relative to the df is small and the values of the alternative fit indices are all in the acceptable range. Thus, we interpreted this model as accounting for the data for the two subgroups well and proceeded to examining the individual parameters of interest.

It was expected from the acceptable fit of the model that the relative significance of the two predictors in terms of the standardised estimates would be similar across the level division, and this was indeed the case as shown in Table 8. Syntax was a better predictor of reading than vocabulary in terms of the regression coefficients and the percentage of reading variance it accounted for. The amount of variance explained by the two predictors together was larger for the lower group, but the relative significance of syntax over vocabulary remained constant.

Table 7 Estimates of model-to-data fit: Study 3 (path coefficients constrained to be constant across groups; $n = 5 8 8 1$ )   

<html><body><table><tr><td>x2</td><td>df</td><td>x2/df</td><td>GFI</td><td>NNFI</td><td>CFI</td><td>RMSEA</td></tr><tr><td>25.917*</td><td>15</td><td>1.728</td><td>.985</td><td>.983</td><td>.992</td><td>.035</td></tr></table></body></html>

Note: $\chi ^ { 2 } \ =$ chi-square, df $=$ degree of freedom, $\mathsf { G F l } =$ goodness of fit index, NNFI $=$ non-normed fit index, CFI $=$ comparative fit index, RMSEA $=$ root mean square error of approximation; $^ { * } p < . 0 5$ .

Table 8 Summary of individual parameter estimates: Study 3 – subgroup analysis   

<html><body><table><tr><td></td><td colspan="2">Reading x</td><td>Syntax x</td></tr><tr><td></td><td>Syntax</td><td>Vocabulary</td><td>Vocabulary</td></tr><tr><td>Higher (n = 343)</td><td></td><td></td><td></td></tr><tr><td>Beta</td><td>.50*</td><td>.19*</td><td></td></tr><tr><td>r</td><td>.62*</td><td>.52*</td><td>.67*</td></tr><tr><td>% explained</td><td>38%</td><td>27%</td><td>45%</td></tr><tr><td>% jointly explained</td><td>41%</td><td></td><td>-</td></tr><tr><td>Lower (n = 245)</td><td></td><td></td><td></td></tr><tr><td>Beta</td><td>.62*</td><td>.26*</td><td>-</td></tr><tr><td>r</td><td>.78* 61%</td><td>.67*</td><td>.67*</td></tr><tr><td>% explained</td><td></td><td>45%</td><td>45%</td></tr><tr><td>% jointly explained</td><td>67%</td><td></td><td>-</td></tr></table></body></html>

Note: $^ { * } p < . 0 5$

# VI Discussion

We set out to investigate the relative significance of syntactic and vocabulary knowledge as predictors of text reading comprehension performance through three separate studies, making efforts to minimise the operations required for discourse-level, meaning extraction or reconstruction in the predictor measures.

For our first preliminary study (Study 1) we were able to obtain data from a sample of EAP students with relatively heterogeneous L1 background and L2 learning experiences, and the results suggested that each of the latent syntax and vocabulary variables accounted for a significant amount of variance in the latent text reading comprehension variable when the effect of the other predictor was controlled for. Between the two, syntax emerged as the stronger predictor.

Our second preliminary study (Study 2) was based on a more homogeneous and relatively weaker sample of EFL learners at Japanese universities, and with this sample again, both the latent syntactic and vocabulary knowledge variables made unique contributions to the prediction of latent text reading comprehension variance, with the syntactic knowledge outperforming vocabulary in predictive power somewhat more clearly than in the first study.

Following these two sets of results, our main study (Study 3) with a significantly larger sample of Japanese EFL readers has also shown that syntactic knowledge was a better predictor of text reading comprehension than vocabulary knowledge. Further, our subgroup analysis indicated that the relative significance of the syntactic variable was not a phenomenon limited to readers of lower ability alone but extended to those with a higher level of ability as well.

The results of our main study, though consistent with qualitative research in the L2 reading field (Bernhardt, 1991), are in conflict with most of the previous quantitative research investigating this issue (Bossers, 1992; Brisbois, 1995; Yamashita, 1999), and so we need to revisit these studies to explore possible sources of the divergence in findings. As already noted, despite their authors’ conclusion that vocabulary was a better predictor of reading than grammar, none of these studies showed vocabulary to be clearly superior. Bossers (ibid) obtained beta coefficients for the two predictors that were not very far from each other (Vocabulary $= . 4 1$ ; Grammar $= . 3 6$ ) based on a restricted sample of only 50 Turkish learners of Dutch. Brisbois (ibid) reported that for her 84 lower-level American learners of French, vocabulary $\prime r = . 3 5 )$ correlated more strongly with reading than did grammar $r = . 2 6 )$ , but the serious concerns with her methodology discussed above must cast some doubt on her findings. It is also notable that both her vocabulary $\overline { { \mathbf { X } } } = 9 . 8 6 \%$ , $\mathrm { S D } = 6 . 4 2 \%$ ) and grammar tests $\mathbf { \overline { { X } } } = 2 2 . 0 2 \%$ , $\mathrm { S D } = 6 . 1 6 \%$ ) were rather difficult for her sample, and we are simply unable to determine whether or not her measures generated a sufficient degree of dispersion and internal consistency within her target sample to permit any valid comparison between her predictor variables to be made. The alpha of .93 that she reported for her grammar measure was based on an extended sample of 265 learners, and its applicability to her actual sample of 84 beginners, from which correlation and regression data derived, is questionable.

In sum, Bossers’s (1992) and Brisbois’s (1995) data cannot be interpreted as clear evidence of the relative significance of vocabulary or insignificance of syntax unless results are replicated with larger samples and on the basis of a more rigorous methodology.

Compared with these studies, Yamashita’s (1999) results are based on a fairly large sample ${ \bf \zeta } _ { n } = 2 4 1$ ) and instruments that seem better matched in difficulty with her sample $\hat { X } = 5 2 . 5 9 \%$ , $\mathrm { S D } = 1 2 . 7 0 \%$ , alpha $\qquad = ~ . 8 1$ for Grammar and $\hat { X } = 6 8 . 2 4 \%$ , $\mathrm { S D } = 1 2 . 1 7 \%$ , alpha $= . 8 7$ for VLT). Among her several predictors of EFL reading comprehension test performance, the significance of the VLT (beta $= . 3 1$ , $r = . 5 7 \AA$ ) was greater than that of grammar (beta $= . 2 4$ , $r = . 4 6 )$ ). However, as noted above, studies based on conventional multiple regression with observed raw scores as independent variables are susceptible to the differential reliabilities of the predictor variables. In fact, informally submitting the raw scores for our own main study to conventional multiple regression resulted in a reversed conclusion $\mathrm { ( b e t a } = . 3 5 ^ { * }$ , alpha $= . 7 9$ for syntax vs. beta $= . 4 0 ^ { * }$ , alpha $= . 9 5$ for vocabulary), which exemplifies the potentially misleading effects of unequal reliabilities.

While both of Yamashita’s predictor instruments demonstrated acceptable levels of internal consistency, the VLT outperformed grammar in this respect. One can in fact note an interesting directionality between the alpha and the correlation coefficients above, although we have no way of verifying the effects of the discrepant reliabilities here. Nevertheless, the stronger prediction by the VLT could well have been mediated by its higher internal consistency, and alternative methods of analysis such as SEM, which are capable of partialling out measurement errors, might have produced coefficients less in favour of vocabulary.

Yamashita’s study and our present main study were both based on Japanese university EFL learners and adopted similar measurement instruments. Both relied on variants of the VLT for vocabulary breadth and on the TOEFL Structure section as part of syntactic/grammar knowledge measures. An important difference is in the criterion reading comprehension index. Both studies employed passage reading followed by MCQ items, but Yamashita additionally used a gap-filling test and based her regression analysis on the composite index of the scores on the two tests. It is notable that when the part of her reading comprehension measure based on the MCQ method (TOEFL’s Reading Comprehension section score) was isolated as the criterion, the correlation data no longer demonstrated any substantive advantage of vocabulary $r = . 3 1 ,$ ) over grammar $r = . 2 9 )$ . We are therefore far from being able to conclude that vocabulary breadth accounts for significantly more variance in any type of reading comprehension test performance than does syntactic knowledge, especially in light of the data generated by our own main study with Japanese participants.

Outside of our Japanese university population, there is also evidence based on an MCQ-format reading ability measure and employing SEM that suggests the importance of syntactic knowledge in predicting reading test performance. Van Gelderen and colleagues’ data (2003; 2004) from their secondary-level EFL learners in The Netherlands indicated that their latent reading variable as measured by the MCQ method correlated better with grammar than with vocabulary.4

The possible effects of test method in estimating text reading comprehension must be addressed. Our main study was based on the MCQ method alone, and thus a conservative interpretation of its findings is that, within the Japanese university population, syntactic knowledge plays a larger role than vocabulary breadth in accounting for the variance in an MCQ-based text reading comprehension test. However there is also a possibility as our preliminary study (Study 1) has indicated, that syntactic knowledge plays a larger role than vocabulary knowledge in explaining text reading comprehension variance in other populations when the assessment is based on alternative methods including short answer, true/false judgment followed by justifications, and table/diagram completion. We recognise that some aspects of Study 1 need improving, and so its results should be interpreted with caution. It contained only ten vocabulary items compared to 30 for syntax, although SEM is relatively more robust in dealing with such disparity as compared to conventional regression. Furthermore, it was a test of contextualised vocabulary knowledge and meant to engage the participants in the processing of sentence semantics, or local reading comprehension. Future studies should endeavour to minimise the overlap in the operations required in the criterion reading and the predictor syntactic and vocabulary knowledge measures. Nevertheless, we feel that this preliminary study was worth reporting as it demonstrated the possibility that syntax can be more significant than vocabulary in predicting performance on a reading test based on multiple methods and even for mixed-L1 EAP students at the point of entry into degree courses in the UK.5

Our main study was based on a sample of Japanese EFL students who were generally lower in L2 proficiency compared to the UK sample above. One interpretation of its results may be that syntactic knowledge remains one of the deciding factors in the performance on text reading comprehension for learners up to a certain level. Future research should thus seek to identify the proficiency levels of their participants in terms of standard scales so that it can provide reference as to where on the proficiency continuum the significance of syntactic knowledge is most evident and where, if it does, it begins to taper in significance.

While syntax clearly overshadowed vocabulary in regression coefficients in our main study, the results must not be interpreted as indicating that vocabulary is unimportant. Vocabulary did correlate significantly with reading $( r = . 7 9 ^ { * } )$ ) but not as strongly as did syntactic knowledge $( r = . 8 5 ^ { * }$ ), and the inter-predictor correlation was high $( r = . 8 4 ^ { * } )$ , which means that syntax has taken away a large portion of the reading variance, much of which vocabulary would have accounted for if entered into the equation alone. Therefore, our attention to the development of lexical competence for improved reading proficiency should continue although our data lead us to recommend rethinking the notion that it is the level of vocabulary knowledge rather than syntactic knowledge that distinguishes better from poorer comprehenders of extended text. Similarly, an implication of our data for L2 pedagogy would be that we should question the notion that, to enhance reading ability, increasing vocabulary size is more effective than developing syntactic knowledge.

The high correlation between syntax and vocabulary found in Studies 2 and 3 $r = . 8 4 ^ { * }$ in both) raises a question regarding the (in)divisibility of the two competencies. It is quite possible that the developments of these putatively distinct elements of linguistic knowledge overlap to a substantial extent such that factor analyses would produce a common underlying variable capable of explaining much of their variances. In fact, a subset of the data for Study 3 has been factor-analysed in Shiotsu (2003), and a common factor indeed accounted for the variances in not only syntactic knowledge and vocabulary breadth but also in the passage reading comprehension measure. In a much larger study, Purpura (1999) factor-analysed his test data and reported that a common “lexico-grammatical ability” accounted for the variances in his “vocabulary”, “grammar”, “word formation”, and “sentence formation” measures well. It is also notable that his SEM analysis showed his lexico-grammatical ability variable to almost perfectly predict his latent reading ability variable (beta $= . 9 8 5 ^ { * }$ ). Previous factor analytic studies thus tend to support the factorial indivisibility of not only grammar/syntactic and lexical knowledge but also reading ability. This is not, in our opinion, in conflict with our interest in investigating the relative significance of syntactic knowledge and vocabulary breadth in the prediction of text reading comprehension performance or our methodology of specifying the competing predictors a priori, which has been customary in studies subjecting ability variables of interest to conventional regression (Brisbois, 1995; Haynes and Carr, 1990; Bossers, 1992; Yamashita, 1999) or SEM (van Gelderen et al., 2003; 2004). To re-examine the claim that vocabulary knowledge more strongly predicts reading ability than does grammar/syntactic knowledge (Brisbois, ibid; Yamashita, ibid), we feel that our methodology offers significant improvement over that based on conventional multiple regression with observed variables.

One of the limitations of the present study is that only one method for testing each variable was available, and thus it was necessary to resort to the practical solution of splitting the observed variables to perform SEM (Schoonen et al., 1998; van Gelderen et al., 2003; 2004). One improvement, therefore, that can be made to our main study is the use of multiple methods in both the criterion reading and the predictor knowledge measures (e.g., Purpura, 1999). The text reading comprehension measure will benefit from the types of tasks adopted in our Study 1. An additional measure for the assessment of lexical skill dimensions other than vocabulary breadth (e.g., Read, 1993; Qian, 1999) might also be illuminative, and syntax knowledge might be more validly estimated if methods other than MCQ are added (cf. van Gelderen et al., 2004). Several different aspects of the component skill areas can be measured with suitable instruments, and candidate abilities on each of them might be estimated on continuous scale using IRT. These ability estimates can form the indicator variables to be submitted to SEM. Such a procedure has the potential to provide a level of stability and generalisability of the results that is extremely difficult to achieve with conventional multiple regression based on raw scores. In fact, we feel that some previous research (e.g., Yamashita, 1999) actually employed instruments that can be easily accommodated in the very type of analysis just described and that reanalysis of such existing data sets should add important insights to our limited database and understanding on the issue of the relative significance of syntax and vocabulary for reading comprehension.

# VII Conclusion

We have identified some problems in previous research studies which claimed that vocabulary was relatively more important for predicting reading test performance than syntactic or grammatical knowledge and made the case for more rigorous research in terms of design, methodology and analysis in future componential studies in this area.

We have reported on three such studies which all offer support for the relative significance of syntactic knowledge over vocabulary breadth in predicting text reading comprehension test performance. Our findings are currently limited to the Japanese university EFL population and the MCQ method of assessing reading ability. Further research will be necessary to investigate their generalisability across different samples and across different response formats.

Future studies might usefully include additional L2 variables such as general comprehension ability as measured by listening tasks, working memory span, and word recognition speed (cf. Shiotsu, 2003). The inclusion of an L1 literacy variable in future monolingual studies subjected to the same methodological rigour provided by SEM might also evince a clearer picture of the relative contribution of this already well-established variable.

The limitations of the present study notwithstanding, we hope it offers useful perspectives on L2 reading ability, test score predictability, the effects of unequal reliabilities, and test content validation.

# Acknowledgements

We gratefully acknowledge the insightful comments made by the Language Testing reviewers and the Editor that helped improve this article. Our thanks also go to all the test candidates and language experts for their generous participation in the study. Aspects of this article were presented at the 26th LTRC held in Temecula in March 2004.

# VIII References

Alderson, J.C. 1984: Reading in a foreign language: a reading problem or a language problem? In Alderson, J.C. and Urquhart, A.H., editors, Reading in a foreign language. Harlow: Longman. 1993: The relationship between grammar and reading in an English for academic purposes test battery. In Douglas, D. and Chapelle, C., editors, A new decade of language testing research: selected papers from the 1990 Language Testing Research Colloquium: dedicated in memory of Michael Canale. Alexandria, VA: Teachers of English to Speakers of Other Languages. 1999: Reading constructs and reading assessment. In Chalhoub-Deville, M., editor, Issues in computer-adaptive testing of reading proficiency. Cambridge: Cambridge University Press. 2000: Assessing reading. Cambridge: Cambridge University Press.   
Arbuckle, J. and Wothke, W. 1999: AMOS 4.0 user’s guide. Chicago: Smallwaters.   
Bachman, L.F. 1986: The Test of English as a Foreign Language as a measure of communicative competence. In Stansfield, C.W., editor, Toward communicative competence testing: Proceedings of the Second TOEFL Invitational Conference, TOEFL Research Report 21, May 1986. Princeton, NJ: Educational Testing Service.   
Baddeley, A., Logie, R., Nimmo-Smith, I. and Brereton, N. 1985: Components of fluent reading. Journal of Memory and Language 24, 119–31.   
Barnett, M.A. 1986: Syntactic and lexical/semantic skills in foreign language reading: importance and interaction. Modern Language Journal 70, 343–9.   
Berman, R.A. 1984: Syntactic components of the foreign language reading process. In Alderson, J.C. and Urquhart, A.H., editors, Reading in a foreign language. Harlow: Longman.   
Bernhardt, E.B. 1991: Reading development in a second language: theoretical, empirical, and classroom perspectives. Norwood, NJ: Ablex. 1999: If reading is reader-based, can there be a computer-adaptive test of reading? In Chalhoub-Deville, M., editor, Issues in computeradaptive testing of reading proficiency. Cambridge: Cambridge University Press. 2000: Second-language reading as a case study of reading scholarship in the 20th century. In Kamil, M.L., Pearson, P.D. and Barr, R., editors, Handbook of reading research. Vol. III. Mahwah, NJ: Lawrence Erlbaum.   
Bernhardt, E.B. and Kamil, M.L. 1995: Interpreting relationships between L1 and L2 reading: consolidating the linguistic threshold and the linguistic interdependence hypotheses. Applied Linguistics 16, 15–34.   
Bossers, B. 1992: Reading in two languages: a study of reading comprehension in Dutch as a second language and in Turkish as a first language. Rotterdam, The Netherlands: Drukkerij Van Driel.   
Brisbois, J.E. 1995: Connections between first- and second-language reading. Journal of Reading Behavior 27, 565–84.   
Byrne, B.M. 2001: Structural equation modeling with AMOS: basic concepts, applications, and programming. Mahwah, NJ: Lawrence Erlbaum.   
Carr, T.H. and Levy, B.A. 1990: Preface. In Carr, T.H. and Levy, B.A., editors, Reading and its development: component skills approaches. San Diego, CA: Academic Press.   
Crystal, D. 1997: The Cambridge encyclopedia of language. Cambridge: Cambridge University Press.   
Cunningham, A.E., Stanovich, K.E. and Wilson, M.R. 1990: Cognitive variation in adult college students differing in reading ability. In Carr, T.H. and Levy, B.A., editors, Reading and its development: component skills approaches. San Diego, CA: Academic Press.   
Grabe, W. and Stoller, F.L. 2002: Teaching and researching reading. Harlow: Longman.   
Green, R. 2000: An empirical investigation of the componentiality of E.A.P. reading and E.A.P. listening through language test data. Unpublished PhD thesis, The University of Reading.   
Harrington, M. and Sawyer, M. 1992: L2 working memory capacity and L2 reading skill. Studies in Second Language Acquisition 14, 25–38.   
Hatch, E.M. and Lazaraton, A. 1991: The research manual: design and statistics for applied linguistics. New York: Newbury House.   
Haynes, M. and Carr, T.H. 1990: Writing system background and second language reading: a component skills analysis of English reading by native speaker-readers of Chinese. In Carr, T.H. and Levy, B.A., editors, Reading and its development: component skills approaches. San Diego, CA: Academic Press.   
Heaton, J.B. 1988: Writing English language tests. London: Longman.   
Henard, D.H. 2000: Item response theory. In Grimm, L.G. and Yarnold, P.R., editors, Reading and understanding more multivariate statistics. Washington, DC: American Psychological Association.   
Hoover, W.A. and Tunmer, W.E. 1993: The components of reading. In Thompson, G.B., Tunmer, W.E. and Nicholson, T., editors, Reading acquisition processes. Clevedon, PA: Multilingual Matters.   
Horrocks, G. 1987: Generative grammar. London: Longman.   
Hu, L.-T. and Bentler, P. M. 1995: Evaluating model fit. In Hoyle, R.H., editor, Structural equation modeling: concepts, issues, and applications. Thousand Oaks, CA: Sage Publications.   
Jackson, M.D. and McClelland, J.L. 1979: Processing determinants of reading speed. Journal of Experimental Psychology: General 108, 151–81.   
Koda, K. 1998: The role of phonemic awareness in second language reading. Second Language Research 14, 194–215.   
Kunnan, A.J. 1998: An introduction to structural equation modelling for language assessment research. Language Testing 15, 295–332.   
Laufer, B. 1992: Reading in a foreign language: how does L2 lexical knowledge interact with the reader’s general academic ability? Journal of Research in Reading 15, 95–103.   
Lee, J.-W. and Schallert, D.L. 1997: The relative contribution of L2 language proficiency and L1 reading ability to L2 reading performance: a test of the threshold hypothesis in an EFL context. TESOL Quarterly 31, 713–39.   
Mardia, K.V. 1970: Measures of multivariate skewness and kurtosis with applications. Biometrika 57, 519–30.   
Nation, I.S.P. 1990: Teaching and learning vocabulary. New York: Newbury House.   
Palmer, J., MacLeod, C.M., Hunt, E. and Davidson, J.E. 1985: Information processing correlates of reading. Journal of Memory and Language 24, 59–88.   
Purpura, J.E. 1999: Learner strategy use and performance on language tests: a structural equation modeling approach. Cambridge, UK: Cambridge University Press. 2004: Assessing grammar. Cambridge, UK: Cambridge University Press.   
Qian, D.D. 1999: Assessing the roles of depth and breadth of vocabulary knowledge in reading comprehension. The Canadian Modern Language Review 56, 282–307.   
Quirk, R., Greenbaum, S., Leech, G. and Svartvik, J. 1972: A grammar of contemporary English. London: Longman.   
Raykov, T. and Marcoulides, G.A. 2000: A first course in structural equation modeling. Mahwah, NJ: Lawrence Erlbaum.   
Read, J. 1993: The development of a new measure of L2 vocabulary knowledge. Language Testing 10, 355–71.   
Richards, J.C., Platt, J. and Platt, H. 1992: Longman dictionary of language teaching and applied linguistics. Harlow: Longman.   
Sampson, G. 1975: The form of language. London: Weidenfeld and Nicolson.   
Schmitt, N., Schmitt, D. and Clapham, C. 2001: Developing and exploring the behaviour of two new versions of the Vocabulary Levels Test. Language Testing 18, 55–88.   
Schoonen, R., Hulstijn, J. and Bossers, B. 1998: Metacognitive and languagespecific knowledge in native and foreign language reading comprehension: an empirical study among Dutch students in grades 6, 8 and 10. Language Learning 48, 71–106.   
Segalowitz, N., Paulsen, C. and Komoda, M. 1991: Lower level components of reading skill in higher level bilinguals: implications for reading instruction. In Hulstijn, J.H., editor, Reading in second languages: AILA review 8.   
Shiotsu, T. 2003: Linguistic knowledge and processing efficency as predictors of L2 reading ability: a component skills analysis. Unpublished PhD thesis, The University of Reading.   
Ulijn, J.M. 1981: Conceptual and syntactic strategies in reading a foreign language. In Hopkins, E. and Grotjahn, R., editors, Studies in language teaching and language acquisition. Bochum, Germany: Brockmeyer. 1984: Reading for professional purposes: psycholinguistic evidence in a cross-linguistic perspective. In Pugh, A.K. and Ulijn, J.M., editors, Reading for professional purposes. London: Heinemann.   
Upton, T.A. and Lee-Thompson, L. 2001: The role of the first language in second language reading. Studies in Second Language Acquisition 23, 469–95.   
Urquhart, A.H. and Weir, C.J. 1998: Reading in a second language: process, product, and practice. New York: Longman.   
van Gelderen, A., Schoonen, R., de Glopper, K., Hulstijn, J., Simis, A., Snellings, P. and Stevenson, M. 2004: Linguistic knowledge, processing speed and metacognitive knowledge in first and second language reading comprehension: a componential analysis. Journal of Educational Psychology 96, 19–30.   
van Gelderen, A., Schoonen, R., de Glopper, K., Hulstijn, J., Snellings, P., Simis, A. and Stevenson, M. 2003: Roles of linguistic knowledge, metacognitive knowledge and processing speed in L3, L2 and L1 reading comprehension: a structural equation modeling approach. International Journal of Bilingualism 7, 7–25.   
Weir, C.J. 1983: Identifying the language needs of overseas students in tertiary education in the United Kingdom. Unpublished PhD thesis, Institute of Education, University of London.   
Weir, C.J., Yang, H. and Jin, Y. 2000: An empirical investigation of the componentiality of L2 reading in English for academic purposes. Cambridge: Cambridge University Press.   
Woods, A., Fletcher, P. and Hughes, A. 1986: Statistics in language studies. Cambridge: Cambridge University Press.   
Yamashita, J. 1999: Reading in a first and a foreign language: a study of reading comprehension in Japanese (the L1) and English (the L2). Unpublished PhD thesis, Lancaster University.   
Yang, H. and Weir, C.J. 1998: Validation study of the College English Test. Shanghai: China National College English Testing Committee.

# Appendix

Knowledge of syntax: test item samples (Study 2)

(Examples of items accepted by the judges)

There are very few areas in the world be grown successfully.

A. where apricots can B. apricots can C. apricots that can D. where can apricots

Caramel is a brown substance by the action of heat on sugar.

A. form B. forming C. formed D. forms

(Examples of items rejected by the judges)

I am taller than you three inches.

A. with B. by C. of D. in

how hard he worked, his tutor never commented on it.

A. Of no account B. No matter C. Without regard D. Mind less