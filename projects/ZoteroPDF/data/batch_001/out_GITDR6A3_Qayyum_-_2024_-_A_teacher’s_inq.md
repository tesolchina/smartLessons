# A teacher's inquiry into diagnostic assessment in an EAP writing course

Rabail Qayyum

University of Hawaii at Manoa, USA

# ARTICLEINFO

# ABSTRACT

Keywords:   
Diagnostic assessment   
Argument-based validation   
English for Academic Purposes writing   
International students

Research into diagnostic assessment of writing has largely ignored how diagnostic feedback information leads to differentiated instruction and learning. This case study research presents a teacher's account of validating an in-house diagnostic assessment procedure in an English for Academic Purposes writing course with a view to refining it. I developed a validity argument and gathered and interpreted related evidence, focusing on one student's performance in and perception of the assessment. The analysis revealed that to an extent the absence of proper feedback mechanisms limited the use of the test, somewhat weakened its impact, and reduced the potential for learning. I propose a modification to the assessment procedure involving a sample student feedback report.

# 1. Introduction

Studies on diagnostic assessment of writing have documented how to identify, operationalize, and measure the underlying constructs (e.g., Dolgova & Sicek, 2019; Kim, 2011; Urmston et l., 2013), yet there remains much to learn about how to use diagnostic instruments (Pellgrino et al., 2016). There is a broad consensus that validity is the most important consideration in evaluating the quality of the uses and interpretations of the results of tests and assessments. Widely cited validit theorists like Kane (2013) and Pellgrino e al. (2016) have conceptualized validity as a multifaceted element that is not an instrument's inherent property; rater it lies in the interpretation of an instrument's use and impact. Whereas some of th current aproaches have attempted to link validity to issue of fss . andllet al, 2024, ae for a social ustice orinttion to valty), the ament-ad valdation aroach is a popular one (Dursun & Li, 2021) that lays out the interpretation and uses of an asessment procedure in as clear and complete as possible terms, and is arelatively traighforward process(Kane, 2021). It general framework and some of theterminologie originate from the work of Toulmin (2003). I is a useful approach in listing the criteria for an asessment instrument and clearly detailing its limitations, so that the purpose of the instrument does not stray.

Among the different approaches in diagnostic assessment such as cognitive diagnostic assessment (Kim, 2011) and automated feedback (Koltovskaia, 2020), the pproach I adopt i based on Alderson (2005) in which the focus i not only on identifying strengths and weaknesse of lerners, but lon following up on these asects. herefore, in this paper, the term dignosti asssment procedure is used to characterize the complete cycle of diagnosis comprising five recuring stages: 1) define what is to be assessed, 2) operationalize it, 3) conduct diagnostic assessment, 4) give feedback, and 5) implement feedback (Huhta et al., 2024).

are teacher-driven initiatives,situated within clssroom contexts. To bridge these gaps, ths case study research is a teachersattempt at validating an in-house diagnostic assessment procedure for an English for Academic Purposes (EAP) course by placing a student's perspective at the center.

By and large, EAP programs are the first among the few opportunities for international students to receive university-level writing instruction. Indeed, these programs do some heavy liting, and the intructors here have to met substantial expectations (Fox, 2009. In this scenario, diagnostic assessment is one avenue to provide beneficial information to teachers, students, and program administrators about students' individual needs. This paper describes how I designed and implemented a validity argument to explore the value and impact f the existing diagnostic asessment procedure. My goali to provide a unifying framework to classroom instructors for spearheading an effort into investigating the validity of their own diagnostic assessments.

In the next section, I describe the findings of key studies on diagnostic asessment of writing and the gaps i it, to build grounds fo. how the present study addresses those gaps.

# 2. Diagnostic assessment of EAP writing

One issue with scholarship on diagnostic asessment of writing is that i often carries a portrayal of diagnostic asssment in both theory and practice that renders it indistinguishable from other asessment forms. In this paper, dignostic asessment refers to assesment wth the function of making inferences about trengths or weakneses in leners writing ailities encomassing linguistic abilities) in order to assign targeted learning activities to yield positive changes in learning (Jang & Sinclair, 2021).

Validity is the fundamental consideration in not only developing tests, but also in evaluating them. Two useful examples for exploring validit f diagnostic instruments are provided by Chapelle et al. (2015) and Knoch and Elder (2016). Chapellet al. (2015) provide argument-based validation examples of two automated-scoring intruments for diagnostic asessment of undergraduate and graduate academic writing. However, these instruments were designed for a particular writing task, and it is unclear how the application of diagnostic fedback generated extended over a semester. On the other hand, Knoch and Elder (2016) analyzed two diagnostic instruments, namely Diagnostic English Language Asessment and Measuring the Academic skilsof Universit Students-termed post-entrance language assessments (PELAs)-using an argument-based validity framework. They determined that neither intrument fully satisfied their established criteria.

Among few studies that explore diagnostic feedback implementation and use, Doe's (2015) study is one notable example. The researcher examined how students interpreted diagnostic asessment in an EAP course at one Canadian university. Drawing on the assesment use argument, this study probed the extent to which diagnostic test feedback enabled students to beneficially adjus ther study practices to improve their aademic skill. The findings related to two assessment claims: interpretations and consequences. The interpretations clam addressed whether students' strengths and weakneses in reading, listening, and reading were meaningful and relevant to student learning. The consequences claim examined whether using the diagnostic test in the EAP course was beneficial to the students. Students responses to relection tas and interviews were used to support or refute the underlying warrants for these claims. Overall the rearcher found that the students interpreted the diagnostc fdack appropriately and benefited from it within a supportive content that enabled them to adjust their study practices.

Other studies that explore diagnostic feedback applications frequently involve implementation of large-scale interventions or introduction of campus-wide policies. Fox et al. (2016) reported on a multistage-evaluation mixed methods study, which evaluated the impact of a diagnostic assessment procedure on the firs-year experience, student engagement, achievement, and retention in an undergraduate engineering program. Their study integrated EAP assessment with mathematical knowledge, and the participants included all students, not just international. In another study, Fox (2009) carried out a large-scale mixed methods study examining he role diagnostic assessment played in moderating policy impact and designing EAP curicular reform. She ascertained that diagnostic assessment was, to a degree, able to mitigate the negative impact of the admission policy.

Overall, there i aneed for more small-cale qualitative studies that delve into applications of diagnostic asssment that is more closely tied to academic course ettings. Such an exploration will also be useful because assessments designed locally to address ocal initiatives and contexts are more likely to portray those contexs accurately and treat the stakeholders fairly than are large-scale assesments (Conference on College Composition and Communication ccc, 2022). Placing diagnostic asessment in classroom contexts and curricular goals willallow a connection of diagnostc feedack with actual actions that students and teachers can take to promote learning. In such micro-level applications of diagnostic feedback, decision making as well as the decision makers willbe noteworthy factors in the course of the procedure.

Since teachers are key stakeholders in the diagnostic ssessment procedure, a teacher-driven intiative can guide further thinking about and research into diagnostic asessment. A teacher's viewpoint merits examination because \*when purposes are set and out. comes measured by those who are beyond the classoom, the teacher is reduced to an element in the equation (Freman, 1998, p. 58) In comparison to macr-level feedback applications, a clasroom-based application can bring out the teacher as the driver and controlle f the assessment proceses, which is crucial fr teacher empowerment (Crusan, 2010). For diagnostc assessment specifically, as proposed by Alderson et al. (2015a), the first principle states that it is not the instrument, but the user who diagnoses.

Furthermore, relegating asessment as the domain of testing experts only and not of writing teachers could be problematic because "testing companie often mysticize asessment through the use of statistical and measurement jargon, making asessment a scary proposition for both teachers and students' (Crusan, 2010, p. 257). Edelenbos and Kubanek-German (2004) emphasize fostering language teachers' diagnostic competence, which remains a neglected area in teacher education (Jang & Sinclair, 2021).

To addres the concerns, this case study reearch is a teacher's attempt at validating an in-house developed diagnostic asessment procedure within an EAP course. Using an argument-based approach, the research integrates attempts I, the teacher, made to make sense of diagnostic test information with a focal student's perspective of receiving, comprehending, valuing, and acting upon the feedback. Both these perspectives are vital because students' understandings may be at odds with those of their teachers (Carles, 2006), enabling different layers of the situation to emerge.

# 3. The present study

The approach of the present study isa case study (Creswell, 2013), which enables close investigation of dynamic social processes as they evolve in the research setig and can provide rich holistic data that contribute to the understanding of complex situations. The strengths f case study method are in its abilit to gain an insider's viewpoint during the reearch process, more in-depth and nuanced findings based on that, and in its flexibility in using different methods.

In contrast tothe existing diagnostic assessment research dominated by the quantitative paradigm, the case study design can help to answer how diagnostic feedback information leads to diffrentiated instruction and learning, how evidence can be sought and interpreted for this purpose, and why certain subskils are more valued. A rich and descriptive case f a classoom-based and courseembedded approach to diagnostic assessment can promote learning by giving a practical framework for investigating validity, elaborating on the nature of evidence required for this purpose and how to analyze it. Case study research is not only an ideal approach to researching diagnostic writing aessment, but i can also capture the richnessofdata nessary to understand the multifaceted aspects of the classroom environment. There is increasing recognition of the situated nature of classroom-based asessment, and a case study research can render that context open.

At the time of data collection, I was a doctoral student in applied linguistics and taught the EAP course as part of my graduate assistantship. I had substantial experience of teaching writing to multilingual learners, tutoring at the writing center, and delivering workshops and attending conferences related to my field. These experiences had given me an insight into the needs of graduate students in general and international students in particular (I happened to be one mysel). I had designed university entrance examinations at my previous place of employment.

I taught the EAP course for the first time in fall 2021 and again in spring 2022. This study deals with spring 2022. As Istarted teaching the course, I became interested in exploring the value and impact of the existing diagnostic asessment procedure to make explicit its expected benefits, provide evidential basis of its expectations, and improve my practice. The point of this exercise was to illustrae an argument-based framework, in which the intended interpretation and use of the existing diagnostic asessment procedure is laid out in some detail, with an emphasis on the inferences being made and the assumptions required for these inferences to be plausible. I ought a coherent rationale for using the existing instrument It was my understanding that this instrument had been used in past iterations of the course, but a validty investigation had not been carried out before. With these ojectives, the study ddresses the following research question: To what extent i the existing diagnostic assessment procedure valid for assessing international graduate students' EAP writing?

Since the present study takes a grassroots level of analysis and provides a glimpse inside a clas, it i necessary to first outline the specific university and course context within which the diagnostic assessment procedure was situated.

# 4. Context

# 4.1. Institutional background

The study took place at a large state university in the Pacific region. 535 international graduate students were enrolled infall 2021. All doctoal students must write adissertatio, and some master's students are expeced to write a thesi or scholarly paper. Incoming students who have Englishas an additional language and do not meet any of the university's criteri for automatic exemption (which include evidence f academic English proficiency e.g., a minimum score of 7 on International English Language Testing System, IELTS) take a placement test and are placed into, or exempted from, EAP courses. Although these courses are non-credit bearing, they are mandatory for completing degree requirements. They cover intermediate and advanced levels of three kill-based areas: listening and speaking, reading, and writing. The course under discussion is one such writing course.

Due to the constraints caused by the CoviD-19 pandemic, students were not offered a placement test, which is otherwise held inperson. They were intead, placed based on proficiency tests and schooing in English-medium contexts. In many instances, students were given the opportunity to provide an academic writing sample to asst with the placement decisions.

Ongoing writing support resides primarily at the writing center that provides free consultations to the university community. The consultants are trained to asst at all stages of the writing proces. Additionall, there is sporadic writing support through depart mental or campus-wide workshops and trainings. It is reasonable to conclude that international graduate students are not heavily scaffold for their language r writing skils enhancement, and the provision for developing writing is deliveed primaril through the EAP or discipline-specific writing-intensive courses.

# 4.2. Curricular context

The 16-week advanced writing course followed a genre analysis aproach (Bhatia, 1997) to help graduate students learn the disciplinary conventions of their fields of study and gain linguistic competence (Swales & Feak, 2012). The main course aim was not to teach students how to compose the specific genres used in their major felds; rather how to observe and comprehend these genres by performing a linguistic and rhetorical analysis of the disciplinary texts. Students were asessed by six assignments as well as through other ungraded written pieces. Consistent with the course aims, the asignment constituted short papers through which students investigated, collcted, and analyzed models of the disciplinary conventions of their fields. Through these tasks, they learned to interpret tas onstrains, pratice intellectal proceses reuired in the ask,crat working drat, reive pr and tcher reonse, and learn to revise efectively based on that response. They participated in teacher-student group conferences (Ching, 2014) before each assinment was due, in which they read each other's papers and offere feedback. They also discussed anonymous samples as models. wales and Feak's Aademic writing for graduate students ssentil sill and tos was used as the textbook. The class met two days per weekfor seventy-five minutes. There was only one course section being offered, so I was the only one teaching this course.

Seven students were in class, hailing from countries like Nepal, Cambodia, and Japan, studying programs like computer science, applied linguistcs, and Asian studie. Their ages ranged from 21 to early-30 s. Allspoke English and at least one additional language.

# 4.3. Instrument

Diagnostic tests can be grounded in either theory or syllabus (Alderson, 2005; Huhta, 2008); the EAP course involved the latter. Students were administered a 70-minute diagnostic test in the first week of the semester through the university's learning management system (LMS). The test comprised a writing with sources task, an independent task that shows how a writer ollows conventions in order to place their text within a network of other texts. The prompt involved five quotations on the topic of time management, and students had to 1) identify one or more points on this topic that they found interesting or important; 2) analyze the point(s); and 3) support their analysis with information from at least two quotations, their own experience, observations, and/or background reading. In this way, the ask primarily involved the cognitive processes of analyzing, evaluating, and synthesizing. No word limit was specified, and dictionary usage was permissible. Students were encouraged to draft and revise ther responses within the 70-minute time frame. assesed the responses through an evaluation criteria, which focused on subskills ik content, organization, ocabulary, and grammar (including mechanics).

As a result, students were expected to demonstrate abilities such as what stance or perspective should be taken toward the topic, how to structure a text, what style to use, how to represent one's persona, and which lexical items were most appropriate to employ. was provided the test material with other instructional resources, and the test material was accompanied by a brief note advising instructors that they \*respond briefly' (emphasis original) and hold individual conferences with students if necessary.

# 5. Method

# 5.1. Participant

Emily? was the only student who volunteered as a participant. She was a motivated student and took great interest in class activities. She was a27-year-old, first-year doctoral student in marine biology. She had completed an undergraduate degree in veterinary medicine in Thailand in 2019 and was admitte directly into the Ph.D. program in fall2021, which was her frst time both visiting the U.S. and studying abroad.

In Thailand Emily had used English for academic purposes including communicating with her (then potential) advisor over Zoom. To prepare for her Ph.D. admission application, she took an IELTS preparation course. After completing the course, she continued to prepare for the exam by practicing writing, which she shared for feedback with her more proficient friends in Thailand. She spent two hours daily for three months to prepare and ended up with a 5.5 band score in writing.

When Emil started the Ph.D. program, shestruggled tocatch up with clasmates and faculty orally discussing scientific concepts. Even though everyday communication was not a barrier, she found it challenging to communicate scientific knowledge. In th irst interview, she highlighted the additional obstacle of switching to a new subject area: Its rell challenge me because first I have language barrer, and second I study in new field that I don't have any background knowledge before, so it's kinda like combination-two things.4 This remark documents the dual challenge that he had to contend with: coping with the intellectual demands of a new discipline and mastering a second language. Moving from one social domain to another requires adjusting writing, learning new skills, and transforming the knowledge one brings from previous experience.

# 5.2. Data collection

The proces for accumulating evidence sanned across the semester. The data sources included the diagnostic instrument, Emily's written response to it, my reflection joural, course matrials (syllabus, lesson plans, and handouts), semi-structured interviews with Emily at mid-semester and at the end of the semester (ee Appendix  for interview guides), Emilys six written ssignments including first drafts) in the EAP course, and two graded assgnments produced for two separate marine biology courses self-selected by Emily). Moss (2013) advocates such a data-driven approach where teachers employ multiple sources of evidence, which also allows to see connections and triangulate findings (Merriam & Tisdell, 2016). I believed that this collection of data adequately represented the

content of instruction and its effects on student learning.

The first interview was held in the seventh week of instruction, and the second was held one week after the course had ended. Both interviews were held over Zoom, audio recorded, conducted in English, and lasted 16 min and 27 min respectively. In both interviews, I shared my screen with Emily to show her the test prompt and her response, in order to stimulate her memory of the test. I acknowledge that one possble limitation of this data was that the teacher-student relationship that I shared with her was likely to have influenc hr reonse. the, I adeery ffot to push hr tocrtilly refet on her reonse. tanrid the interviews using Otter.ai. I exported the downloaded transcript in a Microsoft Word document and listened to the audio again to revise errors in automatic transcription. I aimed to get the transcription as accurate as possible.

Emily's two graded assignments written for her marine biology courses were a study question and a pressrlease. The study question was a page-and-a-half manuscript review of an article meant for journal publication along with guiding questions (Table 2 includes is excerpt). The one-page press release related to sea turtle tumors. The study question was completed in week 7, and the press release in week10. I used these sample to benchmark the disciplinary expectations Emil had to meet and to connect the writing she produced in the EAP classrom with the writing goals that were valued beyond the classroom. These samples provided a window into her larger experience of writing in her major, however brief or subjective. To me, such insight was vital to making informed judgments about her academic writing needs.

# 5.2.1. Developing the Validity Argument

My past experience in the EAP course helped me in designing and conducting the argument-based validation. This process commenced with Chapelle et al.'s (2015) framework. Adapting their framework, I operationalized validity as comprising six core infernces: domain deintion, eluation, fedack, utilization, exrapolation, and decisions. Table 1 prents the clasification of the six inferences, their warrants and underlying assumptions, and their corresponding sources of evidence.

For the framework to serve the context in which the diagnostic test was employed, I made several modifications to Chapelle et al.'s (2015) model, and borrowed from Knoch and Elder (2016) for these changes. The warrants for utilization and decisions inferences were tailored to represent the study's aims, which was to make explicit the expected benefits of the asessment, provide evidential basis of its expectations, and improve my practice. Domin dention rfers to the tart construct or theory about which tes evelopers want to draw conclusions and the decisions those conclusions inform (Moss, 2013). In my approach, the domain was EAP writing, which was operationalized as writing produced for both the EAP course and other discipline-specific writing-intensive courses and included both writing and language ability. Fig. 1 provides a Toulmin diagram to llustrate the evidentiary argument for the first assumption of domain definition inference, drawing on Oliveri et al. (2019). Evaluation, eedback, and utilization inferences were distinguishd in the sensehat theealution nference pertind to the dianostic test and ts reonse eack inference constituted feedback rortig .. intrctions or gudne tht stdents reive r ctionable nfomation for tchrs; and tiiztion inference was what both students and teachers decide to do with the information, and was consequently more action-oriented.

One point of departure from both Chapelle et al. (2015) and Knoch and Elder (2016), was the generalization inference, which I excluded since psychometricreliabilit and the notion of different test occasions were irrlevant to my purposes, nor did I have numeric codes to compute a generalizability score. Instead, I added the assumptions from Knoch and Elder's decisons inference. Additionall, they suggest two more pertinent assumptions under decisions: 1) "Leamers taking up support options improve their English over the course of their studies", and 2) Learners who fail to act on test recommendations are more likely to struggle in their academic studies" (p. 25). These are more ambitious claims and eliciting evidence for these warranted a longitudinal pproach which was beyond the scope of this rearch; consequently, they were not incorporated. In this way, the availability of data afected this framework. In a nutshell the conceptualization of the validity argument views diagnostic asesment as the way the test is designed, the response it yields, and the pedagogical decisions it impacts.

# 5.3. Data analysis

I adopted a more naturalistic inquiry and analyzed the data qualitatively on an ongoing basis, connecting it to the validity in. ferences. As Pellegrino et al. (2016) point out, in contrast to studies of large-scale asessments employing statistical models, in \*the context of lassroom assessment, the interpretation is often made les formal by the teacher and is usuall based on an intuitive or qualitative model rather than a formal statistical one (p. 64). Thus, the analysis was more interpretive and reflective rather than code-focused.

Using the assumptions framing each inference in the argument-based framework that I designed, data analysis started with a consideration of the data type and its relation to the underlying assumptions. I created a table in Microsoft Excel to map the evidence. related inferences according to specific test assumptions and pasted relevant data against each assumption. Table 2 shows an example of how data related to the first assumption of domain definition inference (i.e. The task and evaluation criteria capture aspects of performance that are relevant to EAP course aims and goals") was categorized. used the labels of met partilly met, and did not mee as delimiting categories for analysis. I purposely kept these somewhat simplistic labels to conclusively answer validity questions and to assist other teachers attempting a similar exercise. ooked for patterns in the data and made notes if the evidence was complementary or divergent I also weighted each evidence for the intensit of its coverage of inferences. In instances where I found diversion, I made further investigations by re-examining the data. In cases where I could not reconcile differences, I termed those as partiall met.

Table 1 Argument-based approach for validating diagnostic assessment of EAP writing.   

<html><body><table><tr><td>Inference</td><td>Warrant</td><td>Assumption</td><td>Evidence</td></tr><tr><td>1. Domain definition</td><td>Observations of students&#x27; performance on the diagnostic instrument reveal knowledge, skills, processes, and strategies that align with those required for EAP writing.</td><td>1. The task and evaluation criteria capture aspects of performance that are relevant to EAP course aims and goals. 2. The characteristics of the assessment tasks offered via the test are broadly relevant to the writing tasks required in students&#x27; other university courses.</td><td>1. Diagnostic test document 2. Student&#x27;s diagnostic test response 3. Course syllabus 4. Teacher&#x27;s reflections 5. Student&#x27;s writing samples produced for the course</td></tr><tr><td>2. Evaluation</td><td>The diagnostic instrument is an accurate and useful method of broadly identifying Students&#x27; strengths and weaknesses in EAP writing.</td><td>1. Test instructions, purpose, and tasks are clear to all test takers. 2. Test is pitched at appropriate difficulty level. 3. The task and evaluation criteria have the capacity to broadly identify test takers&#x27; individual strengths and weaknesses as writers.</td><td>Other courses 1. Diagnostic test document 2. Student&#x27;s diagnostic test response 3. Teacher&#x27;s reflections 4. Student&#x27;s interview</td></tr><tr><td>3. Feedback</td><td>Diagnostic feedback is fine-grained, well communicated, and well tied with future. learning.</td><td>1. The descriptive feedback to students identifies their strengths and weaknesses at the subskill level and provides recommendation(s) on future actions. 2. The feedback is detailed, clear, specific, and timely. 3. The follow-up recommendation for students is appropriate and targeted at the subskill level. 4. The follow-up recommendation</td><td>responses 1. Diagnostic test feedback 2. Teacher&#x27;s reflections 3. Student&#x27;s interview responses</td></tr><tr><td>4. Utilization</td><td>Diagnostic results on the quality of EAP writing obtained from the test are useful for students and teachers to set curricular goals and make pedagogical decisions.</td><td>is closely linked to on-campus support. 1. Students can use diagnostic results to set their goals for the course and future improvement. 2. Teachers can use diagnostic feedback to sequence and plan instructional activities.</td><td>1. Student&#x27;s interview responses 2. Teacher&#x27;s reflections 3. Lesson plans and handouts</td></tr><tr><td>5. Extrapolation</td><td>Diagnostic results are relevant to students&#x27; EAP writing.</td><td>1. Test results are good reflectors of students&#x27; overall EAP writing ability and specific strengths and weaknesses in the EAP course. 2. Test results are good reflectors of students&#x27; overall EAP writing ability and specific strengths and weaknesses in other university courses..</td><td>1. Student&#x27;s diagnostic test response 2. Student&#x27;s writing samples produced for the course 3. Student&#x27;s assignments from</td></tr><tr><td>6. Decisions</td><td>The consequences of using the diagnostic and the decisions informed by theo. diagnostic test are beneficial to all stakeholders..</td><td>1. Test takers perceptions of the test and its usefulness are positive. 2. The feedback from the test is useful and directly informs students&#x27; future learning. 3. The feedback from the test is useful for teachers to make effective. instructional decisions. 4. Students act on the test recommendation (i.e., take up the proposed writing.</td><td>other courses 1. Student&#x27;s interview responses 2. Teacher&#x27;s reflections 3. Student&#x27;s writing samples produced for the course</td></tr></table></body></html>

![](img/e3d37534b4ca3bce31a6905a9aadb2531fa34617512fa920a70fb2fb3a69bfd9.jpg)  
Fig. 1. Toulmin diagram of the evidentiary argument for first assumption of domain definition inference.

Table 2 Connecting evidence against the first assumption of domain definition inference.   

<html><body><table><tr><td>Data Type</td><td>Relevant Excerpts</td><td>Meets Criteria</td></tr><tr><td>Diagnostic test document</td><td>Identify one or more points on this topic that you think are interesting or important and write a paper in which you discuss your analysis of the point(s). Support your analysis with information from the quotations,</td><td>Y</td></tr><tr><td>Student&#x27;s diagnostic test responsea</td><td>your own experience, observations, and/or background reading. Emily has understood the task correctly by presenting one synthesized analysis. Her response is in paragraph format. There are some expressions that she has used incorrectly.</td><td>Y</td></tr><tr><td>Course syllabus</td><td>By the end of the course students will be able to: analyze discipline and genre-specific academic English writing conventions and effectively apply that</td><td>Y</td></tr><tr><td>Student&#x27;s writing samples produced</td><td>knowledge to graduate level writing tasks Feedback on assignment 4:</td><td>Y</td></tr><tr><td>for the course Teacher&#x27;s reflections</td><td>Emily chose very useful aspects to base her analysis on. Her use of examples made her analysis very clear. The task of writing from sources is suitable. The test did predict students&#x27; performance. Emily is good. It</td><td>Y</td></tr><tr><td>Student&#x27;s assignments from other</td><td>pinpointed their [students] major areas of weaknesses and strengths. Study question:</td><td>Y</td></tr><tr><td>courses</td><td>Please review this paper on the following criteria:</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td>Data: Are data sufficient? Are methodologies and treatment of data appropriate?</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td>Conclusions: Are they justified by the data presented?</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td>Reasoning: Can it be followed easily?</td><td></td></tr><tr><td></td><td>Writing: Are grammar and style appropriate?</td><td></td></tr><tr><td></td><td>Illustrations: Are quality and quantity appropriate?</td><td></td></tr><tr><td></td><td>Length: Is the manuscript an appropriate length?</td><td></td></tr></table></body></html>

a For Emilys dagnostic est eponse and writig produced for the AP course, used my impressons of her reponse and not the esponse itel, which I felt was more useful.

For each inference, I wrote analytic memos (Saldana, 2009) noting what was achieved and what was not, documenting the rationale for interpretations I drew on past research and iterature in the field to come up with explanations and maintained a critical stance throughout by questioning my assumptions to gauge if my analysis was accurate. Besides, in keeping with the practice of member checking (Merriam & Tisdell 2016), Emily read an earlier draft of this article. She did not suggest any changes to it

# 6. Findings

In this section, I will present my findings for each of the six inferences.

# 6.1. Domain definition inference

The diagnostic instrument appeared to support the domain definition inference (Table 3). The EAP course entails analysis of disciplinary texts, and the cognitive processes captured in the test task were perceived to be aligned. Afer analyzing the test responses, I wrote in my journal: \*The task of writing from sources is suitable. The test did predict students performance. Emily is good. It pinpointed their [students] major areas of weaknesses and strengths."

Furthermore, analyzing Emily's assignments for her marine biology courses, I noted that properties like analytical arguments, personal opinion, and formal tone were required, especially for the study question assgnment. These assignments also demanded knowledge of citing references, which matched with the test task. In this way, I tied the domain of academic writing to the task type and writing and language abilities observed in the test response, evaluated through the evaluation criteria, with both the EAP course and Emily's disciplinary courses.

Nevertheles, one issue with the test prompt was that the quotes did not carr all referencing details (e.g., publisher's name or page number) that were integral to producing an appropriate citation. Revising this element will yield more relevant responses.

# 6.2. Evaluation inference

The evaluation inference was partially met (Table 4). Intilly, my perception was that th firt assumption was completely met. In light of my oserations of students peformance in clas onthe test day, did not note any lack of clarity on their part. However, since Emily's previous EAP writing experience was largely limited to admission preparation, I recognized that diagnostic test tasks set expectations for international graduate students since these are among the first writing assignments they do in universties. These students ften \*ome from edcational ltures with rge classizs that ony value summatie standardized assessments (Doe, 2015, p. 110). This understanding provoked critical questions: Do they understand this asessment form? Are they full aware of is purpose?

These reflections led me to reexamine my initialassumption and look for more evidence to satisfy it full. When I posed this question to Emil in her post-semester interview, her remark reflected that her understanding of what diagnostic asessment is bears more resemblanceto proficiency testing:  think diagnostic tes i writing that help to aess my writing abilit, my sill. She did not regard diagnostc asssment as understanding her trengths and weaknesses as a writer, but instead an evaluation of overallskill Her lack of understanding indicates that I hould have spent more time explaining the tet purpose before administering it t students since they were not familiar with this assessment form. She also admitted that she had never attempted a diagnostic test before. To summarize, I concluded that the first assumption was partially met.

For the second assumption, I observed that the task was at appropriat difficult level: \*The quotes were on a topic general enough to be undertood by all students. hre is niilin-spif fous, which s ie (Techer's refletions).  intrted dfficulty" as the task presenting background knowledge or genre-based chllenges and not being qually relevant to student from all disciplines. In my opinion, the subject mater of time management was general enough and I expected all students to write about it. Explaining the characteristics of an appropriate topic, Read (1990) makes the point that the contents of the question should neither challenge students' knowledge and interests, nor be too simple or predictable.

Therefore, in my view, the task was of appropriat difficult level. The evidence from Emily's first interview, however, was to the contrary. She disclosed that she did not find the quotes very easy" and she had to "spend some time to understand the quotation" Nevertheless the time given for the task was deemed "appropriat." She pointed out that she had not done such a writing-from-sources task before. Emily's viewpoint is understandable because her prior writing experience was limited to her IELTS exam preparation. The two writing tasks in the IELTS exam involve writing from visual sources and an opinion-based essay (IELTS, n.d.). The properties of these tasks are distinct from the diagnostic test task, eliciting different communicative functions.

As far as the third assumption was concerned, I concluded positively. The evaluation criteria flagged individual grammar-related areas, which ranged from plural forms to parallelisms.

Table 3 Domain definition inference.   

<html><body><table><tr><td>Assumption</td><td>Finding</td></tr><tr><td>1. The task and evaluation criteria capture aspects of performance that are relevant to EAP course aims and goals.</td><td>Met</td></tr><tr><td>2.The chtristict at s f vi th t e y e t th wig  re sdt t ore.</td><td>Met</td></tr></table></body></html>

Table 4 Evaluation inference.   

<html><body><table><tr><td>Assumption</td><td>Finding</td></tr><tr><td>1. Test instructions, purpose, and tasks are clear to all test takers.</td><td>Partially met</td></tr><tr><td>2. Test is pitched at appropriate difficulty level.</td><td>Partially met</td></tr><tr><td>3. The task and evaluation criteria have the capacity to broady identify test takers individual strengths and weaknesses as writers.</td><td>Met</td></tr></table></body></html>

# 6.3. Feedback inference

The feedback inference was refuted (Table 5) because there was no structured feedback reporting mechanism provided, making it challenging for me to convey effective feedback. Adhering to the instructions for teachers that they comment \*briefly, I limited my feedback to a few sentences:

Emily, you have understood the task correctly by presenting one synthesized analysis There are some expressions that you have used incorrectly, for instance It is cannot deny' should be It cannot be denied. In the course, intiall focus on paragraphing.

This information poorly identified Emily's strengths and weaknesses at subskills levels, nor did it carry any recommendations. Though the incsion f the subskill crried rarc support (Km, 2011), dd nt make efetive use f thealuation criteria nor did I srive to check students' understanding of my feedback. I also chose to keep the post-test conference optional as suggested. Another drawback of this feedback that Emily identified was that it was restricted to task-level and not broad enough to apply to her general writing competency. In retrospect, this deficiency can be attributed to my own lack of understanding of diagnostic test's purpose. While whatever limited feedback was given may be categorized as clear and specific it was not detailed and seemed inadequate.

Moreover, I was unable to communicate the fedback in a timely manner to the students due to a technical glitch on the LMs. In my first interview with Emily, I discovered that she had not seen my feedback. I spent two weeks trying to reolve the problem, and finally communicated the feedback to the students in week 9.

# 6.4. Utilization inference

The utilization inference was met partiall (Table 6). Even in the absence f clear guidelines as to how the diagnostic fedback wil assist in making pedagogical decisions, I, the teacher, was able to use this information; however, the same cannot be claimed for students. Giventhe absence of any recommendations, it became difficult to investigate the feedback effc on lening and whether or not students followed the recommendations. In her post-semester interview, Emily was unable t identify any particular way in which she used the feedback.

Despite it limitations for student purposes, I used the feedback to make several instructional decsions. It allowed me to see the connections between course content and writing assignments. I analyzed students' performance colectively and identified two common problems: paragraphing and paraphrasing. I prioritized these two areas of need. For paragraphing I felt that more instruction time was required, so I devoted wees 5 and 7 to it. I made these adjustments in order to make connections to students' immediate needs more tangible. I decided to firt focus on composing a coherent paragraph, paying special atention to conclusion paragraphs. In addition, I commented on this subskill in my response to students' writing. For paraphrasing, I designed one lesson in week 14. When designing a sylaus, I usually keep the fous of few sesions to be decided so as to utilize thoseto deal with students spcific needs. capitalized on his space in the syllabus to fus on paraphrasing. Lasty, I trked studets progres in their specific grammar areas in their future asignment fedack. I made sure I commented on these individual subskill in their writing assignments. I relied on my reflection notes throughout the process to track my instructional changes and corresponding students' progress.

# 6.5. Extrapolation inference

It can be concluded that the extrapolation inference was supported (Table 7). As stated previously (Section 6.1), I was able to get a sense of students' overall witing ability and specific trengths and weaknesses for designing intruction through their test responses. In Emily's specific case, I tracked her improvement in two specific subskills: paragraphing and preposition errors. For paragraphing, she continued to display improvement. Her final asignment had information arranged neatly in sections, with well organized paragraphs and logically sequenced sentences. For grammar, I did not notice any particular preposition errors. I did, however, notice two tense errors where she incorrectly referred to a past activit n present tense errors circled in Fig 2). The colored tracked changes are Emily's disciplinary-area instructors' markings.

For the second assumption, I traced the subskills identified as needing improvement in the diagnostic (i.e., paragraphing and preposition usage), in Emily's performance in her marine biology courses as evidenced by her two assgnments. Regarding paragraphing, I noticed that these assnments rquired her to respond briefly and directly in a limited space. Aparently, the paragraphing standards that applied to the test in particular and the AP course in general did not seem to apply in her disciplinary courses. Based on the limited evidence contributed by the two assignment, it can be claimed that paragraphing was perhaps not a subskill most relevant to extrapolation. Since these writing tass were completed in the early to mid-point of the semester, I did not use them for investigating support for the feedack inference, ensuring that the evidence eeded to support a decision was kept relevant t its timescale (Mos, 2013).

Table 5 Feedback inference.   

<html><body><table><tr><td>Assumption</td><td>Finding</td></tr><tr><td>1. he ive   he s a tklee  e ti.</td><td>Not met</td></tr><tr><td>2. The feedback is detailed, clear, specific, and timely.</td><td>Not met</td></tr><tr><td>3. The follow-up recommendation for students is appropriate and targeted at the subskill level.</td><td>Not met</td></tr><tr><td>4. The follow-up recommendation is closely linked to on-campus support.</td><td>Not met</td></tr></table></body></html>

Table 6 Utilization inference.   

<html><body><table><tr><td>Assumption</td><td>Finding</td></tr><tr><td>1. Students can use diagnostic results to set their goals for the course and future improvement.</td><td>Not met</td></tr><tr><td>2. Teachers can use diagnostic feedback to sequence and plan instructional activities.</td><td>Met</td></tr></table></body></html>

Table 7 Extrapolation inference.   

<html><body><table><tr><td>Assumption</td><td>Finding</td></tr><tr><td>1. Test results are good reflctors of students overall EAP writing ability and specific strengths and weaknesses in the EAP course.</td><td>Met</td></tr><tr><td>2. Test results are good reflectors of students overall AP writing ability and specifi strengths and weakneses in other universty course.</td><td>Met</td></tr></table></body></html>

![](img/6b679d4f8ad7dd52a6bc8329675a3197ff89a6baa958107145f959629ef40c3c.jpg)  
Fig. 2. Example from Emily's final graded assignment for the course.

The second subskill was verb form and preposition errors. Samples from her asignments (Figs. 3 and 4) displayed that she made similar errors in her disciplinary courses as well, which provides evidence that this subskill was likely to be more relevant to her performance in other university courses.

Besides, I noticed that the two assgnments were heavily edited. Whereas my written feedback approach focuses more on textual macrostructure, the gist and lines of reasoning employe in the paper, and limits grammar correction to major errors only, it appeared that Emily's disciplinary-area instructors had tried to correct every error. The pres release was evidently meant for publication and hence, was more high-stakes writing that perhaps required higher accuracy level, but the same could not be assumed for her study question assignment. This observation is interesting for two reasons. One, it ffers an explanation why the grammar subskill peared to be more relevant. Due to their knowledge and training on effective written feedback compared to subject/content specialists, writing instructors are likely to focus more on discourse-level aspects and are more tolerant of grammatical rrors, whereas, subject/ content specialists focus on grammar (Kepner, 1991). And two, this observation sheds light on how international students navigate different teacher atitudes regarding written corrective fedback. In her post-semester interview, Emily admitted that most of her feedback in her major courses tended to carry emphasis on grammatical erors, which she appreciated: I think it's rell helpful for me, and I don't mind to learn English grammar from another subject too because it's like, I think writing is not about just leaning in writing clss.It's like about practice. This misalignment in feedback approaches generates some insights for extrapolation inference for diagnostic writing assessment.

![](img/5e19f773c8fe014887b458c6b7055133d22fd466d57bbadbddbd7dc1648b2d03.jpg)  
Fig. 3. Example from Emily's first assignment (Study Question).

![](img/5676be4934053d52c426934a3efaeb97cfe702bb8115cc3088aab7025081c8ff.jpg)  
Fig. 4. Example from Emily's second assignment (Press Release).

# 6.6. Decisions inference

The decisions inference was supported to a limited extent (Table 8). Reflecting on her test experience, Emily found the test frutful in the sense that students could track their progress through the course:

I think about diagnostic test I think it's good to useit as a beginning because so at the end student will know like how they can improve afer taking the class. And I think I like the quotation thing that you can like because its similar to the thing that like got student have to, so I think it's a good way to see what student have and can improve in the next like after you finish the clas. (Emily first interview).

Analysis of Emily's writing produced for the EAP course suggested improvement, particularly in the areas of employing appropriate academic vocabulary and better organization. Information was not arranged in paragraphs in her diagnostic test response i contrast, it was properly organized in her thirdassignment. Though shestil tended to write paragraphs that could be broken down further and were relatively lengthy, she displayed overall improvement in this area. Her final asignment demonstrated further improvement compared to her mid-semester performance. In her post-semester interview, she agreed with this finding. Furthermore, her self. perception of her writing abilitie changed. In the irs interview she rated her competency as \*average"; in her post-semester inerview she rated it as "above average,' though not feeling confident enough to label it as "advanced."

For the ret of the students, I noticed an improvement in paragraphing through their course assgnments. Despite Emily's positive perception and evidence of performance improvement, the absence of follow-up activities or any specified mechanism for tracking progress marred the test's impact. In this way, this inference illuminates the ways in which I used the test and other evidence relevant to students learning in my own context to make decisions about my practice. Hence, the study responds to a wider research call on teachers' using data in educational systems to implement positive change (Moss, 2013).

# 6.7. Synthesis of findings

Overall, out of six inference, the inferences of domain definition and extrapolation were perceived to be met completely, and the evaluation inference was also perceived to be met largely. On the other hand, the utilization inference was considered to be met partially, decsions to a limited extent, and feedback was not met. It seemed to me that more or les the absence of proper feedback mechanism limited the test use, somewhat weakened its impact, and reduced the potential for learning.

Table 8 Decisions inference.   

<html><body><table><tr><td>Assumption</td><td>Finding</td></tr><tr><td>1. Test takers&#x27; perceptions of the test and its usefulness are positive.</td><td>Met</td></tr><tr><td>2. The feedback from the test is useful and directly informs students&#x27; future learning.</td><td>Partially met</td></tr><tr><td>3. The feedback from the test is useful for teachers to make effective instructional decisions.</td><td>Met</td></tr><tr><td>4. Students act on the test recommendation (i.e., take up the proposed writing development strategies).</td><td>Not met</td></tr></table></body></html>

# 7. Discussion and proposed modification

# 7.1. Discussion of findings

This case study presented a more interpretive and qualitative examination of an in-house, classroom-based diagnostic assessment procedure. Unlike existing research dominated by the quantitative paradigm, ths case study helped to answer how diagnostic feedback information leads to differentiated instruction and learning and why certain subskills are more valued. It elaborates on how evidence was gathered and interpreted to confirm and disconfirm the efectivenes, relevance, and appropriatenessof the diagnostic asesment procedure. The rich data from the case study gave opportunities to quote Emily and to bring forward her viewpoint as a valuable data source of validity evidence. It provided me with a window on her thinking, knowledge, and understanding.

Attempting to create and apply the validity framework bolstered my understanding of diagnostic assessment. The argument-based validation approach was a useful tol because the evidence gathered and interpreted in this processallowed me to describe and document learning. During the appraisal stage, I adopted a critical stance (Kane, 2021). As I read literature and reflected on my practice, conducting the study aded to my own diagnostic competence (Edelenbos & Kubanek-German, 2004; Huhta et a., 2024.

Apart from identifying shortcomings in feedback reporting, the validity investigation also identified what changes should be made to the test content (e.g., adding referencing details to the quotes), and what other language assessment ssues are present (e.g, variation in writtenfdback approaches between EAP instructor and discipline-specific instructors. Hence, the validity argument is the centerpiece of the entire assessment process from test development to impact.

The validation investigation fleshed out the tension in the assessment process with differing teacher-student perceptions emerging in two respets: test dfficult and urpose. The finding that mily misperceiveddiagnostic asessment s proficiency teting, reaffirms Doe's (2015) asertion that international graduate students ften have familiarity primarily with proficiency testing. This limited prior experience hapes and colors their understanding of other test types, including diagnostic. They are les likely to have been exposed to diagnostic assessment, highlighting a need to determine ways to raise their awareness of what it is and how it functions.

Besides, the critical inquiry yielded an interesting finding with respect to the extrapolation inference, which is adifference in EAP and non-EAP instructors approaches towards written corrctive feedback. Disciplinary area instructors often disproportionately foreground surface errs relative to other content-based concerns. Therefore, they may be limited in some aspects of their feedback (Kepner, 1991), which perhaps brings to surface a useful quality of a diagnostic test and EAP writing instruction more generally-a capacity to focus on those facets of writing on which students will not get much feedback in their disciplinary courses. On the other hand, EAP teachers tically downplay the stle and correctess fres of writing, in favor of ecouragng exploration, tenttivenes, and eventual understanding. These approaches are underpinned by instructors views of language itself. These conceptions should promote a critical social and rhetorical view of language as opposed to a prescriptivist or standard view. Hence, the validity inves. tigation made explicit the differing values at work in the processes of test design and use.

Overall, the study draws attention to the aspect of diagnostic feedback reporting in course-embedded approaches. The findings underscore the point that a diagnostic test's purpose will be severely compromised if diagnostic feedback is not acted upon by the teacher or delivered to the students. In this regard, the instructions to teachers should be made clear.

7.2. Proposed modification to the diagnostic assessment procedure: meeting the feedback inference

In response to the shortcomings identified in the feedback inference, I put forward one revision to the diagnostic assessment procedure which consists of a sample feedack report for students (Appendix B) and guidelines for teachers. My focus on the feedback inference was driven by the recognition that it was the key inference, and the subsequent inferences of utilization and decisions logically followed from it. The fact that this inference was unmet undermined the other two inferences as well.

The proposed worked example blends key categories from Knoch (2011), write feedback locating trengths and identifying gaps, excerpts from test response (for students to know which specific areas the comments applied to), and fllow up activitie constituting annotated resources. Doe (2015) emphasizes that for feedback to lead to learning, students need resources and communication channels. Table 9 provides a snapshot of how the proposed model compensates for the drawbacks identified earlier so that better informed and more targeted feedback could be given.

Table 9 Modifications to the diagnostic assessment procedure to support the feedback inference.   

<html><body><table><tr><td>Assumption</td><td>Modification</td></tr><tr><td>1. The feedback to students identifies their strengths and weaknesses at the subskill level and provides recommendation(s) on future actions. 2. The feedback is detailed, clear, specific, and timely..</td><td>Written feedback locates strengths and identifies gaps, accompanied by specific recommendations targeted at subskills level. Students are given written feedback report and their original test response, followed by a one-on-one, teacher-student conference with the following agenda: 1. clarify the test purpose,</td></tr><tr><td></td><td>2. explain the feedback report,. 3. understand the processes entailed in the production of the test response by posing reflection questions, and</td></tr><tr><td>3. The follow-up recommendation for students is appropriate and targeted. at the subskill level. 4. The follow-up recommendation is closely linked to on-campus support..</td><td>4. decide a future plan for reporting progress on each area.. The follow-up recommendations constitute annotated resources targeted at subskills level.</td></tr></table></body></html>

For teachers, in line with Huhta et al.'s (2024) recommendation, I suggest that this report should be followed up with a one-on-one. post-tes onference with students to beter communicate the fedback and come up with a plan, deciding about reporting progress on each area. This interaction is vital because merely handing out the feedback report might stil leave room for misinterpretation or technical glitches. Carles (2006) described this process a "aessment dialogue' in which teachers make the asessment criteria explicit to students (p. 230). The conference could be another opportunity for the teacher to clarify the test purpose and actively engage the students in the process.

Though I could not continue this project because I did not teach the same course the following semester, I did share this feedback model with other teachers in our monthly program meeing so that they would have a roadmap of what to concentrate on when they revised assgnment or updated leson plans. The practice of sharing assessment data to drive program and instruction improvement is in line with Conference on College Composition and Communication ccc (2022) guidelines for writig asessment. The proposed feedback model can also be particularly advantageous for institutional contexts with limited language/writing support structures. Practitioners at other institutions could recommend central support centers that they have on campus (such as, academic learning units, graduate communication or academic units, or English lening centers) to connect students with relevant resources.In the study site, since the nature of academic support students receive depends upon their home department, I chose selfacces resources to encourage autonomous learning.

# 8. Limitations and future research

One limitation of the study is that it focuses on only one student's performance and perception as the basis of analysis. Because students in the course come from a range of disciplines, each with their unique rhetorical features, the question of representative sample of students' disciplinary writing arises. This concern of asessment in similar cross-disciplinary curricular spaces is a larger, complex issue (see Sanchez & Kenzie, 2016, for further discussion) and more research is needed in this regard.

Further research can extend the present study's analysis to evaluate the long-term benefits of the proposed modification, whether it leads to any sustained improvement in students' awarenes of scholarly writing and motivation to be slf-directed in their lerning. The data can also be supplemented with lesson video-recordings, especiall on the test day, or students periodic rflections. Longitudinal studies can trace students' engagement with other on-campus resources.

Future research into validity of diagnostcinstruments assesing EAP writing should attend to the difference in EAP and non-EAP instructors' approaches towards written corrective feeback, with respect to the extrapolation inference. This aspect is particularly relevant in light of ongoing citique of notions of tandard English and models of correctness to promote critical language awareness (Shapiro, 2022).

# 9. Conclusion

This case study was a teacher's attempt at validating an in-house developed diagnostic assessment procedure within an EAP course. The research integrated a student's perspective of receiving, comprehending, valuing, and acting upon the diagnostic feedback Together, these perspectives of both key stakeholders shed ight on what meaning isascribed to the diagnostic asesment procedure in promoting teaching and learning.

Instructors in EAP courses routinelyassign a writing task in the first clas to informally diagnose students writing abilities. The research underscores that diagnostic asessment can play a crucial role in living up to high expectations in EAP courses and showcases how informal diagnostic procedures in these classrooms can be systematized to be more accurate. Table 1 is intended as a concetually rich and inclusive tool for other teachers for planning and implementing validation research on existing tests and practices, while the findings provide clear guidance for the validation of particular interpretations and uses.

Unlike other studies that report on selected inferences (e.g., Doe, 2015), the present study makes a valuable contribution to argument-based validation studies by focusing on al six inferences. The research also shows that argument-based validation exercises offer potential grounds for an outreach effrt between EAP teachers and disciplinary area expert to coordinate to support student needs, which is a shared responsibilit. Such collaborative explorations can help to achieve consistency, curricula symmetry, and shared goals in writing instruction.

# CRediT authorship contribution statement

Rabail Qayyum: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Resources, Validation, Writing - original draft, Writing - review & editing.

# Data Availability

The data that has been used is confidential.

# Acknowledgements

I am deeply grateful to Dr. Daniel Ibell, Mr. Kenton Harsch, and Dr. Betsy Gilliland in their ssistance with the project. I am also grateful to Emily for participating in this study. I would also like to thank the reviewers for their constructive fedback.

# Appendix A

# Interview Guides.

Interview 1: Mid-Semester.

Could you tel me about your educational background? Where did you complete graduate and undergraduate education? When did you come to the US?

Wnat aspecis or wriung uo you enyoy?   
What do you want to improve upon in this course?   
What are the challenges you face in your writing?   
Did you face any difficulty in understanding the instructions of the test?   
Did you find the timing of the test adequate in completing it?   
What aspects of feedback on your diagnostic test did you find useful?   
What did you understand about your writing skills after receiving feedback on the diagnostic test? Interview 2: Post-Semester.   
How relevant was the diagnostic test to the activities completed in this course?   
How useful was that information? In what ways can it be made more useful?   
What in your opinion is the purpose of a diagnostic test?   
Did you learn anything about yourself as a writer after getting the feedback?   
Did you take any steps after getting the feedback?   
Would you like there to be a word count for the response?   
Did you use the writing center? Why? Why not?

# Appendix B

Sample diagnostic feedback report for Emily.

<html><body><table><tr><td>Category</td><td>Feature</td><td>Writer&#x27;s strengths and areas to improve</td><td>Sample text</td><td>Follow-up recommendations</td></tr><tr><td>Content</td><td>Task completion</td><td>The response fulfils task requirements by presenting one synthesized analysis using two quotations. However, there is no</td><td>It is cannot deny that time management is the key to success in work and life.</td><td>The following handout outlines how to structure analytical essays: https:// writingcenter.unc.edu/tips-and-tools/ essay-exams/</td></tr><tr><td></td><td>Use of source material</td><td>clear thesis statement. The writer correctly incorporates the ideas from two quotations with no major errors of referencing.</td><td>It&#x27;s true as Guitton said, &quot;It is important to prepare oneself and one&#x27;s environment for peak performance&quot;.</td><td>This handout carries explanation and activities on how to summarize, paraphrase or quote another source: https://writing.wisc.edu/handbook/ assignments/quotingsources/</td></tr><tr><td>Reader/writer interaction</td><td>Style and stance</td><td>Although the writer uses appropriate tone and register throughout the essay, the writer repeatedly uses &#x27;we&#x27;, which at times includes the reader and at times not. It is unclear if &#x27;we&#x27; refers to students</td><td>To be good in time management, we have to access what is the proper environment for work.</td><td>This book presents concrete strategies for improving prose: Williams, J., &amp; Bizup, J. (2016). Style: Lessons in clarity and grace (12th ed.). The University of. Chicago Press.</td></tr><tr><td>Accuracy</td><td>Vocabulary</td><td>specifically or people generally. Overall, the writer uses adequate vocabulary sufficient for the task with minor errors in places.</td><td>To be good in time management, we have to access what is the proper environment for work.</td><td>This resource includes explanation of a wide variety of common usage errors: https://brians.wsu.edu/common- errors/</td></tr><tr><td></td><td>Grammar</td><td>The writer writes accurately on most occasions, but makes few errors in areas of verb tense, missing pronouns, and preposition.</td><td>According to, we have a different preference, for example, some people more productive in the early morning time because it is quieter. In my opinion, I do agree that sometime finish easy work first because it will give me a sense of productive and motivation for</td><td>This self-help book carries activities to improve grammar: Wallwork, A. (2013). English for academic research: Grammar, usage and style. Springer. This website carries interactive exercises to explore various grammar topics: http://sentencesyntax.com/</td></tr></table></body></html>

(continued)   

<html><body><table><tr><td>Category</td><td>Feature</td><td>Writer&#x27;s strengths and areas to improve</td><td>Sample text</td><td>Follow-up recommendations</td></tr><tr><td></td><td></td><td></td><td>But in the same time I don&#x27;t prefer to accumulate big work and procrastinate it until the deadline come.</td><td></td></tr><tr><td>Fluency</td><td>Text length</td><td>The response carries 324 words (class average was 483) which can be improved further.</td><td>What is key success in time management ?</td><td>This handout describes a range of brainstorming strategies: https://writingcenter.unc.edu/tips- and-tools/brainstorming/</td></tr><tr><td>Organization</td><td>Cohesion and coherence</td><td>The paragraphs in the middle are clearly organized, but the introduction and the conclusion carry single sentences and need further development.</td><td>In conclusion, understanding the nature of working environment of ourselves and priorities tasks is the key to the success of time management.</td><td>This handout carries signposting activities with explanation: https:// www.monash.edu/learnhq/write-like- a-pro/improve-your-writing/write- clearly/signpost-to-guide-your-readers</td></tr><tr><td>Mechanics</td><td>Spelling and punctuation</td><td>The response is generally free from major errors in spelling, formatting, and punctuation.</td><td>It&#x27;s true as Glutton said, It is important to prepare oneself and one&#x27;s environment for peak performance&quot;.</td><td>Google documents can be used to compose texts. Errors appear underlined in blue and corrections are also suggested.</td></tr></table></body></html>

# References

Alderson, J. C. (2005). Diagnosing foreign language proficiency: The interface between learning and assessment. Continum.   
Aleron,  t        d    m sptie across diverse fields. Applied Linguistics, 36(2), 236-260. https://doi.org/10.1093/applin/amt046   
Aldrson, J ng    laka, 2015)T dsf r    fo g   
hati (197 is   n  19 Information Agency.   
Cars,  0f  i the k  i  2193./..100500600572132   
Chapel,  o,   . (2015). at mns for dti t sn m witin ato.  et (3) 385405. https://doi.org/10.1177/0265532214565386 power: Theory and practice in peer review and response for the writing classroom (pp. 15-28). Fountainhead Press.   
Conf .2i  /c resources/positions/writingassessment).   
Creswell, J.w. (2013). Qualitative inquiry and research design (3rd ed.). Sage.   
Cran 0   t     252 s   
Do, . 2015.  o ti    , 12110135././01080303.01100292   
Dolgova,  & e 2019. st from th gdp  ad ldtiastc ame rre i at AP context. Journal of English for Academic Purposes, 41. https:/doi.org/10.1016/j.jeap.2019.100771   
urn 1t   t  8  ) al  n     .5-7.m it ./.07/11086989.0.   
Edeenbos, , & banek-an,  (204). cer asment: he ncet of dsic ompeece Lge tin, 213, 259-283. p/oi.org. 10.1191/0265532204lt284oa   
Fox, J..009g  ic t  t   rl  th  o ti Academic Purposes, 8(1), 26-42. https://doi.org/10.1016/j.jeap.2008.12.004   
Fx J.  i    t- d (Ed.), Postadmission language assessment of university students (pp. 43-65). Springer.   
Freeman, D. (1998). Doing teacher research: From inquiry to understanding. Heinle & Heinle.   
Huhta,A2008.sti d ftiveasent n  ky,  t (.), Th doo f ion ingstis (p. 469- 482). el.   
Huhta, A., Harsch, C., Leontev, D., & Nieminen, L. (2024). The diagnosis of writing in a second or foreign language. Routledge.   
ELTS ..  i th  w  022  /g-/r il academic-writing/format#tab-2).   
Jng  r . 1)ti   r  .,   o  p 187-205). Routledge.   
ne, M. 013.l tht  f  s.  f  01) 3././1200   
ane, M . (2021). ticting valdt amet  G uhr, L Hdng ds.), The Rue hdook f lge ttin (. 32-47). Re.   
Kepner, C. . (191). n et in the ohp f tf  fack t o d- wig kil. Thene Journal, 75(3), 305-313.   
Km,Y. H. (201. Din A wig at using the  ri ned mdl. e ting 84), 509-541.ht/i./10.1177 0265532211400860ltj.sagepub.com   
och .11   r tic  i   he  ike a theriri e rsing 162) 81-96. https://doi.org/10.1016/j.asw.2011.02.003   
nch, U. r, . 016) tis  as  sit  tc a t st . .)  n assessment reearch and practice The view from the Middle East and the Pacific Rim (p. 210-230). Cambrige Scholars Publishing.   
Kolka  ti  4 1-12. https://doi.org/10.1016/j.asw.2020.100450   
Mangedr,   ker, 018.  awi ms w  dici df whi do wting Journal of Response to Writing, 4(1), 4-33. (https://scholarsarchive.byu.edu/journalrw/vol4/iss1/2).   
Merriam, S. B., & Tisdell, E. J. (2016). Qualitative research: A guide to design and implementation. John Wiley & Sons.   
Moss, P. A. (2013). Validity in action: Lessons from studies of data use. Journal of Educational Measurement, 50(1), 91-98.   
ivri,    0    ti problem-solving assessments. International Jounadl of Testing, 19, 270-300. https://doi.org/10.1080/15305058.2018.1543308   
Pelrio . i 6. rrk or ing an ig th vt  intil le et. Educational Psychologist, 51(1), 59-81. https://doi.org/10.1080/00461520.2016.1145550   
ndall J., P,  lomp, D, liri,  (2024).r valdit os ike ustce. s your ge etng, 41(1), 20-219 s:/i.o/01177 02655322231202947   
Read, J. (1990). Providing relevant content in an EAP writing test. English for Specific Purposes, 9, 109-121.   
Saldana, J. (2o09). The coding manual for qualitative researchers. Sage Publications Ltd.   
Sanchez, F,& Kenzie D. (2016). Of evolutions and mutations: Aesment as tactics fr actio in WAC parnership. The WAC Joumal, 27 119-141.   
Shapiro, . (202). ivating crticl lange awaes in the wiing clssom (irt ed.). Rtlege tps/i.rg/0.4324/9781003171751   
Swales, J. M., & Fak, C. B. (2012). Acdmic writing for gdate students: sti skils and tasks (rd ed..). University f Michigan Pres.   
Toulmin, S. E. (2003). The uses of argument (Updated ed.). Cambridge University Press.   
ston qel,   . (2013).tic ig f Ho g  dn gish ag rofici Th t d altin o DELTA. Hong Kong Journal of Applied Linguistics, 14(2), 60-82.   
Xi Q. 017t n n i in   o, 1 647 https://doi.org/10.1080/01443410.2016.1202900