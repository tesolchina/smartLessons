We begin this ditorial for Volume 61 of Asessing Writig by reporting the excellent news that Asessing Writig's Impact Factor (IF) according to the Journal Citation Reports 2024 increased from 3.9 to 4.2 for the JCR year 2023. This has placed us as 6th out of 297 journals in Linguisics and 28th out of756 journals in Education and Educational Research. Together with our iteScore of 6.00, which has placed the journal in the 96th percentile and 36/1088 in Language and Lingustics, the IF metrics confirm and accentuate the significant and growing place that our journal has in the linguistics and education fields. We are mindful that journal metrics can fluctuate, and that they reresent just one measure of quality and impact. However, t is good to take a moment to acknowledge where Assesing Writig currently sitsaccording to these metrics and to thank all those stakeholders whose work and endeavours contribute to our standing in the academic community.

Another measure of our success as a journal can be seen in the increasing number f submissions we ultimately accept for pub. lication. Many submissions do not make t into the per review proces, and those that do re subject to rigorous review. In this volume. we present 16 paper covering a range of subjects from a variety of countries. We also include our ols and Tech section, containing an overview article and three papers that sit within that section.

Building on the assertion that prior studies into second language (L2) Chinese writing complexity have neglected various Chinese structures, the study by Hao, Wang, Bin, Yang, and Liu drew on a Chinese L2 writing corpus to examine syntactic complexity and quality in English-speaking Chinese learners' writing. Large-scale analysis revealed T-unit and Clause counts impacted scores by $1 4 . 5 ~ \%$ . It was found that syntactic diversity alone affected scores by $2 4 . 7 ~ \%$ . Also, regression with fine-grained indices explained $4 3 . 7 \%$ of the variance. These insights, the authors argue, emphasise the importance of detailed syntactic analysis for understanding English-speaking Chinese second language (ECsL) writing.

Qayyum's article starts from the premise that research on diagnosing writing sill has overlooked how feedback informs per. sonalised teaching and learning. The author goes on to present a case study that examined a teacher refining an in-house writing assessment for English for Academic Purposes. Validity was established through evidence collction and interpretation, primaril by analysing one student's experience. Findings suggest that inadequate fedback hampers test utilit and learning outcomes. Qayum recommends a revised assessment procedure with sample feedback reports.

The study by Xie explored whether two reading-to-writing tass could enhance the Duolingo English Test's (DET) representation of the writing construct and its ability to predict English academic writing proficiency. Tasks, given to undergraduates from Hong Kong $( \mathtt { n } = 2 0 4 )$ , included summarising two texts and writing an essay based on five texts. Professional raters ${ \mathfrak { m } } = 3$ ) assessed the writing. samples using detailed rubrics. Analyses, including descriptive, regression, and Structural Equation Modeling, confirmed the tass' contribution to how the writing construct is represented in the DET and affirmed its moderate predictive power for academic writing The study discusses practical implications, particularly the intrcate relationship between understanding writing and predictive validity.

Wang and Wang focused on Chinese as a second language, known for it challenging character writing system, and explored the comparabilit of computer-based (CB) and paper-based (PB) writing asessments. Using the many-facet Rasch model (MFRM), dif ferences in text quality were examined. Analysis of keystrokes and handwriting trace data revealed insights into the writing proces. Results indicated that the CB mode produced higher-quality texts with fewer character mistakes and more eficient revision and pause patterns. Findings emphasise the need to consider language and learner characteristics when selecting asessment modes.

The research by Dong, Zhao and Buckingham analyses trends in writing asessment over the past 30 years (1993-2022) using bibliometrics. Their analysis included 1712 articles and 52,092 references, and identified popular research topics, ifluential publications and transformative research. Findings showed increasing interest in technology and cognitive processes. Key publications indicated a shift towards interdisciplinary research, with changing prominent journals over the decades. Recent transformative research suggests future directions, such as integrating computational methods and exploring factors affecting writing quality. The study, they argue, informs the field's development and has implications for researchers and practitioners.

The study by Yasuda investigated how complexity in form and meaning affcts the quality of argumentative essays written by Japanese high school students learning English as a Foreign Language (EFL). Esays from 102 students at various proficiency levels were analysed, focusing on form (like vocabulary and sentencestructure) and meaning (specificall, argument strength). Findings showed that while form complexity plays a role, meaning complexity, particularly effctive argumentation, strongly influences essay scores. This suggests a need to emphasise argument quality over linguistic complexity when teaching and evaluating high school EFL writing.

Abdel Latif, Alsuhaibani, and Alsahil utilised an 8-dimension framework to investigate Saudi university students preferences for English writing feedack compared to their teachers' actual practices. Quantitative and qualitative data were gathered through separate surveys completed by 575 undergraduate English majors and 82 writing istructors acros 11 universte. Analysis revealed significant discrepancies between student preferences and teacher practice across various feedback dimensions. Qualitative insights from open-ended responses provided further context on feedback belief. The study concludes by discussing these findings and their educational implications.

Cai and Yan explain that verbatim source use (vsu) in integrated argumentative writing tasks may increase the complexity of writing. Ths asstance could benefit test takers unevenly based on their writing skill, raising concerns about farness and validity. They note that previous research has focused on how source use relates to proficiency levels, but suggest that it effect on linguistic complexity needs further exploration. Their study analysed 3250 argumentatie esays from a university-level English Placement Test (EPT) using natural language processing. It identified 34 linguistic complexity features and three VU characteristics, finding that VSU had a subtle direct influence on writing complexity, possibly due to diverse writing approaches. However, no significant indirect effect through proficiency were observed. These results hed light onVsU's role in argumentative writing, supporting validity claims in integrated writing assessments..

The paper by Saeed, Abusa'aleek, and Alharbi is built on the premise that it i crucial to explore how different methods like written comments, audio recordings and creencasts impact how teachers evaluate e-feeback linguisically. Their research, using the Systemic Functional Linguistics appraisal framework, examined how these modes influenced an instructor's evaluative language in efeedback and text revisions among 15 pairs of Saudi EFL learners. The analysis revealed distinct patterns: screencasts and audio feedback used more expansive resources to encourage dialogue, while written fdback tended towards more directive language and suggested corrctions. SCreencasts addresse broader writin issues, whereas audio and written feedback focused more on specific detail. While bh ccst and aud feback neraly led omre successul reisions, iffes w td in gloal rvisions This study provides insights for instructors on effectively supporting students' writing development.

Kim and Hammillstart from the premise that directed self-placement (DSP) lets students choose their writing placement, but it is less common in second language (L2) education. Their study examined how students' educational backgrounds, such as attending English-medium high schools or intensive English Programs (IEPs), relate to placement decisions. They compared 804 students' self placements with actual examination results. Most students' DSP choices aligned with their exam placements, but many placed themselves ifferently. Students who attended English-medium high chools tended to place themselves higher than their examresults. The study sugests that DSP can work for L2 programs, but needs careful consideration of learners' backgrounds, skills and selfperception.

The study presented by Pun, Tan, and Li investigated the prevalence and precision of four discourse competence (DC) aspects of writing - informatio flow, stance display, reader engagement, and discourse structure knowledge - in research reports by Hong Kong ESL secondary students. Findings indicated that information flow elements made up less than $9 \%$ of report length, with accuracy between $6 4 . 3 1 \%$ and $8 0 . 3 3 \%$ Also, stance display elements were under $3 \%$ in length, with accuracy from $5 8 . 3 3 \%$ to $9 8 . 1 7 \%$ . Third, reader engagement elements were under $2 \%$ , with accuracy ranging from $3 3 . 3 3 \%$ to $8 8 . 6 4 \%$ . Finally, mastery of discourse structure varied widely from $0 \%$ to $1 0 0 ~ \%$ in accuracy. The results highlight areas for improvement in students' discourse competence and suggest a framework for future research in discipline-specific writing.

Peng suggests that, while research on learner engagement with feedback is growing, how L2 learners interact with feedback under different conditions remains underexplored. Peng's study investigated how individual versus collborative processing of teacher feedback impacts writing development in Chinese lower-secondary EFL learners. Eighty-one 13/14-year-olds with A1-A2 English proficieny participaed, reiing daile feack n the wriin tass and proessig it ether indvidll or collaboratiely over six weeks. Pre, post- and delayed post-ests were conducted to asess writing complexity, acuracy, fluency, content and organisation. Results indicated no signficant ifference in writing complexit or fluency between the condtions, but collaborative rocesing led to more sustainable accuracy improvements. Both conditions significantly enhanced content and organisation scores over time. Implications for teaching and future research on feedback processing are presented.

Peng et al. explored the link between various levels of absolute syntactic complexity (SC) and expert-asssed quality in 446 argumentative writing sample from college-level Chinese EFL learners. Using the computational tols TAAssC and L2SCA, the study found that absolute SC measurements explained $4 2 \%$ of the variance in overall writing scores. It was found that noun phrase (NP) complexity was crucial in expert evaluations of writing quality; large-grained indices like MLC, CN/C, and CN/T effectively repre sented SC and predicted quality; fine-grained elements such as prepositional phrases and relative clauses were significant for NP complexity, relative clauses and adjectival modifiers ofered unique insights into NP complexity, prepositions notably increased large. grained NP complexity. These results contribute to evaluating L2 writing through the lens of absolute syntactic complexity.

Weng, Guanfang Zhao, and Chen start from the premise that, although research on improving students' writing feedack literacy has grown, effective methods remain underexplored. Their study sought to address this gap by examining how peer fdback activities impact various aspects of feedback literacy in EFL writing, including feedback appreciation, judgment, source recognition, affect management, and action-taking. Over a 12-week semester, one class engaged in pee feedback activities (experimental group) while the other received only teacher fdback (control group). Comparing pre- and post-intervention results from a fedack literacy scale, teacher and student interviews, and student writing revisions revealed that peer fedback significantly enhanced students' appreciation of and judgment about fedback. However, no notable improvements were found in other areas. The researchers argue that these findings offer new insights into EFL writing feedback literacy and have important teaching implications.

The study by Zhang and Zhang examined how EFL students' syntactic complexity evolves using Latent Class Growth Analysis (LCGA). The study required college students from Southwest China ${ \bf ( n = 2 1 4 ) }$ ) to write four argumentative essays over a semester. Unconditional LCGA models identified varying developmental paths for six measures of syntactic complexity, indicating diverse L2 development trajectories. Conditional LCGA models, however, found that English proficiency did not predict which developmental path students followed. The findings are discussed with implications for L2 teaching.

Kim, Kim, and Heidari investigated two methods for computerised summary analysis: model-based and text-based aproaches, both rooted in mental models and discourse comprehension theories. The researchers compared these approaches to understand their shared knowledge dimensions and how they asess changes in student summaries over time. Using cases with initial and final summary versions ${ \bf ( n = 1 0 8 ) }$ , they analysed 216 observations using correlations, Principal Components Analysis (PCA) and Linear Mixed-Effects models. The exploratory results identified key tex-based measure, and PCA revealed that both approaches covered the three dimensions of summaries (surface, structure, and semantic). Model-based measures were more efective for tracking surface changes, text-based measures for structure, and both methods were useful for semantics. The study highlights the value of combining multi dimensional measures for formative feedback on students' summaries.

By way of introduction to the journal's Tools and Tech section, Hartwell and Aull note that the 2024 Tools & Technology forum highlights the impact of new writing technologies on assessment practces in the post-coviD-19 era, stressing the need for these innovations to adhere to validity, fainess and equity principles. They suggest that AI tools show potentialfor improvement but must e evaluated to enure they accurately reflect writing constructs, align with ducational gols and support far ssesment practice. They question how well tool capture writing complexity and their effcts on education. Fairness focuses on cultural responsiveness and accessbilit, while equity addresses systemic inequitie and aims to support diverse learning styles. Reviews of tols like PERSUADE 2.0, EvaluMate, and a systematic review app demonstrate how these innovations can enhance valid far and equitable asessments. The forum advocates for continuous discussion and adaptation to foster inclusive and equitable educational experiences.

Crossley et al.'s research methods article presents the open source PERsuADE 2.0 corpus which includes over 25,00 argu. mentative essays written by 6th-12th grade students in the US for 15 prompts on two writing tasks: independent and source-based writing. Furthermore, the corpus presents detailed individual and demographic information for each writer. It is anticipated that the PERsuADE 2.0 corpus will support research into relationships between discourse elements and their effectiveness, quality of writing, writing tasks and prompts, and demographic and individual writer differences.

Guo argues that peer feedbacki crucial for enhancing learning in writing classes, but delivering efective feedback can be chal. lenging for students. To tackle this ssue, this article introduces EvaluMate, an Al-powered per review system utilising ChatGPT, a large language model LLM), to asst students in generating feedback. The article details EvaluMate's design and features, emphasising how it aids students in providing comments on their peers' essays. It also addresses the system's limitations and suggests potential improvements. Aditionally, the article call for further research into student engagement with thi I-driven approach and its effect n learning outcomes. The aim is to encourage exploration of AI technology's potential in writing education and assessment.

According to Agrawal et al., systematic reviews provide detailed insights into the purpose, methodology, and findings of a particular area of focus. Various toos can be used to conduct such reviews. By performing systematic reviews, users can uncover key trends and influential works in their field. They assert that many researchers strengthen their reviews by using the PrisMA-2020 checklist and flow diagram. Their paper introduces a web-based aplication that visualises the PRIsMA analysis and flow diagram, benefiting stakeholders like researchers, students and publication entities. They argue that the application improves article quality assesment and student writing evaluation by offering visual displays of study resuls and reference analysis, including publication year, entry type, authors, keywords and journals.