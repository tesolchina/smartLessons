# ESL Writing Assessment Prompts: How Students Choose

CHARLENE POLIO MARGO GLEW Michigan State University

This qualitative study examines how ESL students choose a prompt from several options on a timed-writing exam. This issue is worth investigating for several reasons: Little is known about the writing process on timed-writing tests; previous quantitative attempts to examine factors affecting student choice have been inconclusive; and opinions vary on whether or not students should be given a choice. Twenty-six students were observed taking a writing exam and were interviewed upon completion. We conclude that students spend little time making a decision; that several factors including their own background knowledge, question type, and specificity of the topic intluence their decision; that attention to the time factor is an overriding consideration.

This article addresses one small part of the writing process of English as a second language (ESL) students in an assessment situation. Specifically, it examines, through qualitative methods, how ESL students choose a prompt on a writing exam when faced with a task that offers them a choice. This issue is worth investigating for several reasons: Little is known about the process of writing under time constraints; previous attempts, using quantitative methods, to examine factors affecting student choice of topics have been inconclusive; and opinions vary on whether or not students should be offered a choice.

Most of the research on the writing process examines, and not inappropriately, the overall writing process from beginning to end. Since it is widely accepted that writing is a recursive process, it is difficult for researchers to focus on the various stages of writing because they are not discrete. Most people argue that as writers write, they are thinking of new ideas and reworking what they have written. During a writing test, when a student is faced with a choice of prompts, he or she must choose a prompt before continuing. This is a step in the writing process in this particular situation, although not necessarily as discrete a step as one might believe. Furthermore, the research on the writing process tends to examine writing over longer periods of time than a student has for a writing test, or over several drafts (Krapels, 1990). One reason is, perhaps, that the current approaches toward writing hold that students need time to plan, write, and revise, and, to use Zamel's (1982) words, "discover meaning" as they write.

In Horowitz's (1986) classic article contesting what he believes to be tenets of the process approach, he states that some people believe that in-class tests do not constitute "real writing"; that is, they are simply artificial classroom tasks. Hamp-Lyons (1986) and most others put little value in the claim that inclass writing, or timed-writing, is not "real." Clearly, as she argues, timed-writing is a process which involves a series of steps, just as writing an essay, article, or some other piece of longer writing does. Timed-writing is a process, however, which, we believe, researchers know much less about. Furthermore, in our opinion, if students are having to do timed-writing in their academic courses, then it is by definition "real." Examining the writing process in timed situations is worthwhile, even if we assume that timed tests are less than ideal tasks on which to assess ESL students' writing (or knowledge in academic courses). Despite a trend toward portfolio assessment, timed-writing tests still exist (Hamp-Lyons & Mathias, 1994), and will most likely continue to be used, particularly as placement tests for ESL students.

A second justification of this study is that it adds to a small but growing body of qualitative research on language testing. Cohen (in press) argues for the value of examining students' test-taking processes. Such research has been conducted on listening tests (Buck, 1991) and reading tests (Cohen, 1984). HampLyons and Mathias (1994) discuss the importance of examining students' testtaking processes on writing exams:

Our study had made clear to us the need to collect observational data on developing writers who are nonnative users of English, to learn what writing skills, processes, or problems they find difficult and what skills, strategies, or solutions work for them. Such studies are now rather common in first language writing research, but they remain the exception in second language writing research, and those that do exist do not take as their target situation the writing assessment experience. (p. 63)

Qualitative methods, such as observations and interviews, are particularly useful in an investigation of prompt choice because quantitative studies have not been conclusive. For example, Chiste and O'Shea (1988) examined university-level ESL students' papers from a test given to both native and nonnative speakers. Each test gave students four options for a 2-hour essay. It appeared that the ESL students were more likely to choose the shorter questions (i.e., those containing fewer words) and those that were placed first or second. But since the shorter questions were placed first or second, the researchers could not make any firm conclusions, and thus they concluded, "Without further student interviews, an explanation for this pattern is not possible"' (p. 683).

The final, and most important, reason for studying this issue is that the lit.

erature on whether or not students should be given a choice of prompts is inconclusive. Opinions can be found on both sides in the literature and in practice. Examining how students choose prompts may give us some insight on whether or not students should be given a choice. As Kroll (1991) points out, some large-scale tests (such as the Test of Written English [TWE] and the Michigan English Language Assessment Battery [MELAB]) do not offer students a choice of prompts, whereas many tests, particularly many university placement tests, do.

The primary reason for offering students a choice of prompts is the belief that students should be allowed to choose a prompt that will enable them to display their best writing. This is assuming, of course, that such prompts are in the realm of the kind of writing being tested. If it is in fact the case that students' writing varies with different task variables, students should not be unfairly penalized, one might claim, by being forced to write on one particular topic. ESL students, in particular, come from a wide range of backgrounds. The same placement exam may be given to a Korean engineering undergraduate who has been attending a U.S. high school for 3 years and to a 40-year-old African student who has been teaching history in his country for 15 years. Can we create a prompt that allows them both to display their best writing skills? In fact, Raimes (1990), in her article raising concerns about the TWE, argues that giving the same task even to both undergraduate and graduate students is problematic.

The extent to which differences on writing exam prompts affect students' writing is, however, not clear. In her review of the research, Hamp-Lyons (1990) cites some studies that show significant differences in scores depending on the prompts, while others do not. This claim is repeated in Hamp-Lyons and Mathias (1994). They state, "Classical statistical methods have typically been used, but are unable to provide sufficiently detailed information about the complex interactions and behaviors that underlie writing ability"' (p. 50).

Carlman (1986), for example, shows that mean scores vary significantly on type of writing but not on topic within each type. However, when the papers examined in this study were divided into dichotomous pass/fail groups, a chisquare test showed significant differences in the scores of individual students based on topic. Spaan (1993) studied MELAB writing exams in which each student was given two different prompts on which to write and found no significant difference in the scores on the two essays.

We still do not really know, however, how the prompt affects writing because conclusions about writing quality are based on the raters' final scores. And as Hamp-Lyons (1990) states, it could be that raters compensate for more difficult questions by giving higher scores. These scores would lead us to believe that the tasks are of equal difficulty when, in fact, they may not be. This point of view is elaborated on in Hamp-Lyons and Mathias (1994) who found that topics judged more difficult by experts received higher scores, and those judged easier received lower scores:

In interpreting these data we must consider the possibility that essay readers are consciously or unconsciously compensating in their scoring for relative prompt difficulty based on their own, internalized, difficulty estimates. Since the two essay readers we used in this study are also actual scorers of the essays in the data set of 8,538 essays used to obtain the mean writing scores for prompts and prompt categories, such a compensatory mechanism would tend to negate the expected effect of prompt difficulty on scores. (pp. 59-60)

They appropriately claim that such strategies on the part of the raters may explain the results.1

In addition to the notion that the difference in the prompts will not affect the final outcome, there are two more reasons for not offering students a choice of prompts. First, offering a choice of tasks, in theory, decreases test reliability on holistic scoring (Evans, 1979; Jacobs, Zinkgraf, Wormuth, Hartfiel, & Hughey, 1981; Hughes, 1989; Kroll, 1991; Ruth & Murphy, 1988); giving students a choice adds an additional source of measurement error. There are dissenting opinions. Hoetker and Brossell (1986) have argued that more variety on writing exams may keep the readers more alert and interested, thus increasing reliability, yet they do not provide empirical evidence to support this claim.

The second reason for not offering students a choice is that it has been claimed there is no evidence that when students choose a prompt, they choose one that allows them to display their best writing skills, and that forcing them to make a choice simply wastes their time. We know of no study, however, that investigates differences in students' performance under the two conditions (i.e., with and without a choice).2 Kroll (1991), citing Ruth and Murphy (1988), argues that students may not choose prompts wisely. Tracing this claim to empirical research led to two studies on native speaker writing, one involving U.S. university students (Freedman, 1983) and one with Canadian secondary students (Evans, 1979). Freedman (1983) claims that the prompts in her study which students found to be "dull and difficult' received the same scores (i.e., no statistically significant difference) as prompts the students found "easy and interesting."' Since the students' opinions of the prompts were determined before they wrote on them, Freedman claims it could be that those prompts perceived as easy were discovered to be difficult when the student wrote the essay or, similarly, dull prompts became interesting once the student delved into the topic. An alternative explanation is the previously mentioned possibility that raters were compensating for the more difficult topic, and thus no difference in essay quality could be detected.

Evans (1979) bases his claims on a report of a colleague's unpublished study which shows that less able students (as indicated by scores on a reading and language test) were more likely to choose one prompt, while the better students were more likely to choose two other prompts. This could be what was happening in the Hamp-Lyons and Mathias (1994) study; the so-called easy prompts had lower mean scores because the poorer students chose them. Evans claims that these choices disadvantage poor students further, but this is not a reasonable conclusion. It could be that the poorer students are doing better than they would have had they been forced to write on the prompts chosen by the better students. He uses this study to say that restricting the students' choice is more fair than offering them a choice.

The point most agreed upon in the above literature is that giving students a choice can affect test reliability.3 Other than that, much of the research is contradictory and inconclusive. Furthermore, we know very little about why students choose various prompts and what processes they go through in doing so. Thus, we propose the following research questions in this study.

1. How long does it take students to choose a prompt?   
2. Do they change their minds and waste time?   
3. What criteria do they use when choosing?   
4. Do students prefer having a choice of prompts?

While the answers to such questions cannot definitively tell us whether or not it is better to give students a choice, seeing how students choose a prompt should help clarify some of the above claims and be a basis for further empirical research.

# METHOD

# Participants

Twenty-six students enrolled in either an intensive English program (IEP) or an English for academic purposes (EAP) program at a large U.S. state university participated in this study. The IEP students were from the third-level IEP class, the class one must pass in order to be allowed to matriculate at the university. (Usually, the majority of the students in this course intend to go on to study at a U.S. university.) The EAP students were concurrently enrolled in university courses but showed a deficiency on the writing portion of the placement exam.

These groups were chosen for three reasons. First, these two levels have the largest number of students providing the largest pool of potential participants. Second, the students' spoken proficiency was good enough to participate in a postwriting interview conducted in English. Third, and most important, the writing exam used in this study has a great effect on the lives of these students since it is both the placement and exit exam for both groups of students. The IEP students did not pass thc placcment exam (which includes also listening, reading, and grammar) and thus had to enroll in the IEP courses. Their subsequently being able to enroll at the university depends on their passing the exit exam. The EAP students were allowed to enroll in their major courses but had to take an additional ESL course costing them time and money. The EAP students not passing the exit exam must repeat the course. The participants included 9 Chinese speakers, 7 Koreans, 7 Japanese, and 1 each Arab, Dutch, and Malaysian students.

# Exam

The writing exam given to the students was in the format of the actual placement/exit exam used for ESL students at the university. Normally, three prompts, chosen randomly from a database of about 200, are presented to the student. The one- or two-sentence prompts in the database elicit the rhetorical modes dcscription (personal narrative), comparison/contrast, and argumentation. Some call for highly personal writing such as in "Describe one of your grandparents," while others require the students to write about general current events (such as space exploration) or controversial issues (such as smoking in public places). The students have 30 minutes to write, and the essays are graded on a scale which includes language use, vocabulary, organization, content, and mechanics. While this test may be less than ideal, we wanted to use a test that was similar to the actual test our students had taken and would be taking again. For the simulated exam in this study, we chose three questions that we felt represented the three different rhetorical modes possible on the exam. We also wanted to include at least one personal and one less personal topic so that we could determine if these factors influenced the students' choices. The three questions are listed in Appendix A along with the number of students who chose each option.

Students in the classes chosen were asked if they would be willing to participate in a practice exam which was the same format as the placement exam and the final exam for their class. Almost all agreed since we offered to give them feedback upon completion.

The students all participated at various times in the second half of the semester (i.e., after having completed about 7 weeks of writing instruction). They were first given a consent form to read and sign. While they read the form, one of the researchers adjusted a video camera focusing on the student and the essay. The student was then given the instructions with the three questions, each written on a different piece of paper and stapled together so that they could read only one question at a time. The researcher usually left the room at this point and returned 30 minutes later. The student was interviewed following a guided-interview procedure which was videotaped. The questions from the interviews are listed in Appendix B. At the end, the student was given feedback on the essay.

The videos were examined to see what kind of behavior the students exhibited when choosing a topic. The students' behavior at the beginning of each writing session was observed, and it was noted how many seconds each student spent reading the instructions and looking at each question. The remainder of the tape was watched to see if the student, at any point, went back and looked at any other prompt during the exam.

Observation was chosen over talk-aloud protocols, in part, because we believe that the protocols can often change the writing process. Specifically for this study, students would probably take more time choosing a topic if they had to talk about it while they were doing it. Furthermore, we were interested in only the first few minutes, and it would not have been worthwhile to explain to the students how to go through the procedure for the entire exam when we were concerned only with the beginning. Having them talk about what they were doing only for the first few minutes would probably have caused them to focus too much on that part of the writing process.

# RESULTS

# Observations

Figure 1 shows two examples of how the students' behavior was recorded. The number next to each prompt is the time in seconds that they spent looking at each question, and the arrows show the sequence of where their attention was focused. The first example is typical, and the second example is quite unusual.

In presenting the data, we can only add up the amount of time the students spent reading each prompt before they actually began writing notes or the essay; we cannot make any inferences about what the students were actually doing during that time. (Only 6 out of the 26 students wrote notes of some kind before beginning the essay; the other 20 began writing the essay immediately.) This period of time represents the maximum amount of time it could have taken a student to choose a prompt, because it is likely that choosing a prompt is not a discrete step; while the students are reading the prompts they are probably actively thinking of ideas and/or attempting to plan the essay.

![](img/83ab2c0ed520070023ba9c89c173f71f66027019577f0e705ac63836d1df9df3.jpg)

![](img/7458a546bc0535b3f06562c1a7ebca9ef919f88afe1aa0f529f56fc8ae2dbc50.jpg)  
Figure 1. Students' prewriting behavior.

Examining the amount of time before the students began writing helps us address the issue of whether or not students are wasting valuable time choosing a prompt. The median time before students began writing was 59 seconds, with a range of 18 to 182 seconds. (With a mean of 73, these times were not normally distributed but, rather, positively skewed.) Fifteen of the 26 students looked at each prompt only once before beginning to write, and only 1 student started to write and then changed prompts. Even being conservative about what the students were actually doing before writing, that is, assuming every minute before writing was spent choosing a prompt, the amount of time the students took to choose the prompt was not very large, and very few seemed to have difficulty deciding.

# Interviews

The interviews provided us with the most revealing information about how the students felt about the prompts and how they made their final decision. The first question was exploratory, asking general information about their writing process on the 30-minute essay. Despite the superficial nature of the question and brief responses, it was useful to gain a little information about the steps the students said they went through when writing an essay in such a situation. While all of them said they brainstormed or outlined in their heads, most said they had no time to write anything before beginning the essay. Furthermore, most said that they did not have time to go back and reread what they had written.

Twenty-one out the 26 said they were able to choose a topic "quickly." Of those 5 who said it took a while to decide, 1 took about 2 minutes, while another took only 39 seconds. One who said it took 5 to 10 minutes took 122 seconds. In general, most students did not feel this step took a long time. They seemed to understand the importance of choosing quickly on a timed test.

Only two of the students said they regretted the choice they made and felt that they could havc written a better essay if they had chosen a different prompt. Three of those who felt they had made the right choice emphasized that there was no time to even consider the other prompts; they directed their thoughts only toward the prompt they had chosen..

Various reasons emerged as to what affected the students' choices. These are listed in Table 1. They include a student's perceived background knowledge of the topic, the generality or specificity of a prompt's topic, the rhetorical structure elicited by the prompt, a student's interest in the topic, and a student's knowledge of the appropriate English vocabulary.

Twenty-two of the students said that they chose or did not choose prompts based on how familiar they were with a topic or how much they had to say about it. For example, many said of the smoking prompt that they simply did not like smoking but could not support their opinion; they would need evidence from medical studies or knowledge of law. On the other hand, some students said they wrote on the topic because they had a strong opinion about it. A few students rejected Prompt #2 because they believed that they had lived in the U.S. too long to write about aspects of their native country, or that they had not been in the U.S. long enough to know about its culture. Others felt that Prompt #2 was easy because they had recently been thinking a great deal about the differences between the U.S. and their native country. Prompt #1 was chosen by some students because they had an experience that they had recently been thinking about and thus felt close to the topic.

TABLE 1 Reasons For Choosing Specific Prompts   

<html><body><table><tr><td>Reason</td><td>Students</td></tr><tr><td>Perceived familiarity or background knowledge</td><td>22</td></tr><tr><td>Generality or specificity of a prompt</td><td>12</td></tr><tr><td>Perceived rhetorical structure elicited by a prompt</td><td>7</td></tr><tr><td>Interest level in topic</td><td>2</td></tr><tr><td>Knowledge of appropriate English vocabulary</td><td>2</td></tr></table></body></html>

Twelve students said they did not choose topics that were too general since it would take too much time to narrow the topic and decide what to write about. Of the three questions, Prompt #3 was the most narrow in that the actual topic of the essay was given to the students. To write on Prompt #1 or Prompt #2, the student had to choose an experience to write about or choose which aspect or aspects of the U.S. and their native country they wanted to compare. The 5 who chose the smoking question all mentioned that they did not choose the others because they were too broad and would take too much time to narrow down. Yet the 7 others who mentioned this still chose one of the broader topics.

Seven students stated that rhetorical structure or essay type was a factor in their choice. One student said that his teacher had told them that compare/contrast was the easiest type of essay to organize, and 1 student said that his teacher had emphasized this type of essay in class. Another student felt that description/narration was easy, while argumentation was hard. Two felt that personal description essays were difficult to organize; 1 student specifically said that it was not academic writing. Yet 2 others believed that personal narratives were easier since they were more like telling a story.

Two students commented that they wanted to write about topics that were fun or interesting. One of them specifically stated that Prompt #2 was not fun because during the past year he had constantly been comparing the U.S. and his native country and he was bored with doing so. He chose Prompt #1 even though it was difficult, because, "I like to have fun when I write."

And finally, only 2 students mentioned the importance of being able to write about the topic in English, particularly the importance of knowing the correct English vocabulary. One of these students wrote about a bad experience he had had with a bank. His essay contained rather sophisticated banking vocabulary he had learned from the experience. Another had wanted to write about being rejected for a U.S. visa in Taiwan, but did not, since she felt her vocabulary of the appropriate English terms was lacking. She said she had learned the importance of this when taking the placement exam; she had begun to write about a topic she knew much about but ran into difficulty when she realized that she did not know the appropriate English vocabulary.

In answer to the last question, 24 of the students believed that it was better to have a choice of prompts on a timed-writing test. When asked how many choices they should have, most said three, 1 said two, and 5 said more than three. Only 2 students believed they should not be offered a choice. One student felt that he was too indecisive, and the other felt that even though he preferred to have a choice, having essays on different topics made it harder to compare the essays when grading them; that is, reliability was sacrificed.

# DISCUSSION

This study raises a number of issues. First is the question of whether or not students waste time wavering between questions taking valuable time from their writing. Even assuming that students spend all their time before beginning to write on choosing, we cannot say that they spend very much time making a decision. Most students felt that they chose quickly, and most preferred having a choice. However, all of the students in this study had taken a 30-minute writing exam at least twice before, and many, probably more often. Their attention to the time factor pervaded their interviews. For many, the time factor changed their writing process; they said they did not have time to go through steps they normally went through in writing an essay and, more significantly, steps they were being taught in their ESL classes. The time factor also affected their choice of prompts; some felt they did not have time to narrow down a broad topic. Another student chose a prompt (the personal narrative) that she believed did not require her to plan what she would say before writing so that she could begin writing more quickly. This contradicted what she had been taught in writing class. This attention to the time factor is probably indicative of the students' experience with such tests. We do not know how students taking the placement exam without such experience deal with the choice of prompts. It is certainly possible that they do waste valuable time choosing a prompt.

It was clear from the interviews that students had various strategies for choosing a prompt and writing a timed essay. Regardless of the fact that some of the strategies may be inappropriate for out-of-class writing, the fact that students are using such strategies does not imply that timed-writing tasks for assessment should be eliminated. We must first consider the appropriateness, outside the context of this research, of the timed-writing task. If students in our context, a U.S. university, have to do timed-writing in their academic courses, then we should, we believe, teach and test it in our ESL composition courses.

We do not really know without further inquiry how much timed, in-class writing students at even our own university are required to do. Those involved in writing instruction discourage other faculty from having students do timed, inclass writing, yet a survey of faculty done 2 years ago at our university shows that this kind of writing in content classes is far from obsolete (Cook, 1991). Furthermore, despite a trend toward papers and portfolios, it is difficult to envision anything other than a timed essay for a placement exam. Assuming then that we are going to be teaching this kind of writing and testing it, one way to exploit students' strategies is to have students discuss such strategies after a practice exam in the classroom. Perhaps, for example, if students know that they should choose a prompt for which they have the appropriate English vocabulary, they will not run into difficulties as one student said she did on the placement test.

With the exception of 2 of the 26 students who felt that they should have chosen a different prompt, the students in this study believed that they chose the question that allowed them to display their best writing ability, and that they would have been hindered had they been forced to choose a different prompt. Without further research, however, we cannot substantiate this belief. Two students claimed they could not write about something from their native country because they had been away for too long. (One student had been away for 1 year and one for 3 years.) Some thought they would have difficulty narrowing certain topics. One could argue that these are writing problems and that a lack of ability to write on a given topic (to some extent) simply indicates that one is a poor writer. These students may have had difficulties with any topic, and thus giving them a choice may not really give them any advantage.

Although this research does not conclusively show that students should or should not be offered a choice, it has shown that students perceive that they need a choice. One might argue that a more pragmatic reason for offering students a choice is the face validity of the test among the students. All except 2 participants thought they should be offered a choice. The ESL students in this study stated that they could not write on topics they had nothing to say about and that students from different backgrounds need different topics. The importance of face validity is a controversial one. (See Bachman, 1990, and Hughes, 1989, for opposing viewpoints.) Nevertheless, university ESL program administrators know the potential problems that can arise if a placement exam lacks face validity with students and other university faculty and administrators.

And finally, the issue of reliability has been raised, since this seems to be the primary reason for denying students a choice of prompts. Denying students a choice may increase reliability, but it is possible that forcing them to write on a particular topic renders the test less valid. Essays written on prompts students are forced to use may not be a good indicator of what a writing exam claims to be testing, even though such a test has high interrater reliability. The conflict between reliability and validity has been discussed elsewhere in the testing literature. Tedick (1993) and Weigle (1994a, 1994b), for example, argue in their studies of ESL writing exam raters that reliability may be achieved at the expense of validity. Their studies found that raters sometimes give scores so that they are in agreement with other raters, despite their actual feeling toward a piece of writing. Henning (1987) states that reliability and content validity may not be compatible. With regard to topic choice, if a student writes on a topic that is not suitable, one might argue that the task is not valid as a test of the students' writing ability.

The pros and cons of offering students a choice of prompts have been presented. This research has shed some light on previous claims and given rise to questions that can be answered only by further empirical research. One important empirical study would be to examine various student characteristics to see if there is a correlation with specific prompts. If it is the case that certain students are choosing certain prompts, then one might argue that it is important to offer students a choice so that one particular group is not disadvantaged. Another empirical study would be to randomly give half a group of students a choice of prompts and the other half no choice to determine if their final score is affected. Further discussion and research is clearly indicated.

# Notes

1. Weigle (1994a) showed that raters are influenced by features of the prompt for the essays they are rating. However, she found that raters actually believed that the easier prompts disadvantaged students because such prompts demanded less sophisticated essays. Nevertheless, this does not preclude raters from compensating for what they believe to be more difficult prompts.

2. Kroll (1991) states that Spaan's (1989, cited in Kroll, 1991) study showed that when students were given a one-item writing test (with no choice) and a two-item writing test (with two choices), $10 \%$ of the students received different holistic scores.. Spaan's study (1993) compared students' scores on two different versions of the test using two different prompts. In neither case were students offered a choice (Spaan, 1993; M. Spaan, personal communication, 1993).

3. One anonymous JSLw reviewer emphasized that this is true only in theory.

# REFERENCES

Bachman, L. (1990). Fundamental considerations in language testing. Oxford: Oxford University Press.   
Buck, G. (1991). The testing of listening comprehension: An introspective study. Language Testing, 8, 67-91.   
Carlman, N. (1986). Topic differences on writing tests: How much do they matter? English Quarterly, 19, 39-47.   
Chiste, K., & O'Shea, J. (1988). Patterns of question selection and writing performance of ESL students. TESOL Quarterly, 22, 681-684.   
Cohen, A. (1984). On taking language tests: What the students report. Language Testing, 1, 71--81.   
Cohen, A. (in press). Strategies and processes in test-taking and SLA. In L. Bachman & A. Cohen (Eds.), Interfaces between second language acquisition and language testing research. Cambridge: Cambridge University Press.   
Cook, H.G. (1991). English language center curriculum development project. Unpublished manuscript, Michigan State University.   
Evans, P. (1979). Evaluation of writing in Ontario: Grades 8, 12, and 13. Review and Evaluation Bulletins, 1, Ontario.   
Freedman, S. (1983). Student characteristics and essay test writing performance. Research in the Teaching of English, 17, 313-325.   
Hamp-Lyons, L. (1986). No new lamps for old yet, please. TEsOL Quarterly, 20, 790-796.   
Hamp-Lyons, L. (1990). Second language writing: Assessment issues. In B. Kroll (Ed.), Second language writing: Research insights for the classroom (pp. 69-87). Cambridge: Cambridge University Press.   
Hamp-Lyons, L., & Mathias, S.P. (1994). Examining expert judgments of task difficulty on essay tests. Journal of Second Language Writing, 3, 49-68.   
Henning, G. (1987). A guide to language testing. New York: Newbury House.   
Hoetker, J., & Brossell, G. (1986). A procedure for writing content-fair essay examination topics for large scale writing assessments. College Composition and Communication, 37, 328-335.   
Horowitz, D. (1986). Process not product: Less than meets the eye. TEs0L Quarterly, 20, 141-144.   
Hughes,A. (1989). Testing for language teachers. Cambridge: Cambridge University Press.   
Jacobs, H., Zinkgraf, S., Wormuth, D., Hartfiel, V., & Hughey, J. (1981). Testing ESL composition: A practical approach. Rowley, MA: Newbury House.   
Krapels, A. (1990). An overview of second language writing process research. In B. Kroll (Ed.), Second language writing: Research insights for the classroom (pp. 37-56). Cambridge: Cambridge University Press.   
Kroll, B. (1991). Understanding TOEFL's test of written English. RELC Journal, 22, 20-33.   
Raimes, A. (1990). The TOEFL test of written English: Causes for concern. TESOL Quarterly, 24, 427-442.   
Ruth, L., & Murphy, S. (1988). Designing writing tasks for the assessment of writing. Norwood, NJ: Ablex.   
Spaan, M. (1989, March). Essay tests? What's in a prompt? Paper presented at TESOL Conference, San Antonio, TX.   
Spaan, M. (1993). The effect of prompt in essay examinations. In D. Douglas & C. Chapelle (Eds.), A new decade of language testing research: Selected papers from the 1990 Language Testing Colloquium (pp. 98-121). Alexandria, VA: TESOL.   
Tedick, D. (1993, March). A multidimensional exploration of raters' judgments of ESL writing. Paper presented at TESOL Conference, Atlanta.   
Weigle, S. (1994a). Effects of training on raters of English as a second language compositions: Quantitative and qualitative approaches. Unpublished doctoral dissertation, University of California, Los Angeles.   
Weigle, S. (1994b). Effects of training on raters of ESL compositions. Language Testing, II, 197-223.   
Zamel, V. (1982). Writing: The process of discovering meaning. TEsOL Quarterly, 16, 195-209.

# APPENDIX A

# Essay Prompts

Instructions: You will see three topics attached to these instructions. Choose one of the three topics to write your essay on. You will have 30 minutes to write as much as you can. You will be graded on content, organization, and language.

#1. Describe the best or worst experience you have had in the past year. [chosen by 9 students]   
#2. Compare and contrast some aspect of life in your country with some aspect of life in America. [chosen by 12 students]   
#3. Should smoking in public places be against the law? Why or why not? [chosen by 5 students]

# APPENDIX B

# Interview Questions

1. When you write an essay during a test situation, when you only have 30 minutes, can you explain how you go through the steps of writing an essay, what do you do first and so on?

1a. [If no response] Do you take time to think, do you plan, do you go back   
and edit at the end?   
2. Why did you choose this topic?   
3. Which was the easiest topic? Why?   
4. Which was the hardest topic? Why?   
5. Were you able to decide immediately?   
6. When you were writing, were you happy that you had chosen this topic   
or did you ever think, "I should have chosen another topic?   
7. Did you change your mind at any point?   
8. How do you usually decide on a topic during a test situation?   
9. Do you like having a choice during a test?   
9a. [If yes] How many is a good number of choices: 2,3,4,5?