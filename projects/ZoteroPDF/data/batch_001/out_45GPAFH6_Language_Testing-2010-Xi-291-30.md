# Automated scoring and feedback systems: Where are we and where are we heading?

Xiaoming Xi Educational Testing Service, USA

Advances in natural language processing (NLP) and speech recognition and processing technologies (Dodigovic, 2005; Heift and Schulze, 2007; Holland and Fisher, 2008; Randall and Haggstrom, 2006; Shermis and Burstein, 2003) have led to increased efforts in developing automated scoring and feedback systems that support language learning and assessment. The development of automated scoring and feedback systems for the assessment and learning of a language is an extremely complex endeavor and requires expertise in a number of areas: first or second language acquisition (SLA), language assessment, educational measurement, and computational linguistics. Some areas, such as second language learning, language assessment, and educational measurement, have seen cross-fertilizations due to the work of researchers who are well-versed in multiple areas (Bachman and Cohen, 1998; Chalhoub-Deville and Deville, 2006). Individuals in other areas, such as second language learning & assessment and computational linguistics, have started to learn from each other’s perspectives but have yet to see significant progress in integrating perspectives across areas (Chapelle, 1997; 2008).

Initially, the development of automated scoring and feedback systems occurred primarily outside language learning and assessment with little direct influence from current language learning or testing theories. Fortunately, in recent years, language learning and assessment theories have exerted a growing influence on research and development efforts into automated scoring and feedback systems. More and more applied linguistics and computational linguistics conferences have taken the initiative to invite or encourage the participation of researchers from the other discipline to share and debate issues of common interest. Recent collaborative efforts between applied linguists and computational linguists to build automated scoring and feedback systems have allowed some applied linguists to develop expertise in applying test validity theories to the validation of automated scoring systems (Bernstein et al., 2010; Carr and Xi, in press; Xi et al., 2008). Specialized interdisciplinary programs have also emerged to provide formal training in both language learning and assessment and computer technologies (e.g., Applied Linguistics and Computer Technologies at Iowa State University). Such collaborative and cross-disciplinary efforts may lead to the development of more computer-assisted language learning and assessment systems that draw on solid language learning and assessment theories and practices.

This journal issue describes language testers’ concerted engagement in automated scoring and feedback systems. Computer technologies have transformed the delivery systems of language assessments and supported the creation of innovative assessment tasks that tap into new language constructs (Chapelle, 2008; Chapelle and Douglas, 2006). However, the use of computer technologies in scoring constructed-response tasks and in providing performance feedback is a relatively new area of research in language assessment, despite its wide applications in commercial contexts (Chapelle and Cho, 2010).

This issue also attempts to bring together in a cross-disciplinary fashion a number of originally distinct areas that are actively engaged in the development, evaluation and validation of automated scoring and feedback systems for the assessment or learning of a language. Given the enormous practical impact of NLP and speech technologies on the practices of language learning and assessment, it is imperative that guidelines for best practices be established. Collaborative work between researchers in all of these disciplines will eventually lead to solid guidelines for best practices. Efforts to establish best practices would not be fruitful if individuals in these areas continued to work in isolation.

This issue starts with Chapelle and Cho’s state-of-the-art review of automated scoring and feedback systems, with a focus on automated scoring. Their review provides great insights into how automated scoring and feedback for language assessment and learning has grown out of a few related yet diverse research disciplines. They also critically evaluate a few major automated essay and speech scoring systems, comparing the aspects of the construct the scoring features represent and the agreement between human and automated scores. They then briefly touch upon feedback systems, laying out an interpretative argument for system evaluation using an automated feedback system in a university ESL writing class as an example. Finally, they identify the need for cross-disciplinary collaboration between validation research and automated language processing research, so that each field can inform, enrich, and refine the theories and practices of the other.

Following the review article, three articles are featured that highlight different aspects of the validation of writing or speaking assessments that use automated scoring. Another article discusses the potential of developing automated speech fluency features in a principled way. The remaining two articles focus on the evaluation of automated grammatical error and pronunciation feedback systems.

In the subsequent sections, I will first provide some background on the automated scoring research followed by a description of the papers. The same structure is followed for the automated feedback research. Finally, I will discuss some critical problems to grapple with for automated scoring and automated feedback research respectively.

# Automated Scoring Research

As discussed in Chapelle & Cho and Enright & Quinlan in this issue, current test validation theories provide guiding principles for the validation and evaluation of automated scoring and feedback systems (Clauser et al., 2002; Xi, 2008; Xi et al., 2008). These guiding principles translate well into a series of questions that we can ask and seek answers to in our validation and evaluation work. These questions are interrelated yet focus on different aspects of an argument to support the validity of an automated scoring or feedback system.

For automated scoring systems, the following fundamental questions should be asked. They correspond to the different inferential steps in a validity argument (Chapelle et al., 2008; Kane, 1992; 2006).

Does the use of assessment tasks constrained by automated scoring technologies lead to construct under- or misrepresentation? (Domain representation)   
Do the automated scoring features under- or misrepresent the construct of interest? (Explanation)   
Is the way the scoring features are combined to generate automated scores consistent with theoretical expectations of the relationships between the scoring features and the construct of interest? (Explanation)   
Does the use of automated scoring change the meaning and interpretation of scores provided by trained raters? (Explanation)   
Does automated scoring yield scores that are accurate indicators of the quality of a test performance sample? Would examinees’ knowledge of the scoring algorithms of an automated scoring system impact the way they interact with the test tasks, thus negatively affecting the accuracy of the scores? (Evaluation)   
Does automated scoring yield scores that are sufficiently consistent across measurement contexts (e.g., across test forms, across tasks in the same form)? (Generalization) Does automated scoring yield scores that have expected relationships with other test or non-test indicators of the targeted language ability? (Extrapolation) Do automated scores lead to appropriate score-based decisions? (Utilization) Does the use of automated scoring have a positive impact on examinees’ test preparation practices? (Utilization)   
Does the use of automated scoring have a positive impact on teaching and learn ing practices? (Utilization)

The aspects of the validity argument that require full support are dictated by the way automated scoring is used (e.g., used alone or in combination with human scoring) and the intended use of the scores. For example, the areas of emphasis for validating an automated scoring system intended for a practice environment may differ from those for a system deployed in an assessment to support high-stakes decisions. If an automated scoring system is used alone to score an assessment to support high-stakes decisions, full supporting evidence should be sought to provide satisfying answers to all the questions raised above. The questions related to the Domain Representation, Explanation and Utilization inferences are especially important because they are central to the validity argument yet are the areas that are most vulnerable to technological constraints.

The types of assessment tasks or aspects of a performance sample that can be accurately scored by a machine are constrained by NLP and speech technologies. For automated essay scoring, current NLP technologies have enabled the use of various surface linguistic features such as essay length, grammatical accuracy, use of expected vocabulary, vocabulary sophistication, and overall organization structure to predict overall writing proficiency.

However, the state-of-the-art automated essay scoring technologies are not sophisticated enough to score organization, coherence, content and meaning the same way as a trained human rater does (Weigle, 2010).

Automated scoring of speech faces the same constraints as automated essay scoring and poses additional challenges including the accurate recognition of spontaneous accented speech, and the modeling of prosodic aspects of speech such as intonation and stress patterns. Spoken discourse is more fragmented and repetitious and less structured than written discourse, adding to the difficulty in automating the evaluation of speech quality.

To attempt to tackle these challenges, existing research and development work has taken two approaches. One approach, represented by the fully automated VersantTM Speaking tests owned by Pearson (Pearson, 2009), has attempted to bypass the problem of recognizing and processing free speech by applying automated scoring to highly constrained speech elicited through tasks such as reading sentences aloud and giving antonyms of words. These tasks do not call for extended, spontaneous speech production but measure instead ‘facility in $_ { \textrm L 2 } ,$ (p. 355, Bernstein et al., 2010), a set of basic speaking skills that may be correlated with those required for spontaneous, extended and meaningful speech production such as grammatical range, complexity and accuracy, vocabulary diversity, sophistication and precision, organization, coherence, and topic development. In other words, the Versant tests intend to predict speaking proficiency, but do not directly measure communicative competence (Bachman, 1990; Bachman and Palmer, 1996).

Other automated speech scoring efforts have been based on free, extended speech elicited through communicative speaking tasks but focus on using speech features that can be reliably extracted using current speech technologies. For example, Cucchiarini and her colleagues (Cucchiarini et al., 2000; 2002) have used automatically extracted fluency features, which are not heavily reliant on accurate speech recognition, to predict speaking proficiency. SpeechRater SM, developed at the Educational Testing Service (ETS), has taken a similar approach but has expanded the representation of the speaking construct by including pronunciation, vocabulary and grammar features in addition to fluency features (Xi et al., 2008). It has been deployed in a speaking practice test environment. The current SpeechRater model uses features that represent only a subset of the criteria evaluated by human raters and its prediction accuracy (correlation with human scores is around 0.70) is adequate for low-stakes practice purposes but not for highstakes decisions. However, with advances in speech recognition and processing technologies, it is conceivable that more high-level scoring features could be added to expand the construct coverage and improve the accuracy of an automated scoring model.

Currently, neither approach has successfully tackled the problem of under- or misrepresentation of the construct of speaking proficiency in either the test tasks used or the automated scoring methodologies, or both. Given the limitations of current speech scoring technologies, if they were used to score an assessment to support high-stakes decisions, the assessment may be vulnerable to new types of test-taking or cheating strategies. Test takers would be prompted to write or speak to the less sophisticated machine grader to try to achieve undeserved high scores. This would negatively impact the trustworthiness of the scores. Further, if the test tasks or the automated scoring model under- or misrepresent the construct of interest, test takers may be led to place an inappropriate focus on wrong skills or to omit important skills in their test preparation. An automated test that under- or misrepresents the construct may also bring about negative washback effects on teaching and learning and compromise the credibility of the test program.

Against the backdrop of the fundamental questions for validating automated scoring systems and the current state of NLP and speech technologies, four articles are featured in this issue to demonstrate varying philosophies and approaches to the development of automated scoring systems, and different validation approaches.

In the second article in this special issue, Enright and Quinlan make the case for using e-rater $^ \mathrm { \textregistered }$ in conjunction with human scoring for the TOEFL iBT essay writing task (the independent task). They synthesize and evaluate a wide-ranging program of research that supports different inferential links in the validity argument for using e-rater as a complement to human scoring. The research addresses the accuracy of the e-rater scores in comparison to human scores, the reliability of the e-rater scores, the linkage of the e-rater scoring features to the construct, the fairness of e-rater scores for different L1 groups, the relationship between e-rater scores and external criterion writing measures, and the expected consequences of using e-rater along with human raters. They argue that human raters and e-rater possess complementary strengths. Thus, combining the consistency of e-rater with the comprehensiveness and sophistication of human judgment increases scoring efficiency while maintaining high scoring quality. However, the consequences of using automated scoring remains an open issue. Enright and Quinlan raise the need to investigate how examinees’ knowledge of their essays being scored by a human, e-rater, or some combination of the two may change the way they approach the writing task and their writing products.

Although e-rater and human scores are strongly correlated $\mathrm { r } = 0 . 7 6$ for a single writing task as reported in Attali, 2008), evidence beyond human-machine agreement is needed to strengthen the validity argument of using e-rater in conjunction with a human rater to score the TOEFL iBT independent writing task. Weigle’s research, described in the third article in this special issue, focuses on validating e-rater using a variety of external criterion measures including students’ self-assessments of their writing ability, faculty members’ assessment of their writing ability, and trained raters’ evaluations of students’ actual course papers. The overall conclusion is that e-rater scores and human scores have moderate but similar relationships with scores on the criterion measures. While being optimistic about the use of e-rater in combination with human judgment for making high-stakes decisions, Weigle cautions that e-rater can not replace human judgment completely since it does not score an essay the same way a trained human rater does.

In the fourth article, Bernstein, van Moere, and Cheng provide validity evidence for a suite of VersantTM automated speaking tests. They argue that the Versant tests could be used for pre-screening purposes as one of a battery of language tests in various domains. The evidence presented includes a conceptual analysis of the test tasks, the scoring approach and algorithms, and the concurrent validity evidence linking Versant automated scores to human scores on interactive speaking tests. The tests use predominantly simple, highly constrained tasks such as sentence repetition, reading sentences aloud, building sentences, and providing the opposites of words, and are scored on a few aspects of speech using an automated system. Some language versions have started to include tasks that elicit more extended and spontenous speech such as retelling spoken passages, although scoring is limited to the similarity between words used in a test response and words expected to be used. Bernstein et al. make the case for a ‘facility in L2’ construct and argue that the basic speaking skills measured by the automated tests underlie the more complex skills required for communicative speaking tests used for a variety of domains.

In the fifth article, Ginther, Dimova, and Yang report on a study that examines the relationships between temporal variables of speech and human holistic ratings in the context of a local speaking test for screening international teaching assistants. A potential contribution of this piece of basic research to automated scoring is its theory-driven approach combined with empirical analyses. The fluency variables were meticulously selected to represent different dimensions of fluency based on theoretical definitions of fluency; detailed analyses were conducted to examine both the general linear relationships between fluency variables and human holistic scores and whether temporal features of speech distinguish speakers between particular levels of speaking proficiency. Given the heavy use of fluency scoring features in automated speech scoring systems, this study provides both theoretical and empirical justifications for the use of some key fluency variables in automated speech scoring systems that may be strong predictors of general speaking proficiency.

# Automated Feedback Research

For automated feedback systems, the following questions are to be asked. Some overlap with those for automated scoring systems while others are unique to automated feedback systems.

Does the automated feedback system accurately identify learner performance characteristics or errors? (Evaluation)   
Does the automated scoring feedback system consistently identify learner performance characteristics or errors across performance samples? (Generalization) Is the automated feedback meaningful to students’ learning? (Explanation) Does the automated feedback lead to improvements in learners’ performances? (Utilization)   
Does the automated feedback lead to gains in targeted areas of language ability that are sustainable in the long term? (Utilization)   
Does the automated feedback have a positive impact on teaching and learning? (Utilization)

The first three questions pertain to the accuracy, reliability and meaning of the feedback (Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., forthcoming). Many feedback systems aim to identify errors (e.g., pronunciation, grammar, etc.) in learners’ writing or speaking samples. A typical set of metrics used for evaluating the accuracy of automated feedback are precision and recall, percentage of errors identified by the machine that are true errors and percentage of true errors detected by the machine, respectively. Another commonly used metric is kappa between the true errors and errors identified by the machine or the correlation between human and machine judgment of a particular aspect of learners’ performance samples.

The last three questions concern the impact of automated feedback on student learning. This line of research has addressed the extent to which automated feedback impacts the products or the process of writing (Warden, 2000; Yao and Warden, 1996), and how automated feedback is perceived by learners in classroom settings (Chen and Cheng, 2008).

In the sixth article, Franco et al. explore the more basic questions about a Spanish pronunciation scoring and feedback system. Is the automated feedback (both in terms of overall pronunciation quality and individual mispronounced phones) accurate? To evaluate the accuracy of the system’s evaluation of overall pronunciation quality, a few automatically computed features were used to predict the pronunciation score classes human raters assigned. The system was able to produce pronunciation scores that had reasonable correlations with the human scores (0.62 for a non-native speech sample and 0.79 for a native & non-native speech sample). Another focus of this paper is to examine the accuracy of automated feedback on mispronounced phones. By comparing machine predictions with the dichotomous judgments on individual phones by human phoneticians (correct vs. mispronounced), Franco and his colleagues found that the classification error rate of the system was slightly higher than the average human disagreement rate.

In the last article, Chodorow, Tetreault, and Gordon compared two grammatical error feedback systems: CriterionTM is developed by ETS and used in an educational context; ESL Assistant is developed by Microsoft and used in a commercial environment. They evaluated the effectiveness of the feedback on the accuracy of the resulting revisions. For Criterion, it was found that the use of the provided article error feedback reduced the relative frequency of article errors in ESL/EFL students’ revisions. For ESL Assistant, suggested corrections for article and preposition errors plus actual example sentences from the Web containing the suggested forms were provided to the users. Given the imperfect feedback provided by ESL Assistant, this study examined how the users interacted with the ESL Assistant suggestions when composing e-mail messages. It was found that the users made informed judgments based on the suggestions and were able to select correct suggestions at a higher rate than the baseline performance rate of ESL Assistant. Both studies focus on the short-term effects of automated error feedback on users’ revised texts. A natural extension of this line of research would be longitudinal studies that track learners’ growth in specific language skills with the aid of automated feedback.

# Outlook for the Future

Computer technologies will undoubtedly advance and become even more pervasive in our language learning and assessment practices. This under-the-hood look into the inner workings of some of the widely used commercial systems casts some light on what they can do, and what they can not do. This close look may help dispel some of the misconceptions and unwarranted skepticisms about automated scoring and feedback systems (Ericsson and Haswell, 2006). Computer capabilities, if used appropriately and responsibly, can expand the resources and improve the efficiency of language learning and assessment. However, the current limitations of NLP and speech technologies also call for responsible and cautious use of them and call into question the appropriateness of using them alone in scoring assessments for high-stakes decisions. As Fox and Fraser (2009) have argued, making unsubstantiated marketing claims about automated scoring could raise unrealistic expectations and eventually discredit the many advantages that automated scoring could bring. The debate on the appropriateness of claims based on automated scores expects to attract more concentrated research efforts regarding the interactions among automated scoring, test constructs, score meaning, and test consequences.

Automated feedback systems afford efficient, instantaneous feedback and have the potential to transform and enhance learners’ language learning experiences. However, it is also important to realize that the accuracy of feedback given by computers, although acceptable in low-stakes practice environments with instructor support, leaves considerable room for improvement to emulate the judgment of trained linguists. Further, no current research has addressed the stability of automated feedback across performance samples. Finally, the effects of automated feedback on learning, especially over an extended period of time, are under-explored and deserve special research efforts.

The studies in this issue represent an important first step to encourage and engage language learning and assessment researchers to read the literature critically, and to reflect on and debate issues that are reshaping the landscape for language learning and assessment. Although some of the contributions in this issue do not read like anything we are used to in Language Testing, I hope you will find them interesting, gratifying and stimulating to read.

# References

Bachman LF (1990). Fundamental considerations in language testing. Oxford: Oxford University Press.   
Bachman LF, Palmer AS (1996). Language testing in practice: Designing and developing useful language tests. Oxford: Oxford University Press.   
Bachman LF, Cohen A (eds) (1998). Interfaces between second language acquisition and language testing research. Cambridge: Cambridge University Press.   
Bernstein J, Van Moere A, and Cheng J (2010). Validating automated speaking tests. Language Testing, 27(3): 355–377.   
Carr N, Xi X (in press). Automated scoring of short answer reading items: Implications for constructs. Language Assessment Quarterly.   
Chalhoub-Deville M, Deville C (2006). Old, borrowed, and new thoughts in second language testing. In R. Brennan (ed), Educational measurement, (4th ed., pp. 517–530). Westport, CT: Praeger.   
Chapelle CA (1997). CALL in the year 2000: Still in search of research paradigms? Language Learning and Technology, 1(1), 19–43.   
Chapelle CA (2008). Utilizing technology in language assessment. In E. Shohamy (Ed.), Encyclopedia of language education, vol. 7: Language testing and assessment (2nd ed., pp. 123–134). Heidelberg, Germany: Springer.   
Chapelle CA, Enright MK, and Jamieson JM (eds) (2008). Building a validity argument for the test of English as a foreign language™. Mahwah, NJ: Lawrence Erlbaum.   
Chapelle CA, Douglas D (2006). Assessing language through computer technology. Cambridge: Cambridge University Press.   
Chapelle CA and Chung Y-R (2010). The Promise of NLP and speech processing technologies in language assessment. Language Testing, 27(3): 301–315.   
Chen CE, Cheng W (2008). Beyond the design of automated writing evaluation: Pedagogical practices and perceived learning effectiveness in EFL writing classes. Language Learning and Technology, 12(2), 94–112.   
Clauser BE, Kane MT, and Swanson DB (2002). Validity issues for performance-based tests scored with computer-automated scoring systems. Applied Measurement in Education, 15, 413–432.   
Cucchiarini C, Strik H, and Boves L (2000). Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology. Journal of the Acoustical Society of America, 107, 989–999.   
Cucchiarini C, Strik H, and Boves L (2002). Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech. Journal of the Acoustical Society of America, 111(6), 2862–2873.   
Dodigovic M (2005). Artificial intelligence in second language learning: Raising error awareness. Clevedon: Multilingual Matters.   
Ericsson PF, Haswell R (eds) (2006). Machine scoring of student essays: Truth and consequences. Logan, UT: Utah State University Press.   
Fox J, Fraser W (2009). The Versant SpanishTM Test. Test review. Language Testing, 26(2), 313–322.   
Gamon M, Leacock C, Brockett C, et al. (forthcoming). Using statistical techniques and Web search to correct ESL errors. To appear in: CALICO Journal, special edition on ‘Automatic Analysis of Lerner’s Language’.   
Han N-R Chodorow M, and Leacock C (2006). Detecting errors in English article usage by nonnative speakers. Natural Language Engineering, 12(2), 115–129.   
Heift T, Schulze M (2007). Errors and intelligence in computer-assisted language learning: Parsers and pedagogues. London and New York: Routledge.   
Holland M, Fisher P (eds) (2008). The path of speech technologies in computer assisted language learning: From research toward practice. London and New York: Routledge.   
Kane MT (1992). An argument-based approach to validity. Psychological Bulletin, 112, 527–535.   
Kane MT (2006). Validation. In R. L. Brennan (ed), Educational measurement (4th ed., pp. 18–64). Washington, DC: American Council on Education/Praeger.   
Pearson (2009). Versant Spanish Test: Test description and validation summary. Pearson Knowledge Technologies, Palo Alto, California. Available online at https://www.ordinate.com/ technology/VersantSpanishTestValidation.pdf (accessed January 2010).   
Randall PD, Haggstrom MA (eds) (2006). Changing language education through CALL. London and New York: Routledge.   
Shermis MD, Burstein J (eds) (2003). Automated essay scoring: A cross-disciplinary perspective. Hillsdale, NJ: Lawrence Erlbaum.   
Tetreault J, Chodorow M (2008). The ups and downs of preposition error detection. Manchester, UK: COLING.   
Warden CA (2000). EFL business writing behavior in differing feedback environments. Language Learning, 50(4), 573–616.   
Weigle SC (2010). Validation of automated scoring of TOEFL iBT tasks against non-test indicators of writing Language Testing, 27(3): 335–353.   
Xi X (2008). What and how much evidence do we need? Critical considerations for using automated speech scoring systems. In Chapelle CA, Chung Y-R, and Xu J (eds), Towards adaptive CALL: Natural language processing for diagnostic language assessment (pp. 102–114). Ames, IA: Iowa State University.   
Xi X, Higgins D, Zechner K, and Williamson DM (2008). Automated scoring of spontaneous speech using SpeechRater v1.0 (ETS Research Rep. No. RR-08-62). Princeton, NJ: Educational Testing Service.   
Yao YC, Warden CA (1996). Process writing and computer correction: Happy wedding or shotgun marriage. CALL Electronic Journal. Available at http://www.tell.is.ritsumei.ac.jp/callej/1-1/ Warden1.html.

# Errata

# Erratum

The publisher would like to apologize for the error in the article titled ‘A multi-method analysis of evaluation criteria used to assess the speaking proficiency of graduate student instructors’ published in volume 27 issue 2, 235–260 (DOI: 10.1177/0265532209349469).

The author corrections were missed and not incorporated in the print version of the article. The correct version of the article is available online.

# Erratum

The publisher would like to apologize for the error in Language Testing in the editorial titled ‘Automated scoring and feedback systems: Where are we and where are we heading?’ published in volume 27 issue 3, 291–300 (DOI: 10.1177/0265532210364643).

In the editorial, the last name of Yoo Ree Chung was incorrect in the in-text references to the article of Carol A. Chapelle and Yoo Ree Chung. It should be ‘Chung’ instead of ‘Cho’ throughout the editorial.