# Using AR filters in L2 pronunciation training: practice, perfection, and willingness to share

Jingjing Zhu, Xi Zhang & Jian Li

To cite this article: Jingjing Zhu, Xi Zhang & Jian Li (2024) Using AR filters in L2 pronunciation training: practice, perfection, and willingness to share, Computer Assisted Language Learning, 37:5-6, 1364-1396, DOI: 10.1080/09588221.2022.2080716

To link to this article: https://doi.org/10.1080/09588221.2022.2080716

# Using AR filters in L2 pronunciation training: practice, perfection, and willingness to share

Jingjing Zhu, Xi Zhang and Jian Li

School of Foreign Studies, Shanghai University of Finance and Economics, Shanghai, The People’s Republic of China

# ABSTRACT

Traditional L2 pronunciation teaching puts too much emphasis on explicit phonological knowledge (‘knowing that’) rather than on procedural knowledge (‘knowing how’). The advancement of mobile-assisted language learning (MALL) offers new opportunities for L2 learners to proceduralize their declarative articulatory knowledge into production skills more efficiently through systematic practice. Within the framework of skill acquisition theory, and using a mixed-methods design including production tests, articulatory awareness quizzes, and a semi-structured interview, this study aims to investigate the effectiveness of using an augmented reality (AR) filter app during pronunciation practice to improve Chinese L2 students’ segmental production and articulatory awareness of the targeted English consonants (dark/ɫ/,/ð/and/θ/), and their attitudes towards this application. The experimental results indicate that the app with AR technology, facial detection technology, and digital zoom technology played a positive role in promoting students’ segmental production and raising their articulatory awareness. The interview results show that the AR filter helped to alleviate the learners’ speaking anxiety and embarrassment related to interacting with the teacher for corrective feedback, and encouraged daily practice by providing more fun. Combining all the qualitative and quantitative results, we conclude that an AR filter app can offer academic, psychological, and social support for L2 pronunciation skill learning.

# KEYWORDS

AR filter; L2   
pronunciation training; mobile-assisted language learning; articulatory   
awareness; skill   
acquisition

# Introduction

Research on second language acquisition (SLA) has reported that language learners frequently experience difficulties with the acquisition and production of particular L2 speech sounds (Flege et  al., 1992; Guion et  al., 2000; Piske et  al., 2001). This may cause problems for listener understanding by native speakers or L2 users from different L1 backgrounds, according to the intelligibility principle (Levis, 2005). Although pronunciation is a vital but challenging competence for L2 learners, pronunciation teaching is often “marginalized” (Derwing & Munro, 2005, p. 382) and has taken a back seat to the more prominent instruction of vocabulary and grammar in L2 classrooms (Fouz-González, 2020). The limited amount of pronunciation instruction that is typically integrated into regular classes or with other language skills (listening, reading, writing) gives learners few opportunities to practice and get corrective feedback (CF) from teachers (Darcy et  al., 2012), who assign more time to the delivery of explicit phonological knowledge to facilitate word reading and spelling. However, according to the skill acquisition theory (DeKeyser, 2017), without sufficient time for deliberate practice and CF, ‘knowledge that’ (declarative knowledge) cannot be easily proceduralized into ‘knowledge how’ (procedural knowledge), and thus production behaviors or skills are difficult to acquire.

To address practice and CF issues in language skills acquisition and improve their efficiency and effectiveness, increasing numbers of researchers and practitioners have begun to resort to mobile-assisted technology and AR technology. It is reported that mobile-assisted language learning (MALL) enables learners to benefit from continuity and spontaneity of access to the learning material (Kukulska-Hulme, 2012a) and promote CF and learners’ learning motivation by alleviating students’ L2 anxiety (Hwang et  al., 2017; Reinders & Wattana, 2014; Wu, 2018). It is also worth noting that as a symbol of the cutting-edge developments in human computer interaction, mobile-based AR applications have been used as an educational tool in the language classrooms, demonstrating its success in supporting linguistic gains (Parmaxi & Demetriou, 2020) and improving learners’ intrinsic motivation, engagement and enjoyment (Chen, 2020; Perry, 2015; Solak & Cakir, 2015). Although the AR studies in language learning reported “a wide range of benefits for vocabulary acquisition, writing, reading, speaking, comprehension, pronunciation and phonics” (Parmaxi & Demetriou, 2020, p.7), pronunciation skill acquisition in AR studies is often neglected, especially the segmental production which calls for more visual aids from AR to develop procedural knowledge of articulatory gestures.

In view of these previous studies, we focus on the AR filter technology used by a mobile selfie and social app, with the following two goals: first, to help learners to proceduralize and automatize the declarative knowledge of articulatory features which is delivered by the teacher and seen as a necessary and prior step to the acquisition of segmental production skills; and second, to help alleviate learners’ shyness and anxiety in their interactions with the teacher for CF, and engage the learners in a deliberate practice routine by providing more fun. Hence, the present study aims to examine the effectiveness of using an AR filter in the pronunciation practice routine in terms of improving Chinese L2 students’ segmental production and their articulatory awareness of the targeted English consonants, as well as their attitudes toward this new learning tool.

# Literature review

# Theoretical background

The theoretical framework on which we base our assumptions about how computer-assisted language learning (CALL), specifically AR filters, works in general and for segmental pronunciation training in particular is skill acquisition theory (DeKeyser, 2017). Within this theoretical paradigm, deliberate practice is necessary to turn declarative knowledge into procedural knowledge. This transition from ‘knowledge that’ to ‘knowledge how’ is called proceduralization (DeKeyser, 2017). This theory is particularly appropriate for L2 segmental production skill acquisition. Non-native segmental production demands good control of the articulatory gestures, some of which might be absent in the learners’ first language. Even if they are taught and understand the explicit articulatory knowledge, learners cannot successfully produce these phonemes if they are not able to reproduce those movements (Ouni, 2014). Articulatory awareness may exert a powerful influence on L2 pronunciation learning, and may be further divided into noticing and explicit articulatory knowledge (Saito, 2018). The role of noticing involves altering input processing, concerned with the conversion of input to intake, which is conducive to the proceduralization of explicit knowledge and thus aids skill acquisition (VanPatten & Cadierno, 1993).

According to skill acquisition theory, deliberate and systematic practice that marked effects of noticing is crucial to achieving higher levels of proceduralization and automatization. In the narrowest sense, practice means deliberate practice, which Brown et  al. (2014, p.183) characterize as follows: “If doing something repeatedly might be considered practice, deliberate practice is a different animal: it’s goal-directed, often solitary, and consists of striving to reach beyond your current level of performance.” In this regard, mobile-assisted language learning (MALL), especially the built-in digital zoom-in technology, can reinforce learners’ visual feedback of their own articulators and thus draw their conscious attention to articulatory gestures during systematic pronunciation practice. This type of deliberate practice with a clear goal of improving articulatory awareness may lead to the proceduralization of declarative knowledge and higher levels of non-native segmental production.

Another issue that is inseparable from skill acquisition and practice is feedback, which prompts students to monitor their own declarative knowledge during production and thus proceduralize and automatize it (Sato & Lyster, 2012). During computer-assisted pronunciation training (CAPT) learners can obtain simultaneous visualized feedback that enables them to locate their errors. Lyster and Saito (2010) emphasized that prompts (providing negative feedback) were more effective than recasts (providing mostly positive feedback). CF from teachers is crucial for learners to obtain diagnostic information (Knoch & Macqueen, 2017), and the application of technology has been proven successful in increasing CF (Loewen & Sato, 2017): first, technology is capable of engaging learners in practice routines, and therefore promotes and sustains student-teacher relationships and interaction (Mazer et  al., 2007); and second, it can help alleviate some learners’ shyness and anxiety in the teacher-student interaction for CF (Hwang et  al., 2017). Studies such as the foregoing have suggested that skill acquisition can be enhanced with the help of technology which takes on a positive role in both practice and feedback processes (Loewen & Sato, 2017).

Drawing on the theoretical insights outlined previously, this study chooses the after-class deliberate practice and assignment submission for CF as the research context.

# From CALL to MALL

During the past two decades, sophisticated technology has facilitated a boom in mobile-assisted language learning (MALL). Advances in portable communication technology have meant that powerful smartphones are now affordable and accessible, leading to a rise in worldwide mobile phone use in various fields, such as communication, entertainment, mobile money, and education (Karlsson et  al., 2018). As Kukulska-Hulme (2009) noted, the ownership and use of mobile devices tends to generate and facilitate more language learning opportunities for learners.

Generally speaking, MALL differs from computer-assisted language learning (CALL) in terms of the mobility of the learner and the portability of handheld devices. MALL ensures that language learning and practice can take place at any time and anywhere (Kukulska-Hulme & Shield, 2008; Palalas, 2011). Owing to the use of portable devices, MALL gives learners continuity and spontaneity of access to learning materials (Kukulska-Hulme, 2012a). Moreover, the use of portable devices in language learning allows more interactions between learners across different contexts, provides students with opportunities to engage in authentic communication, integrates formal and informal language learning, and engages learners in deliberate practice routines (Kukulska-Hulme, 2012a).

As the learner should be prioritized ahead of technologies (Colpaert, 2004), how to develop the learner’s learning motivation and improve their learning experience should always be the core issue in language learning supported by advanced technology. MALL can offer learners a personalized language learning experience (Kukulska-Hulme, 2012b). In self-directed non-formal language learning settings it is important for learners to autonomously dominate, manage, and regulate the whole language process (Zhang & Pérez-Paredes, 2021). MALL helps to create more activities for self-training or practice, allowing language learners to ‘express and follow their interests, to work towards personal goals, and to engage in content creation and sharing, while raising awareness of strategies in speaking and listening’ (Kukulska-Hulme, 2016, p.2).

Scholars have reported the effectiveness of MALL in developing learners’ second language acquisition (SLA) from perceptive, cognitive, and attitudinal perspectives (Viberg & Grönlund, 2012). Researchers who study the psychological effects of mobile-assisted learning strategies on language learners have claimed that MALL can benefit learners’ learning motivation (Hwang et  al., 2017; Wu, 2018), encourage engagement and student-teacher interactions (Mompean & Fouz-González, 2016), and alleviate students’ English anxiety (Ebadijalal & Yousofi, 2021; Reinders & Wattana, 2014), since most of them hold positive attitudes toward MALL (Duman et  al., 2015). For instance, Reinders and Wattana (2014) used a digital game play to explore its effects on learner’s Willingness to Communicate and academic gains, and the results showed that the computer assisted game could alleviate students’ anxiety and enhance significant improvement. Ebadijalal and Yousofi (2021) also reported the effectiveness of using mobile-assisted peer feedback on enhancing the learners’ oral proficiency and lowering their foreign language anxiety.

Traditional CALL has hindered many cutting-edge technologies from being used in educational context and presenting their potential to revolutionize learning and improve students’ learning performance. For example, the application of AR and virtual reality (VR) technologies has been restricted to laboratories equipped with head-mounted displays (HMD). The characteristics of MALL mean that it lends itself well to application in pedagogical settings. AR technology, as a variation of VR technology, supplements reality by integrating virtual objects or information over real-world settings (Azuma, 1997; Craig, 2013; Ruan & Jeong, 2012). There is an emerging body of literature that endorses the application of AR in L2 language learning and machine learning (Salunkhe & Mehta, 2016; Sharma et  al., 2017; Yang, 2004). As a good means to create an interactive language learning environment, the use of mobile-based AR in L2 education can not only increase students’ motivation and contribute to their learning outcomes, but also enhance learners’ sense of enjoyment and arouse their curiosity (Perry, 2015; Solak & Cakir, 2015; Wu, 2021). For example, in Perry’s Perry (2015) study, the students described their gamified French learning as fun, engaging and playful, which encouraged more interactions with teammates. Solak and Cakir (2015) suggested that materials designed with AR technology had a positive impact on increasing undergraduate students’ motivation towards vocabulary learning at elementary level. Wu (2021) also proved a mobile-AR game, Pokémon Go, could make language learning an interesting and enjoyable process, and therefore provided more opportunities for peer feedback and closer interactions between students and teachers. Collectively, all these studies used mobile AR games to gamify L2 learning.

There are other interesting AR mobile applications that remain unexplored in L2 education, such as selfie applications and AR filters, which may be capable of supporting self-practice by arousing learners’ learning interest, and encouraging teacher-and-student interactions. It should also be pointed out, however, that most mobile-based AR studies in language learning focused on promoting vocabulary, reading, speaking and writing skills, rather than pronunciation. To the best of our knowledge, there is currently no published study that explores the use of mobile-based AR in facilitating L2 segmental production. In light of this gap in the literature, we extend the use of AR technology into L2 pronunciation skill acquisition. In the ensuing section, we zoom in on the literature review of the pronunciation training and articulatory feedback assisted with advanced technologies.

# CAPT and articulatory feedback aided by MALL

Advances in technology, particularly the recent proliferation of web-based and mobile applications, have made possible the use of CAPT for individual L2 pronunciation practice without direct instruction from a teacher. CAPT adopts technologies such as automatic speech recognition (ASR), speech visualization, artificial intelligence (AI), and so on to dynamically evaluate and automatically provide feedback, such as automated assessment and visual display, for learners to improve their L2 pronunciation and raise their awareness of speech errors (Engwall, 2012; Rogerson-Revell, 2021; Tsai, 2019). Traditionally, automated assessment aids L2 pronunciation education by providing a global numerical score derived from a standard model, according to a holistic evaluation of pronunciation (Rogerson-Revell, 2021), or based on waveforms and spectrograms of speakers’ speech samples (Motohashi-Saigo & Hardison, 2009). The assessment can stimulate learners’ intrinsic motivation and improve students’ pronunciation. However, with feedback only in the form of raw data based on acoustic analysis, students may not be able to derive any indication on how to actually improve their pronunciation while looking at the spectrograms or waveforms (Engwall & Bälter, 2007). Another issue is that this type of visual feedback tells learners little about the articulatory causes of their errors, and therefore students can only make random attempts to correct their pronunciation, which may eventually end up in frustration if they are unable to correct fossilized errors (Rogerson-Revell, 2021).

Visual display of the lips and oral cavity provides another articulatory-based visual input which has proven effective in improving both perception and production of pronunciation. Li and Somlak (2017) displayed images of native speakers’ faces so that students could observe articulatory gestures during classes, and reported significant academic gains. Massaro and Light (2003) reported the effectiveness of applying talking heads to illustrate the correct articulatory gestures in improving students’ L2 segmental pronunciation. Badin et  al. (2010) also showed the positive effects of providing frontal visual displays in training learners in the pronunciation of vowels and consonants. However, none of these studies provided real-time, personalized, visual self-feedback of students’ own articulatory gestures to facilitate their pronunciation practice and teacher-student interaction.

Articulatory-based visual feedback enhances learners’ pronunciation by providing explicit articulatory information, especially articulatory gestures, and by directing students’ conscious attention to enlarged articulators. An articulatory gesture not only entails the movement and location of certain articulators, but is also central to the representation of phonemes in the brain (Liberman, 1999). Saito (2018) believed that explicit articulatory knowledge and attention are two integral parts of awareness, and it has been reported that using visual illustrations (e.g., of explicit articulatory information) could improve articulatory awareness (Haldin et  al., 2018).

When it comes to CAPT in the classroom-based context, articulatory awareness has been neglected. More than forty years ago a single study shed light on the effectiveness of using mirrors to provide articulatory-based feedback in L2 pronunciation classrooms (De BOT, 1980), but students cannot observe the location of their articulators clearly without an augmented image of their oral cavity. The same thing happens with the built-in front-facing cameras of mobile phones and tablets, which are not equipped with digital zoom technology.

In general, the existing literature on articulatory-based CAPT is empirically thin and theoretically light. The implementation of articulatory-based visual CAPT does not seem to be designed based on a theoretical grounding, such as sociocultural, situated and skill acquisition theory, which will support purposeful pedagogical learning activities. Even the limited studies on articulatory-based visual feedback failed to associate the application meaning with the development of learners’ procedural knowledge or link to activity design for deliberate practice and noticing. Based on skill acquisition theory, we resort to an AR-supported articulatory-based tool to provide real-time, personalized, reinforced visual self-feedback of students’ own articulatory gestures for the optimization of noticing, deliberate pronunciation practice and teacher-student interaction.

# AR filters

As an important branch of AR technology, AR filters applies three major technologies: AR technology, facial detection technology, and digital zoom technology. Facial detection technology is a vital part of AR filters, using pixel data from a camera to identify faces. Facial landmark detection technology aims to automatically identify the locations of key landmark points on facial images, and convert them into information. Facial information is important for human and computer interaction, entertainment, security surveillance, and medical applications (Wu & Ji, 2019).

AR filters first scan the user’s face with the built-in front-view camera, and then detect the place of each landmark feature on the user’s face using the facial landmark technology. AR technology can integrate virtual pictures with the image of user’s face (or a specific feature) according to the facial information, replacing the unwanted real facial features, and hence reinforcing the remaining features. However, the size of the images that are captured by the facial landmark detection technology is fixed and the image of the user’s face may not be augmented enough, since the front-facing cameras built into mobile phones and tablets are generally not equipped with digital zoom technology. Digital zoom technology uses software to magnify a subset of an image captured by a lens, yielding an overall augmented view of the subject. With this technology, users can better observe their facial features and change the size of the images compared to just using the front camera.

AR filters are used to take selfies and are widely applied in both social (e.g., Snapchat, Instagram, TikTok) and selfie applications (e.g., B612) for entertainment, socializing, and advertising purposes. Previous studies have shown that users may take pleasure in producing these augmented selfies (Hawker & Carah, 2021). However, AR filters have never been adopted in the area of L2 education, especially L2 pronunciation learning and practice before.

# L2 pronunciation learning problems for chinese students

Pronunciation is essential for L2 speakers in enabling them to achieve a communicative goal, which has been further discussed as to produce intelligible and comprehensible communication, and to avoid accentedness to facilitate listeners’ perceptions (Offerman & Olson, 2016). Despite the large English-speaking population and long history of EFL education in China, Chinese L2 education has always been exam-oriented and vocabulary-driven, with pronunciation instruction receiving relatively little attention. According to Carey (2002), many errors in second language pronunciation are caused by transfer. Learners tend to rely on their native phonological systems to pronounce L2 sounds. Due to the Chinese negative transfer to English, dental fricatives/ð/and/θ/(Li & Sewell, 2012) and dark/ɫ/(Ao & Low, 2012) are the most difficult English consonants for Chinese students to pronounce. For example, students of L1 Chinese tend to vocalize the word-final dark/ɫ/into a back vowel/ʊ/ (e.g., bell [beʊ], fill [fiʊ]) or delete it when it appears after a back vowel/ʊ/(e.g., ball [bɔː], pull [puː]) (Deterding, 2007).

The errors caused by these consonants may be easily fossilized in students’ English pronunciation acquisition because of nonnative English teachers’ insufficient attention to students’ pronunciation and their lack of articulatory knowledge. Furthermore, these fossilized errors usually affect the intelligibility of certain words—for example, many Chinese tend to pronounce ‘thank’ incorrectly as ‘sank’, and ‘fall’ as ‘for’. This study will focus on these problematic consonants which require the most explicit articulatory instruction based on the literature.

In addition to the phonemic obstacles induced by intra-linguistic factors, previous studies have reported some psychological issues among Chinese speakers during L2 pronunciation learning, such as shyness (Paulhus et  al., 2002) and anxiety that may be experienced by many Chinese students when speaking English (Mak, 2011), and which may lead to unwillingness to interact with teachers or peers in the learning process. We seek to use an AR filter to replace unwanted facial features with virtual pictures, and only display the articulators, including tongues, teeth and lips. We are hoping that this might remove or alleviate students’ shyness or speaking anxiety and attract learners’ attention to the articulaors to promote students’ practice routines and student-teacher interaction.

# The current study

In their study of an audiovisual talking head, Badin et  al. (2010) found that learners paid less attention to tongue movements than to lips. This study thereby focuses on the learners’ attention to tongue movements when investigating the AR filter’s impact on their segmental production and articulatory awareness of the targeted English consonants, since consonant production requires accurate tongue placement and movements.

The application $\mathbf { \widetilde { B 6 1 2 } } ^ { \mathbf { \prime } }$ that we adopted includes hundreds of AR filters (Figure 1, left) catering for users’ various interest and needs. Users can take selfies and record videos with the AR filters (Figure 1, right), as well as sharing with other users or uploading to other platforms.

Compared with standard front-view cameras, the $\mathbf { \widetilde { B } 6 1 2 } ^ { \prime }$ application can significantly augment the users’ facial features (tongue, lips and teeth) and eliminate unwanted facial characteristics, which are irrelevant to articulators (as shown in Figure 2). We hypothesize that the use of this mobile application will be helpful in developing students’ segmental production skills, enabling accurate tongue placement and movements, and improving their articulatory awareness, and that using a variety of interesting filters may increase students’ self-practice by arousing their learning interest, as well as reducing or eliminating their unwillingness to interact with teachers caused by shyness or L2 anxiety. On the whole, this study will address the following three research questions:

RQ1: To what extent, if any, does the use of an AR filter facilitate the participants’ targeted segmental production?

![](img/56ee9a73729f31731cb6ee4c5d24b02cfb0453c36a58e7cc8085e7726da0a0c8.jpg)  
Figure 1. S creenshots from the $" { \mathsf { B } } 6 1 2 ^ { \prime }$ app.

![](img/f70374c7ba3242d4ccf833675ad3c330672760c3f0261e4e6b137c69c692346f.jpg)  
Figure 2. S elfies taken by standard front-view camera (left) and using AR filter (right) when producing dark/ɫ/.

RQ2: To what extent, if any, does the intervention cultivate overall articulatory awareness?

RQ3: What is the participants’ attitude towards the use of AR filters in their production practice?

# Methodology

To answer the research questions, this study adopted a mixed-methods design. Production tests and articulatory awareness quizzes completed by the participants were first analyzed quantitatively, and then the qualitative data from semi-structured interviews were further analyzed to interpret and supplement the quantitative findings.

# Participants

The 55 participants in this study were non-English major sophomores who had voluntarily enrolled in a phonetic course during the Spring semester at a prestigious university in Shanghai, China. The participants came from 21 different provinces in China, and were randomly divided into the experimental group $( \mathrm { N } = 2 7$ , 9 males and 18 females) and the control group $\mathrm { N } = 2 8$ , 2 males and 26 females). All the participants were native speakers of Mandarin and had learned English as their L2 for 9 to 12 years; all had intermediate or advanced English proficiency. None had participated in any phonetic courses before. All parts of the data collection of this study were mandatory components of the students’ homework. Students were only included as participants in the study if they submitted $8 0 \%$ of the homework assignments (Martin, 2020). Hence, 5 participants from each group were excluded from the initial participant pool, leaving 55 participants in the final pool.

# Instruments

The study consisted of classroom instruction, training implementation, and data collection. During the experimental and data collection sessions, the study was conducted in a classroom with high quality audio-visual services. Students were required to read aloud on their own desktop computer equipped with headsets and microphones. During the in-class and after-class practice, the students from the experimental group were asked to practice their articulatory gestures using an AR filter on their own smartphones, while the control group used mirrors or front-view cameras. All the AR filters adopted in the experiment were from the selfie application $^ { \mathrm { e } } { \mathrm { B } } 6 1 2 ^ { \mathrm { \ ' } }$ .

# Procedure

# Instruction

Both the experimental group and the control group were enrolled in a phonetic course over a period of a term. The quasi-experiment lasted for 8 weeks in total. Instruction related to the targeted consonants was only carried out for the first two weeks. The teacher delivered explicit instruction of the relevant articulatory knowledge to both groups. All the participants received relevant in-class instruction for approximately 90 minutes a week. Figure 3 illustrates the processes included in the quasi-experiment: an awareness quiz pretest, a production pretest, teaching of the target phonemes, after-class phonetic training, a production posttest, a delayed production test, an awareness quiz posttest, and one-on-one interviews. Other control variables like educator, place of teaching, teaching materials, and teaching approaches were controlled for.

During the first week of the class the instructor demonstrated the use of AR filters to the experimental group. Learners were allowed to use the AR filter anywhere and at any time, and they were given some guidance on how to use the app during the class at the beginning of the semester. For the control group, the students were encouraged to use traditional mirrors or the standard front-view cameras in smartphones (without augmented visual mode). For the reasons discussed above in the literature review, we chose dark/ɫ/,/θ/, and/ð/as our target sounds. The target consonants were divided into two groups (dark/ɫ/; dental fricatives/θ/and/ð/) according to the different articulatory gestures. Each group of consonants was taught in one session.

# Practice

The teaching materials and stimuli for the experimental and control groups’ daily practice were obtained from an English phonetic textbook which includes explicit phonetic knowledge of English vowels and consonants. For each group of consonants, the training material for the participants was around 30 words with all the possible phonological environments for each consonant (e.g., syllable onset and coda positions). The after-class was set as a daily homework routine and took 10 minutes per day as required. The participants were required to complete a self-reported survey everyday which was used to keep a track of their practice time. The results showed that the actual practice time of both groups took approximately 10 minutes per day (Experimental group: avg. time $= 1 0 . 9 4 \mathrm { m i n }$ ; control group: avg. time $= \ 1 0 . 3 2 \mathrm { m i n } )$ . After practicing with the app the experimental group were asked to use AR filters to record videos of themselves completing the read-aloud task, while the control group audio-recorded the reading task. After all the participants had uploaded the videos or recordings to a homework-collection platform, the teacher gave written corrective feedback.

![](img/bc2e7b0fcbd39e3cce99fa8f46fa486b715109fc6811d606ff67645a046018bd.jpg)  
Figure 3. Quasi-experiment procedure.

# Production tests

In order to evaluate and compare the participants’ improvements in pronunciation quality at the word level, we conducted a read-aloud task which included 10 words for each target consonant. All the possible phonological environments for each consonant and syllable positions of the consonants that were taught during the sessions were covered in the test. The participants were given 3 seconds to read each word shown on the screens of their desktop computers.

The production tests were conducted individually in the same audio-visual classroom that was used for the class instruction. All the participants had been asked to take the pretest before the instruction. After the 90-minute instruction and a week of production practice, they took the posttest. The data collection included audio-recordings of each participant’s reading performances during the pre-, post-, and delayed test.

# Articulatory awareness quiz

This task (Appendix B) was based on the articulatory awareness test used by Ruscello et  al. (1980), who investigated the articulatory awareness of normal speakers and speakers with an articulatory disorder. An example item is: Do your lips touch when you say/p/as in pig? (Yes/ No/Don’t know). The articulatory gesture for each consonant was taught explicitly in class by the instructor. This study combined the quiz with the phoneme-video pairing test conducted by Montgomery (1981), in which the participants were shown colored pictures of a sagittal view of the human face, with each item depicting a particular constellation of tongue, teeth, and lips during the articulation of a particular phoneme. The participants’ task was to identify which of the schematic drawings represented the articulation of the target phoneme. With the development of technology and in order to better match the teaching techniques used in the phonetic training procedure, we adopted zoom-in short videos rather than the static articulatory gesture pictures in the quiz, which showed the dynamic movement of a real person’s pronunciation of the target consonants. An example is illustrated in Figure 4: Does the video clip show the correct pronunciation of the highlighted part in the word “think”? (Yes/No). The questions were presented in Chinese.

The awareness quiz was conducted individually in the same audio-visual classroom that was used for the classroom instruction. All the participants were asked to take the pretest before the instruction, and the posttest after they had finished all the production tests.

# Semi-structured interviews

In order to investigate the participants’ perceptions of the use of the AR filter app, a semi-structured interview was conducted at the end of the semester. For the face-to-face in-depth interviews, we selected 4 students who had shown significant academic improvement in terms of their segmental production and 4 students who had shown a lower production improvement rate, according to the results of production tests. However, since the interview was conducted just before the exam period, only 3 students with significant academic improvement participated in the interview. The semi-structured interviews consisted of questions investigating four distinct aspects: attitude towards AR filter, anxiety, interest, and articulatory awareness. These are shown in Appendix C in both the Chinese and English versions.

![](img/204b434c9de92916b036316db0e12e171e7bd8fdfad66f5aa294f3a9f54a29ff.jpg)  
Figure 4. S creenshot of a question in the awareness quiz.

# Data analysis

The participants’ production recordings were then rated by two native English speakers. A third non-native expert in English pronunciation (L1 Chinese), who is also an experienced EFL teacher, was asked to adjudicate disagreements. If the two judges’ answers were different, the study adopted the third judge’s rating. The ratings were always dichotomous (1 if the target sound was pronounced accurately, 0 if it was mispronounced). The raters only judged whether the targeted phoneme was pronounced correctly or not, rather than the whole word, and they could play each recording as many times as they needed. Interrater reliability was assessed using the Cronbach alpha coefficient $\scriptstyle ( \mathtt { a } = . 9 6 9 )$ , indicating high reliability. Intra-rater reliability was assessed using the Pearson correlation coefficient $\left( \mathtt { p } = 0 . 9 8 2 \right.$ (Rater P) and $\mathrm { p } { = } 0 . 9 5 7$ (Rater W)), also indicating high reliability.

The articulatory awareness quiz was marked manually by the first author according to a given key. The percent correct scores were calculated for statistical comparison between the experimental and control groups in terms of different phonemes (Griffiths & Frith, 2002). Due to the limited sample size, not all of the data were normally distributed for parametric testing. Therefore, the differences in both the production and articulatory awareness quiz results between the experimental and control groups were tested with Mann-Whitney U tests. The interviews were recorded and later transcribed and translated into English for qualitative analysis using a thematic approach.

# Results

# Production tests

The results from the production tests are divided into two distinct parts according to different articulatory gestures: dark/ɫ/, and dental fricatives/θ/and/ð/.

Figure 5 offers a visual representation of the production results for the consonant dark/ɫ/(see Appendix D for descriptive statistics). Mann-Whitney tests showed a significant difference in the production results of dark/ɫ/between the experimental and control groups at posttest $_ { ( \mathrm { p = . 0 8 9 } ) }$ . Moreover, the difference between the experimental and control groups at the delayed test was even more marked $\left( \mathtt { p } \mathrm { = } . 0 3 5 \right)$ . Mann-Whitney tests also revealed a significant difference between the pretest and posttest scores $\scriptstyle ( \mathtt { p } < . 0 0 1 )$ within the experimental group.

Mann-Whitney tests did not show a significant difference in the production results for dental fricatives between the experimental and control groups at posttest $\left( \mathrm { p } { = } . 7 9 6 \right)$ (see Appendix D for descriptive statistics). However, when we examined the dental fricatives/θ/and/ð/ closely and analyzed them separately, the statistics showed different patterns between the voiceless/θ/and voiced/ð/(see Figure 6).

Mann-Whitney tests did not reveal a significant difference in the production results for/ð/between the experimental and control groups at posttest and delayed test $( \mathtt { p } { = } . 4 2 1$ and $\mathrm { p } { = } . 6 9 0$ , respectively). However, there was a significant difference between the two groups at posttest for the consonant/θ/ $\scriptstyle ( \mathtt { p } = . 0 5 6 )$ . A Mann-Whitney test also revealed a significant difference between the experimental and control groups at the delayed test $\left( \mathrm { p } { = } . 0 9 5 \right)$ , and the mean scores for voiced/ð/were lower than those of voiceless/θ/for both groups in all three tests. This means that voicing or the vibration of vocal cords affected the production performances of the experimental group. These results may reveal that the AR filters did not work for segmental production involving articulatory gestures other than those of mouth articulators.

![](img/3084f96ed5d4729169e8863493edbbfa225eb27f082462e06458139e1053ae78.jpg)  
Figure 5. M ean production scores for dark/ɫ/at pretest, posttest, and delayed test.

![](img/6bec44b8ea0be23e3a761cf325a4f3abebed580b7c2ff3fd8a99981f16a65c10.jpg)  
Figure 6. M ean production scores for/θ/and/ð/at pretest, posttest, and delayed test.

# Articulatory awareness quiz

The posttest articulatory awareness quiz percent correct scores for the experimental and control groups are shown in Table 1. The fillers have already been taken out of the table. The scores indicate that in both the experimental and control groups the majority of the participants correctly answered the questions concerning the articulatory gestures utilized in the production procedure presented in the videos.

In the articulatory awareness quiz we did not adopt ‘don’t know’ as a third option. The Fisher exact probability test (Siegel, 1956) may be employed when a response category contains less than four subject choices, and therefore this test was adopted for the statistical comparison between the experimental and control groups in relation to individual words. The probability levels indicate that all the differences for dark/ɫ/ successfully reached the predetermined rejection level of $\mathtt { p } = 0 . 1 0$ . That is, the experimental group performed at a significantly higher level for dark/ɫ/.

Table 1. P ercent correct scores of the articulatory awareness posttest.   

<html><body><table><tr><td colspan="7"></td></tr><tr><td>Questions</td><td>/0/ in think</td><td>Dark/t/ in people</td><td>Dark /t/ in girl</td><td>/0/ in thank</td><td>// in with</td><td>Dark /t/ in pull</td></tr><tr><td>Experimental Group</td><td>92.59%</td><td>81.48%</td><td>81.48%</td><td>96.30%</td><td>100.00%</td><td>74.07%</td></tr><tr><td>Control Group</td><td>82.14%</td><td>57.14%</td><td>64.29%</td><td>92.86%</td><td>92.86%</td><td>25.00%</td></tr><tr><td>Probability Level</td><td>0.17</td><td>0.036</td><td>0.089</td><td>0.389</td><td>0.255</td><td>0.00027</td></tr></table></body></html>

$\mathsf { ^ { \ast } p } \mathsf { < } = 0 . 1 0$ .

We used Mann-Whitney tests to analyze the between-group differences in students’ articulatory awareness before and after the experiment. The articulatory awareness quiz results for the consonant dark/ɫ/are illustrated in Figure 7 (see Appendix E for descriptive statistics). A Mann-Whitney test revealed that the experimental group performed significantly better than the control group with the use of AR filters. However, as shown in Figure 8, the differences in the percent correct scores between the experimental and control groups were still not statistically significant for dental fricatives (see Appendix E for descriptive statistics).

According to a correlation analysis, there are significant and positive correlations between the articulatory awareness scores and the production posttest scores $\scriptstyle ( \mathrm { r } = . 4 2 5$ , $\mathrm { p } { < } . 0 1 $ ), and between the articulatory awareness scores and the production delayed test scores $( \mathrm { r } { = } . 4 3 5 , \ \mathrm { p } { < } . 0 1 )$ .

![](img/a4fe455188183c7bb84f39ac3f1f9485c890ff37bb4b2978ee693a6949fdda5e.jpg)  
Figure 7. P ercent correct scores of articulatory awareness for dark/ɫ/at pretest and posttest.

![](img/f6b554642b8d524670fdc4b0e905c8c54c7751aac63026de6f2621dfc7356c4b.jpg)  
Figure 8. P ercent correct scores of articulatory awareness for/θ/and/ð/at pretest and posttest.

# Interview results

The interview results answered the third research question about the participants’ attitudes toward the use of AR filters in their production practice. Moreover, it generally corroborated the quantitative results of articulatory awareness quizzes, and partially answered the second research question that the intervention cultivates overall articulatory awareness.

Based on the thematic analysis, the interview results were categorized into three themes: the noticing process, interaction with the teacher, and motivation for practice routine. In the following paragraphs extracts from the seven interviewees are used to illustrate these three themes. Pseudonyms are used to protect the participants’ confidentiality.

Whether they showed higher or lower improvement in their production performance, most of the interviewees (6 out of 7) reported that the use of AR filters meant that they could observe their articulators more clearly compared with standard front-view cameras or traditional mirrors. Their attention was directed to the enlarged images of their articulators by their use of the digital zoom built into the app:

1. “Obviously, the AR filter works better than traditional mirrors when we’d like to check the openness of the mouths. And it can help us raise our awareness to correct the pronunciation errors in our practice.” (Sammy)   
2. “Indeed, I can observe the shape of my mouth and tongue movement more clearly with the zoom-in in the AR filter.” (Ruby)   
3. “I have noticed some details that had been neglected before using the AR filter.” (Eve)   
4. Some of them offered further explanations of why their attention was directed to the articulators:   
5. “Compared with the front-view camera, the digital zoom-in helped me pay more attention to the pronunciation practice itself, focusing on my mouth features instead of my facial appearance or something else irrelevant to the practice.” (Sammy)   
6. “I can completely concentrate on my mouth using this app, since other facial features are eliminated or blurred.” (Eve)

Compared with previous tools for visualized feedback, the AR filters provided real-time, augmented, personalized visual self-feedback of students’ articulatory gestures during their production practice. Assisted by the app, the students’ attention was successfully drawn to problematic

L2 articulatory features. Moreover, they were able to reflect on the explicit knowledge of articulatory gestures that the teacher had delivered. This means that the students engaged in some degree of metalinguistic reflection, becoming more aware of the explicit or declarative knowledge delivered by the teacher, suggesting that the AR technology promoted the noticing process in pronunciation acquisition. Therefore, we have the reason to claim that the self-report data are consistent with the quantitative results from the articulatory awareness quiz.

Furthermore, the interview data also revealed that the AR filter was beneficial for the interaction between teacher and students, especially in the corrective feedback (CF) process. All the students were required to upload their practice recordings or videos to a homework collection platform every day, and the teacher then gave CF. A pilot survey about homework submission forms indicated that when they were asked about submitting recorded videos for CF, most students expressed reluctance because they felt awkward or shy about showing their faces to the teacher. Five of the interviewees confirmed that the recording function of the AR filter app meant that they were more willing to upload videos because their embarrassment, shyness, and even speaking anxiety were reduced:

1. “This way (with AR filter) of practice is really good. I worried a lot when the teacher asked us to upload video every day. That would be very embarrassing for me without AR filter.” (Ruby)   
2. “If I have to upload practice videos to teachers, I would definitely prefer recording videos without my own facial features other than those necessary parts.” (Lucy)   
3. As well as reducing these negative psychological factors, the app could also help with practical issues related to interacting with the teacher, making practice and uploading more convenient:   
4. “Because the AR video only shows your mouth, you do not have to worry about the background and the clothes you are wearing. It is more convenient since you can practice and upload the homework wherever you are.” (Nancy)   
5. In addition, two interviewees mentioned that the recorded videos helped them to review and correct their errors according to teacher’s comments:   
6. “It provides me with a record of the practice for a review.’ (Jenny)

The processes of practicing, uploading, commenting, and reviewing were all improved by using the app. Therefore, the interactions between teacher and students were greatly promoted.

Finally, the qualitative data indicated that the AR filter app could bring a lot of fun and enjoyment to the learners’ practice. Six interviewees reported that using the AR filter app was a novel way to carry out their pronunciation practice. Four of them recalled that they showed great interest in using this app and had a lot of fun with it: “Every time I used AR filters to practice in the dorm, my roommates were very happy. All of us agree it is really funny.” (Jenny)

These comments provide evidence that the new mobile-assisted AR filter app aroused the learners’ interest and enjoyment, and motivated them to keep practicing every day.

# Discussion and implications

The quantitative and qualitative data have both provided a rich amount of information in response to the research questions. The production results demonstrated the success of AR filters in facilitating the learners’ targeted segmental production. The articulatory awareness quiz results showed that the intervention significantly cultivated the articulatory awareness of dark/ɫ/. The interview data not only corroborated the result of the articulatory awareness quiz, but revealed that the students hold an overall positive attitude towards the use of this app, describing the experience as motivating, enjoyable and engaging which is consistent with the findings of previous studies on mobile AR tools in language learning (Perry, 2015; Solak & Cakir, 2015; Wu, 2021). In this section the results are discussed in relation to previous research and their contribution to the field of L2 learning with AR technology and apps.

# What do AR filters augment?

The AR filter includes digital zoom technology, which, as Glaser et  al. (2017) found in their eye-movement experiment, plays a positive role in directing attention when looking at visual learning material. Our interview data revealed that the students’ attention was successfully guided to the problematic L2 articulatory features by the digital zoom, and that they began to reflect on the explicit knowledge about articulatory gestures delivered by their teacher. This suggests that the students had engaged in some degree of metalinguistic reflection, becoming more aware of the explicit or declarative knowledge they had acquired in class. These findings can also be substantiated by the results of the articulatory awareness quiz, where the experimental group showed significant improvement with the use of the AR filter for dark/ɫ/compared to the control group.

For the dental fricatives/θ/and/ð/the differences between the percent correct scores of the experimental and control groups were not statistically significant, with both groups performing well in the articulatory awareness pretest (the experimental group scored slightly higher). We may assume that the traditional explicit verbal instruction given to both groups worked effectively in enhancing the students’ articulatory perception of dental fricatives, but the use of the AR filter app in their practice helped the experimental group to proceduralize and autonomize their articulatory knowledge of dental fricatives/θ/and/ð/into production behavior. This explains why the experimental group performed significantly better than the control group in producing fricatives in both the post- and delayed tests. However, for the more problematic consonant dark/ɫ/, where both groups performed badly in the production pretest, the AR filter app played a more important role in cultivating the students’ articulatory awareness by cueing their attention through enlarged images of their articulators (lips and tongue).

According to skill acquisition theory (DeKeyser, 2017) and the Noticing Hypothesis (Schmidt, 1990), conscious attention to form in the input or feedback that students receive is necessary for skill learning to take place. In the proceduralization and automatization of declarative knowledge into production behaviors, noticing is a necessary process that aids skill acquisition. Modern technology should be designed and used to promote this noticing, especially during production practice. Through correlation analysis we found that there was a positive correlation between articulatory awareness and production posttest scores, and a larger positive correlation between articulatory awareness and production delayed test scores.

Therefore, combining all the quantitative results and the qualitative interview data, we believe that the AR filter app, with its augmented images of learners’ articulators, is conducive to guiding their attention to articulatory gestures, raising their articulatory awareness, proceduralizing explicit articulatory knowledge and thereby facilitating academic gains in segmental production.

# What do AR filters filter out?

The interview data provided us with a detailed explanation for the participants’ academic production gains and their improved articulatory awareness. In addition to augmenting relevant features of the required articulators, the AR app also filters out irrelevant facial features. This filter function not only helps to increase articulatory awareness by concealing other facial features except for the mouth, but also reduces or eliminates some negative psychological factors that might inhibit L2 learning and production.

Multimedia learning is an effective teaching technique because multiple cues can be used to direct learners’ attention to relevant elements or segments (Van Gog, 2014), and this is especially true for L2 pronunciation learning. However, the majority of the interviewees from our study mentioned that they might have felt embarrassed or reluctant if they had been asked to submit a video showing other facial features to the teacher. Most of the interviewees felt relieved because of the use of the AR filter. According to skill acquisition theory, pronunciation instruction requires ample practice and CF from teachers (DeKeyser, 2017). However, these processes involve at least two types of specific L2 anxieties: speaking anxiety, and fear of negative evaluation (Horwitz, 1986). As discussed in the literature review, Chinese L2 learners tend to suffer from psychological issues such as speaking anxiety and shyness (Mak, 2011; Paulhus et  al., 2002). Therefore, the negative psychological issues that the AR filter helped to reduce in this study can be seen as a guarantee of production gains, since this was helpful in motivating the learners to engage with the necessary practice and CF processes. Moreover, the elimination of the speaker’s background display made it easier and more convenient for the students to practice, record and submit their production. This also contributed to their acceptance of this new technology.

# What L2 learning processes can the selfie app be used in?

In raising learners’ articulatory awareness by augmenting articulators and reducing negative psychological factors by covering the non-articulatory features, the selfie app also brought other benefits for the learners. Most of the interviewees indicated that they were very happy with the numerous types of filters, which included animals, statues, fruits, emojis, aliens, beauties, ghosts, and so on. These imaginative and novel filters provided them with extra fun and enjoyment, even contributing to a happy atmosphere among their peers. These positive emotions kept the learners highly motivated in terms of their deliberate, repeated practice routine. In addition, the app’s recording function facilitated the learners’ interaction with the teacher for CF, which is closely linked to practice and is beneficial and necessary for moving learners forward in their L2 skill acquisition (DeKeyser, 2017). The recording also helped learners to review their production attempts on their own in conjunction with the comments from the teacher. Although the app’s social (or sharing) function was not used in this quasi-experiment, we might assume that the learners would be keen to share their production with their peers, given that some of them confirmed that their roommates also derived a lot of enjoyment when they practiced with the app in the dorm.

Synthesizing all the qualitative and quantitative results, we can conclude that the selfie and social app supported by AR filters could be advantageous to other aspects of the L2 pronunciation learning processes such as systematic practice, assignment submission for CF, self-monitored training, and group practice, by providing multidimensional (academic, psychological, and social) aids.

# Conclusions

This article examined the extent to which a selfie app supported by AR filter technology could benefit L2 segmental production and the cultivation of articulatory awareness. The study also investigated learners’ attitudes (including their learning experience, satisfaction, and acceptance) towards the app’s use. Within the framework of skill acquisition theory, we focused on the effectiveness of the app in the learning processes of noticing, systematic practice, and CF.

The findings indicate that experimental group showed a significantly bigger improvement in the production of/ɫ/and/θ/than the control group, but no significant differences were reported between the two groups for the voiced dental fricative $\mathtt { \mathtt { N } } /$ . This demonstrates that the AR filter facilitated the production of phonemes involving articulatory gestures in the oral cavity, but failed to solve production problems involving the vocal cords, such as voicing. The results of the articulatory awareness quiz and the interview revealed that the use of the AR filter with digital zoom technology played a positive role in guiding learners’ attention to the relevant articulatory features as well as eliciting reflections on teachers’ explicit articulatory knowledge instruction, thereby raising their articulatory awareness. In addition there was a positive correlation between articulatory awareness and the learners’ scores in a post production test, and a higher positive correlation between articulatory awareness and scores in a delayed test. This confirms the value of the noticing process in skill acquisition. Combining all the qualitative and quantitative results, we can conclude that the selfie and social app, equipped with AR, facial detection, and digital zoom technologies, provided academic, psychological, and social support for L2 pronunciation skill learning.

The study is significant to the field in several ways. First, our results extend pronunciation skill acquisition theory by providing empirical evidence that noticing, as promoted by AR filter technology, may help to enhance articulatory awareness, thereby proceduralizing declarative articulatory knowledge into production behaviors acting on that knowledge. Second, this study promotes the use of AR-assisted apps in various L2 pronunciation learning processes such as systematic practice, assignment submission for CF, self-monitored training, and group practice.

Therefore, it switches the focus of CAPT from diagnostic visual feedback for pronunciation measurements to procedural visual feedback for pronunciation practice and CF. This shift helps L2 learners to proceduralize ‘knowing that’ to ‘‘knowing why’ and ‘knowing how’, which is conducive to awareness raising and explicit learning. Third, there is a dearth of research on learner awareness of pronunciation and learner autonomy beyond the classroom, because L2 learners tend to be unaware of their pronunciation difficulties and the gaps between their articulatory gestures and more desirable ones (Trofimovich et  al., 2016). Therefore, based on our findings we recommend that AR filter apps should be widely used to promote learner autonomy out of class and their awareness of pronunciation acquisition. Finally, given that the AR filter app may help to relieve some negative psychological issues in L2, especially foreign language anxiety, the app could also be used in other L2 speech fluency training for L2 oral courses, such as speeches and debating.

This study has at least four limitations. First, we had the limited interview data, including only seven interviewees. Second, due to technical limitations we could not investigate the pronunciation training of phonemes that require more side-view visual feedback of the mouth, such $\mathrm { { a s } / \mathrm { { J } / \mathrm { { a n d } / \mathrm { { 3 } } } } } /$ . We suggest that AR filter apps could be developed to provide multi-view avatars for L2 learning. Third, more natural elicited data at the sentence level could be collected from the production tests, in addition to the word-level read-aloud task. This would offer a more authentic reflection of the learners’ pronunciation skill acquisition. Finally, due to the rating difficulties, vowels were not considered in the current study. For the future research, we suggest that with the feasibility of AR technologies to annotate objects (Azuma, 1997), AR filter apps can be further used to remind students of the shape and size of their articulators when practicing vowels.

# Acknowledgments

We are indebted to the research participants who generously shared their ideas and experiences with us. We would also like to thank the anonymous reviewers for comments on an earlier version of this paper.

# Disclosure statement

No potential conflict of interest was reported by the authors.

# Funding

This work was supported by the [College Students’ Innovative Entrepreneurial Training Plan Program] under grant [2021120142].

# Notes on contributors

Jian $L i$ is an associate professor at School of Foreign Studies, Shanghai University of Finance and Economics, the People’s Republic of China. She conducts research on the effects of mobile-assisted technology (e.g., digital zoom, games, AR, online videoconference) on L2 pronunciation learning and teaching. She is a member of Pronunciation Teaching and Phonetic Research Council of China Association for Comparative Studies of English and Chinese.

Jingjing Zhu is a student at School of Foreign Studies, Shanghai University of Finance and Economics, the People’s Republic of China. Her research interests include phonetics and the use of mobile technologies in L2 pronunciation learning and teaching.

Xi Zhang is a student at School of Foreign Studies, Shanghai University of Finance and Economics, the People’s Republic of China. Her research focuses on statistical analysis and computer-assisted language learning.

# References

Ao, R., & Low, E. L. (2012). Exploring pronunciation features of Yunnan English. English Today, 28, 27–33. https://doi.org/10.1017/S0266078412000284   
Azuma, R. T. (1997). A survey of augmented reality. Presence: Teleoperators and Virtual Environments, 6(4), 355–385. https://doi.org/10.1162/pres.1997.6.4.355   
Badin, P., Tarabalka, Y., Elisei, F., & Bailly, G. (2010). Can you ‘read tongue movements’? Evaluation of the contribution of tongue display to speech understanding. Speech Communication, 52(6), 493–503. https://doi.org/10.1016/j.specom.2010.03.002   
Brown, P. C., Roediger, H. L. I., & McDaniel, M. A. (2014). Make it stick: The science of successful learning. Belknap/Harvard.   
Carey, M. D. (2002). An L1-specific CALL pedagogy for the instruction of pronunciation with Korean learners of English [Unpublished doctoral thesis]. Macquarie University.   
Chen, C. (2020). AR videos as scaffolding to foster students’ learning achievements and motivation in EFL learning. British Journal of Educational Technology, 51(3), 657–672. https://doi.org/10.1111/bjet.12902   
Colpaert, J. (2004). From courseware to coursewear? Computer Assisted Language Learning, 17(3-4), 261–266. https://doi.org/10.1080/0958822042000319575   
Craig, A. B. (2013). Understanding augmented reality: Concepts and applications. Elesvier.   
Darcy, I., Ewert, D., & Lidster, R. (2012). Bringing pronunciation instruction back into the classroom: An ESL teachers’ pronunciation “toolbox. In J. Levis & K. LeVelle (Eds.), Proceedings of the 3rd Annual Pronunciation in Second Language Learning and Teaching Conference (pp. 93–108). Iowa State University.   
De BOT, C. L. J. (1980). The role of feedback and feedforward in the teaching of pronunciation - An overview. System, 8, 35–45. https://doi.org/10.101 6/0346-251X(80)90022-6   
DeKeyser, R. (2017). Knowledge and skill in ISLA. In S. Loewen & M. Sato (Eds.), The Routledge handbook of instructed second language acquisition (pp. 15–32). Routledge.   
Derwing, T. M., & Munro, M. J. (2005). Second language accent and pronunciation teaching: A research-based approach. TESOL Quarterly, 39(3), 379–397. https://doi. org/10.2307/3588486   
Deterding, D. (2007). Singapore English. Edinburgh University Press. https://doi. org/10.3366/edinburgh/9780748625444.001.0001   
Duman, G., Orhon, G., & Gedik, N. (2015). Research trends in mobile assisted language learning from 2000 to 2012. ReCALL, 27(02), 197–216. https://doi.org/10.1017/ S0958344014000287   
Ebadijalal, M., & Yousofi, N. (2021). The impact of mobile-assisted peer feedback on EFL learners’ speaking performance and anxiety: Does language make a difference? The Language Learning Journal, online published, https://doi.org/10.1080/09571736.2 021.1957990   
Engwall, O. (2012). Analysis of and feedback on phonetic features in pronunciation training with a virtual teacher. Computer Assisted Language Learning, 25(1), 37–64. doi.org/ https://doi.org/10.1080/09588221.2011.582845   
Engwall, O., & Bälter, O. (2007). Pronunciation feedback from real and virtual language teachers. Computer Assisted Language Learning, 20(3), 235–262. doi.org/ https://doi. org/10.1080/09588220701489507   
Flege, J. E., Munro, M. J., & Skelton, L. (1992). Production of the word-final English /t/-/d/ contrast by native speakers of English, Mandarin, and Spanish. The Journal of the Acoustical Society of America, 92(1), 128–143. https://doi.org/10.1121/1.404278   
Fouz-González, J. (2020). Using apps for pronunciation training: An empirical evaluation of the English File Pronunciation app. Language Learning & Technology, 24(1), 62–85.   
Glaser, M., Lengyel, D., Toulouse, C., & Schwan, S. (2017). Designing computer-based learning contents: Influence of digital zoom on attention. Educational Technology Research and Development, 65(5), 1135–1151. https://doi.org/10.1007/s11423-016- 9495-9   
Griffiths, S., & Frith, U. (2002). Evidence for an articulatory awareness deficit in adult dyslexics. Dyslexia, 8(1), 14–21. doi.org/ https://doi.org/10.1002/dys.201   
Guion, S. G., Flege, J. E., & Loftin, J. D. (2000). The effect of L1 use on pronunciation in Quichua–Spanish bilinguals. Journal of Phonetics, 28(1), 27–42. 10.1006/ jpho.2000.0104   
Haldin, C., Acher, A., Kauffmann, L., Hueber, T., Cousin, E., Badin, P., Perrier, P., Fabre, D., Perennou, D., Detante, O., Jaillard, A., Loevenbruck, H., & Baciu, M. (2018). Speech recovery and language plasticity can be facilitated by sensori-motor fusion training in chronic non-fluent aphasia. A case report study. Clinical Linguistics & Phonetics, 32(7), 595–621. https://doi.org/10.1080/02699206.2017.1402090   
Hawker, K., & Carah, N. (2021). Snapchat’s augmented reality brand culture: Sponsored filters and lenses as digital piecework. Continuum, 35(1), 12–29. doi.org/ https://doi. org/10.1080/10304312.2020.1827370   
Horwitz, E. K. (1986). Preliminary evidence for the reliability and validity of a foreign language anxiety scale. TESOL Quarterly, 20(3), 559–562. doi.org/ https://doi. org/10.2307/3586302   
Hwang, G. J., Hsu, T. C., Lai, C. L., & Hsueh, C. J. (2017). Interaction of problem-based gaming and learning anxiety in language students’ English listening performance and progressive behavioral patterns. Computers & Education, 106, 26–42. doi.org/ https:// doi.org/10.1016/j.compedu.2016.11.010   
Karlsson, M., Penteniari, G., & Croxson, H. (2018). Accelerating affordable smartphone ownership in emerging markets. Walbrook: GSMA.   
Knoch, U., & Macqueen, S. (2017). Assessment in the L2 classroom. In S. Loewen & M. Sato (Eds.), The Routledge handbook of instructed second language acquisition (pp. 181–202). Routledge.   
Kukulska-Hulme, A. (2009). Will mobile learning change language learning? ReCALL, 21(2), 157–165. doi.org/ https://doi.org/10.1017/S0958344009000202 The encyclopedia of applied linguistics (pp.3701–3709). Malden, MA: Wiley-Blackwell.   
Kukulska-Hulme, A. (2012b). Mobile learning and the future of learning. International HETL Review, 2, 13–18.   
Kukulska-Hulme, A. (2016). Personalization of language learning through mobile technologies. Cambridge University Press.   
Kukulska-Hulme, A., & Shield, L. (2008). Overview of mobile assisted language learning: From content delivery to supported collaboration and interaction. ReCALL, 20(3), 271–289. doi.org/ https://doi.org/10.1017/S0958344008000335   
Levis, J. (2005). Changing contexts and shifting paradigms in pronunciation teaching. TESOL Quarterly, 39(3), 369–377. doi: 39. https://doi.org/10.2307/3588485   
Liberman, A. M. (1999). The reading researcher and the reading teacher need the right theory of speech. Scientific Studies of Reading, 3(2), 95–111. https://doi.org/10.1207/ s1532799xssr0302_1   
Li, S. Q., & Sewell, A. (2012). Phonological features of China English. Asian Englishes, 15(2), 80–101. doi.org/ https://doi.org/10.1080/13488678.2012.10801331   
Li, Y., & Somlak, T. (2017). The effects of articulatory gestures on L2 pronunciation learning: A classroom-based study. Language Teaching Research, 23(3), 352–371. doi. org/ https://doi.org/10.1177/1362168817730420   
Loewen, S., & Sato, M. (2017). The Routledge handbook of instructed second language acquisition. Routledge.   
Lyster, R., & Saito, K. (2010). Oral feedback in classroom SLA: A meta-analysis. Studies in Second Language Acquisition, 32(2), 265–302. https://doi.org/10.1017/ S0272263109990520   
Mak, B. (2011). An exploration of speaking-in-class anxiety with Chinese ESL learners. System, 39(2), 202–214. https://doi.org/10.1016/j.system.2011.04.002   
Martin, I. A. (2020). Pronunciation development and instruction in distance language learning. Language Learning & Technology, 24(1), 86–106.   
Massaro, D., & Light, J. (2003). Read my tongue movements: Bimodal learning to perceive and produce non-native speech/r/and/l/. Eurospeech, 8, 2249–2252.   
Mazer, J. P., Murphy, R. E., & Simonds, C. J. (2007). I’ll see you on Facebook: The effects of computer mediated teacher self-disclosure on student motivation, affective learning, and classroom climate. Communication Education, 56(1), 1–17. https://doi. org/10.1080/03634520601009710   
Mompean, J. A., & Fouz-González, J. (2016). Twitter-based EFL pronunciation instruction. Language Learning & Technology, 20(1), 166–190.   
Montgomery, D. (1981). Do dyslexics have difficulty accessing articulatory information. Psychological Research, 43, 235–243. doi.org/ https://doi.org/10.1007/BF00309832   
Motohashi-Saigo, M., & Hardison, D. M. (2009). Acquisition of L2 Japanese geminates training with waveform displays. Language Learning and Technology, 13(2), 29–47.   
Offerman, H. M., & Olson, D. J. (2016). Visual feedback and second language segmental production: The generalizability of pronunciation gains. System, 59, 45–60. doi. org/ https://doi.org/10.1016/j.system.2016.03.003   
Ouni, S. (2014). Tongue control and its implication in pronunciation training. Computer Assisted Language Learning, 27(5), 439–453. https://doi.org/10.1080/09588221.2012.761637   
Palalas, A. (2011). Mobile-assisted language learning: Designing for your students. In S. Thouësny & L. Bradley (Eds.), Second language teaching and learning with technology: Views of emergent researchers (p. 7194). Research-publishing.net.   
Parmaxi, A., & Demetriou, A. A. (2020). Augmented reality in language learning: A state‐of‐the‐art review of 2014–2019. Journal of Computer Assisted Learning, 36(6), 861–875. https://doi.org/10.1111/jcal.12486   
Paulhus, D. P., Duncan, J. H., & Yik, M. S. M. (2002). Patterns of shyness in East-Asian and European-heritage students. Journal of Research in Personality, 36, 442–462. doi. org/ https://doi.org/10.1016/S0092-6566(02)00005-3   
Perry, B. (2015). Gamifying French language learning: A case study examining a quest-based, augmented reality mobile learning-tool. Procedia - Social and Behavioral Sciences, 174, 2308–2315. doi.org/. https://doi.org/10.1016/j.sbspro.2015.01.892   
Piske, T., MacKay, I. R. A., & Flege, J. E. (2001). Factors affecting degree of foreign accent in an L2: A review. Journal of Phonetics, 29(2), 191–215. https://doi.org/10.1006/ jpho.2001.0134   
Reinders, H., & Wattana, S. (2014). Can I say something? The effects of digital game play on willingness to communicate. Language Learning & Technology, 18(2), 101–123.   
Rogerson-Revell, P. M. (2021). Computer-Assisted pronunciation training (CAPT): Current issues and future directions. RELC Journal, 52(1), 189–205. https://doi. org/10.1177/0033688220977406   
Ruan, K., & Jeong, H. (2012 An augmented reality system using QR code as marker in Android Smartphone [Paper presentation]. 2012 Spring Congress on Engineering and Technology (S-CET). https://doi.org/10.1109/SCET.2012.6342109   
Ruscello, D. M., Moreau, V. K., & Sholtis, D. (1980). Awareness of ceratin articulatory gestures in normal-speaking and articulatory-defective children. Journal of Communication Disorders, 13(1), 59–64. https://doi.org/10.1016/0021-9924(80)90022-2   
Saito, K. (2018). Individual differences in second language speech learning in classroom settings: Roles of awareness in the longitudinal development of Japanese learners’ English/ɹ/pronunciation. Second Language Research, 35(2), 149–172. https://doi. org/10.1177/0267658318768342   
Salunkhe, P., & Mehta, V. (2016). Intelligent mirror: Detecting skin cancer (Melanoma) using convolutional neural network with augmented reality feedback. International Journal of Computer Applications, 154(6), 4–7. https://doi.org/10.5120/ijca2016912149   
Sato, M., & Lyster, R. (2012). Peer interaction and corrective feedback for accuracy and fluency development. Studies in Second Language Acquisition, 34(4), 591–626. https://doi.org/10.1017/S0272263112000356   
Schmidt, R. W. (1990). The role of consciousness in second language learning. Applied Linguistics, 11(2), 129–158.   
Sharma, D., Bhatia, A., & Chethan, S. (2017). Augmented reality in academia-changing landscape of education. Journal of Engineering and Applied Science, 12(12), 3113–3116. https://doi.org/10.3923/jeasci.2017.3113.3116   
Siegel, S. (1956). Nonparametric statistics for the behavioral sciences. McGraw-Hill.   
Solak, E., & Cakir, R. (2015). Exploring the effect of materials designed with augmented reality on language learners’ vocabulary learning. Journal of Educators Online, 12(2), 50–72.   
Trofimovich, P., Isaacs, T., Kennedy, S., Saito, K., & Crowther, D. (2016). Flawed self-assessment: Investigating self- and other-perception of second language speech. Bilingualism: Language and Cognition, 19(1), 122–140. https://doi.org/10.1017/ S1366728914000832   
Tsai, P. H. (2019). Beyond self-directed computer-assisted pronunciation learning: A qualitative investigation of a collaborative approach. Computer Assisted Language Learning, 32(7), 713–744. https://doi.org/10.1080/09588221.2019.1614069   
Van Gog, T. (2014). The signaling (or cueing) principle in multimedia learning. In R. E. Mayer (Ed.), The Cambridge handbook of multimedia learning (pp. 263–278). Cambridge University Press. https://doi.org/10.1017/CBO9781139547369.014   
VanPatten, B., & Cadierno, T. (1993). Explicit instruction and input processing. Studies in Second Language Acquisition, 15, 225–241. http://dx.doi.org/10.1017/ S0272263100011979   
Viberg, O., & Grönlund, A. (2012 Mobile assisted language learning: A Literature review [Paper presentation]. 11th World Conference on Mobile and Contextual Learning.   
Wu, M. H. (2021). The applications and effects of learning English through augmented reality: A case study of Pokémon Go. Computer Assisted Language Learning, 34(5–6), 778–812. https://doi.org/10.1080/09588221.2019.1642211   
Wu, T. T. (2018). Improving the effectiveness of English vocabulary review by integrating ARCS with mobile game-based learning. Journal of Computer Assisted Learning, 34(3), 315–323. https://doi.org/10.1111/jcal.12244   
Wu, Y., & Ji, Q. (2019). Facial landmark detection: A literature survey. International Journal of Computer Vision, 127(2), 115–142. https://doi.org/10.1007/s11263-018-1097-z   
Yang, M. H. (2004). Recent advances in face detection. In IEEE ICPR 2004 Tutorial. http://vision.ai.uiuc.edu/mhyang/papers/icpr04_tutorial.pdf   
Zhang, D. Y., & Pérez-Paredes, P. (2021). Chinese postgraduate EFL learners’ self-directed use of mobile English learning resources. Computer Assisted Language Learning, 34(8), 1128–1153. https://doi.org/10.1080/09588221.2019.1662455

# Appendices

The words in bold are fillers.

<html><body><table><tr><td> Pre-test</td><td>Post-test</td><td>Delayed-test</td></tr><tr><td>south</td><td>month</td><td>athlete</td></tr><tr><td>health</td><td> wealth</td><td>thank</td></tr><tr><td>third</td><td>faith</td><td>three</td></tr><tr><td>nothing</td><td>thought</td><td>nothing</td></tr><tr><td>forth</td><td>Thursday</td><td>health</td></tr><tr><td>that</td><td>neither</td><td>breathe</td></tr><tr><td>smooth</td><td>smooth</td><td>clothe</td></tr><tr><td>these</td><td>those</td><td>that</td></tr><tr><td>father</td><td>their</td><td>these</td></tr><tr><td>other</td><td>bathe</td><td>further</td></tr><tr><td>pressure</td><td>machine</td><td>seizure</td></tr><tr><td>catch</td><td> giant</td><td>action</td></tr><tr><td>elbow</td><td>middle</td><td>puzzle</td></tr><tr><td>twelve</td><td>film</td><td>field</td></tr><tr><td>belt</td><td>belt</td><td>angle</td></tr><tr><td>film</td><td>table</td><td>apple</td></tr><tr><td>battle</td><td>solve</td><td>twelve</td></tr><tr><td>help</td><td> soul</td><td>shelf</td></tr><tr><td>shelf</td><td>apple</td><td> deal</td></tr><tr><td>table</td><td>folk</td><td>bowl</td></tr><tr><td>milk</td><td>elbow</td><td>feel</td></tr><tr><td>cool</td><td>cool</td><td>flm</td></tr></table></body></html>

![](img/83b3b01bb2669a778acd590d25e9c4504cee575fdf0eb7e1f6a0f14e4267bdd1.jpg)

![](img/5717cb757a36c31a275a26d12483bc1e8090877f83b9b9ccf7c21fca8cf77988.jpg)

Appendix C. Semi-structured interview questions.   

<html><body><table><tr><td colspan="2">Themes</td><td>Questions</td><td>English version</td></tr><tr><td rowspan="3">Overview</td><td></td><td>1. 2i###4? 2. &amp;? EH</td><td>1. Have you ever studied/ practiced your own pronunciation before? 2. If so, by what means? Have you looked at your</td></tr><tr><td>?</td><td>3. X?</td><td>articulators? 3. Do you think it is helpful to practice your pronunciation by looking at your own articulators?</td></tr><tr><td></td><td>4.Z*#$/ I ne ny? 5.AR</td><td>4. Have you used a selfie app/feature like this before? 5. How do you feel about this pronunciation</td></tr><tr><td rowspan="4">Anxiety</td><td></td><td>H+t1e ? 1.</td><td>practice using AR selfie filter? 1. Do you get nervous when you speak English in</td></tr><tr><td></td><td>ky? H+? 2.</td><td>front of others? Why?</td></tr><tr><td></td><td>/KT y?</td><td>2. If you have to show your pronunciation in front of others, will you feel embarrassed/nervous about doing it?</td></tr><tr><td></td><td>3. F T E#</td><td>3. When you upload your homework, do you feel. embarrassed to upload it using built-in front view camera to record the video? How about in</td></tr><tr><td rowspan="4">Interest</td><td></td><td>Fa? 4.Fi</td><td>the case of using software to record? 4. Did AR filters make you pronounce more</td></tr><tr><td></td><td>ky?</td><td>confidently/ nervously?</td></tr><tr><td></td><td>1. */</td><td>1. Do you think this new way of practicing pronunciation is any more interesting than</td></tr><tr><td></td><td>Fy? 2. t $j j ? 3.</td><td>practicing with a book or looking at a mirror? 2. If so, in which specific aspects? 3. Do you think it is a novel way to practice using selfie apps?</td></tr></table></body></html>

<html><body><table><tr><td>Awareness</td><td>4. ? 5. #? i Tk? 6.i#j H2xi##bjjn? k1 Aiy? can be improved? Do you have any suggestions?.</td><td>7. What do you think are the advantages of selfie apps compared with traditional practice. methods? 8. What do you care most about when practicing? Word spelling, mouth gestures or examples of. standard pronunciation recordings? 9. In which ways do you think this kind of practice</td></tr></table></body></html>

Appendix D. Descriptive statistics for production tests.   

<html><body><table><tr><td rowspan="2"></td><td colspan="2">Pretest</td><td colspan="2">Posttest</td><td colspan="2">Delayed test</td></tr><tr><td>M</td><td>SD</td><td>M</td><td>SD</td><td>M</td><td>SD</td></tr><tr><td>Production - dark I</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Experimental group (n =27)</td><td>0.185</td><td>0.669</td><td>7.037</td><td>2.426</td><td>6.889</td><td>2.439</td></tr><tr><td>Control group (n =28)</td><td>0.607</td><td>0.939</td><td>5.214</td><td>2.691</td><td>4.679</td><td>2.804</td></tr><tr><td>Production - /0/</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Experimental group (n=27)</td><td>3.630</td><td>1.281</td><td>4.667</td><td>0.544</td><td>4.370</td><td>0.909</td></tr><tr><td>Control group (n=28)</td><td>2.500</td><td>1.150</td><td>4.071</td><td>0.884</td><td>3.500</td><td>1.018</td></tr><tr><td>Production - /0/</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Experimental group (n=27)</td><td>4.037</td><td>1.290</td><td>4.889</td><td>0.314</td><td>4.481</td><td>0.631</td></tr><tr><td>Control group (n=28)</td><td>4.071</td><td>1.307</td><td>4.000</td><td>1.102</td><td>3.786</td><td>0.977</td></tr></table></body></html>

# Appendix E. Descriptive statistics for articulatory awareness quizzes.

<html><body><table><tr><td rowspan="2"></td><td colspan="2">Pretest</td><td colspan="2">Posttest</td></tr><tr><td>M</td><td>SD</td><td></td><td>SD</td></tr><tr><td>Experimental group (n=27)</td><td>1.296</td><td>0.656</td><td>2.407</td><td>0.828</td></tr><tr><td>Control group. (n = 28)</td><td>1.741</td><td>0.750</td><td>1.536</td><td>0.731</td></tr><tr><td>Experimental group (n=27)</td><td>2.259</td><td>0.798</td><td>2.889</td><td>0.314</td></tr><tr><td>Control groupe (n = 28)</td><td>2.333</td><td>0.816</td><td>2.643</td><td>0.666</td></tr></table></body></html>