# A large-scale corpus for assessing written argumentation: PERSUADE 2.0

S.A. Crossleya,\*, Y. Tianb, P. Baffourc, A. Franklinc, M. Bennerc, U. Boserc

a Vanderbilt University, United States b Digital Harbor Foundation, United States c The Learning Agency, United States

# ARTICLEINFO

# ABSTRACT

Keywords: Corpus linguistics Writing assessment Argumentation Individual differences

This research methods article introduces the open source PERSUADE 2.0 corpus. The PERSUADE 2.0 corpus comprises over 25,000 argumentative essays produced by 6th-12th grade students in the United States for 15 prompts on two writing tasks: independent and source-based writing. The PERSUADE 2.0 corpus also provides detailed individual and demographic information for each writer. The goal of the PERsUADE 2.0 corpus is to advance research into relationships between discourse elements, their effectiveness, writing quality, writing tasks and prompts, and demographic and individual differences.

# 1. Introduction

Argumentation is a logical appeal that involves stating claims and offering support to justify or rfute beliefs to influence others (Newell Beach, Smith & VanDerHeide, 2011). The ability to persuade with good argumentation skill lies at the core of critical thinking and has long been valued in personal, professonal, and academic contexts. Given the important role of argumentation in students' cognitive development and academic learning, the K-12 Common Core State Standards (Ccs, 2010) were developed in the United States to highlight the cultivation of argumentation sils in writing instruction. The ccs for argumentative writing stipulate that students should be able to introduce claims, distinguish the claims from alternative claims, and organize the clams logicall. Additionall, students should be able to support claims and counterclaims with logical reasoning and accurate data and evidence. However, many students in the United te stil struggle to construct ffective auments in writing due to the cognitiely demanding nature of argumentation. According to the 2012 National Assessment of Educational Progres (NAEP) Writing Report Card, a nationally representative and continuing assessment of America's students knowledge, only about $2 5 ~ \%$ of students' argumentative essays were scored as competent.

To better understand student dfficultie n argumentation, it i nessary to systmatically analye argument production in writing samples. One way to do this is through large-scale corpus analyses wherein a collection of student essays is annotated for argumen tation, argumentation effectiveness and overall writing quality. Quantitative analyse can then be conducted to better understand links between the podction of amentative elments, the efftivenessof thee ments, and ratings of quality. hese analyses can provide information on argument construction and efiacy that inform classroom practices. Additionally, computational modeling of argumentation and argumentative quality may provide opportunities for learning plaforms to provide students with granular feedback on writing quality which in turn could lead to more deliberative practice and writing development.

This paper introduces a corpus that can help address research and pedagogical interventions in student argumentation: the Persuasive Eays for Rating, Selecting, and Understanding Argumentative and Discourse Elements (PERsUADE) corpus 2.0. PERSUADE 2.0 builds on the PERsUADE 1.0 corpus (Crossley et al., 202), which comprises over 25,00 persuasive essays (both independent and dependent essays) annotated for discourse elements (e.g, position, claims, and evidence) as well as hierarchical relationships between these elements. The PERsUADE 1.0 corpus also includes detailed demographic information for the writers.

The PErsuADE 2.0 corpus provides holistic essay scores for each persuasive essay as well a effectivenes scores for each discourse element found in the PERSUADE 1.0 corpus. Like PERSUADE 1.0, PERSUADE 2.0 is open source. The purpose of the PERSUADE 2.0 corpus is to provide tool to increase knowledge of persuasive writing within the reearch community and to advance the development of unbiased computational algorithms that can predict overall writing quality and discourse element quality. The knowledge gleaned from the corpus wil help researchers and practitioners develop clasroom interventions to promote proficient writing skill. Algo. rithms developed from the corpus could inform automated writing evaluation (AWE) systems to beter asses student writing samples, provide more focused feedback, and promote better revising practices. The corpus is discussed in depth below.

# 2. PERSUADE 2.0 corpus

The PERSUADE corpus 2.0 corpus is an augmented version of the PERSUADE corpus 1.0, which comprises source-based essays $( { \mathfrak { n } } =$ 12,875) and independent essays $( \mathbf { n } = 1 3 , 1 2 1 ) .$ . The source-based essays were written on seven different writing prompts and each prompt had asingle source. Writers were sampled from 6th to 10th grades. The source-based essays were writen on eight different writing prompts sampled from writers from the 8th to 12th grade.

The essays included in the PERsuADE 1.0 corpus reflect the diversity of the student body found in the United States. Each essay is linked to the writers' gender and race/ethnicity with the majority of the writers identifying as White $( 4 5 \% )$ followed by Hispanic (25 $\%$ ) and Black $( 1 9 \% )$ . Most of the essays $\mathbf { ( n = 2 0 , 7 5 9 ) }$ are also linked to student eligibility for federal assistance programs that help identify students with economic disadvantages. The PERsuADE 1.0 essay contains anotations for discourse elements and includes relationships between the elements. Discourse elements include leads, positions, claims, countercaims, rebuttals, evidence and concluding summaries. All anotations were provided by expert raters based on a standardized rubric developed to identify common discourse elements similar to those found in Toulmin's argumentative framework (1958). More information about the PERsuADE 1.0 corpus is reported in Crossley et al., 2022.

The PERSUADE corpus 2.0 includes the same data as the PERsUADE corpus 1.0, but it also includes effectivenes scores for the individual discourse elements along with holistic scores for the overall quality of each essay.

# 2.1. Effectiveness scores for discourse elements

An outside consulting firm that specializes in educational data hired all raters and completed all effectivenes ratings. All ratings were conducted on a third-party annotation platform. Raters were trained on a standardized rubric? and were asked to score each element as fective, adequate, or ineffective. An effective score indicated that the writersucesfull fulilled the rhetorical or argumentative function of the element. Adequate indicated that the writer sufficiently fulflled the function of the element while ineffective indicated that the writer did not successully fulfil the function of the element. A description was given for each category along with an example of a discourse element that it that categorical score. The expert raters used a double-blind rating process and raters were trained by prompt. Raters were trained on independent esays first followed by source-based essays. After training, raters independently annotated esays for the discourse elements reported in the PERsuADE 1.0 corpus. They then provided effectiveness scores for each element.

Inter-rater reliabilit (IRR) is difficult to calculate for the effectiveness because raters segmented essays into discourse elements before scoring those elements for effectiveness In many cases, the segmented elements between two raters do not overlap perfectly, making alignment f annttions and effetiveness cores difficult. To calulat R, we examined sements where the first and second rater had at least $5 0 \%$ overlap in the discourse elements they annotated. Within the essays, there were 159,228 elements that had at least $5 0 \%$ overlap. Of these, the raters scored ${ \sim } 1 2 0 { , } 0 0 0$ segments as adequate, ${ \sim } 3 2 , 0 0 0$ as effective, and ${ \sim } 6 0 0 0$ as ineffective. Two statistics were calulated: exact agrement and a weighted Cohens Kappa. Exact agreement for ll clsses was.718. For specifi classes, exact agreement was between.68 and.81. Exact agreement was general higher in dependent essays as compared to independent essays. Cohen's Kappa was quite low for the quality ratings reporting $\kappa$ between.17 and.38 for specific classes. In general, Cohen's Kappa was higher in dependent essays in comparison to independent essays. The overall Cohen's Kappa for all classes was $\kappa = . 3 1 6$ indicating fir agrement. In the eents of effctivnes rating differece, a thid, exprt ratr erved as the adudicto to make  inal decision. All IRR statistics are reported in Table 1.

# 2.2. Holistic writing quality scores

The same raters that annotated the discourse elements for the essays and scored each element for effectiveness also scored each essay holisticall. For the independent esays, raters were trained on astandardized SA holistic esay scoring rubric. The rubric used a 1 to 6 scale that has interval levels (ie., the raters were told that the distance between each grade was equal). A score of 6 indicates an essay that shows clear and consistent mastery of writing. The same SAT rubric was slightly modifie for the source-based esays. The main difference was the incusion of evidencetaken from the source as a criterion for writing qualit. Toillustrate, the wording for an essay scored 6 from the independent rubric states "A typical essy...(uses clearly aropriate examples, reasons, and other evidence to support its position." For the dependent rubric, this wording was modified to state A typical esay.. uses clearly appropriate examples, reasons, and other evidence taken from the source text(s) to support its position. The two rubrics were matched as closely as possible to ensure that independent and source-based essays were comparable. Like the efectivenes rating, expert raters used a double-blind rating process followed by $1 0 0 \%$ adjudication such that each essay was scored holistically by two raters and adjudicated by a third rater f necessary. Raters were trained by prompt and essay type (independent essays were scored first ollowed by source. based essays). Inter-rater reliability before adjudication indicated strong agreement between the expert raters (weighted $\kappa = . 7 4 5$ $r =$ .750).

Table 1 IRR statistics for PERSUADE effectiveness ratings.   

<html><body><table><tr><td>Discourse Element</td><td>Exact Overlap All</td><td>Exact Overlap Independent</td><td>Exact Overlap Dependent</td><td>Cohen&#x27;s K All</td><td>Cohen&#x27;s k Independent</td><td>Cohen&#x27;s k Dependent</td></tr><tr><td>Lead</td><td>0.725</td><td>0.710</td><td>0.744</td><td>0.378</td><td>0.385</td><td>0.353</td></tr><tr><td>Position</td><td>0.807</td><td>0.809</td><td>0.804</td><td>0.274</td><td>0.241</td><td>0.303</td></tr><tr><td>Claim</td><td>0.677</td><td>0.674</td><td>0.682</td><td>0.165</td><td>0.144</td><td>0.191</td></tr><tr><td>Counterclaim</td><td>0.679</td><td>0.677</td><td>0.682</td><td>0.188</td><td>0.17</td><td>0.204</td></tr><tr><td>Rebuttal</td><td>0.676</td><td>0.663</td><td>0.701</td><td>0.256</td><td>0.23</td><td>0.281</td></tr><tr><td>Evidence</td><td>0.731</td><td>0.716</td><td>0.749</td><td>0.432</td><td>0.403</td><td>0.405</td></tr><tr><td>Concluding Statement</td><td>0.738</td><td>0.729</td><td>0.747</td><td>0.370</td><td>0.359</td><td>0.375</td></tr></table></body></html>

# 3. Possible futures

Around $8 0 \%$ of the PERsUADE 2.0 corpus was released in a Kaggle competition in late 2022. However, Kaggle competitors had no access tothe holisic qualit scores or the demographic and individual dfference scores. The competition asked data scientists to develop models to assign effectiveness ratings to annotated discourse elements. The competition used a subset of the PERSUADE corpus (und 690 t of the 6,0esays, or a itl er one quarer. ssas were et to esure alace acros efetivenes by discourse element. Feedback 2.0 prioritized computationally eficient algorithms that were simple and fast, but stil achieved high accuracy because complex models can negatively impact the evironment through energy consumption. Aditionally complex models are les suitable for use in real-world applications. Over 1500 teams and almost 2000 competitors participated in this challenge, generating nearly 30,o00 submissions. Winning models were able to predict human judgments of discourseffectiveness at between ${ \sim } 3 0 ~ \%$ and ${ \sim } 9 0 ~ \%$ accuracy. Accuracy was lowest for ineffective discourse elements which were often predicted to be adequate. Accuracy was highest for effective discourse elements (see Fig. 1).

While the entire corpus was not made available in the Kaggle competition, the entire PErsuADE 2.0 corpus has been made available for researchers including data for demographic information, individual dfferences, and holistic writing quality. Our hope is that the PERsUADE 2.0 corpus wil help advance research into relationships betwen argumentative and discourse elements, their effectiveness, writing quality, writing tass and prompts, and demographic and individual difference. The size of the data along with the reliability of human annotations should ensure that research findings derived from the data will forward our knowledge of the writing product and how the written product interacts with a myriad of textual and non-textual features.

It i important to note that the PERsuADE 2. corpus is the first large-scale corpus to identify discourse elements instudent writing in English. More importantly, these elements are scored for efectiveness. Research capitalizing on the PERsuADE 2.0 corpus should lead to more valid writing asessment over time. In terms of construct validity, we have confidence that the data contained in PERSUADE accurately represents written argumentation including annotations for argument elements,relationships among those elements, and the quality of those elements. Thus, we have some certainty that the labels in the PERsUADE corpus can be used to measure the construct of written argumentation. The PERsuADE corpus likely does not represent all aspects of written argumentation, and thus may have lower content validity. Additionall, models derived from the PERsuADE corpus that annotate discourse elements and their quality can be validated based on how well they predict overal essy quality using the holistic scores provided. The models can also be extended to other available corpora of scored esays to aess how well predicted discourse elements and their qualit are related to holistic scores, providing additional validation checks.

The ability to assess the validit of models developed using the PERsUADE corpus should help develop more reliable and repre. sentative writing interventions. For instance, models developed in the Kaggle competition can be validated internally and externall. Once validated, the models can be integrated into automatic writing evaluation (AWE) systems to help provide meaningful feedback for students at the discourse level. Primaril, the algorithms can assist students in identifyig the presence or absence of important argumentative elements such as the use of positions, clams, and eidence and f present, the likely efectivenes of those elements.f secondary importance, the holistic scores can be used in AWE systems to provide summative feedback to learners about the overall quality of their writing.

![](img/dab84c218eb3b50cd87e511ae7e49839d759f5f20ebea35071dcb9e18482e26d.jpg)  
Fig. 1. Accuracy of winning algorithm to predict discourse effectiveness.

The PERsuADE 2.0 corpus also provides important research possbilities. Immediately, the corpus can be used to compare dif ferences in independent and dependent writing not only in terms of holistic scores, but also with respect to the types of argumentation that each writing task produces. Additionall, rearch could be carred out to examine differences in the inguistic fatures produced within each task. Such studies have been carried out on non-native writer of English (Guo, Crossley, & McNamara, 2013; Tywoniw & Crossley, 2019), but not with native writers..

The demographic and individual difference measures in the PERsUADE 2.0 corpus also allow for research that examines how these non-language features co-vary with asessment of discourse element quality and overall holistic scores of qualit. Importantly, the demographic and individual difference measures can be used to asses whether models and algorithms derived from the corpus suffer from bias. Initial analysis of the winning models from the Kaggle competition based on predicting discourse element effectiveness indicate some bias in terms of race/ethnicity, economic status, and English Language Learner (ELL) status (Baffour, Saxberg, & Crossley, in press). Newly developed models can be developed specifically to avoid bias in output.

# 4. Connections

There are a few available corpora that focus on annotating argumentation in persuasive writing, but none are annotated for discourse features like leads or concluding summaries. More importantly, there are no corpora available that provide effctiveness scores for discourse elements. While there are some available corpora that provide holistic scores for essay quality, the corpora are problematic in terms of overlap in task, prompts, scoring mechanisms and none, to our knowledge, include student meta-data see the ASAP corpus, Shermis, 2014). Whil some existing corpora provide annotations for discourse elements, the corpora are relatively small ae annotated for only a few discourse features, do not provide effectiveness scores for those features, and do not contain meta-data for the student writers (Stab and Gurevych, 2014, 2017).

# 5. Limitations

The PERsuADE 2.0 corpus is the largest writing dataset i terms of ize, types of annotations, and available meta-data. However, there are stil limitations. For example, the corpus only focuses on 6-12th grade writer. Thus, intelligence gleaned from the corpus may not generalize to younger writers that are just developing proficiency in writing skils and to older, more proficient writers. The corpus also has a limited number of prompts $( \mathrm { N } = 1 5$ ) and only focuses on independent and integrated writing tasks. The corpus could be augmented in the future by including essays from a greater number of prompts and a greater variety of writing tasks including compare and contrast esays, analysis papers, and rearch reports. astly, inter-rater reliability for parts of the corpus was difficult to calculate because of segmentation differences among annotators and the difficulty in labeling discourse elements. This speaks to the general difficulty in annotating discourse elements in student writing because boundarie between the start and end of elements are nebulous and many discourse elements (e.g., positions, claims, and counterclaims) share similar semantics. These aspects of discourse make agrement for human raters difficult to obtain and may impact validity and fairness when modeling the data. Low reliability in the human ratings may transfer into automatic writing evaluation systems developed using the PERsUADE 2.0 corpus, potentially leading to bias or validity concerns in the feedback such systems provide. Initial analyses suggest some bias in the models developed from the Kaggle competition indicating the need for newer models that more strongly address theseconcerns. It ill be of paramount importance to ensure that the next generation of models developed on the PERsUADE 2.0 corpus mitigate bias and support fairnes. The inclusion of demographic and individual difference measures in PERsUADE 2.0 make this more feasible.

# Data availability

Data is available at https://github.com/scrosseye/persuade_corpus_2.0.

# References

Baffr,  x   .n      ak Prize Competition Series. In Proceedings of the 2023 BEA 18th Workshop on Innovative Use of NLP for Bulding Educational Aplications.   
s Retrieved from (http://www.corestandards.org).   
Gu, L    13     m A comparison study. Assessing Writing, 18(3), 218-238.   
National enter fo ction Stisic ED). (2013). The Nion's Rport Cad: Tends in Aadic Proges 2012. NCE 2013-456.IC Clernghouse.   
ewell . h  , J, d . 011 g  g tve  tig  f r h Quarterly, 46(3), 273-304.   
Shi, 1     2, 53-76. in Natural Language Processing (EMNLP) (pp. 46-56).   
Stab, C., & Gurevych, I. (2017). Parsing argumentation structures in persuasivesays. Computational Lingusics, 43(3), 619-659.   
Toulmin, S. (1958). The uses of argument. Cambridge: Cambridge University Press,.   
,  1 i    t   in Assessment, 2(3), 90-103.