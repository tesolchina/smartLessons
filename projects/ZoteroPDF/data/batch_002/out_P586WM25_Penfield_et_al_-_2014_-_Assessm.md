# Assessment, evaluations, and definitions of research impact: A review

Teresa Penfield1 , Matthew J. Baker1 , Rosa Scoble2 and Michael C. Wykes1,\*

$I$ University of Exeter, Innovation Centre, Rennes Drive, Devon EX4 4RN, UK and 2 Brunel University, Kingston Lane, Uxbridge UB8 3PH, UK \*Corresponding author. Email: m.c.wykes@exeter.ac.uk

This article aims to explore what is understood by the term ‘research impact’ and to provide a comprehensive assimilation of available literature and information, drawing on global experiences to understand the potential for methods and frameworks of impact assessment being implemented for UK impact assessment. We take a more focused look at the impact component of the UK Research Excellence Framework taking place in 2014 and some of the challenges to evaluating impact and the role that systems might play in the future for capturing the links between research and impact and the requirements we have for these systems.

Keywords: impact; research evaluation; assessment; evidence.

# 1. Introduction, what is meant by impact?

When considering the impact that is generated as a result of research, a number of authors and government recommendations have advised that a clear definition of impact is required (Duryea, Hochman, and Parfitt 2007; Grant et al. 2009; Russell Group 2009). From the outset, we note that the understanding of the term impact differs between users and audiences. There is a distinction between ‘academic impact’ understood as the intellectual contribution to one’s field of study within academia and ‘external socioeconomic impact’ beyond academia. In the UK, evaluation of academic and broader socio-economic impact takes place separately. ‘Impact’ has become the term of choice in the UK for research influence beyond academia. This distinction is not so clear in impact assessments outside of the UK, where academic outputs and socio-economic impacts are often viewed as one, to give an overall assessment of value and change created through research.

The Oxford English Dictionary defines impact as a ‘Marked effect or influence’, this is clearly a very broad definition. In terms of research impact, organizations and stakeholders may be interested in specific aspects of impact, dependent on their focus. In this case, a specific definition may be required, for example, in the

Research Excellence Framework (REF), ‘Assessment framework and guidance on submissions’ (REF2014 2011b), which defines ‘impact’ as,

an effect on, change or benefit to the economy, society, culture, public policy or services, health, the environment or quality of life, beyond academia

Impact is assessed alongside research outputs and environment to provide an evaluation of research taking place within an institution. As such research outputs, for example, knowledge generated and publications, can be translated into outcomes, for example, new products and services, and impacts or added value (Duryea et al. 2007). Although some might find the distinction somewhat marginal or even confusing, this differentiation between outputs, outcomes, and impacts is important, and has been highlighted, not only for the impacts derived from university research (Kelly and McNicol 2011) but also for work done in the charitable sector (Ebrahim and Rangan, 2010; Berg and Ma˚ nsson 2011; Kelly and McNicoll 2011). The Social Return on Investment (SROI) guide (The SROI Network 2012) suggests that ‘The language varies “impact”, “returns”, “benefits”, “value” but the questions around what sort of difference and how much of a difference we are making are the same’. It is perhaps assumed here that a positive or beneficial effect will be considered as an impact but what about changes that are perceived to be negative? Wooding et al. (2007) adapted the terminology of the Payback Framework, developed for the health and biomedical sciences from ‘benefit’ to ‘impact’ when modifying the framework for the social sciences, arguing that the positive or negative nature of a change was subjective and can also change with time, as has commonly been highlighted with the drug thalidomide, which was introduced in the 1950s to help with, among other things, morning sickness but due to teratogenic effects, which resulted in birth defects, was withdrawn in the early 1960s. Thalidomide has since been found to have beneficial effects in the treatment of certain types of cancer. Clearly the impact of thalidomide would have been viewed very differently in the 1950s compared with the 1960s or today.

In viewing impact evaluations it is important to consider not only who has evaluated the work but the purpose of the evaluation to determine the limits and relevance of an assessment exercise. In this article, we draw on a broad range of examples with a focus on methods of evaluation for research impact within Higher Education Institutions (HEIs). As part of this review, we aim to explore the following questions:

What are the reasons behind trying to understand and evaluate research impact?   
What are the methodologies and frameworks that have been employed globally to assess research impact and how do these compare? What are the challenges associated with understanding and evaluating research impact?   
. What indicators, evidence, and impacts need to be captured within developing systems

# 2. Why evaluate research impact?

What are the reasons behind trying to understand and evaluate research impact? Throughout history, the activities of a university have been to provide both education and research, but the fundamental purpose of a university was perhaps described in the writings of mathematician and philosopher Alfred North Whitehead (1929).

‘The justification for a university is that it preserves the connection between knowledge and the zest of life, by uniting the young and the old in the imaginative consideration of learning. The university imparts information, but it imparts it imaginatively. At least, this is the function which it should perform for society. A university which fails in this respect has no reason for existence. This atmosphere of excitement, arising from imaginative consideration transforms knowledge.’

In undertaking excellent research, we anticipate that great things will come and as such one of the fundamental reasons for undertaking research is that we will generate and transform knowledge that will benefit society as a whole.

One might consider that by funding excellent research, impacts (including those that are unforeseen) will follow, and traditionally, assessment of university research focused on academic quality and productivity. Aspects of impact, such as value of Intellectual Property, are currently recorded by universities in the UK through their Higher Education Business and Community Interaction Survey return to Higher Education Statistics Agency; however, as with other public and charitable sector organizations, showcasing impact is an important part of attracting and retaining donors and support (Kelly and McNicoll 2011).

The reasoning behind the move towards assessing research impact is undoubtedly complex, involving both political and socio-economic factors, but, nevertheless, we can differentiate between four primary purposes.

(1) HEIs overview. To enable research organizations including HEIs to monitor and manage their performance and understand and disseminate the contribution that they are making to local, national, and international communities.   
(2) Accountability. To demonstrate to government, stakeholders, and the wider public the value of research. There has been a drive from the UK government through Higher Education Funding Council for England (HEFCE) and the Research Councils (HM Treasury 2004) to account for the spending of public money by demonstrating the value of research to tax payers, voters, and the public in terms of socioeconomic benefits (European Science Foundation 2009), in effect, justifying this expenditure (Davies Nutley, and Walter 2005; Hanney and Gonza´ lezBlock 2011).   
(3) Inform funding. To understand the socio-economic value of research and subsequently inform funding decisions. By evaluating the contribution that research makes to society and the economy, future funding can be allocated where it is perceived to bring about the desired impact. As Donovan (2011) comments, ‘Impact is a strong weapon for making an evidence based case to governments for enhanced research support’.   
(4) Understand. To understand the method and routes by which research leads to impacts to maximize on the findings that come out of research and develop better ways of delivering impact.

The growing trend for accountability within the university system is not limited to research and is mirrored in assessments of teaching quality, which now feed into evaluation of universities to ensure fee-paying students’ satisfaction. In demonstrating research impact, we can provide accountability upwards to funders and downwards to users on a project and strategic basis (Kelly and McNicoll 2011). Organizations may be interested in reviewing and assessing research impact for one or more of the aforementioned purposes and this will influence the way in which evaluation is approached.

It is important to emphasize that ‘Not everyone within the higher education sector itself is convinced that evaluation of higher education activity is a worthwhile task’ (Kelly and McNicoll 2011). The University and College Union (University and College Union 2011) organized a petition calling on the UK funding councils to withdraw the inclusion of impact assessment from the REF proposals once plans for the new assessment of university research were released. This petition was signed by 17,570 academics (52,409 academics were returned to the 2008 Research Assessment Exercise), including Nobel laureates and Fellows of the Royal Society (University and College Union 2011). Impact assessments raise concerns over the steer of research towards disciplines and topics in which impact is more easily evidenced and that provide economic impacts that could subsequently lead to a devaluation of ‘blue skies’ research. Johnston (Johnston 1995) notes that by developing relationships between researchers and industry, new research strategies can be developed. This raises the questions of whether UK business and industry should not invest in the research that will deliver them impacts and who will fund basic research if not the government? Donovan (2011) asserts that there should be no disincentive for conducting basic research. By asking academics to consider the impact of the research they undertake and by reviewing and funding them accordingly, the result may be to compromise research by steering it away from the imaginative and creative quest for knowledge. Professor James Ladyman, at the University of Bristol, a vocal adversary of awarding funding based on the assessment of research impact, has been quoted as saying that ‘. . .inclusion of impact in the REF will create “selection pressure,” promoting academic research that has “more direct economic impact” or which is easier to explain to the public’ (Corbyn 2009).

Despite the concerns raised, the broader socio-economic impacts of research will be included and count for $20 \%$ of the overall research assessment, as part of the REF in 2014. From an international perspective, this represents a step change in the comprehensive nature to which impact will be assessed within universities and research institutes, incorporating impact from across all research disciplines. Understanding what impact looks like across the various strands of research and the variety of indicators and proxies used to evidence impact will be important to developing a meaningful assessment.

# 3. Evaluating research impact

What are the methodologies and frameworks that have been employed globally to evaluate research impact and how do these compare? The traditional form of evaluation of university research in the UK was based on measuring academic impact and quality through a process of peer review (Grant 2006). Evidence of academic impact may be derived through various bibliometric methods, one example of which is the H index, which has incorporated factors such as the number of publications and citations. These metrics may be used in the UK to understand the benefits of research within academia and are often incorporated into the broader perspective of impact seen internationally, for example, within the Excellence in Research for Australia and using Star Metrics in the USA, in which quantitative measures are used to assess impact, for example, publications, citation, and research income. These ‘traditional’ bibliometric techniques can be regarded as giving only a partial picture of full impact (Bornmann and Marx 2013) with no link to causality. Standard approaches actively used in programme evaluation such as surveys, case studies, bibliometrics, econometrics and statistical analyses, content analysis, and expert judgment are each considered by some (Vonortas and Link, 2012) to have shortcomings when used to measure impacts.

Incorporating assessment of the wider socio-economic impact began using metrics-based indicators such as Intellectual Property registered and commercial income generated (Australian Research Council 2008). In the UK, more sophisticated assessments of impact incorporating wider socio-economic benefits were first investigated within the fields of Biomedical and Health Sciences (Grant 2006), an area of research that wanted to be able to justify the significant investment it received. Frameworks for assessing impact have been designed and are employed at an organizational level addressing the specific requirements of the organization and stakeholders. As a result, numerous and widely varying models and frameworks for assessing impact exist. Here we outline a few of the most notable models that demonstrate the contrast in approaches available.

The Payback Framework is possibly the most widely used and adapted model for impact assessment (Wooding et al. 2007; Nason et al. 2008), developed during the mid-1990s by Buxton and Hanney, working at Brunel University. It incorporates both academic outputs and wider societal benefits (Donovan and Hanney 2011) to assess outcomes of health sciences research. The Payback Framework systematically links research with the associated benefits (Scoble et al. 2010; Hanney and Gonza´ lez-Block 2011) and can be thought of in two parts: a model that allows the research and subsequent dissemination process to be broken into specific components within which the benefits of research can be studied, and second, a multi-dimensional classification scheme into which the various outputs, outcomes, and impacts can be placed (Hanney and Gonzalez Block 2011). The Payback Framework has been adopted internationally, largely within the health sector, by organizations such as the Canadian Institute of Health Research, the Dutch Public Health Authority, the Australian National Health and Medical Research Council, and the Welfare Bureau in Hong Kong (Bernstein et al. 2006; Nason et al. 2008; CAHS 2009; Spaapen et al. n.d.). The Payback Framework enables health and medical research and impact to be linked and the process by which impact occurs to be traced. For more extensive reviews of the Payback Framework, see Davies et al. (2005), Wooding et al. (2007), Nason et al. (2008), and Hanney and Gonza´ lez-Block (2011).

A very different approach known as Social Impact Assessment Methods for research and funding instruments through the study of Productive Interactions (SIAMPI) was developed from the Dutch project Evaluating Research in Context and has a central theme of capturing ‘productive interactions’ between researchers and stakeholders by analysing the networks that evolve during research programmes (Spaapen and Drooge, 2011; Spaapen et al. n.d.). SIAMPI is based on the widely held assumption that interactions between researchers and stakeholder are an important pre-requisite to achieving impact (Donovan 2011; Hughes and Martin 2012; Spaapen et al. n.d.). This framework is intended to be used as a learning tool to develop a better understanding of how research interactions lead to social impact rather than as an assessment tool for judging, showcasing, or even linking impact to a specific piece of research. SIAMPI has been used within the Netherlands Institute for health Services Research (SIAMPI n.d.). ‘Productive interactions’, which can perhaps be viewed as instances of knowledge exchange, are widely valued and supported internationally as mechanisms for enabling impact and are often supported financially for example by Canada’s Social Sciences and Humanities Research Council, which aims to support knowledge exchange (financially) with a view to enabling long-term impact. In the UK, UK Department for Business, Innovation, and Skills provided funding of £150 million for knowledge exchange in 2011–12 to ‘help universities and colleges support the economic recovery and growth, and contribute to wider society’ (Department for Business, Innovation and Skills 2012). While valuing and supporting knowledge exchange is important, SIAMPI perhaps takes this a step further in enabling these exchange events to be captured and analysed. One of the advantages of this method is that less input is required compared with capturing the full route from research to impact. A comprehensive assessment of impact itself is not undertaken with SIAMPI, which make it a less-suitable method where showcasing the benefits of research is desirable or where this justification of funding based on impact is required.

The first attempt globally to comprehensively capture the socio-economic impact of research across all disciplines was undertaken for the Australian Research Quality Framework (RQF), using a case study approach. The RQF was developed to demonstrate and justify public expenditure on research, and as part of this framework, a pilot assessment was undertaken by the Australian Technology Network. Researchers were asked to evidence the economic, societal, environmental, and cultural impact of their research within broad categories, which were then verified by an expert panel (Duryea et al. 2007) who concluded that the researchers and case studies could provide enough qualitative and quantitative evidence for reviewers to assess the impact arising from their research (Duryea et al. 2007). To evaluate impact, case studies were interrogated and verifiable indicators assessed to determine whether research had led to reciprocal engagement, adoption of research findings, or public value. The RQF pioneered the case study approach to assessing research impact; however, with a change in government in 2007, this framework was never implemented in Australia, although it has since been taken up and adapted for the UK REF.

In developing the UK REF, HEFCE commissioned a report, in 2009, from RAND to review international practice for assessing research impact and provide recommendations to inform the development of the REF. RAND selected four frameworks to represent the international arena (Grant et al. 2009). One of these, the RQF, they identified as providing a ‘promising basis for developing an impact approach for the REF’ using the case study approach. HEFCE developed an initial methodology that was then tested through a pilot exercise. The case study approach, recommended by the RQF, was combined with ‘significance’ and ‘reach’ as criteria for assessment. The criteria for assessment were also supported by a model developed by Brunel for ‘measurement’ of impact that used similar measures defined as depth and spread. In the Brunel model, depth refers to the degree to which the research has influenced or caused change, whereas spread refers to the extent to which the change has occurred and influenced end users. Evaluation of impact in terms of reach and significance allows all disciplines of research and types of impact to be assessed side-by-side (Scoble et al. 2010).

The range and diversity of frameworks developed reflect the variation in purpose of evaluation including the stakeholders for whom the assessment takes place, along with the type of impact and evidence anticipated. The most appropriate type of evaluation will vary according to the stakeholder whom we are wishing to inform. Studies (Buxton, Hanney and Jones 2004) into the economic gains from biomedical and health sciences determined that different methodologies provide different ways of considering economic benefits. A discussion on the benefits and drawbacks of a range of evaluation tools (bibliometrics, economic rate of return, peer review, case study, logic modelling, and benchmarking) can be found in the article by Grant (2006).

Evaluation of impact is becoming increasingly important, both within the UK and internationally, and research and development into impact evaluation continues, for example, researchers at Brunel have developed the concept of depth and spread further into the Brunel Impact Device for Evaluation, which also assesses the degree of separation between research and impact (Scoble et al. working paper).

# 4. Impact and the REF

Although based on the RQF, the REF did not adopt all of the suggestions held within, for example, the option of allowing research groups to opt out of impact assessment should the nature or stage of research deem it unsuitable (Donovan 2008). In 2009–10, the REF team conducted a pilot study for the REF involving 29 institutions, submitting case studies to one of five units of assessment (in clinical medicine, physics, earth systems and environmental sciences, social work and social policy, and English language and literature) (REF2014 2010). These case studies were reviewed by expert panels and, as with the RQF, they found that it was possible to assess impact and develop ‘impact profiles’ using the case study approach (REF2014 2010).

From 2014, research within UK universities and institutions will be assessed through the REF; this will replace the Research Assessment Exercise, which has been used to assess UK research since the 1980s. Differences between these two assessments include the removal of indicators of esteem and the addition of assessment of socio-economic research impact. The REF will therefore assess three aspects of research:

(1) Outputs (2) Impact (3) Environment

Research impact is assessed in two formats, first, through an impact template that describes the approach to enabling impact within a unit of assessment, and second, using impact case studies that describe the impact taking place following excellent research within a unit of assessment (REF2014 2011a). HEFCE indicated that impact should merit a $2 5 \%$ weighting within the REF (REF2014 2011b); however, this has been reduced for the 2014 REF to $20 \%$ , perhaps as a result of feedback and lobbying, for example, from the Russell Group and Million $^ +$ group of Universities who called for impact to count for $1 5 \%$ (Russell Group 2009; Jump 2011) and following guidance from the expert panels undertaking the pilot exercise who suggested that during the 2014 REF, impact assessment would be in a developmental phase and that a lower weighting for impact would be appropriate with the expectation that this would be increased in subsequent assessments (REF2014 2010).

The quality and reliability of impact indicators will vary according to the impact we are trying to describe and link to research. In the UK, evidence and research impacts will be assessed for the REF within research disciplines. Although it can be envisaged that the range of impacts derived from research of different disciplines are likely to vary, one might question whether it makes sense to compare impacts within disciplines when the range of impact can vary enormously, for example, from business development to cultural changes or saving lives? An alternative approach was suggested for the RQF in Australia, where it was proposed that types of impact be compared rather than impact from specific disciplines.

Providing advice and guidance within specific disciplines is undoubtedly helpful. It can be seen from the panel guidance produced by HEFCE to illustrate impacts and evidence that it is expected that impact and evidence will vary according to discipline (REF2014 2012). Why should this be the case? Two areas of research impact health and biomedical sciences and the social sciences have received particular attention in the literature by comparison with, for example, the arts. Reviews and guidance on developing and evidencing impact in particular disciplines include the London School of Economics (LSE) Public Policy Group’s impact handbook (LSE n.d.), a review of the social and economic impacts arising from the arts produced by Reeve (Reeves 2002), and a review by Kuruvilla et al. (2006) on the impact arising from health research. Perhaps it is time for a generic guide based on types of impact rather than research discipline?

# 5. The challenges of impact evaluation

What are the challenges associated with understanding and evaluating research impact? In endeavouring to assess or evaluate impact, a number of difficulties emerge and these may be specific to certain types of impact. Given that the type of impact we might expect varies according to research discipline, impact-specific challenges present us with the problem that an evaluation mechanism may not fairly compare impact between research disciplines.

# 5.1 Time lag

The time lag between research and impact varies enormously. For example, the development of a spin out can take place in a very short period, whereas it took around 30 years from the discovery of DNA before technology was developed to enable DNA fingerprinting. In development of the RQF, The Allen Consulting Group (2005) highlighted that defining a time lag between research and impact was difficult. In the UK, the Russell Group Universities responded to the REF consultation by recommending that no time lag be put on the delivery of impact from a piece of research citing examples such as the development of cardiovascular disease treatments, which take between 10 and 25 years from research to impact (Russell Group 2009). To be considered for inclusion within the REF, impact must be underpinned by research that took place between 1 January 1993 and 31 December 2013, with impact occurring during an assessment window from 1 January 2008 to 31 July 2013. However, there has been recognition that this time window may be insufficient in some instances, with architecture being granted an additional 5-year period (REF2014 2012); why only architecture has been granted this dispensation is not clear, when similar cases could be made for medicine, physics, or even English literature. Recommendations from the REF pilot were that the panel should be able to extend the time frame where appropriate; this, however, poses difficult decisions when submitting a case study to the REF as to what the view of the panel will be and whether if deemed inappropriate this will render the case study ‘unclassified’.

# 5.2 The developmental nature of impact

Impact is not static, it will develop and change over time, and this development may be an increase or decrease in the current degree of impact. Impact can be temporary or long-lasting. The point at which assessment takes place will therefore influence the degree and significance of that impact. For example, following the discovery of a new potential drug, preclinical work is required, followed by Phase 1, 2, and 3 trials, and then regulatory approval is granted before the drug is used to deliver potential health benefits. Clearly there is the possibility that the potential new drug will fail at any one of these phases but each phase can be classed as an interim impact of the original discovery work on route to the delivery of health benefits, but the time at which an impact assessment takes place will influence the degree of impact that has taken place. If impact is short-lived and has come and gone within an assessment period, how will it be viewed and considered? Again the objective and perspective of the individuals and organizations assessing impact will be key to understanding how temporal and dissipated impact will be valued in comparison with longer-term impact.

# 5.3 Attribution

Impact is derived not only from targeted research but from serendipitous findings, good fortune, and complex networks interacting and translating knowledge and research. The exploitation of research to provide impact occurs through a complex variety of processes, individuals, and organizations, and therefore, attributing the contribution made by a specific individual, piece of research, funding, strategy, or organization to an impact is not straight forward. Husbands-Fealing suggests that to assist identification of causality for impact assessment, it is useful to develop a theoretical framework to map the actors, activities, linkages, outputs, and impacts within the system under evaluation, which shows how later phases result from earlier ones. Such a framework should be not linear but recursive, including elements from contextual environments that influence and/or interact with various aspects of the system. Impact is often the culmination of work within spanning research communities (Duryea et al. 2007). Concerns over how to attribute impacts have been raised many times (The Allen Consulting Group 2005; Duryea et al. 2007; Grant et al. 2009), and differentiating between the various major and minor contributions that lead to impact is a significant challenge.

Figure 1, replicated from Hughes and Martin (2012), illustrates how the ease with which impact can be attributed decreases with time, whereas the impact, or effect of complementary assets, increases, highlighting the problem that it may take a considerable amount of time for the full impact of a piece of research to develop but because of this time and the increase in complexity of the networks involved in translating the research and interim impacts, it is more difficult to attribute and link back to a contributing piece of research.

This presents particular difficulties in research disciplines conducting basic research, such as pure mathematics, where the impact of research is unlikely to be foreseen. Research findings will be taken up in other branches of research and developed further before socio-economic impact occurs, by which point, attribution becomes a huge challenge. If this research is to be assessed alongside more applied research, it is important that we are able to at least determine the contribution of basic research. It has been acknowledged that outstanding leaps forward in knowledge and understanding come from immersing in a background of intellectual thinking that ‘one is able to see further by standing on the shoulders of giants’.

# 5.4 Knowledge creep

It is acknowledged that one of the outcomes of developing new knowledge through research can be ‘knowledge creep where new data or information becomes accepted and gets absorbed over time. This is particularly recognized in the development of new government policy where findings can influence policy debate and policy change, without recognition of the contributing research (Davies et al. 2005; Wooding et al. 2007). This is recognized as being particularly problematic within the social sciences where informing policy is a likely impact of research. In putting together evidence for the REF, impact can be attributed to a specific piece of research if it made a ‘distinctive contribution’ (REF2014 2011a). The difficulty then is how to determine what the contribution has been in the absence of adequate evidence and how we ensure that research that results in impacts that cannot be evidenced is valued and supported.

![](img/41c3a1429c7a28f9664066e85104abbb5bf4ae2a67187834cd119fdd9ba287fc.jpg)  
Figure 1. Time, attribution, impact. Replicated from (Hughes and Martin 2012).

# 5.5 Gathering evidence

Gathering evidence of the links between research and impact is not only a challenge where that evidence is lacking. The introduction of impact assessments with the requirement to collate evidence retrospectively poses difficulties because evidence, measurements, and baselines have, in many cases, not been collected and may no longer be available. While looking forward, we will be able to reduce this problem in the future, identifying, capturing, and storing the evidence in such a way that it can be used in the decades to come is a difficulty that we will need to tackle.

# 6. Developing systems and taxonomies for capturing impact

Collating the evidence and indicators of impact is a significant task that is being undertaken within universities and institutions globally. Decker et al. (2007) surveyed researchers in the US top research institutions during 2005; the survey of more than 6000 researchers found that, on average, more than $40 \%$ of their time was spent doing administrative tasks. It is desirable that the assignation of administrative tasks to researchers is limited, and therefore, to assist the tracking and collating of impact data, systems are being developed involving numerous projects and developments internationally, including Star Metrics in the USA, the ERC (European Research Council)

Research Information System, and Lattes in Brazil (Lane 2010; Mugabushaka and Papazoglou 2012).

Ideally, systems within universities internationally would be able to share data allowing direct comparisons, accurate storage of information developed in collaborations, and transfer of comparable data as researchers move between institutions. To achieve compatible systems, a shared language is required. CERIF (Common European Research Information Format) was developed for this purpose, first released in 1991; a number of projects and systems across Europe such as the ERC Research Information System (Mugabushaka and Papazoglou 2012) are being developed as CERIFcompatible.

In the UK, there have been several Jisc-funded projects in recent years to develop systems capable of storing research information, for example, MICE (Measuring Impacts Under CERIF), UK Research Information Shared Service, and Integrated Research Input and Output System, all based on the CERIF standard. To allow comparisons between institutions, identifying a comprehensive taxonomy of impact, and the evidence for it, that can be used universally is seen to be very valuable. However, the Achilles heel of any such attempt, as critics suggest, is the creation of a system that rewards what it can measure and codify, with the knock-on effect of directing research projects to deliver within the measures and categories that reward.

Attempts have been made to categorize impact evidence and data, for example, the aim of the MICE Project was to develop a set of impact indicators to enable impact to be fed into a based system. Indicators were identified from documents produced for the REF, by Research Councils UK, in unpublished draft case studies undertaken at King’s College London or outlined in relevant publications (MICE Project n.d.). A taxonomy of impact categories was then produced onto which impact could be mapped. What emerged on testing the MICE taxonomy (Cooke and Nadim 2011), by mapping impacts from case studies, was that detailed categorization of impact was found to be too prescriptive. Every piece of research results in a unique tapestry of impact and despite the MICE taxonomy having more than 100 indicators, it was found that these did not suffice. It is perhaps worth noting that the expert panels, who assessed the pilot exercise for the REF, commented that the evidence provided by research institutes to demonstrate impact were ‘a unique collection’. Where quantitative data were available, for example, audience numbers or book sales, these numbers rarely reflected the degree of impact, as no context or baseline was available. Cooke and Nadim (2011) also noted that using a linearstyle taxonomy did not reflect the complex networks of impacts that are generally found. The Goldsmith report (Cooke and Nadim 2011) recommended making indicators ‘value free’, enabling the value or quality to be established in an impact descriptor that could be assessed by expert panels. The Goldsmith report concluded that general categories of evidence would be more useful such that indicators could encompass dissemination and circulation, re-use and influence, collaboration and boundary work, and innovation and invention.

While defining the terminology used to understand impact and indicators will enable comparable data to be stored and shared between organizations, we would recommend that any categorization of impacts be flexible such that impacts arising from non-standard routes can be placed. It is worth considering the degree to which indicators are defined and provide broader definitions with greater flexibility.

It is possible to incorporate both metrics and narratives within systems, for example, within the Research Outcomes System and Researchfish, currently used by several of the UK research councils to allow impacts to be recorded; although recording narratives has the advantage of allowing some context to be documented, it may make the evidence less flexible for use by different stakeholder groups (which include government, funding bodies, research assessment agencies, research providers, and user communities) for whom the purpose of analysis may vary (Davies et al. 2005). Any tool for impact evaluation needs to be flexible, such that it enables access to impact data for a variety of purposes (Scoble et al. n.d.). Systems need to be able to capture links between and evidence of the full pathway from research to impact, including knowledge exchange, outputs, outcomes, and interim impacts, to allow the route to impact to be traced. This database of evidence needs to establish both where impact can be directly attributed to a piece of research as well as various contributions to impact made during the pathway.

Baselines and controls need to be captured alongside change to demonstrate the degree of impact. In many instances, controls are not feasible as we cannot look at what impact would have occurred if a piece of research had not taken place; however, indications of the picture before and after impact are valuable and worth collecting for impact that can be predicted.

It is now possible to use data-mining tools to extract specific data from narratives or unstructured data (Mugabushaka and Papazoglou 2012). This is being done for collation of academic impact and outputs, for example, Research Portfolio Online Reporting Tools, which uses PubMed and text mining to cluster research projects, and STAR Metrics in the US, which uses administrative records and research outputs and is also being implemented by the ERC using data in the public domain (Mugabushaka and Papazoglou 2012). These techniques have the potential to provide a transformation in data capture and impact assessment (Jones and Grant 2013). It is acknowledged in the article by Mugabushaka and Papazoglou (2012) that it will take years to fully incorporate the impacts of ERC funding. For systems to be able to capture a full range of systems, definitions and categories of impact need to be determined that can be incorporated into system development. To adequately capture interactions taking place between researchers, institutions, and stakeholders, the introduction of tools to enable this would be very valuable. If knowledge exchange events could be captured, for example, electronically as they occur or automatically if flagged from an electronic calendar or a diary, then far more of these events could be recorded with relative ease. Capturing knowledge exchange events would greatly assist the linking of research with impact.

The transition to routine capture of impact data not only requires the development of tools and systems to help with implementation but also a cultural change to develop practices, currently undertaken by a few to be incorporated as standard behaviour among researchers and universities.

# 7. Indicators, evidence, and impact within systems

What indicators, evidence, and impacts need to be captured within developing systems? There is a great deal of interest in collating terms for impact and indicators of impact. Consortia for Advancing Standards in Research Administration Information, for example, has put together a data dictionary with the aim of setting the standards for terminology used to describe impact and indicators that can be incorporated into systems internationally and seems to be building a certain momentum in this area. A variety of types of indicators can be captured within systems; however, it is important that these are universally understood. Here we address types of evidence that need to be captured to enable an overview of impact to be developed. In the majority of cases, a number of types of evidence will be required to provide an overview of impact.

# 7.1 Metrics

Metrics have commonly been used as a measure of impact, for example, in terms of profit made, number of jobs provided, number of trained personnel recruited, number of visitors to an exhibition, number of items purchased, and so on. Metrics in themselves cannot convey the full impact; however, they are often viewed as powerful and unequivocal forms of evidence. If metrics are available as impact evidence, they should, where possible, also capture any baseline or control data. Any information on the context of the data will be valuable to understanding the degree to which impact has taken place.

Perhaps, SROI indicates the desire to be able to demonstrate the monetary value of investment and impact by some organizations. SROI aims to provide a valuation of the broader social, environmental, and economic impacts, providing a metric that can be used for demonstration of worth. This is a metric that has been used within the charitable sector (Berg and Ma˚ nsson 2011) and also features as evidence in the REF guidance for panel D (REF2014 2012). More details on SROI can be found in ‘A guide to Social Return on Investment’ produced by The SROI Network (2012).

Although metrics can provide evidence of quantitative changes or impacts from our research, they are unable to adequately provide evidence of the qualitative impacts that take place and hence are not suitable for all of the impact we will encounter. The main risks associated with the use of standardized metrics are that

(1) The full impact will not be realized, as we focus on easily quantifiable indicators   
(2) We will focus attention towards generating results that enable boxes to be ticked rather than delivering real value for money and innovative research.   
(3) They risk being monetized or converted into a lowest common denominator in an attempt to compare the cost of a new theatre against that of a hospital.

# 7.2 Narratives

Narratives can be used to describe impact; the use of narratives enables a story to be told and the impact to be placed in context and can make good use of qualitative information. They are often written with a reader from a particular stakeholder group in mind and will present a view of impact from a particular perspective. The risk of relying on narratives to assess impact is that they often lack the evidence required to judge whether the research and impact are linked appropriately. Where narratives are used in conjunction with metrics, a complete picture of impact can be developed, again from a particular perspective but with the evidence available to corroborate the claims made. Table 1 summarizes some of the advantages and disadvantages of the case study approach.

By allowing impact to be placed in context, we answer the ‘so what?’ question that can result from quantitative data analyses, but is there a risk that the full picture may not be presented to demonstrate impact in a positive light? Case studies are ideal for showcasing impact, but should they be used to critically evaluate impact?

# 7.3 Surveys and testimonies

One way in which change of opinion and user perceptions can be evidenced is by gathering of stakeholder and user testimonies or undertaking surveys. This might describe support for and development of research with end users, public engagement and evidence of knowledge exchange, or a demonstration of change in public opinion as a result of research. Collecting this type of evidence is timeconsuming, and again, it can be difficult to gather the required evidence retrospectively when, for example, the appropriate user group might have dispersed.

The ability to record and log these type of data is important for enabling the path from research to impact to be established and the development of systems that can capture this would be very valuable.

# 7.4 Citations (outside of academia) and documentation

Citations (outside of academia) and documentation can be used as evidence to demonstrate the use research findings in developing new ideas and products for example. This might include the citation of a piece of research in policy documents or reference to a piece of research being cited within the media. A collation of several indicators of impact may be enough to convince that an impact has taken place. Even where we can evidence changes and benefits linked to our research, understanding the causal relationship may be difficult. Media coverage is a useful means of disseminating our research and ideas and may be considered alongside other evidence as contributing to or an indicator of impact.

Table 1. The advantages and disadvantages of the case study approach   

<html><body><table><tr><td>Benefits</td><td>Considerations</td></tr><tr><td>Uses quantitative and qualitative data</td><td>Automated collation of evidence is difficult</td></tr><tr><td>Allows evidence to be contextualized and a story told</td><td>Incorporating perspective can make it difficult to assess critically</td></tr><tr><td>Enables assessment in the absence of quantitative data</td><td>Time-consuming to prepare and assess</td></tr><tr><td>Allows collation of unique datasets</td><td>Difficult to compare like with like.</td></tr><tr><td>Preserves distinctive account or disciplinary perspective</td><td>Rewards those who can write well, and/or afford to pay for external input</td></tr></table></body></html>

![](img/6bbb03f7d659338313ca2c0e220c7ff8b1a46bdb8ca1586467793bbc8a150155.jpg)  
Figure 2. Overview of the types of information that systems need to capture and link.

The fast-moving developments in the field of altmetrics (or alternative metrics) are providing a richer understanding of how research is being used, viewed, and moved. The transfer of information electronically can be traced and reviewed to provide data on where and to whom research findings are going.

# 8. Conclusions and recommendations

The understanding of the term impact varies considerably and as such the objectives of an impact assessment need to be thoroughly understood before evidence is collated.

While aspects of impact can be adequately interpreted using metrics, narratives, and other evidence, the mixedmethod case study approach is an excellent means of pulling all available information, data, and evidence together, allowing a comprehensive summary of the impact within context. While the case study is a useful way of showcasing impact, its limitations must be understood if we are to use this for evaluation purposes. The case study does present evidence from a particular perspective and may need to be adapted for use with different stakeholders. It is time-intensive to both assimilate and review case studies and we therefore need to ensure that the resources required for this type of evaluation are justified by the knowledge gained. The ability to write a persuasive well-evidenced case study may influence the assessment of impact. Over the past year, there have been a number of new posts created within universities, such as writing impact case studies, and a number of companies are now offering this as a contract service. A key concern here is that we could find that universities which can afford to employ either consultants or impact ‘administrators’ will generate the best case studies.

The development of tools and systems for assisting with impact evaluation would be very valuable. We suggest that developing systems that focus on recording impact information alone will not provide all that is required to link research to ensuing events and impacts, systems require the capacity to capture any interactions between researchers, the institution, and external stakeholders and link these with research findings and outputs or interim impacts to provide a network of data. In designing systems and tools for collating data related to impact, it is important to consider who will populate the database and ensure that the time and capability required for capture of information is considered. Capturing data, interactions, and indicators as they emerge increases the chance of capturing all relevant information and tools to enable researchers to capture much of this would be valuable. However, it must be remembered that in the case of the UK REF, impact is only considered that is based on research that has taken place within the institution submitting the case study. It is therefore in an institution’s interest to have a process by which all the necessary information is captured to enable a story to be developed in the absence of a researcher who may have left the employment of the institution. Figure 2 demonstrates the information that systems will need to capture and link.

(1) Research findings including outputs (e.g., presentations and publications)   
(2) Communications and interactions with stakeholders and the wider public (emails, visits, workshops, media publicity, etc)   
(3) Feedback from stakeholders and communication summaries (e.g., testimonials and altmetrics)   
(4) Research developments (based on stakeholder input and discussions)   
(5) Outcomes (e.g., commercial and cultural, citations)   
(6) Impacts (changes, e.g., behavioural and economic)

Attempting to evaluate impact to justify expenditure, showcase our work, and inform future funding decisions will only prove to be a valuable use of time and resources if we can take measures to ensure that assessment attempts will not ultimately have a negative influence on the impact of our research. There are areas of basic research where the impacts are so far removed from the research or are impractical to demonstrate; in these cases, it might be prudent to accept the limitations of impact assessment, and provide the potential for exclusion in appropriate circumstances.

# Funding

This work was supported by Jisc [DIINN10].

# References

Australian Research Council. (2008) ERA Indicator Principles (Pubd online) <http://www.arc.gov.au/pdf/ERA_Indicator_ Principles.pdf> accessed 6 Oct 2012.   
Berg, L.O. and Ma˚ nsson, C. (2011) A White Paper on Charity Impact Measurement (Pubd online) <http://www.charitystar. org/wp-content/uploads/2011/05/Return_on_donations_a white_paper_on_charity_impact_measurement.pdf> accessed 6 Aug 2012.   
Bernstein, A. et al. (2006) A Framework to Measure the Impact of Investments in Health Research (Pubd online) <http://www. oecd.org/science/innovationinsciencetechnologyandindustry/ 37450246.pdf> accessed 26 Oct 2012.   
Bornmann, L. and Marx, W. (2013) ‘How Good is Research Really?’, European Molecular Biology Organization (EMBO) Reports, 14: 226–30.   
Buxton, M., Hanney, S. and Jones, T. (2004) ‘Estimating the Economic Value to Societies of the Impact of Health Research: A Critical Review’, Bulletin of the World Health Organization, 82: 733–9.   
Canadian Academy of Health Sciences Panel on Return on Investment in Health Research. (2009) Making an Impact. A Preferred Framework and Indicators to Measure Returns on Investment in Health Research (Pubd online) <http:// www.cahs-acss.ca/wp-content/uploads/2011/09/ROI_FullRep ort.pdf> accessed 26 Oct 2012.   
Cooke, J. and Nadim, T. (2011) Measuring Impact Under CERIF at Goldsmiths (Pubd online) <http://mice.cerch.kcl. ac.uk/wp-uploads/2011/07/MICE_report_Goldsmiths_final. pdf> accessed 26 Oct 2012.   
Corbyn, Z. (2009) Anti-Impact Campaign’s ‘Poster Boy’ Sticks up for the Ivory Tower. Times Higher Education (Pubd online) <http://www.timeshighereducation.co.uk/story.asp?story Code $=$ 409614&sectioncode $= 2 6 >$ accessed 26 Oct 2012.   
Davies, H., Nutley, S. and Walter, I. (2005) Assessing the Impact of Social Science Research: Conceptual, Methodological and Practical Issues (Pubd online) <http:// www.odi.org.uk/rapid/Events/ESRC/docs/background_paper. pdf> accessed 26 Oct 2012.   
Decker, R. et al. (2007) A Profile of Federal-Grant Administrative Burden Among Federal Demonstration Partnership Faculty (Pubd online) <http://www.iscintelli gence.com/archivos_subidos/usfacultyburden_5.pdf> accessed 26 Oct 2012.   
Department for Business, Innovation and Skills. (2012) Guide to BIS 2012-2013 (Pubd online) <http://www.bis.gov.uk/ About> accessed 24 Oct 2012.   
Donovan, C. (2008) ‘The Australian Research Quality Framework: A live experiment in capturing the social, economic, environmental and cultural returns of publicly funded research’. In: Coryn, C. L. S. and Scriven, M. (eds) Reforming the Evaluation of Research. New Directions for Evaluation, 118, pp. 47–60. (2011) ‘Impact is a Strong Weapon for Making an Evidence-Based Case Study for Enhanced Research Support but a State-of-the-Art Approach to Measurement is Needed’, LSE Blog, (Pubd online) <http://blogs.lse.ac.uk/impactofsocialsciences/tag/claire-donovan $>$ accessed 26 Oct 2012.   
Donovan, C. and Hanney, S. (2011) ‘The “Payback Framework” explained’, Research Evaluation, 20: 181–3.   
Duryea, M., Hochman, M. and Parfitt, A. (2007) Measuring the Impact of Research, Research Global, 27, 8-9 (Pubd online) <http://www.atn.edu.au/docs/Research%20Global%20-%20 Measuring%20the%20impact%20of%20research.pdf> accessed 26 Oct 2012.   
Ebrahim, A. and Rangan, K. (2010) ‘The Limits of Nonprofit Impact: A Contingency Framework for Measuring Social Performance’, Working Paper. Harvard Business School. (Pubd online) <http://www.hbs.edu/research/pdf/10-099. pdf> accessed 26 Oct 2012.   
European Science Foundation. (2009) Evaluation in National Research Funding Agencies: Approaches, Experiences and Case Studies (Pubd online) <http://www.esf.org/index. php?eID $=$ tx_ccdamdl_file&p[file] $= 2 5 6 6 8 \& p [ \mathrm { d l } ] = 1 \& p [ \mathrm { p i d } ] =$ 6767&p[site] $=$ European%20Science%20Foundation&p[t] $= 1$ 351858982&hash $=$ 93e987c5832f10aeee3911bac23b4e0f&l $=$ en> accessed 26 Oct 2012.   
Jones, M.M. and Grant, J. (2013) ‘Methodologies for Assessing and Evidencing Research Impact. RAND Europe’. In: Dean. et al. (eds) 7 Essays on Impact. DESCRIBE Project Report for JISC. University of Exeter.   
Grant, J. (2006) Measuring the Benefits from Research, RAND Europe (Pubd online) <http://www.rand.org/pubs/research_ briefs/2007/RAND_RB9202.pdf> accessed 26 Oct 2012.   
Grant, J. et al. (2009) Capturing Research Impacts. A Review of International Practice (Pubd online) <http://www.rand.org/ pubs/documented_briefings/2010/RAND_DB578.pdf> accessed 26 Oct 2012.   
HM Treasury, Department for Education and Skills, Department of Trade and Industry. (2004) ‘Science and Innovation Framework 2004–2014’. London: HM Treasury.   
Hanney, S. and Gonza´ lez-Block, M. A. (2011) ‘Yes, Research can Inform Health Policy; But can we Bridge the ‘DoKnowing it’s been Done’ Gap?’, Health Research Policy and Systems, 9: 23.   
Hughes, A. and Martin, B. (2012) ‘Council for Industry and Higher Education, UK Innovation Research Centre. Enhancing Impact. The Value of Public Sector R&D’, Summary report, (Pubd online) <http://ukirc.ac.uk/object/ report/8025/doc/CIHE_0612ImpactReport_summary.pdf> accessed 26 Oct 2012.   
Husbands-Fealing, K. (2013) ‘Assessing impacts of higher education systems’, University of Minnesota, in Dean et al. (eds) 7 Essays on Impact. DESCRIBE project report for JISC. University of Exeter.   
Johnston, R. (1995) ‘Research Impact Quantification’, Scientometrics, 34: 415–26.   
Jump, P. (2011) ‘HEFCE Reduces Points of Impact in REF’, Times Higher Education, (Pubd online) <http://www. timeshighereducation.co.uk/story.asp?storyCode $=$ 415340& sectioncode $= 2 6 >$ accessed 26 Oct 2012.   
Kelly, U. and McNicoll, I. (2011) ‘National Co-ordinating Centre for Public Engagement’, Through a Glass, Darkly: Measuring the Social Value of Universities (Pubd online) <http://www.publicengagement.ac.uk/sites/default/files/8009 6%20NCCPE%20Social%20Value%20Report.pdf> accessed 26 Oct 2012.   
Kuruvilla, S. et al. (2006) ‘Describing the Impact of Health Research: A Research Impact Framework’, BMC Health Services Research, 6: 134.   
LSE Public Policy Group. (2011) ‘Maximising the Impacts of Your Research: A Handbook for Social Scientists’ (Pubd online) <http://www2.lse.ac.uk/government/research/resgroups/LSE PublicPolicy/Docs/LSE_Impact_Handbook_April_2011.pdf> accessed 26 Oct 2012.   
Lane, J. (2010) ‘Let’s Make Science Metrics More Scientific’, Nature, 464: 488–9.   
MICE Project. (2011) Measuring Impact Under CERIF (MICE) Project Blog (Pubd online) <http://mice.cerch.kcl.ac.uk $^ { \prime } >$ accessed 26 Oct 2012.   
Mugabushaka, A. and Papazoglou, T. (2012) ‘Information systems of research funding agencies in the “era of the Big Data”. The case study of the Research Information System of the European Research Council’. In: Jeffery, K. and Dvorˇ a´ k, J. (eds) E-Infrastructures for Research and Innovation: Linking Information Systems to Improve Scientific Knowledge, Proceedings of the 11th International Conference on Current Research Information Systems (June 6–9, 2012), pp. 103–12. Prague, Czech Republic.   
Nason, E. et al. (2008) ‘RAND Europe’, Health Research— Making an Impact. The Economic and Social Benefits of HRB-funded Research. Dublin: Health Research Board.   
Reeves, M. (2002) ‘Arts Council England’, Measuring the Economic and Social Impact of the Arts: A Review, (Pubd online) <http://www.artscouncil.org.uk/media/uploads/docu ments/publications/340.pdf> accessed 26 Oct 2012.   
REF 2014. (2010) Research Excellence Framework Impact Pilot Exercise: Findings of the Expert Panels, (Pubd online) <http://www.ref.ac.uk/media/ref/content/pub/researchexcelle nceframeworkimpactpilotexercisefindingsoftheexpertpanels/ re01_10.pdf> accessed 26 Oct 2012. (2011a) Assessment Framework and Guidance on Submissions, (Pubd online) <http://www.ref.ac.uk/media/ref/ content/pub/assessmentframeworkandguidanceonsubmissions /02_11.pdf> accessed 26 Oct 2012. (2011b) Decisions on Assessing Research Impact, (Pubd online) <http://www.ref.ac.uk/media/ref/content/pub/assessm entframeworkandguidanceonsubmissions/GOS%20including $\% 2 0$ addendum.pdf> accessed 19 Dec 2012. (2012) Panel Criteria and Working Methods, (Pubd online) <http://www.ref.ac.uk/media/ref/content/pub/panelcriteriaan dworkingmethods/01_12.pdf> accessed 26 Oct 2012.   
Russell Group. (2009) Response to REF Consultation, (Pubd online) <http://www.russellgroup.ac.uk/uploads/REF-consult ation-response-FINAL-Dec09.pdf> accessed 26 Oct 2012.   
Scoble, R. et al. (2009) Research Impact Evaluation, a Wider Context. Findings from a Research Impact Pilot, (Pubd online) <http://www.libsearch.com/visit/1953719> accessed 26 Oct 2012. (2010) ‘Institutional Strategies for Capturing SocioEconomic Impact of Research’, Journal of Higher Education Policy and Management, 32: 499–511.   
SIAMPI Website (2011) (Pubd online) <http://www.siampi.eu/ Pages/SIA/12/625.bGFuZz1FTkc.html> accessed 26 Oct 2012.   
Spaapen, J. et al. (2011) SIAMPI Final Report (Pubd online) <http://www.siampi.eu/Content/SIAMPI/SIAMPI_Final%20 report.pdf> accessed 26 Oct 2012.   
Spaapen, J. and Drooge, L. (2011) ‘Introducing ‘Productive Interactions’ in Social Impact Assessment’, Research Evaluation, 20: 211–18.   
The Allen Consulting Group. (2005) Measuring the Impact of Publicly Funded Research. Canberra: Department of Education, Science and Training.   
The SROI Network. (2012) A Guide to Social Return on Investment (Pubd online) <http://www.thesroinetwork.org/ publications/doc_details/241-a-guide-to-social-return-on-inve stment- $2 0 1 2 >$ accessed 26 Oct 2012.   
University and College Union. (2011) Statement on the Research Excellence Framework Proposals, (Pubd online) <http://www. ucu.org.uk/media/pdf/n/q/ucu_REFstatement_finalsignatures. pdf $\mathrm { \dot { > } }$ accessed 26 Oct 2012.   
Vonortas, N. and Link, A. (2013) Handbook on the Theory and Practice of Program Evaluation. Cheltenham, UK: Edward Elgar Publishing Limited.   
Whitehead, A. (1929) ‘Universities and their function’, The Aims of Education and other Essays. New York: Macmillan Company.   
Wooding, S. et al. (2007) ‘RAND Europe’, Policy and Practice Impacts of Research Funded by the Economic Social Research Council, (Pubd online) <http://www.esrc.ac.uk/_images/Case_ Study_of_the_Future_of_Work_Programme_Volume_2_tcm 8-4563.pdf> accessed 26 Oct 2012.