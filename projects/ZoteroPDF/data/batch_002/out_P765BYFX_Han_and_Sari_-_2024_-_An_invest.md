# An investigation on the use of automated feedback in Turkish EFL students’ writing classes

Turgay Han & Elif Sari

To cite this article: Turgay Han & Elif Sari (2024) An investigation on the use of automated feedback in Turkish EFL students’ writing classes, Computer Assisted Language Learning, 37:4, 961-985, DOI: 10.1080/09588221.2022.2067179

To link to this article: https://doi.org/10.1080/09588221.2022.2067179

# An investigation on the use of automated feedback in Turkish EFL students’ writing classes

Turgay Hana $\textcircled{1}$ and Elif Sarib

a Department of English Language and Literature, Faculty of Science and Letters, Ordu University, Ordu, Turkey; bSchool of Foreign Languages, Karadeniz Technical University, Trabzon, Turkey

# ABSTRACT

Feedback is generally regarded as an integral part of EFL writing instruction. Giving individual feedback on students’ written products can lead to a demanding, if not insurmountable, task for EFL writing teachers, especially in classes with a large number of students. Several Automated Writing Evaluation (AWE) systems which can provide automated feedback on written texts have been developed to reduce the time and effort teachers need to give individual feedback on students’ writings. Employing a quasi-experimental research design, this study aimed to examine how automated feedback impacted students’ writing scores and writing accuracy. The data were collected from 75 Turkish EFL university students. The experimental group students were exposed to combined automated-teacher feedback while the control group students received full teacher feedback. Both quantitative and qualitative data were collected through pre-test/post-test writing tasks, Criterion error analysis reports, and student reflections. The results of the study revealed that the students who received combined automated-teacher feedback improved their analytic writing scores as much as the students who received full teacher feedback. However, combined automated-teacher feedback was more effective than full teacher feedback in reducing the students’ grammar and mechanics errors. The qualitative findings obtained from the student reflections about the Criterion feedback helped to understand its impact on writing improvement. The study provided implications for effective use of AWE in EFL writing classrooms.

# KEYWORDS

Automated writing evaluation (AWE); automated feedback; teacher feedback; technology and writing instruction

# Introduction

Although research has shown conflicting views regarding the role of written corrective feedback in second/foreign language writing (e.g., Bitchener, 2012; Ferris, 2010; Truscott, 1996), language teachers believe in the power of corrective feedback in writing progress (Chen et  al.,

2009). Similarly, language learners expect their teachers to identify the problematic areas of their writing that need to be reformed and help students overcome the difficulties and problems in their writing (Shute, 2008). In a typical ESL/EFL writing classroom, an instructor is mainly responsible for providing feedback on all students’ writing errors, which leads to a great amount of workload for teachers especially in classes with a large number of students (Attali et  al., 2010; Dikli, 2006; Kellogg et  al., 2010; Warschauer & Grimes, 2008). Therefore, teachers can avoid assigning writing tasks, or they cannot provide individual feedback on each written product submitted by their students in order not to shoulder this workload (Chen et  al., 2009). To address this problem, some computer programs have been developed to provide instant and individualized feedback on students’ written texts (e.g., Criterion by ETS, MyAccess by Vantage Learning, and WriteToLearn by Pearson Knowledge Technologies) (Chen & Cheng, 2008; Dikli, 2006; Warschauer & Grimes, 2008). These programs are called Automated Writing Evaluation (AWE) systems. Many teachers and administrators from K-12 classrooms, colleges, and universities are increasingly using AWE systems as educational tools in their EFL writing classes (Chen & Cheng, 2008; Grimes & Warschauer, 2010; Stevenson & Phakiti, 2014).

The use of AWE in ESL/EFL writing contexts has been subject to many studies for the last two decades. The vast majority of studies centred on the impact AWE had on the quality of the written products (the increase in scores and the reduction in errors) using either within-group or between-group designs while others investigated students’ and/or teachers’ perceptions of using AWE. Table 1 provides a summary of these studies.

Table 1. S tudies reviewed.   

<html><body><table><tr><td>Focus of the Study</td><td>Research Design</td><td>Studies Reviewed</td><td>Results</td></tr><tr><td></td><td></td><td>Attali, 2004 Chou et al., 2016 Ebyary &amp; Windeatt, 2010</td><td>Reduced errors in grammar, usage, mechanics, and style Increased holistic and analytic scores Increased holistic scores</td></tr><tr><td></td><td>Within-group design Between-group design</td><td>Li et al., 2017 Liao, 2015 Kellogg et al., 2010</td><td>Improved grammar accuracy Reduced grammar errors Reduced errors in grammar, usage, and mechanics</td></tr><tr><td>The Impact of AWE on Written Products Students&#x27; and/or</td><td></td><td>Link et al., 2020 Rock, 2007 Tang &amp; Rich, 2017</td><td>More retention in accuracy Increased analytic scores</td></tr><tr><td>Teachers&#x27; Perceptions of AWE</td><td>Within &amp; between group design Grimes &amp; Warschauer, 2010 Facilitated classroom management</td><td>Wang et al., 2013 Wilson &amp; Czik, 2016 Fang, 2010</td><td>Improved holistic scores Reduced sentence-level errors. No gain in terms of writing quality Increased revisions of assignments</td></tr></table></body></html>

Within-group studies generally showed that students’ writing scores increased, and the number of errors decreased across their drafts after they received AWE feedback. For example, Ebyary and Windeatt (2010) found that trainee EFL teachers’ holistic scores provided by Criterion showed a significant improvement both between the two submissions (the first draft and the final draft of the same task) and among the four drafts in total. In the same vein, a study by Chou et  al. (2016) revealed student improvement in terms of holistic and analytic scores and and text length. Attali’s Attali (2004) study showed that the feedback provided by Criterion was effective for students to improve their essays by reducing their errors of grammar, usage, mechanics, and style and increasing the length of their essays. Similarly, Liao (2015) showed that Criterion helped college students reduce their number of errors in four identified types of grammatical errors (e.g., fragment, subject-verb agreement, run-on sentences, ill-formed verbs) in both revisions and new texts although the effect was changeable for each error category. Finally, Li et  al. (2017) found that the students improved their grammar accuracy in all of the nine categories in the short term and they attained accuracy only in the category of run-on sentences in the long term. However, the fact that these studies lacked a control group makes it unconvincing whether the students’ writing improvement was the result of the automated feedback they received or could be attributed to some other factors such as maturation (Rogers & Revesz, 2020; Stevenson & Phakiti, 2014).

Several studies employed between-group designs where an experimental group’s performance was compared with that of a control group. For example, in their studies, Kellogg et  al. (2010) concluded the students who received continuous feedback reduced their number of errors in the categories of mechanics, usage, and grammar than the students who did not receive any feedback although the students’ holistic scores showed no significant gains of feedback in general. Similarly, Wang et  al. (2013) found student gains in terms of decreasing their errors after the AWE treatment. Rock’s Rock (2007) study showed that the students who received feedback from Criterion received higher analytic scores on their essays which they wrote at the end of the treatment than the students who received feedback from their teacher although no difference was found between the two groups regarding their holistic scores. These studies investigated the impact of AWE systems on students’ writing improvement when they were used as the only feedback provider, but the literature suggests integrating AWE feedback with teacher feedback as AWE systems were developed to complement teacher feedback not to replace it (Burstein et  al., 2003).

Regarding the impact of AWE on students’ writing performance as a supplementary feedback tool, few studies have been conducted so far.

For example, Wilson and Czik (2016) revealed that the feedback given to the students in the teacher $^ +$ automated feedback condition was mostly related to higher-level writing skills (ideas and elaboration, organization, style). In addition, the students who received teacher $^ +$ automated feedback were more motivated and persistent in their writing; however, no differences emerged between the two groups in terms of writing quality. In the same vein, a study by Choi (2010) where the impact of AWE on students’ writing scores and accuracy was investigated using three different AWE integration models (No-AWE, Optional-AWE, and Integrated-AWE) revealed no significant difference across the three groups from pre- to post-test. In their quasi-experimental study, Tang and Rich (2017) reported that the experimental group students who received integrated automated-teacher feedback achieved greater writing improvement than the control group students. More recently, Link et  al. (2020) concluded that the students revised their teachers’ feedback on the lower-level structural problems of their writing more frequently than they revised automated feedback. However, the students in the integrated AWE-teacher feedback condition had more retention regarding their improvement in accuracy. It should be noted that except for the study by Link et  al. (2020), these studies do not provide detailed explanation regarding the nature of the teacher feedback the control group students receive. It is important to clarify the type (e.g., direct or indirect) and focus (e.g., form-focused or content-focused) of the teacher feedback provided in the control group in order to make a meaningful comparison between the groups in terms of the feedback procedures, and thus to attain more reliable results regarding the effectiveness of AWE tools in EFL writing classrooms. In addition, it is equally important to know whether the AWE system and the teacher identified the same errors in this kind of comparative studies.

The studies investigating students’ and/or teachers’ perceptions of AWE revealed contradicting results. Some of these studies showed that using AWE in writing classes facilitated classroom management and the students felt themselves more confident in writing as they found machine judgement less threatening than human judgement (Grimes & Warschauer, 2010); AWE helped students increase their revision of their assignments as they actively participated in the assessment process and took responsibility of their own progress (Fang, 2010; Tang & Rich, 2017); and AWE was useful for students to get rid of their repeated mistakes (Tsuda, 2014). Teachers also reported positive perceptions of the AWE tool they used. In Wilson and Roscoe (2020) study, teachers stated that the AWE tool let them provide more feedback on content and idea development. They added that the students who received automated feedback had more positive writing self-efficacy.

On the other hand, some studies revealed negative perceptions towards the use of AWE. For example, in Lai’s study Lai, Lai, (2010), the students favoured peer feedback over automated feedback with regards to process, product, and perspective. The students accepted their peers as real audiences and valued their comments on their writings. In addition, they found automated feedback too general, vague, and fixed while they thought peer feedback was more direct and explicit. In the same vein, in a study by Wang (2015), the students found automated feedback misleading or incorrect. Regarding the effectiveness of the diagnostic feedback provided by Criterion, students found the feedback on grammar and usage more beneficial than the feedback on mechanics and style. Related to the correctness of the diagnostic feedback messages, students reported that some messages were incorrect and confusing, and some basic errors in grammar and usage were missed by the program. It should be noted that in these studies AWE was used in isolation of teacher feedback and it was not made clear whether the participating students had the necessary skills to use technology in their instruction and whether they were given training on how to use the AWE tools they were exposed to, which impacts students’ experiences with using AWE systems (Grimes & Warschauer, 2010; Warschauer & Grimes, 2008).

Based on the review of the previous research on AWE, we can conclude that the studies which investigated the impact of using AWE on students’ writing performance have shown some limitations as follows:

experimental-control group design studies are scarce;   
existing experimental-control group design studies lack detailed explanation regarding the feedback procedure the experimental and the control groups were exposed to;   
existing experimental-control group design studies do not clarify whether the AWE system and the teacher identified the same errors; and   
they do not expand on the students’ familiarity with technology and the training they received on using the AWE system.

To address these gaps in the previous research, the present study aimed to investigate the impact of using an AWE tool on Turkish EFL students’ writing improvement in terms of increasing their analytic writing scores and reducing their writing errors. For this purpose, an experimental-control group design was adopted in which the students were assigned either to a combined automated-teacher feedback model or to a full teacher feedback model. Additionally, the study investigated students’ perceptions of receiving automated feedback. Specifically, the following three research questions guided this study:

1. How does the combined automated-teacher feedback impact Turkish EFL students’ analytic writing scores compared to full teacher feedback?   
2. How does automated feedback impact Turkish EFL students’ writing errors in the categories of grammar, usage, and mechanics compared to teacher feedback?   
3. What are the Turkish EFL students’ perceptions of receiving automated feedback on their writing performance?

# Method

# Research design

This study employed a quasi-experimental research design (pretest-posttest control group design). One of the two intact classes was assigned as the experimental group and the other was assigned as the control group. Both quantitative and qualitative data were collected to answer the research questions. The quantitative part of the study examined the changes in students’ analytic writing scores and the number of errors after the intervention of combined automated-teacher feedback through the use of pre-test and post-test writing tasks. The qualitative data obtained from student reflections helped to reveal students’ perceptions of receiving automated feedback on their writing performance. The research questions and the corresponding data collection methods and analysis are given in Table 2.

# Setting and participants

This study was conducted in the English Language Teaching Department at a Turkish state university. In this department, the undergraduate English majors are offered 2-credit writing courses (Writing Skills I in the first semester and Writing Skills II in the second semester) in their first year. Data for the current study were collected in the second semester when the students were practicing writing a five-paragraph essay at the upper-intermediate level. Students’ grammatical accuracy and correct use of language were among the main objectives of the course.

A total of 75-second semester students, whose ages ranged from 18 to 26 with a mean of 19.5, were conveniently sampled for the study. Of those students, while 37 students were conveniently assigned to the experimental group, the number of students was 38 in the control group (please see Table 3). All of the participants were C1 (upper-intermediate) level EFL learners following the descriptions of the Common European Framework of Reference for Languages (CEFR).

Table 2. R esearch questions and corresponding data collection methods and analysis.   

<html><body><table><tr><td>Purpose</td><td>Research Questions</td><td>Data Collection Method</td><td>Data Analysis</td></tr><tr><td>Investigate the impact of combined automated-teacher feedback on students&#x27; writing proficiency</td><td>RQ1. How does the combined automated-teacher feedback impact Turkish EFL students&#x27; analytic writing scores compared to full teacher feedback?</td><td>Pre-test writing task Post-test writing task Criterion error analysis reports</td><td>Inferential analysis</td></tr><tr><td>Investigate the impact of combined automated-teacher feedback on students&#x27; writing errors</td><td>RQ2. How does automated feedback impact Turkish EFL students&#x27; writing errors in the categories of grammar, usage, and mechanics compared to</td><td>Pre-test writing task Post-test writing task Criterion error analysis reports</td><td>Inferential analysis</td></tr><tr><td>Investigate students&#x27; perceptions RQ3. What are the Turkish of combined automated-teacher feedback</td><td>teacher feedback? EFL students&#x27; perceptions of receiving automated feedback on their writing performance?</td><td>Student reflections</td><td>Content analysis</td></tr></table></body></html>

Table 3. P rofiles of the students in the experimental and the control groups.   

<html><body><table><tr><td></td><td></td><td colspan="2">Control</td><td colspan="2">Experimental</td><td colspan="2">Total</td></tr><tr><td></td><td></td><td>n</td><td>%</td><td>n</td><td>%</td><td>n</td><td>%</td></tr><tr><td>Gender</td><td>Female</td><td>25</td><td>65.8</td><td>26</td><td>70.3</td><td>51</td><td>68</td></tr><tr><td></td><td>Male</td><td>13</td><td>34.2</td><td>11</td><td>29.7</td><td>24</td><td>32</td></tr></table></body></html>

In order to ensure that the two classes were identical in terms of writing proficiency, all participants in the experimental group and the control group were administered a pre-test writing task before the study started. The pre-test scores of both groups were compared through a Mann-Whitney U test since the data followed a non-normal distribution $( \mathtt { p } { < } . 0 5 )$ according to the results of the Kolmogorov-Smirnov test. The results of the Mann-Whitney U test showed that there was no significant difference between the pre-test scores of the experimental group and the control group in terms of their total score $( \mathrm { U } = 7 0 2 . 5 0 0$ , $\mathsf { p } { > } . 0 5 )$ and the scores they received from the subcategories of the analytic scoring scale $\mathrm { ( U = 6 9 8 . 0 0 0 }$ , $\mathrm { p } { > } . 0 5$ for the category of grammar; $\mathrm { U } = 6 8 1 . 5 0 0$ , $\mathrm { p } { > } . 0 5$ for the category of content; $\mathrm { U } = 5 6 6 . 0 0 0$ , $\mathrm { p } { > } . 0 5$ for the category of organization; $\mathrm { U } = 6 2 0 . 0 0 0$ , $\mathrm { p } { > } . 0 5$ for the category of style and quality of expression; and $\mathrm { U } = 5 6 4 . 0 0 0$ , $\mathrm { p } { > } . 0 5$ for the category of mechanics).

Further, the students were given a computer literacy survey that was adapted from Dikli (2006) to obtain information about their familiarity and experience with using a computer and the Internet. The results of the survey revealed that $9 0 . 7 \%$ of the students owned a computer at home and $8 5 . 3 \%$ had an Internet connection at home. In addition, $7 6 \%$ of the students considered themselves to be very comfortable or comfortable while using basic computer programs, like Word, PowerPoint,

Table 4. M ann-Whitney U test comparing the computer literacy of the experimental and the control groups.   

<html><body><table><tr><td colspan="3"></td><td colspan="3"></td><td colspan="3">Mean Sum of</td></tr><tr><td></td><td></td><td>n</td><td>M</td><td>SD</td><td>Ranks</td><td>Ranks</td><td>U z</td><td>p</td></tr><tr><td rowspan="2">Do you have a computer at Experimental home?</td><td></td><td>37</td><td>1.10</td><td>.314</td><td>38.55</td><td>1426.50</td><td>682.500 -.431.666</td><td></td></tr><tr><td>Control</td><td>38</td><td>1.07</td><td>.273</td><td>37.46</td><td>1423.50</td><td></td><td></td></tr><tr><td>Do you have an Internet connection at home?</td><td>Experimental</td><td>37</td><td>1.10</td><td>.314</td><td>36.55</td><td>1352.50</td><td>649.500 -.925 .355</td><td></td></tr><tr><td rowspan="3">Are you comfortable using basic computer</td><td>Control</td><td>38</td><td>1.18</td><td>.392</td><td>39.41</td><td>1497.50</td><td>1407.50 701.500 -.017 .986</td><td></td></tr><tr><td>Experimental</td><td>37</td><td>1.97</td><td>.763 38.04</td><td></td><td></td><td></td><td></td></tr><tr><td>Control</td><td>38</td><td>2.00 .900</td><td>37.96</td><td></td><td>1442.50</td><td></td><td></td></tr></table></body></html>

Internet Explorer and the use of email. Table 4 compares the two groups in terms of their computer literacy.

Table 4 shows that there was no significant difference between the groups regarding their access to a computer $( \mathrm { U } = 6 8 2 . 5 0 0 $ , $\mathsf { p } { > } . 0 5 )$ and the Internet $\mathrm { ' } \mathrm { U } = 6 4 9 . 5 0 0$ , $\mathsf { p } { > } . 0 5 )$ and their attitude towards the use of basic computer programs $\mathrm { ( U = 7 0 1 } . 5 0 0$ , $\mathsf { p } { > } . 0 5 )$ .

The participating instructor was a full-time EFL instructor at the state university where this study was conducted. She majored in the English language teaching department for her B.A. degree and English linguistics for her M.A. and PhD. She had been teaching at this university for twelve years and had ten years of experience in teaching writing. She had used another online writing program in the previous years so she did not have difficulty in using the AWE program employed in this study.

# The AWE tool used in the study

The AWE tool used in the present study was Criterion. Criterion was launched by Educational Testing Service (ETS) in September 2002 with the contribution of 15 developers who are experienced in programming automated scoring engines (Burstein et  al., 2003). Criterion offers a holistic score reflecting the overall quality of the writing sample submitted by users and detailed diagnostic feedback on errors in various domains within seconds. It provides instructors with individual reports which enable them to review their students’ progress. Also, it provides students with a comprehensive “Writer’s Handbook” which includes feedback definitions, explanations of errors with examples, and suggestions on how to correct the errors and how to improve the organization and development (Dikli, 2006). All of the student’s writing samples, scores, diagnostic feedback, and teacher comments are saved to a portfolio that is accessible both by the teacher and the student. Teachers can assign a writing prompt from the Criterion topic library which includes more than 400 writing prompts in different genres at varying levels from $4 ^ { \mathrm { t h } }$ grade to university level. Teachers can also create their writing topics (ETS, 2019). Further, teachers can write their comments on students’ writings using the “Comment” or “Dialogue” sections.

# Data collection instruments

# Pre- and post-test writing tasks

For both pre-test and post-test, the students were required to write a 300 to 350-word length persuasive essay on a single topic that had been selected for all students. Both tasks were selected from the essay topics of “Criterion Topic Library” and were assumed to be parallel regarding the topic familiarity by the writing course teacher in terms of the students’ level of proficiency, experiences of writing classes, educational interests, and cultural characteristics. The students were given 60 minutes to write their essays using Microsoft Word. The course teacher accepted the essays through text-matching software, Turnitin, to ensure the originality of the essays. The students wrote their essays on the following topics which had not been discussed with the students beforehand.

# Pre-test writing topic

“What makes a professor great? Prominence in his or her field? A hot new book? Good student reviews every semester? What standards should be used to assess the quality of college faculty members? Support your position with reasons and examples from your own experiences, observations or reading.”

# Post-test writing topic

“After they complete their university studies, some students live in their hometowns. Others live in different towns or cities. Which do you think is better, living in your hometown or living in a different town or city? Give reasons for your answer.”

# Criterion error analysis reports

Criterion records all submissions of a writing assignment along with holistic scores given on each submission for each student or the whole group. The program provides and records error analysis reports including the total number of mistakes made in each category (e.g., grammar, usage, mechanics, and style) and subcategory (e.g., subject-verb agreement, preposition error, missing comma, etc.) (Please see Appendix for the feedback categories and subcategories provided by Criterion). These reports were used to compare the number of student errors before and after the treatment. However, since other studies (e.g., Guo et  al., 2021; Jiang & Yu, 2020; Ranalli, 2021) found that AWE systems might mislabel a correct usage as erroneous, the second author of the study and an independent researcher checked the accuracy of errors identified by Criterion following the procedure applied by Guo et  al. (2021). An error identified by Criterion was accepted as accurate if the original usage was unacceptable. If the original usage was acceptable, the error was deemed inaccurate. A two-way mixed, consistency, average-measures intra-class correlation (ICC) model was used to measure the consistency between the raters (McGraw & Wong, 1996). The resulting ICC was in the excellent range for all categories $\operatorname { \Pi } ( \operatorname { I C C } = \ 0 . 9 9$ for grammar, 0.98 for usage and mechanics) (Cicchetti, 1994). Then, every discrepancy was discussed until reaching an agreement. As a result, the number of true errors was obtained to make a comparison between the groups.

# Student reflections

The experimental group students were asked to write their reflections on the process of writing their assignments in the combined automated-teacher feedback condition. They wrote the reflections towards the end of the treatment when they were considered to have enough experience regarding the feedback procedure they were exposed to. The researchers formed some questions to guide the students to express their experiences with and perceptions of receiving automated feedback from Criterion. The students were allowed to write their reflections in Turkish because it was believed that the students would express their opinions better in their mother tongue. The quotations given in this paper were translated into English by the first author of the study.

# Data collection procedure

All students in the experimental and control groups were informed about the purpose and procedures of the study and provided with a written consent form. Official permission was also obtained to involve the students in the study from the Dean’s Office of the Faculty where the students were enrolled. We conducted this quasi-experimental study in a natural teaching context by integrating it into the writing course the students had to take during one semester. The students were required to produce six writing assignments of different genres during the semester, which would constitute $5 0 \%$ of their final grades for the course. The assignments were adapted in a way that could be used for the purpose of this study in order not to impose extra work on the students. The students in both groups were asked to fill out the computer literacy survey and were administered the pre-test writing task. Then, the instructor held a training session with the experimental group students to familiarize the students with the Criterion service. The intervention lasted 12 weeks and each class met two hours per week. Intending to minimize the effects of confounding factors, the syllabus, the instruction, writing topics, and writing process were the same for both groups. Also, the two groups were taught by the same instructor following the same procedure. When the instructor declared the essay topic, the students were given one day to develop ideas and create their first drafts. Then, the students received feedback on their first drafts, revised their essays based on the feedback they were provided, and created their final drafts. All students received feedback on form (sentence level grammar and punctuation errors) and content (meaning-related problems) on each of their six assignments. The only difference between the groups was that the experimental group students received automated feedback on form accompanied by teacher feedback on content while the control group students received teacher feedback on form and content. The experimental group students were asked to write their reflections on the combined automated-teacher feedback they received after they submitted all of their six writing assignments. Finally, all participants in both groups were administered a post-test writing task in order to see the impact of the intervention on their writing scores and errors. Table 5 summarizes the data collection procedure.

# Feedback procedure for the control group

The feedback procedure for the control group included only written teacher feedback on form and content. The teacher provided feedback on the form (e.g., grammar, usage, and mechanics) by underlining the words, phrases, or sentences in the students’ essays which needed to be corrected or improved. The feedback was unfocused and indirect. The kind of problem in the underlined parts was indicated by error correction codes without giving the correct form. The instructor was allowed to use her own set of error codes which she had used before the study in order to ensure a natural feedback process for the instructor. The teacher’s error codes and the feedback categories of Criterion were compared before the feedback sessions started to make sure that the teacher and Criterion would identify the same errors (please see Appendix for the comparison of the feedback categories provided by Criterion and the teacher). In addition, since these codes do not tell anything about the content and the organization of the essay, the instructor wrote some notes suggesting revision in terms of meaning-related problems and organization of the text. The instructor completed the feedback procedure within three days after the submission deadline. After all essays were given feedback, the students were required to revise their essays based on the feedback, and complete and hand in their second (final) drafts within the following three days.

Table 5. D ata collection procedure.   

<html><body><table><tr><td></td><td>Experimental Group</td><td>Control Group</td></tr><tr><td rowspan="2">Pre-test measure</td><td> literacy</td><td>Demographic data and computer : Demographic data and computer literacy Writing proficiency and writing errors.</td></tr><tr><td>Writing proficiency and writing errors</td><td></td></tr><tr><td>Treatment conditioning : Six writing assignments</td><td>each followed by combined automated-teacher feedback</td><td>Six writing assignments each followed. by full teacher feedback</td></tr><tr><td rowspan="2">Post-test measure</td><td>Writing proficiency and writing errors</td><td>.Writing proficiency and writing errors.</td></tr><tr><td>Reflections on combined automated-teacher feedback</td><td></td></tr></table></body></html>

# Feedback procedure for the experimental group

The students in the experimental group were exposed to an integration of automated feedback given by Criterion and teacher feedback. The previous studies suggest that it is an ideal combination when students receive automated feedback on their earlier drafts and then teacher feedback on their later drafts (e.g., Chen & Cheng, 2008; Grimes & Warschauer, 2010). Therefore, the students performed their writing assignments using the Criterion program and first received feedback on their sentence-level errors from the program. Then, since the program was not able to give feedback regarding the content of the written text, the teacher provided feedback on content by writing comments through the “Comment” and “Dialogue” tools presented by the program within two days. The students revised their essays again based on the teacher feedback within another two days and handed in their final drafts.

The feedback of Criterion was unfocused and mostly indirect. When students submit their essays, Criterion presents the number of errors under each error category and subcategory. When the students click on the subcategories which show an error, the problematic areas are highlighted. Moving the mouse over the highlighted areas, the students are provided with advisory messages about how to correct the error without directly offering the correct form. However, sometimes the program offers the correct form through its suggestions. After correcting their errors based on the Criterion feedback within two days, the students submitted their essays for teacher revision.

# Data analysis procedures

Quantitative data obtained through the error analysis reports of Criterion, and by scoring the pre-test and post-test writing tasks were analysed through the use of Statistical Package for Social Sciences (SPSS), version 25. The pre-test and post-test essays were scored by two independent raters using a 10-point analytic scoring scale which is comprised of five categories: a) grammar (3 pts.), b) content (2 pts.), c) organization (2 pts.), d) style and quality of expression (1.5 pts.), and e) mechanics (1.5 pts.). A two-way mixed, consistent, average-measures ICC model was employed to measure the degree that the raters provided consistency in their ratings (McGraw & Wong, 1996). The resulting ICC was in the good range for the pre-test tasks and the post-test tasks $\mathrm { ( I C C = ~ } 0 . 8 6$ and 0.83 respectively) indicating that the two tests were rated consistently (Cicchetti, 1994). In several instances where the discrepancy between the two raters’ scores was greater than two points, a third rater was asked to score the essay using the same rubric. The average of the closest two scores was accepted as the final score.

The data obtained through student reflections were analyzed based on the Grounded Theory (Glaser & Strauss, 1967) as it allows the researchers to develop codes and categories based on the emerging patterns rather than predetermined ones through constant comparisons (Miles & Huberman, 1994). The second author of the study compiled the student answers under each question. Then, she read through the data several times in order to get a general sense, looking for both similarities and differences in the data. Then, similar data were grouped under a category. New data was constantly compared to previous data and placed in the relevant category. In order to check the accuracy of the findings, a PhD candidate who had experience in analysing qualitative data was asked to form codes and categories out of the same data. The consistency between the two coders was calculated by dividing the number of agreements by the sum of the number of agreements and the number of disagreements, then multiplied by 100. A high consistency $( 9 4 \% )$ was found between the coders on the coding of the data. The discrepancies regarding the coding and the categorization of the data were resolved through discussion.

# Results

# RQ1: The impact of combined automated-teacher feedback on Turkish EFL students’ analytic writing scores

In order to investigate the impact of combined automated-teacher feedback on the students’ analytic writing scores, the pre-test and post-test writing scores of the experimental and the control groups were compared. As it was stated in the setting and participants section, the Mann-Whitney U test showed no significant difference between the pre-test scores of the experimental group and the control group.

After the treatment, another Mann-Whitney U test was conducted to compare the post-test scores of the two groups in order to see whether one group showed a higher improvement of writing proficiency than the other group both in total and in each category of the analytic scoring scale. The results are given in Table 6.

Table 6. M ann-Whitney U test comparing the post-test scores of the experimental and the control groups.   

<html><body><table><tr><td colspan="3"></td><td colspan="3">Mean</td><td colspan="4">Sum of</td></tr><tr><td></td><td></td><td>N</td><td>M</td><td>SD</td><td>Ranks</td><td>Ranks</td><td>U</td><td>z</td><td>p</td></tr><tr><td rowspan="2">Grammar</td><td>Experimental</td><td>37</td><td>2.77</td><td>.07</td><td>43.78</td><td>1620.00</td><td>489.000</td><td>2.281</td><td>.023*</td></tr><tr><td>Control</td><td>38</td><td>2.70</td><td>.10</td><td>32.37</td><td>1230.00</td><td></td><td></td><td></td></tr><tr><td>Content</td><td>Experimental</td><td>37</td><td>1.66</td><td>.22</td><td>33.45</td><td>1237.50</td><td>534.500</td><td>-1.794</td><td>.073</td></tr><tr><td></td><td>Control</td><td>38</td><td>1.73</td><td>.27</td><td>42.43</td><td>1612.50</td><td></td><td></td><td></td></tr><tr><td>Organization</td><td>Experimental</td><td>37</td><td>1.73</td><td>.22</td><td>27.85</td><td>1030.50</td><td>327.500</td><td>4.033</td><td>.000**</td></tr><tr><td></td><td>Control</td><td>38</td><td>1.86</td><td>.22</td><td>47.88</td><td>1819.50</td><td></td><td></td><td></td></tr><tr><td>Style</td><td>Experimental</td><td>37</td><td>1.30</td><td>.10</td><td>33.68</td><td>1246.00</td><td>543.000</td><td>-1.729</td><td>.084</td></tr><tr><td>Mechanics</td><td>Control</td><td>38</td><td>1.33</td><td>.15</td><td>42.21</td><td>1604.00</td><td></td><td></td><td></td></tr><tr><td></td><td>Experimental</td><td>37</td><td>1.36</td><td>.25</td><td>43.43</td><td>1607.00</td><td>502.000</td><td>2.183</td><td>0.29*</td></tr><tr><td>TOTAL</td><td>Control</td><td>38</td><td>1.30</td><td>.27</td><td>32.71</td><td>1243.00</td><td>643.000</td><td></td><td></td></tr><tr><td></td><td>Experimental</td><td>37</td><td>8.85</td><td>.79</td><td>36.38</td><td>1346.00</td><td></td><td>-.636</td><td>.524</td></tr><tr><td></td><td>Control</td><td>38</td><td>8.95</td><td>.77</td><td>39.58</td><td>1504.00</td><td></td><td></td><td></td></tr></table></body></html>

$\mathbf { \bar { \rho } } _ { p \mathrm { ~ < ~ } . 0 5 }$ . $^ { * * } p < . 0 0 1$

As shown in Table 6, the total scores of the two groups revealed no significant difference $\mathrm { \Delta V = 6 4 3 . 0 0 0 }$ , $\mathsf { p } { > } . 0 5 )$ . The average of the post-test scores revealed that the two groups demonstrated very similar performance in the post-test $\mathbf { \check { M } } = 8 . 8 5$ , $\mathrm { S D } { = } . 7 9$ for the experimental group, $\mathrm { M } = 8 . 9 5$ , $\mathrm { { S D = . 7 7 } }$ for the control group). When the mean scores of the two groups were compared between the pre-test and the post-test, it was found that both groups improved their writing scores as a result of the feedback procedures they were exposed to (e.g., combined automated-teacher feedback and full teacher feedback). However, none of the two groups outperformed the other in terms of their analytic writing scores, which indicated that both feedback procedures had the same degree of impact on the students’ writing proficiency. When the two groups’ scores in each category were compared, it was seen that there was a significant difference in the categories of grammar $( \mathrm { U } = 4 8 9 . 0 0 0$ , $\mathsf { p } { < } . 0 5 )$ , organization $\mathrm { ( U } = 3 2 7 . 5 0 0$ , $\mathrm { p } { < } . 0 0 1 )$ , and mechanics $\mathrm { ' U } = 5 0 2 . 0 0 0$ , $\mathsf { p } { < } . 0 5 )$ . The mean scores showed that the experimental group students significantly outperformed the control group students in the categories of grammar $( \mathrm { E x p . M } = 2 . 7 7$ , Cont. $\mathrm { M } = 2 . 7 0 $ ) and mechanics $( \mathrm { E x p . M } = 1 . 3 6 .$ , Cont. $\mathbf { M } = 1 . 3 0 $ ) while the control group students outperformed their counterparts in the category of organization $( \mathrm { E x p . M } = 1 . 7 3$ , Cont. $\mathbf { \boldsymbol { M } } = 1 . 8 6$ ). According to Cohen’s Cohen (1988) standards, a large effect size was found for the grammar category $\left( \mathrm { d } \mathrm { = } . 8 1 \right)$ indicating that the difference between the groups was large while a small effect size was found for the mechanics category $\left( \mathrm { d } \mathrm { = } . 2 3 \right)$ . For the organization category, the effect size was medium $\mathrm { ( d } \mathrm { = } . 5 9 \mathrm { ) }$ ).

RQ2: The impact of automated feedback on Turkish EFL students’ writing errors in the categories of grammar, usage, and mechanics

The numbers of student errors in the categories of grammar, usage, and mechanics were compared between the pre-test and the post-test in order to reveal the impact of the feedback procedure on the students’ writing accuracy. The numbers of errors were standardized by dividing students’ total number of errors in each category by the total number of words used to create the essay and multiplying the result by 100 as the number of errors would vary according to the essay length. Non-parametric tests were applied to compare the number of errors because the data did not distribute normally.

Using a Mann-Whitney U test, the mean error frequencies of the two groups were compared to see whether there were any significant differences between the groups in terms of their writing errors prior to the treatment. The results of the Mann-Whitney U test showed that there was no significant difference between the experimental and the control groups in terms of their writing errors in the three error categories $\mathrm { ' } \mathrm { U } = 6 0 9 . 0 0 0$ , $\mathrm { p } { > } . 0 5$ for the category of grammar; $\mathrm { U } = 6 7 6 . 5 0 0$ , $\mathrm { p } { > } . 0 5$ for the category of usage; $\mathrm { U } = 6 7 3 . 0 0 0$ , $\mathrm { p } { > } . 0 5$ for the category of mechanics).

In order to find out whether any group outperformed the other in terms of reducing their writing errors, another Mann-Whitney $\mathrm { U }$ test was applied on the mean error frequencies the groups made in the post-test. The results of the Mann-Whitney U test are given in Table 7.

As shown in Table 7, the results of the Mann-Whitney U test showed a statistically significant difference in the categories of grammar $\mathrm { ' } \mathrm { U } = 3 7 6 . 5 0 0$ , $\mathrm { p } { < } . 0 0 1 $ ) and mechanics $\mathrm { ( U = 4 7 1 . 5 0 0 }$ , $\mathsf { p } { < } . 0 5 )$ . When the mean error frequencies were examined, it was seen that the experimental group students made fewer errors than the control group students in the categories of grammar $\mathrm { ' E x p . M = } . 0 8$ , Cont. $\mathrm { M } { = } . 3 2 $ ) and mechanics $( \mathrm { E x p . M } { = } . 3 6 $ , Cont. $\mathrm { M } { = } . 6 2 $ ) while there was no significant difference for the category of usage $\mathrm { \Delta } ^ { \prime } \mathrm { U } = 5 9 4 . 5 0 0 $ , $\mathsf { p } { > } . 0 5 )$ . According to Cohen’s Cohen (1988) standards, a large effect size was found for the grammar category $( \mathrm { d } = 1 . 0 7 )$ indicating that the difference between the groups was large while a medium effect size was found for the mechanics category $\left( \mathrm { d } \mathrm { = } . 5 5 \right)$ . This result corresponds to the results obtained from the analytic scoring scale, which showed that the experimental group outperformed the control group in the categories of grammar and mechanics.

Table 7. M ann-Whitney U test comparing the mean numbers of errors the experimental and the control group students made in the post-test.   

<html><body><table><tr><td></td><td></td><td></td><td></td><td></td><td>Mean</td><td>Sum of</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>N</td><td>M</td><td>SD</td><td>Ranks</td><td>Ranks</td><td>U</td><td>z</td><td>p</td></tr><tr><td>Grammar</td><td>Experimental</td><td>37</td><td>.08</td><td>.119</td><td>29.18</td><td>1079.50</td><td>376.500</td><td>-3.618</td><td>.000**</td></tr><tr><td></td><td>Control</td><td>38</td><td>.32</td><td>.293</td><td>46.59</td><td>1770.50</td><td></td><td></td><td></td></tr><tr><td>Usage</td><td> Experimental</td><td>37</td><td>.67</td><td>.545</td><td>35.07</td><td>1297.50</td><td>594.500</td><td>1.150</td><td>.250</td></tr><tr><td></td><td>Control</td><td>38</td><td>.78</td><td>.504</td><td>40.86</td><td>1552.50</td><td></td><td></td><td></td></tr><tr><td>Mechanics</td><td>Experimental</td><td>37</td><td>.36</td><td>.365</td><td>31.74</td><td>1174.50</td><td>471.500</td><td>2.457</td><td>.014*</td></tr><tr><td></td><td>Control</td><td>38</td><td>.62</td><td>.557</td><td>44.09</td><td>1675.50</td><td></td><td></td><td></td></tr></table></body></html>

$\overline { { \overline { { p } } \mathrm { ~< ~ } . 0 5 } }$ . ${ } ^ { \ast \ast } p < . 0 0 1$ .

Table 8. S tudents’ perceptions of automated feedback.   

<html><body><table><tr><td>Themes</td><td>Categories</td><td>Frequency of codes (N)</td></tr><tr><td rowspan="3">Positive Perceptions of Automated Feedback</td><td>Useful</td><td>35</td></tr><tr><td> Enjoyable</td><td>32</td></tr><tr><td>Motivating</td><td>29</td></tr><tr><td rowspan="8">Negative Perceptions of Automated Feedback</td><td>Detailed</td><td>27</td></tr><tr><td>Rich in examples.</td><td>23</td></tr><tr><td>Total</td><td>146</td></tr><tr><td>No focus on content.</td><td>27</td></tr><tr><td>Not understandable</td><td>14</td></tr><tr><td>Unclear feedback categories</td><td>11</td></tr><tr><td>False messages</td><td>10</td></tr><tr><td>Old interface</td><td>6</td></tr><tr><td></td><td>Total</td><td>68</td></tr><tr><td></td><td>Sum</td><td>214</td></tr></table></body></html>

# RQ3: Turkish EFL students’ perceptions of receiving automated feedback on their writing performance

The analysis of the data derived from the student reflections resulted in two themes (e.g., positive perceptions and negative perceptions) and 10 categories (please see Table 8). Each category is presented below with representative quotations taken from the reflection papers.

# Positive perceptions of automated feedback

Students mostly reported positive perceptions regarding the automated feedback they received in their writing course $( \mathrm { N } = 1 4 6 )$ ). First of all, the students found automated feedback useful $( \mathrm { N } = 3 5 )$ ) for their writing performance. They stated that the most effective side of automated feedback was that it was immediate and accessible to the students on the condition that they have an Internet connection. They believed that the immediate feedback made it easier for them to correct their errors when the thoughts and structures they used to create their essays were still fresh in their minds. They also asserted that the immediate correction of errors enhanced permanent learning. These points were made clear by the following statements provided by one of the participants:

“What I most liked about automated feedback was that I was able to reach it as soon as I submitted my essay. I believe making errors is very beneficial for learning and being aware of these errors as soon as I make them is really helpful for more effective learning. In addition, correcting my errors was easier for me because what I tried to express in my essay was still fresh in my mind. When I receive feedback on my errors after a while, I have to think about what I try to say here, which makes me tired and unwilling to correct my errors.”

The students also described automated feedback as enjoyable $( \mathrm { N } = 3 2 )$ ) as they liked to take the advantage of technology in their writing course. They reported that writing an essay on the computer was easier and more enjoyable than writing with pen and paper. Further, they reflected that receiving a score soon after they submitted their written products was motivating $( \mathrm { N } = 2 9 )$ for them. One of the participants commented as follows:

“Receiving a holistic score from the program was motivating for me. Each time I did my best to produce a better essay in order to get the highest score, 6. In addition, as I corrected my errors and submitted my essay again, I could see that my score increased immediately, which encouraged me to correct all of my errors and to write error-free essays for the next assignments.”

The students thought that automated feedback was useful because it was detailed $( \mathrm { N } = 2 7 )$ and provided rich examples $\left( \ N = 2 3 \right)$ in the Writer’s Handbook, which helped them understand the source of their errors and how to correct them. On this point, one of the participants expressed his thoughts through the following words:

“The program provides detailed feedback under some categories. Seeing my errors under these categories let me grasp the general picture related to my writing performance, so I realized my strengths and weaknesses in writing. Moreover, the Writer’s Handbook presents plenty of examples which are very useful for correcting sentence-level writing errors.”

# Negative perceptions of automated feedback

When the two categories are compared, it is obvious that the students’ negative perceptions are much less $( \mathrm { N } = 6 8 )$ ). What the students most criticized about Criterion was that it cannot provide feedback on content $( \mathrm { N } = 2 7 )$ ). They also stated that the advisory messages are difficult to understand $\left( \ N = 1 4 \right)$ because they do not directly tell the correct form of the error. One of the participants reflected his thought as follows:

“Automated feedback only shows and categorizes our mistakes, but it would be more beneficial if it provides the correct form. I had difficulty in finding the correct form of my errors because I could not understand the advisory messages.”

The students reported that the feedback given in the category of organization and development was not clear $( \mathrm { N } = 1 1$ ). Criterion uses colour codes to show the organizational elements of the submitted essay, such as thesis statement, main ideas, supporting sentences, and concluding sentences. The students reported that the advisory messages that appear when you click on the colour codes are repetitive and not informative. One of the participants revealed his comment on this issue using the following words:

“Each time I submitted my essay, I received the same advisory messages in terms of the development and organization of my essay. I think these messages are too general and do not give me detailed information regarding the organizational elements in my essay.”

Moreover, the participants stated that the program sometimes generates false error messages $( \mathrm { N } = 1 0 )$ ). Regarding this issue, the students presented the following examples:

“The program says there is a grammar mistake in the highlighted part of the sentence, but in fact there is no problem.”   
“Criterion showed a repetition warning although I repeated the word only a few times.”   
“Criterion showed the title of the essay as a fragment error, thus I did not write a title for my later submissions.”

Finally, the students reported that the user interface of the program should be renewed as it looked old and dull $\left( \mathrm { N } = 6 \right)$ .

# Discussion and conclusion

The results of this study showed that the experimental group students increased their writing scores and decreased their errors after they received combined automated-teacher feedback as it was revealed by the previous studies (Attali, 2004; Chou et  al., 2016; Ebyary & Windeatt, 2010; Li et  al., 2017; Liao, 2015). Different from these studies, however, the present study compared the experimental group students’ writing achievement with that of the control group who received full teacher feedback. The results showed no significant difference between the two groups in terms of the total score they got from the analytic scoring scale. This finding is in line with those reported by Choi (2010) and Wilson and Czik (2016) who concluded that teacher $^ +$ automated feedback condition and teacher feedback-only condition had the same impact on students’ writing improvement. However, it contradicts with that of Tang and Rich (2017) who indicated that the students in the AWE-integrated feedback condition achieved greater writing improvement than the students in the teacher feedback condition. It is important to note that these studies (i.e., Choi, 2010; Tang & Rich, 2017; Wilson & Czik, 2016) did not give details regarding the provision of teacher feedback (e.g., whether it was direct or indirect and what aspects of writing it addressed) in the control group. There might be differences between the feedback conditions applied in those studies and the feedback conditions applied in the present study. Variation of feedback procedures might reveal contradictory results.

The comparison of the error frequencies indicated that while there was no significant difference between the two groups in terms of reducing their errors in the category of usage, the experimental group students significantly outperformed the control group students in the categories of grammar and mechanics. The students’ reflections can explain this significant difference. They believed that automated feedback was more effective for learning since it allowed them to correct their errors when the thoughts were still fresh in their minds. Also, the AWE system provided them with the individual support they needed anytime and anywhere. Having a chance to correct their errors before they submitted their tasks for teacher revision might have increased the students’ autonomy creating a learner-focused environment which enabled them to conduct their own learning process. Moreover, the difference between the two groups regarding the improvement in grammar and mechanics might have been due to the fact that Criterion picked out much more mistakes than the instructor, which increased the students’ awareness of their own mistakes. Although the instructor was trained to provide comprehensive feedback like Criterion, she might have missed some errors because of fatigue. The result that automated feedback has positive impact on grammar and mechanics is parallel with those reported by Kellogg et  al. (2010) and Wang et  al. (2013) while it contradicts that of Choi (2010) who found no significant difference between the two groups who received either automated feedback or teacher feedback in terms of reducing the number of their errors in the given categories. This might be because the teachers in Choi’s study could not integrate automated feedback in their writing instruction properly due to the large number of students and lack of time as it was stated by the author. In other words, the teachers did not urge their students to use the AWE program before receiving teacher feedback and they did not follow their use of it throughout the feedback procedure. Therefore, the students did not use the program as much as it was expected. However, in the current study, the instructor checked whether the students corrected their form-related errors using the automated feedback before they sent their assignments to the instructor. Also, the students were reminded that both the first and the final drafts of these assignments would be considered in the evaluation of their final grades. Therefore, it was made sure that all of the students used automated feedback to revise their first drafts.

Most of the participants believed that automated feedback helped them improve their writing skills in general. The most effective aspect of automated feedback stated by the students was that it was immediate and accessible at any time. The participants stated that receiving immediate feedback made them willing to complete their assignments as soon as possible because they were curious about their performance; and they were encouraged to revise their essays more because their scores increased as they corrected the problematic parts. These positive perceptions coincide with the findings revealed by the previous studies (Ebyary & Windeatt, 2010; Fang, 2010; Grimes & Warschauer, 2010; Link et  al., 2014; Tang & Rich, 2017; Tsuda, 2014; Wilson & Roscoe, 2020). The participants also stated that receiving immediate feedback was more effective in learning because it enables the students to revise their errors when the ideas are still fresh in their minds, which helped them reduce their repeated errors as it was previously presented by Tsuda (2014). Few students reported negative reflections on the AWE system such as providing false messages and vague feedback categories as shown by some previous studies (Lai, 2010; Wang, 2015). This might be because the students in the present study were English majors with the upper-intermediate level of English language proficiency. Also, they had a high level of computer literacy as shown by the survey and were given a detailed training on using the AWE tool.

In conclusion, the present study showed that the students who received combined automated-teacher feedback improved their analytic writing scores as much as the students who received full teacher feedback. However, the students who received automated feedback on the form made fewer errors in the categories of grammar and mechanics than the students who received full teacher feedback. In an AWE-integrated writing classroom, students have the opportunity to self-correct their errors on the form (sentence-level grammar and punctuation errors), which might increase learner autonomy enabling the learners to self-regulate and to reflect on their own learning in an anxiety-free environment. Besides, AWE-integrated writing instruction decreases the time teachers will spend responding to a single assignment as well as the number of revision rounds. Thus, as it was suggested by the previous studies, EFL instructors can integrate automated feedback in their writing instruction in order to reduce their burden and provide more detailed feedback on the meaning related problems of writing, which is significant for the communicative dimension of writing (Burstein et  al., 2003; Li et  al., 2014; Link et  al., 2014; Tang & Rich, 2017). In addition, instead of marking tens of papers every day, EFL instructors can spend their time and energy on looking for ways to provide more effective writing instruction practices for their students. Also, integrating technology in writing instruction can be interesting and enjoyable for EFL learners who are accustomed to conventional pen-and-paper writing instruction. It is necessary to note that, the effectiveness of AWE integration depends on several factors such as students’ familiarity with technology, teachers’ readiness and willingness to integrate technology in their instruction, and training both students and teachers on how to use the selected AWE program (Grimes & Warschauer, 2010; Warschauer & Grimes, 2008). These factors should be considered to make decisions about integrating AWE in writing class.

The current study has four limitations that should be considered in interpreting its findings. First, in the current study, the participants were English Language Teaching major students who were motivated and willing to receive any kind of feedback that would help them improve their writing performance. Thus, the outcomes of the study may not be generalized to non-English major learners. Future studies can be conducted on non-English majors and the results can be compared to those obtained from English majors. Second, the fact that the current study was carried out in an instructional setting made it impossible to employ a true experimental research design. Random sampling and assignment of the participants might yield more reliable results. Third, the study provided data regarding the students’ perceptions of receiving automated feedback but not teacher feedback. Providing the perceptions of receiving teacher feedback as well as the perceptions of automated feedback could be valuable for a comparison between the two feedback practices. Finally, a delayed post-test was not administered in the current study because the authors could not communicate with the same students after the study. Future studies can employ e delayed post-test to assess the longer-term impacts of AWE. Moreover, future studies can compare the number of errors picked out by AWE systems and instructors in order to back up the speculation that instructors might miss some errors because of fatigue.

# Authors’ note

This study was derived from a PhD dissertation by Elif SARI under the supervision of Turgay HAN and it was approved by the Faculty of Educational Sciences of Atatürk University in Turkey.

# Acknowledgements

We would like to express our gratitude to TUBITAK (The Scientific and Technological Research Council of Turkey) for the financial support provided through 2211-National Graduate Scholarship Programme. We also owe our thanks to Educational Testing Services (ETS) for supporting this study by donating licenses to use Criterion Online Writing Evaluation Service.

# Disclosure statement

No potential conflict of interest was reported by the authors.

# Notes on contributors

Assoc. Prof. Dr. Han acts as the director of English Language and Literature department at Ordu University. His research areas include L2 measurement and assessment issues with a special focus on individual differences in language learning, and generalizability (G-) theory. Dr. Han has published widely on the examination of affective factors in L2, the use of mobile applications in L2 learning, and the application of different assessment frameworks in assessing L2 writing performance.

Dr. Sari works as an EFL instructor at the School of Foreign Languages at Karadeniz Technical University in Turkey. Her research areas include EFL writing assessment, automated scoring, and automated feedback. She also conducted studies on grammar teaching and pronunciation anxiety in the EFL context.

# ORCID

Turgay Han $\textcircled{1}$ http://orcid.org/0000-0002-9196-0618   
Elif Sari $\textcircled{1}$ http://orcid.org/0000-0002-3597-7212

# References

Attali, Y. (2004). Exploring the feedback and revision features of Criterion. Paper presented at the National Council on Measurement in Education (NCME), San Diego, CA.   
Attali, Y., Bridgeman, B., & Trapani, C. (2010). Performance of a generic approach in automated essay scoring. Journal of Technology, Learning, and Assessment, 10(3), 4–16.   
Bitchener, J. (2012). A reflection on ‘the language learning potential’ of written CF. Journal of Second Language Writing, 21(4), 348–363. https://doi.org/10.1016/j.jslw.2012.09.006   
Burstein, J., Chodorow, M., Leacock, C. (2003, August). Criterion online essay evaluation: An application for automated evaluation of student essays. In Proceedings from the 15th Annual Conference on Innovative Applications of Artificial Intelligence.   
Chen, C., & Cheng, W. (2008). Beyond the design of automated writing evaluation: Pedagogical practices and perceived learning effectiveness in EFL writing classes. Language Learning & Technology, 12(2), 94–112. http://doi.org/10.125/44145   
Chen, H. H. J., Chiu, S. T. L., & Liao, P. (2009). Analyzing the grammar feedback of two automated writing evaluation systems: My Access and Criterion. English Teaching and Learning, 33(2), 1–43. https://doi.org/10.6330/ETL.2009.33.2.01   
Choi, J. (2010). The impact of automated essay scoring (AES) for improving English language learner’s essay writing (pp. 1–208). University of Virginia. https://doi.org/10. 18130/V3TC0J   
Chou, H. N. C., Moslehpour, M., & Yang, C. Y. (2016). My access and writing error corrections of EFL college pre-intermediate students. International Journal of Education, 8(1), 144–161. https://doi.org/10.5296/ije.v8i1.9209   
Cicchetti, D. V. (1994). Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology. Psychological Assessment, 6(4), 284. https://doi.org/10.1037/1040-3590.6.4.284   
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Erlbaum.   
Dikli, S. (2006). An overview of automated scoring of essays. Journal of Technology, Learning, and Assessment, 5(1), 1–35. http://www.jtla.org.   
Ebyary, K., & Windeatt, S. (2010). The impact of computer-based feedback on students’ written work. International Journal of English Studies, 10(2), 121–142. https://doi. org/10.6018/ijes/2010/2/119231   
Educational Testing Service (ETS). (2019). http://www.ets.org   
Fang, Y. (2010). Perceptions of the computer-assisted writing program among EFL college learners. Journal of Educational Technology & Society, 13(3), 246–256.   
Ferris, D. R. (2010). Second language writing research and written corrective feedback in SLA: Intersections and practical applications. Studies in Second Language Acquisition, 32(2), 181–201. https://doi.org/10.1017/S0272263109990490   
Glaser, B. G., & Strauss, A. (1967). The discovery of grounded theory. Chicago: Aldine.   
Grimes, D., & Warschauer, M. (2010). Utility in a fallible tool: A multi-site case study of automated writing evaluation. Journal of Technology, Language, and Assessment, 8(6), 1–43.   
Guo, Q., Feng, R., & Hua, Y. (2021). How effectively can EFL students use automated written corrective feedback (AWCF) in research writing? Computer Assisted Language Learning, 1–20. https://doi.org/10.1080/09588221.2021.1879161   
Jiang, L., & Yu, S. (2020). Appropriating automated feedback in l2 writing: Experiences of Chinese EFL student writers. Computer Assisted Language Learning, 1–25. https:// doi.org/10.1080/09588221.2020.1799824   
Kellogg, R. T., Whiteford, A. P., & Quinlan, T. (2010). Does automated feedback help students learn to write? Journal of Educational Computing Research, 42(2), 173–196. https://doi.org/10.2190/EC.42.2.c   
Lai, Y. H. (2010). Which do students prefer to evaluate their essays: Peers or computer program. British Journal of Educational Technology, 41(3), 432–454. https://doi. org/10.1111/j.1467-8535.2009.00959.x   
Li, Z., Feng, H. H., & Saricaoglu, A. (2017). The short-term and long-term effects of AWE feedback on ESL students’ development of grammatical accuracy. Calico Journal, 34(3), 355–375. https://doi.org/10.1558/cj.26382   
Li, Z., Link, S., Ma, H., Yang, H., & Hegelheimer, V. (2014). The role of automated writing evaluation holistic scores in the ESL classroom. System, 44, 66–78. https:// doi.org/10.1016/j.system.2014.02.007   
Liao, H. C. (2015). Using automated writing evaluation to reduce grammar errors in writing. Elt Journal, 70(3), 308–319. https://doi.org/10.1093/elt/ccv058   
Link, S., Dursun, A., Karakaya, K., & Hegelheimer, V. (2014). Towards better ESL practices for implementing automated writing evaluation. Calico Journal, 31(3), n3. https://doi.org/10.11139/cj.31.3.323-344   
Link, S., Mehrzad, M., & Rahimi, M. (2020). Impact of automated writing evaluation on teacher feedback, student revision, and writing improvement. Computer Assisted Language Learning, 1–30. https://doi.org/10.1080/09588221.2020.1743323   
McGraw, K. O., & Wong, S. P. (1996). Forming inferences about some intraclass correlation coefficients. Psychological Methods, 1(1), 30. https://doi.org/10.1037/1082- 989X.1.1.30   
Miles, M. B., & Huberman, A. M. (1994). Qualitative data analysis: An expanded sourcebook. Sage.   
Ranalli, J. (2021). L2 student engagement with automated feedback on writing: Potential for learning and issues of trust. Journal of Second Language Writing, 52, 1–16. https:// doi.org/10.1016/j.jslw.2021.100816   
Rock, J. (2007). The impact of short-term use of Criterion on writing skills in ninth grade. (Research Report 07-07). Educational Testing Service. https://doi. org/10.1002/j.2333-8504.2007.tb02049.x   
Rogers, J., & Revesz, A. (2020). Experimental and quasi-experimental designs. Routledge.   
Shute, V. J. (2008). Focus on formative feedback. Review of Educational Research, 78(1), 153–189. https://doi.org/10.3102/0034654307313795   
Stevenson, M., & Phakiti, A. (2014). The effects of computer-generated feedback on the quality of writing. Assessing Writing, 19, 51–65. https://doi.org/10.1016/j.asw. 2013.11.007   
Tang, J., & Rich, C. S. (2017). Automated writing evaluation in an EFL setting: Lessons from China. JALT CALL Journal, 13(2), 117–146. https://doi.org/10.29140/jaltcall. v13n2.215   
Truscott, J. (1996). The case against grammar correction in L2 writing classes. Language Learning, 46(2), 327–369. https://doi.org/10.1111/j.1467-1770.1996.tb01238.x   
Tsuda, N. (2014). Implementing criterion (automated writing evaluation) in Japanese college EFL classes. Language and Culture: The Journal of the Institute for Language and Culture, 18, 25–45. http://doi.org/10.14990/00000561   
Wang, P. (2015). Effects of an automated writing evaluation program: Student experiences and perceptions. Electronic Journal of Foreign Language Teaching, 12(1), 79–100.   
Wang, Y. J., Shang, H. F., & Briody, P. (2013). Exploring the impact of using automated writing evaluation in English as a foreign language university students’ writing. Computer Assisted Language Learning, 26(3), 234–257. https://doi.org/10.1080/09588221. 2012.655300   
Warschauer, M., & Grimes, D. (2008). Automated writing assessment in the classroom. Pedagogies, 3(1), 52–67. https://doi.org/10.1080/15544800701771614   
Wilson, J., & Czik, A. (2016). Automated essay evaluation software in English Language Arts classrooms: Effects on teacher feedback, student motivation, and writing quality. Computers & Education, 100, 94–109. https://doi.org/10.1016/j.compedu.2016.05.004   
Wilson, J., & Roscoe, R. D. (2020). Automated writing evaluation and feedback: Multiple metrics of efficacy. Journal of Educational Computing Research, 58(1), 87–125. https:// doi.org/10.1177/0735633119830764

# Appendix

Comparison of criterion and teacher feedback categories.

<html><body><table><tr><td>Criterion category</td><td>Criterion subcategory</td><td>Equivalent instructor feedback</td></tr><tr><td rowspan="7">Grammar</td><td>Fragment or missing comma</td><td>Sentence fragment (fr)</td></tr><tr><td>Run-on sentences</td><td>Run-on sentence (ro)</td></tr><tr><td>Garbled sentences</td><td>Sentence structure error (ss)</td></tr><tr><td>Subject-verb agreement</td><td>Subject-verb agreement (sv)</td></tr><tr><td>III-formed verbs</td><td>Verb form error (vf)</td></tr><tr><td>Pronoun errors</td><td>Pronoun reference error (ref)</td></tr><tr><td>Possessive errors</td><td>Possessive error (poss)</td></tr><tr><td></td><td>Wrong or missing word.</td><td>Wrong word (ww) Missing word (mw)</td></tr><tr><td rowspan="6">Usage</td><td>Proofread this</td><td>Meaning is not clear (?)</td></tr><tr><td>Wrong article</td><td>Article error (art)</td></tr><tr><td>Missing or extra article</td><td>Article error (art)</td></tr><tr><td></td><td>Extra word (X)</td></tr><tr><td>Confused words</td><td>Wrong word (ww)</td></tr><tr><td>Wrong form of word</td><td>Word form error (wf)</td></tr><tr><td></td><td>Faulty comparisons</td><td>Word form error (wf)</td></tr><tr><td></td><td>Preposition error</td><td>Preposition error (pr)</td></tr><tr><td></td><td>Nonstandard word form.</td><td>Word form error (wf)</td></tr><tr><td>Mechanics</td><td>Negation error</td><td>Verb form error (vf)</td></tr><tr><td rowspan="9"></td><td>Spelling</td><td></td></tr><tr><td></td><td>Spelling error (sp)</td></tr><tr><td>Capitalize proper nouns</td><td>Capitalization error (C)</td></tr><tr><td>Missing initial capital letter in a sentence</td><td>Capitalization error (C)</td></tr><tr><td>Missing question mark</td><td>Punctuation error (P)</td></tr><tr><td>Missing final punctuation</td><td>Punctuation error (P)</td></tr><tr><td>Missing apostrophe</td><td>Punctuation error (P)</td></tr><tr><td>Missing comma</td><td>Punctuation error (P)</td></tr><tr><td>Hyphen error</td><td>Punctuation error (P)</td></tr><tr><td></td><td>Fused words</td><td>Spelling error (sp)</td></tr><tr><td></td><td>Compound words</td><td>Word form error (wf)</td></tr></table></body></html>