# A measure of EFL argumentative writing cognitive load: Scale development and validation

Jiu Lia, Jianhua Wangb,c, \*

a School of Foreign Languages, Shanxi Normal University, No. 339 Taiyu Road, Xiaodian District, Taiyuan 030031, China   
b School of Foreign Languages, Renmin University of China, Beiing, China   
c School of Foreign Languages, Huaqiao University, Xiamen, China

# ARTICLEINFO

# ABSTRACT

Keywords:   
Writing cognitive load   
Argumentative writing   
Scale development   
Validation

Writing requires substantial cognitive processing; when writing, cognitive load emerges due to the limited working memory. Research has investigated cognitive load as an explanatory concept in L2 writing, although there are a few instruments for measuring. Accordingly, the present study aims to develop and validate the EFL Argumentative Writing Cognitive Load Scale. Initially, stimulated recall from 13 undergraduates' cognitive load during a writing task was used to generate a pool of scale items. Subsequently, 685 undergraduate students participated, with 300 at the scale development stage and 385 at the validation stage. The final version of the scale contains 39 items assessing six dimensions: argumentation, organization, language expression, word spelling, introduction and conclusion writing and post-writing monitoring. Both exploratory and confirmatory factor analyses showed that the scale's psychometric properties are acceptable. The scale enables writing researchers and teachers to better understand L2 learners' writing process and deploy targeted intervention strategies to optimize writing cognitive load.

# 1. Introduction

Cognitive load is \*a multidimensional construct representing the load that performing a particular task imposes on the learner's cognitive system" (Pas & Van Merrenboer, 1994, p.353). Determined by the interaction between tass and learners, cognitive load in second language writig (SLW)i often used to describe learners implicit information processng mechanism in the black box" of the writing process An accessible writing cognitive load measurement tol would have important implications for both teaching and learning writing, as well as task complexity research more broadly. However, there is a lack of such measurement cales in the literature. Aordingly, we developed the EFL Argumentative Writing Cognitive Load Scale (EFL-AWCLS), based upon cognitive load theory, the cognitive models of writing, and EFL learners reports of cognitive load experiences. We chose the genre of argumentative writing, because i i important in academic contexts (Le & Deakin, 2016) and is oten used to test L2 leaners writing proficiency in high-stakes tests (Hirvela, 2017). Practicing writing teachers who use the scale may gain improved understanding of their students cognitive resources distribution and accordingly optimize intervention strategies to improve students' writing proces management. EFL learners can use the scale to self-diagnose their cognitive load in argumentative writing and achieve self-regulated learning. Finally, the scale makes i possible to measure learners writing conitive load directly and ensures the validity of independent variable in writing task complexity research.

# 2. Literature review

# 2.1. Writing cognitive load

Cognitive load derives from the cognitive processing that ocurs when people perform a complex task. In this regard, pioneering work was conducted by Swellr (1988), who examined cognitie load from the perspective of cognitive reources alocation. Humans limited cognitive capacity makes it dificult for individuals to process multiple pieces of information simultaneously. Thus, tasks requiring such simultaneous processing increase cognitive load. During simultaneous processing, individuals must make decisions about allocating cognitive resources.

From a cognitive perspective, writing isa problem-solving activity involving multiple cognitive sub-processes. Thus, any attempt to define writing cognitive load must identify the specific cognitive sub-processes of writing. Based on the early cognitive models, the writing sub-processes generally include planning, translating, and reviewing (Flower & Hayes, 1981) or monitoring (Kelogg, 1996) Planning contains setting writing goals, generating and organizing ideas to achieve the goals; translating means encoding the generated ideas into written language; reviewing or monitoring involves evaluating and revising of the written text or unwriten thoughts. Considering that transcribing also competes with other sub-processes for cognitive resources, in his most recent model, Hayes (2012) added transcring including spling and orthography. Allthese sub-processes carry out non-automated activities in working memory and how its components support the orchestration of the sub-processes is precisely described in Kellgg's model (1996). The sub-- processes parallel characteristics and conflicting demands on the limited working memory make them in many respects contolled and effortful (Haye, 1996; Kellog, 196) and gnerate cognitive overload on writers 2 writers, retricted by their language proficiency, just have to invest more cognitive resources during writing and thus are likely to feel more cognitively overloaded. For example, compared with L1 writers, 2 writer had more dificulty with planning and revision and felt laorious when transcribing (Silva, 1993) In terms of time alocation, formulation took up the largest part (Roca de Larios et al., 2008) and revisions were also time consuming (Hall, 1990). Although L2 and 1 writer display different mental ffort distriution, L2 writing follows asimilar composition process to L1 writing (Raimes, 1985; Zamel, 1982) and containing the same sub-processes as has been mentioned.

In spite of the consensus reached on the sub-processes of writing, these sub-processes contained in diffrent cognitie models are sometimes labeled differently or have different denotations. For example, monitoring in Kellogg's (1996) model is equivalent to reviewing in Flower and Hayes' (1981) model; difering from these two models, Hayes (2012) updated model considers planing and reviewing as writing activitie involving a whole writing process rather than as the sub-processes. However, dfferent from monitoring, reviewing in some writing processtdie ., Sila, 193 is used in ts narrow sense, only referring to evaluating and not including revision. To avoid the narrow interpretation of reviewing, in our study monitoring was chosen covering reviewing and revising Meanwhile, we hold that as part of a whole writing task, planning and reviewing in the written form also exert cognitive load on learners. Against this background, we theoretically defined writing cognitive load as the load imposed on learners by planning, translating, transcribing and monitoring when they conduct a writing task.

# 2.2. Writing cognitive load measurement

The cognitive load can be measured by three indice: mental load istask-oriented, determined by task complexity and independent of individual charctrisics, ment ffortis lnerrientd and belieed  reet leers atual contive resource allocation in task completion (Paas & Van Merrienboer, 1994); task perfomance is used to indirctly infer leaners cognitive load level during task completion based on its correlation with cognitive load. While mental load and task performance are a priori and posterior estimation of cognitie od respectively, mental efor rpresets the amount f capacity or reoures actuall allocatd o acommodate the ask demands when learners are working on a task (Paas et al, 2003). Taking mental ffort as the measurement index, the slf-ratings used to measure cognitive load so far mainly comprised a single-item scale cited from Paas (1992). Learners were asked to complete a task and then to rate their level of mental efort during the tas completion on a 9-point scale ranging from1extremely low) to 9extremely high). Sometimes lerners were required to indicate \*how dificulty the task was" also ona-point scale ranging from1 (extremely easy) to 9 (extremely dfficult). However, mental effort and perceived difficulty are independent and not linearly related. Learners who perceive greater difficulty during task completion do not necessarily invest acorresponding amount of mental ffort. When a learner perceives a task to be extremely dificult, they may not be motivated to invest much effort (Van Gog & Paas, 2008). Therefore, we cannot judge how much cognitive efort the learner has invested by observing their perceived task dificulty. As we are interested in learners' subjective engagement during the writing process, mental effort was used as the measurement index in this study.

In SLW studies, cognitive load often acts as the mediator in the relationship between task complexity and writing performance. However, cognitive load's causal factors, task complexity and leaner factors (e.g., prior knowledge, language proficiency, working memory, ec. rather than cognitive load itself have received the most attention. Based on Robinsons (2001b) triadi framework, task complexity has usually been manipulated along the dimensions of planning (e.g., Johnson et al., 2012), number of elements (e.g., Kuiken & Vedder, 2008), reasoning demands (e.g., Ruiz-Funes, 2015), and prior knowledge (e.g., He & Shi, 2012), and the resuls regarding eect size and whether task complexity manipulations actually produced sinificantly different language peformances were inconclusive and even contradictory (Johnson, 2017). Some researchers (e.g., Revesz, 2014) attributed these divergent findings to the inadequacy of current research practices in the measurement of the independent factors and the causal processes. Cognitive load resulted from the manipulated task often failed to get measurement and was merely assumed in most cases. To resolve the controversy, researchers must validate the assumption of cognitive task complexity (Sasayama, 2016), i., provide separate evidence that the task manipulation has indeed led to the desired changes i cognitive load, and that these changes have actually trigered the predicted causal procese (Revesz, 2014; Reves e al., 2016). Thus, the measurement of cognitive load induced by different task manipulations has become an issue of fundamental importance. A growing number of studies have realized the methodological shortcomings and began to examine whether task manipulation based on the triadic framework actually resulted in the intended changes in learners cognitive load. Some introduced the writing proces to find evidence (e.g., Tabari, 2021), while most measured cognitive load through self-ratings, expert judgment, time-on-task, dual task methodology or their combined use.

Self-ratings used in most studies were adapted and extended from Robinson's (2001a) questionnaire, and used a 9-point scale (e.g., Revesz et al., 2017), a 5-point scale (e.g., Golparvar & Rashidi, 2021), a 6-point cale (e.g., Zalbidea, 2017) or a 4-point scale (e., Choong, 2014) to asses participants perceptions of overall mentaleffort, task difficulty or both to validate the designed task complexity. All the self-ratings employed in these studies were concerned with learners' overall perceptions of cognitive load in finishing the whole task. Only a few studies asked what leaners found difficult (e., Choong, 2014) or how much mental ffort they invested in sub-tasks, such as planning (e.g, Revesz et al., 2017). To better validate tak complexity, some researchers combined self-ratings with expert judgments, time-on-task or a dual-task methodology. In Rahimi and Zhang's (2018) study, both teachers and students judged task complexity. Participants in Choong's (2014) tudy not only needd to indicat perceived difficulty, but also had to report the time required to finish the task. Lee (2020) combined subjective elf-ratings, experts judgment, and time-on-task. A 9-point Likert scale regarding respondents overall perceived task difficulty and mental effort was completed by both learners and experts; learners also needed to indicate answers regarding their stressand reported estimated time spent in pre-task stage and time pressure during task completion. Further, Xu et al. (2023) triangulated data from self-ratings, expert judgment and dual-task methodology to validate the task complexity manipulations.

Considering the necessity of differentiating between mental effort and perceived difficulty mentioned above in this ection, some researchrs augd both, u et al., 2023). Neertheles, the mental effort scale employed in these studies was usually single-item and aimed to tap the overall amount of mental effort learners invested, whil lessconsideration has been given to the mental effort disribution in writing processes. Furthermore,cognitive load is largely task-dependent; argumentative tasks are assumed to be more cognitively demanding than narrative tasks (Ruiz-Funes, 2015; Yoon, 2021), but there is a paucity of instrument designed for measuring learners' cognitive load in specific writig genres. To fillthis gap, and considering the academic importance of argumentative writing for leaners, the present study uses mental effort as the measurement index and attempts to develop and validate a scale for measuring EFL learners cognitive load in argumentative writing (EFL-AWCLS), specificall, to address the following two questions: (1) What is the internal structure f the EFL-AwCLs? (2) Is the scale psychometricly reliable and valid to measure EFL learners' writing cognitive load in argumentative writing?

# 3. Method

# 3.1. Participants

At the stage of the scale's tentative structure development and item generation, we recruited 13 EFL non-English major un. dergraduates at a university in Chinese mainland. They were eight female students and five male students, aged from 17 to 20 years. Five of these were sophomores and had passed the College English Test (CET)-Band 6, roughly equivalent to B2 in terms of CEFR; eight freshmen had not yet but was going to take CE-Band 4 soon, the threshold of which is basically equivalent to B1 in terms of CEFR. All the participants signed an informed consent form.

At the stage of scale development and validation, 685 non-English majors through convenience sampling from the first author's university participated in the study. All the participants were sophomores aged from 18 to 21 years old $M = 1 9$ $S D = 0 . 8$ ), comprising 185 male students and 500 female students. Like most of the participants at the previous stage, they were also going to take CET-Band 4 soon. Based on the previous experience athis tge generally most of them could pass the Test and even for those who could not, their scores would be near to the threshold of CET-4. Thus, we estimated their English proficiency as B1 in terms of CEFR. They had learned English for at least nine years but had no learning experiences abroad and had not received specific argumentative writing training. The participants were divided into two samples, with the exploratory factor analysis (EFA) at the development stage conducted on Sample 1 $( N = 3 0 0 )$ ) and the confirmatory factor analysis (CFA) at the validation stage conducted on Sample 2 $( N = 3 8 5$ . Participants both in Sample 1 and 2 were balanced on discipline background.

# 3.2. Procedures

To answer RQ1, a tentative structure of EFL-AWCLS was first developed, based on which the scale items were generated, and then item analysis and construct validity test through EFA were conducted; to answer RQ2, the reliability test was conducted and CFA was performed to verify the scale structure.

# 3.2.1. Developing a tentative structure of EFL-AWCLS

To develop a tentative structure of EFL-AwCLs and inform later item generation, we firstly defined the writing cognitive load construct by reviewing of cognitive load theory (Swellr, 1988) and writing process theory (Flower & Hayes, 1981; Kellgg, 1996; Hayes 2012) (see the Literature Review above). Subsequently, we explored and disentangled the construct through a qualitative study. In the study, 13 participants were required to complete an argumentative writing task on the topic \*Are you in favor of learning by reading or learning by doing?" in no essthan 120 words within 30 minutes. The writing prompt was presented in English and students were notalowed to use tools such as dictionaries or web search during writing. In line with the CET-Band 4 writing requirements, both a 30-minute time limit and a lower word limit of 120 were set because they were in congruence with most of the participants' English proficiency levels and these student writers' actual cognitive load generallycould be uncovered under such limited conditions.

Each participant met with the firs author individually to do the writing task. Their writing proces was recorded and later a stimulated recall was administered in Chinese to find out how they allocated their metal effort during writing. Data collection proceeded as follows: Inputlog 7.0.0.11 and Camstudio were used in combination to track and record the participants real-time writing process fter writing, the video was played back to stimulate participants' rel of their cognitive activities during pause andrevision the threshold of a pause was set to $2 0 0 0 \mathrm { m } s$ in line with previous studies (e.g., Wengelin, 2006). During the pause and revision, questions were asked such as "why did you pause here?", \*what were you thinking during the pause?" and \*what revision did you try to make here?" In addition, participants were encouraged to interrupt the playback at any time to trace their psychological activities during writing. To avoid memory los, the stimulated recall was conducted 10 minutes fter writing. The participants'recall was recorded with their permission.

The stimulated recall protocols were transcribed and manually coded twice by one of the authors; the first coding was carried out immediately after each participant finished the stimulated recall and the second about one month later aferall 13 participants had completed the task. In a similar vein as Revesz et al.s work (2019), open coding and axial coding were conducted successively. The identified codes afer axial coding were grouped into the writing cognitive load construct defined in the Literature review. A high intra-coder reliability was yielded on all the identified codes (Cohen's kappa $\ge 0 . 8 4$ . Codes' definitions and examples are provided in the coding scheme (see Table 1).

# 3.2.2. Scale development and validation

3.2.2.1. Item generation. Based on the EFL-AwCLS tentative structure, an initial pol of 72 items was generated. Modeled afer Paas's (1992) MentalEffort Rating Scale each item wasated in the form,  inest much effort i..". Responses were scored on a 6-point Likert scale from 1 (strongly disgree to 6(strongly agree). The even design of responses aimed to inhibit participants' over reliance on a neutral attitude. Allitems were worded positively, so higher scores always represented higher levels of mental effort.

To check the scale's content validit, 13 English teachers from different universities were consulted whether the initia items can reflect the writing cognitive lod we defined before, and whether there were duplicated items or ther deiciencies. These teachers had at least five years of English writing teaching experience and they were familiar with students' writing process and had a good understanding of what shaped students writing cognitie load. Subsequently, two pilot tests were conducted successvely with 28 and 31 undergraduates to improve the items' comprehensibility and readability based on these students comments and suggestions. These recruited students shared a similar background with the participants in the study. After the content validity and pilot tes, some items were removed or revised and finally 59 items remained.

The Chinese version of the intial scale was distributed to 761 students in a paper form in September 2018 and 685 were effectively received, with the effective rate of $9 1 . 8 \%$ . When completing the scale, students were asked to just reflect on their argumentative writing activities. The data pre-processing showed that there were no duplicate values and outliers; the missing values were replaced employing Mean of Nearby Points. After pre-processing, item analysis, construct validity and reliabilit tests were conducted successively via SPSS 24.0.

Table 1 Coding scheme.   

<html><body><table><tr><td>Category</td><td>Sub-category</td><td>Definition</td><td>Example</td></tr><tr><td>Planning</td><td>Topic examining</td><td>The effort to figure out the task requirements</td><td>During this time, I tried to clarify the writing prompt. When I read it for the first time, I was not...you know, kind of confused. I feel like I didn&#x27;t see it very clearly, and then I read it several times and finally figured out, oh, there are two different opinions, and you have to</td></tr><tr><td></td><td>Generating ideas</td><td>The effort to explore possible opinions, to propose one&#x27;s own position and points of arguments</td><td>make a comment... (Student H) The topic is mainly about ways of learning, so I was thinking which is better. Then I came up with three points to argue why. (Student M)</td></tr><tr><td></td><td>Organizing ideas</td><td>The effort to arrange the generated ideas in a logical and coherent way</td><td>I was considering, first to state both ways are important, then to. argue the negative effect of taking only one way of learning, and finally to make the conclusion. (Student L)</td></tr><tr><td>Translating</td><td>Argumentation</td><td>The effort to argue and elaborate one&#x27;s own position</td><td>I was thinking how to develop the logical deduction, for example, how does A get to B? Maybe it goes from A to a, then from a to b and finally to B. (Student A)</td></tr><tr><td></td><td>Linguistic representation</td><td>The effort to linguistically encode the writing content, including language retrieval and polishing</td><td>During writing, I tried to make the language more appropriate, various, and no Chinglish. (Student A).</td></tr><tr><td>Transcribing</td><td>Word spelling</td><td>The effort to recall how to spell the words.</td><td>I forget how to spell the word &quot;mist&#x27; and I am not sure whether it is m-i-s-t. I remembered it is not so short, so I deleted it. (Student E) I read back the writing prompt, because I felt what I have written</td></tr><tr><td>Monitoring</td><td>Reviewing</td><td>The effort to check the content pertinence, argumentation logic, accuracy and readability, etc.. of the written text.</td><td>was a little off the topic. (Student D)</td></tr><tr><td></td><td>Revising</td><td>The effort to make improvements on the aspects. identified in reviewing.</td><td>I found what I was going to express was not causally related to the above sentence and replaced the cohesive word &quot;so&quot;&#x27; with &quot;but&#x27;. (Student B)</td></tr></table></body></html>

3.2.2.2. Item analysis. To reduce the weakness of relying on single methods and ensure the validity of item selection (Denzin, 1978), item analysis was performed through a triangulation of critical ratio (CR), correlation and homogeneity tests to check whether the scale items were discriminant. In the CR test, rfrrng to Kell (1939), we defined the respondents whose total cale scores were in the top $2 7 \%$ as the higher score group $\left( N = 8 2 \right)$ ), and the bottom $2 7 \%$ as the lower score group $\left( N = 8 3 \right)$ . An independent sample t-test was conducted to identify the differences between the two extreme groups on each item. When the two groups were significantlydifferent $\left( t > 3 \right)$ , a larger tvalue meant the item was more discriminant. In the correlation test both the coefficient between the score on each item and the ttal core onthe overal scale and the coficient between the score on ach item and the score on the remaining items in total were calculated. An item would be retained only when its score was at least moderately related (Pearson correlation cofficient $r$ $\geq 0 . 4 )$ to both the total score of the overallscale and the sum of the remaining items. In the homogeneity test, both Cronbach $\alpha$ test and Principal Component Analysis (PCA) were conducted. According to the Cronbach's $\alpha$ calculation formula $\alpha = \frac { \mathrm { { K } } } { \mathrm { { K } } - 1 } \left( 1 - \frac { \sum { S _ { i } ^ { 2 } } } { S ^ { 2 } } \right) ( \mathrm { { I } }$ < stands for the number of the scale items), a scale's Cronbach's $\alpha$ will generally be higher if it contains more items, so if the Cronbach's $\alpha$ increased or remained the same when a certain item was removed, it meant that what the item measured was dfferen from the other items and it should be discarded. Through PCA, both the commonality and factor loading were observed to check each item's homogeneity with the overal scale; following Tabachnick & Fidells (2007) criteri, we deleted the item when is commonality was less than 0.2 and the factor loading was lower than 0.45.

3.2.2.3. Validity and relibilit tests. The construct validity test was conducted by EFA to determine the factor structure of the scale before EFA, Kaiser-Meyer-Olkin (KMO) measure f sampling adequacy and Bartlett's Test of sphericity were calculated to ensure the data's suitability for EFA. Internal and external reliability were checked through the Cronbach's $\alpha$ test. Six weeks after the first test, 36 randomly chosen participants were retested to check the scale's stability. The coeficient of the production-moment correlation of the two tests was calculated; a higher coefficient indicated a better external reliability and stability.

3.2.2.4. Scale validation. FA was conducted later with Amos 2. toverify the preliminary factor structure. Both the overall model fit (absolute fit, incremental fit and parsimonious fit) and internal structure model fit (convergent and discriminant validity) were checked to evaluate the scale's stability and fit on other samples. Absolute fit was evaluated through $X ^ { 2 }$ $X ^ { 2 } / d f ,$ root mean square error of approximation (RMsEA), goodnessof-fit index (GFI) and adjusted goodnes-of-fit index (AGFI), incremental fi through TuckerLewis index (TLI), comparative fit index (CFI) and incremental fit index (IFI) and parsimonious fit through parsimonious goodness of-fit index (PGFI), parsimonious normed fit index (PNFI), parsimonious comparative-fit-index (PCFI) and critical number of the sample size (CN).

# 4. Results

# 4.1. Tentative structure of EFL-AWCLS

As mentioned above, we categorized learners' writing cognitive load into the following dimensions: planning, translating, tran. scribing, and monitoring. The coding result of the qualitative study further revealed that, in argumentative writing, EFL learners mental effrt on planning was specificall invested in topic examining, generating and organizing ideas, on translating in argumentation and linguistic representation, on transcribing in word speling, and on monitoring in reviewing and revising. Integrating the theoretical definition with the empirical findings, we established a tentative sructure of EFL-AWCLS which was used for later item generation (see Fig. 1).

![](img/301a1bcf1dbb3076d36f8b08035ab014ba655b050415091f86f853d7a72b41dc.jpg)  
Fig. 1. Tentative structure of EFL-AWCLS.

# 4.2. Item analysis

The CR test result showed that respondents scores on each item were significantly different between the higher and lower score groups at a $9 5 \%$ confidence interval, and the CR range [4.780, 11.986] was above 3. Correlation analysis showed that scores on each item were significantly correlated with total scores, but on Item 1 and 2, the coefficient was lower than 0.4 (Item 1 $; r = . 3 5 1$ ; Item $2 \colon r$ $= . 3 2 8 \AA$ ; similarly, although the scores on each item were significantly related to the scores of the sum of the remaining items, the coefficients for Items 1, 2, 15 and 35 were also lower than 0.4 (Item $1 \colon r = . 3 1 9$ ; Item 2: $r = . 2 9 7$ ; Item 15: $r = . 3 9 4$ ; Item 35: $r = . 3 9 3 $ Regarding the homogeneity test, Cronbach's $\alpha$ did not change when Items 1 and 2 were deleted, and PCA indicated that the commonality and factor loadings of Items 1, 2 3 15, and 35 were below the threshold at which items should be retained. Acording to the item exclusion principle we set that items would be discarded when the results of any two methods simultaneously suggested that they were not discriminant, Item 1, 2, 15, and 35 were removed from the original scale and finally 55 items were retained.

# 4.3. Construct validity and reliability test

The KMO value (0.906) indicated that the corrlation between the 55items was meritorious and suitabl fr factor analysis (Kaier, 1974). Bartlett's test of sphericity results $( X ^ { 2 } = 6 3 0 6 . 5 2 6 ; p < 0 . 0 0 1 )$ showed that common factors could be extracted in the correlation matrix. Kaiser rule and scree plot test were used in a combined way to determine how many factors to be extracted. Ten factors eigenvalues were greater than 1, but the scree plot (se ig. 2) showed that it was appropriate to extract three or five factors, which are closer tothe tentative structure of EFL-AwCLS developed before. The Kaiser rule led to overfactoring and we determined the number of factors based on the scre plot. EFA with principal axis factoring and oblimin rotation were adopted for factor extraction considering the data are in abnormal distribution (Shapiro-Wilk: $d f = 3 0 0$ $p < 0 . 0 0 1 $ and the extracted factors are correlated.

Both three- and five-factor structure analyses were carried out in the first run of EFA and the following criteria were observed to decide whether items should be included: (1) each item's factor loading should be larger than 0.4 (Tabachnick & Fidell, 207); (2) each item should have no crossfactor loading larger than 0.4; (3) within each factor, the number of items should be no les than 3. The three-factor structure analysis result showed that 17 items did not meet the inclusion criteria. When these items were deleted, the three-factor structure's cumulative explanation variance was $4 8 . 1 5 8 \%$ . By contrast, the five-factor structure analysis revealed that 16 items should be discarded, and the cumulative explanation variance was $5 4 . 3 6 2 \%$ after deletion. Considering that the five-factor structure (see Table 2) had a higher explanation power and was theoretically closer to the tentative structure of EFL-AwCLS pro posed initially, it was chosen as the scale's factor structure.

However, in the five-factor structure, Factor 1, 2, and 3 contained more than 7 items each and what those items measured was not exactly homogeneous. Accordingly, we further conducted a second run of EFA of the items within these threefactors. Following the same procedure as in the first run, KMO and Bartlett test results indicated that the sample was suitable for EFA (Factor 1: $\operatorname { K M O } = 0 . 9 1 1$ $X ^ { 2 } = 1 6 7 9 . 9 4 6$ $p < 0 . 0 1$ ; Factor 2: $\mathrm { K M O } = 0 . 8 7 6$ $X ^ { 2 } = 1 2 2 9 . 8 1 9$ $p < 0 . 0 1$ ; Factor 3: $\mathrm { K M O } = . 8 2 3 ,$ $X ^ { 2 } = 1 0 1 0 . 3 0 0$ $p < 0 . 0 1 { \dot { . } }$ . Varimax rotation was conducted and two common factors were identified based on the scree plot. The cumulative explanation variance of each two-factor structure was either more than or close to $6 0 \%$ (Factor $1 { : } 6 2 . 1 2 4 \%$ ; Factor 2: $6 8 . 4 3 8 \%$ ; Factor 3: $5 9 . 6 5 5 \%$ , indicating that the items had higher factor loadings. After the first run of five-factor EFA and the second run of sub-factor analysis, a total of eight factors were extracted. These factors are named and explained in Table 3.

![](img/5816b4a0f86f10dee1a48487d051d079f95172ce289517d87564cfbff5aa5722.jpg)  
Fig. 2. Scree plot of items measuring writing cognitive load.

Table 2 Factor loadings of the items.   

<html><body><table><tr><td>Items</td><td>F1</td><td>F2</td><td>F3</td><td></td><td></td><td>F5</td></tr><tr><td>46. After writing, I invest much effort in improving the content pertinence.</td><td>0.77</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>45. After writing, I invest much effort in checking the content pertinence.</td><td>0.75</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>49. After writing, I invest much effort in checking the reasoning.</td><td></td><td>0.68</td><td></td><td></td><td></td><td></td></tr><tr><td>50. After writing, I invest much effort in improving the reasoning.</td><td></td><td>0.69</td><td></td><td></td><td></td><td></td></tr><tr><td>54. After writing, I invest much effort in improving the exemplification.</td><td>0.66</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>53. After writing, I invest much effort in checking the exemplification.</td><td></td><td>0.65</td><td></td><td></td><td></td><td></td></tr><tr><td>44. After writing, I invest much effort in checking whether I have understood the task requirements.</td><td></td><td>0.63</td><td></td><td></td><td></td><td></td></tr><tr><td>51. After writing, I invest much effort in checking whether I properly used</td><td></td><td>0.60</td><td></td><td></td><td></td><td></td></tr><tr><td>transitions. 48. After writing, I invest much effort in supplementing content.</td><td></td><td>0.60</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>52. After writing, I invest much effort in revising the transition structures.</td><td>0.60</td><td></td><td></td><td></td><td></td></tr><tr><td>important information.</td><td>47. After writing, I invest much effort in checking whether I have missed</td><td>0.59</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>31. During writing, I invest much effort in thinking how to use grammar</td><td></td><td>0.82</td><td></td><td></td><td></td></tr><tr><td>correctly.</td><td>32. During writing, I invest much effort in checking whether I have used.</td><td></td><td>0.78</td><td></td><td></td><td></td></tr><tr><td>the grammar correctly.</td><td>30. During writing, I invest much effort in choosing the proper</td><td></td><td>0.76</td><td></td><td></td><td></td></tr><tr><td>grammatical structure.</td><td>33. During writing, I invest much effort in revising the grammatical errors.</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>29. During writing, I invest much effort in using collocations properly.</td><td></td><td>0.71 0.69</td><td></td><td></td><td></td></tr><tr><td>28. During writing, I invest much effort in choosing more appropriate words.</td><td></td><td></td><td>0.63</td><td></td><td></td><td></td></tr><tr><td>27. During writing, I invest much effort in retrieving words to formulate</td><td></td><td></td><td>0.55</td><td></td><td></td><td></td></tr><tr><td>content.</td><td></td><td></td><td>0.40</td><td></td><td></td><td></td></tr><tr><td>34. During writing, I invest much effort in avoiding using simple words. 18. During writing, I invest much effort in improving the reasoning.</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>17. During writing, I invest much effort in checking the reasoning.</td><td></td><td></td><td></td><td>0.67</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>0.66</td><td></td><td></td></tr><tr><td>14. During writing, I invest much effort in checking whether I properly used transitions.</td><td></td><td></td><td></td><td>0.66</td><td></td><td></td></tr><tr><td>21. During writing, I invest much effort in improving the exemplification.</td><td></td><td></td><td></td><td>0.61</td><td></td><td></td></tr><tr><td>20. During writing, I invest much effort in checking the exemplification.</td><td></td><td></td><td></td><td>0.61</td><td></td><td></td></tr><tr><td>13. During writing, I invest much effort in choosing transition structures.</td><td></td><td></td><td></td><td>0.54</td><td></td><td></td></tr><tr><td>16. During writing, I invest much effort in reasoning for argumentation.</td><td></td><td></td><td></td><td>0.54</td><td></td><td></td></tr><tr><td>19. During writing, I invest much effort in exemplifying for argumentation.</td><td></td><td></td><td></td><td>0.49</td><td></td><td></td></tr><tr><td>12. During writing, I invest much effort in combining the sentences.</td><td></td><td></td><td></td><td>0.48</td><td></td><td></td></tr><tr><td>42. During writing, I invest much effort in checking the word spelling. 43. During writing, I invest much effort in revising the word spelling</td><td></td><td></td><td></td><td></td><td>0.76</td><td></td></tr><tr><td>mistakes.</td><td></td><td></td><td></td><td></td><td>0.77</td><td></td></tr><tr><td>58. After writing, I invest much effort in revising the word spelling mistakes. 40. During writing, I invest much effort in how to spell words..</td><td></td><td></td><td></td><td></td><td>0.74</td><td></td></tr><tr><td>57. After writing, I invest much effort in checking the word spelling.</td><td></td><td></td><td></td><td></td><td>0.70</td><td></td></tr><tr><td>41. During writing, I invest much effort in changing the word forms.</td><td></td><td></td><td></td><td></td><td>0.69</td><td></td></tr><tr><td>7. Before writing, I invest much effort in thinking how to begin the writing.</td><td></td><td></td><td></td><td></td><td>0.56</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.68</td></tr><tr><td>9. During writing, I invest much effort in writing the introduction part.</td><td></td><td></td><td></td><td></td><td></td><td>0.67</td></tr><tr><td>22. During writing, I invest much effort in writing the conclusion part. 10. During writing, I invest much effort in making the opening appealing to</td><td></td><td></td><td></td><td></td><td></td><td>0.61</td></tr><tr><td>readers.</td><td></td><td></td><td></td><td></td><td></td><td>0.60</td></tr><tr><td>23. During writing, I invest much effort in making the ending appealing to readers.</td><td></td><td></td><td></td><td></td><td></td><td>0.60</td></tr></table></body></html>

Table 3 The factor naming.   

<html><body><table><tr><td>Factor</td><td>Name</td><td>Definition</td><td>Items included</td></tr><tr><td>F1</td><td>logic monitoring</td><td>Mental effort for checking and improving the logic of argumentation after writing</td><td>49-54</td></tr><tr><td>F2</td><td>pertinence monitoring</td><td>Mental effort for checking and improving the comprehensiveness and pertinence of content after</td><td>44-48</td></tr><tr><td>F3</td><td>grammar use</td><td>writing Mental effort for choosing and using grammatical structure correctly during writing</td><td> 30-33</td></tr><tr><td>F4</td><td> lexical retrieval</td><td>Mental effort for retrieving and polishing lexical forms during writing</td><td>27-29, 34</td></tr><tr><td>F5</td><td>argumentation</td><td>Mental effort for argumentation, checking and revising the logic during writing</td><td>16-21</td></tr><tr><td>F6</td><td> organization</td><td> Mental effort for organizing ideas and using transition devices during writing</td><td>12-14</td></tr><tr><td>F7</td><td>word spelling</td><td>Mental effort for spelling words, checking and correcting spelling errors during and after writing</td><td> 40-43,57,58</td></tr><tr><td>F8</td><td>introduction and conclusion writing</td><td>Mental effort invested in writing the introduction and conclusion part to develop position or to attract readers</td><td>7,9,10,22,23</td></tr></table></body></html>

Cronbach's $\alpha$ showed that both the total scale $\scriptstyle ( \alpha = . 9 4 2$ and its subscales $( \alpha > . 7 0 )$ had good internal reliability. About one month later, 36 students took a retest and altogether 32 were valid; the production-moment orelation cofficient of both the total scale and subscales were acceptable, indicating that the scale had measuring constancy. The internal and external reliabilit results are presented in Table 4.

# 4.4. Scale structure validation

After the construct validity and reliability test, the structure of EFL-AwCLS was determined and a total of 39 items remained and were reordered. The cale structure identified through EFA was verified using CFA via Amos 22.0. The maximum liklihood estimation result of the first-order CFA showed that the itial model basically fi the data but did not reach an ideal level. At the same time, the modification indices value between multiple pairs of residuals in the initial model was relatively large; the corresponding items of these pairs of residuals did have a close relationship, which meant revision was necessary. As suggested by the modification indices, there was a high correlation betwee checking and revising on the dimensions of word spelling, grammar use, content pertinence, and logic; in addition, close connections existed between thinking how to begin writig at the pre- and during- writing phases, writing the introduction and conclusion, spelling words and making inflections, improving the reasoning and exemplification, and checking the task requirements and pertinence monitoring. Based on the modification indices, we established a covariant rlationship betwen those pairs of residuals and revised the initial model.

The estimated parameters of the revised model all reached significance $( p < . 0 1 )$ ; all error variances were positive, and the standard error of each estimated parameter was very small the factor loading of each item in the scale was between 0.480-0.848. All these indices showed that the revised model met the it requirements and could identify the data. The revised model beter fit the data. Both the commonly followed criteria for fit indices and the actual model fit indices are presented in Table 5.

The revised model's internal structure model fit was tested through convergent and discriminant validity. Regarding the convergent validity (see Table 6), the factor loadings of almost all items were above 0.55; the reliability coefficient $( R ^ { 2 } )$ showed the latent variables could explain $2 3 . 1 \% - 7 2 \%$ of variance in the items, and even the minimum was within the acceptable range (Tabachnick & Fidell, 2007); the combined reliability (OMB-R) of each latent variable was higher than 0.7, indicating the convergent validity of the latent variables was acceptablealthough the average variance extraction (AVE) was slightly lower than the cutoff f 0.5 (Fornell & Larcker, 1981).

The discriminant validity was judged by comparing the AVE of each latent variable with the correlation cofficient betwen latent variables. When the $\scriptstyle { \sqrt { \mathrm { A V E } } }$ is greater than the correlation coefficient, it is safe to claim that the latent variables have a high discriminant validity. The result (see Table 7) showed that almost every latent variable's $\surd \mathrm { A V E }$ value was higher than its correlation coefficient with other latent variables, except for two pars: lexical rerieval/gammar use, pertinence montoring/logic monitoring, ind. cating that each pair may belong to a higher-order latent construct, which underpinned a second-order CFA. All other items had discriminant validity.

Based on the theoretical assumption and first-order CFA, we constructed a second-order model, with "lexical retrieval' and grammar use" actig as firs-order factors and language expression acting as the second-order factor. imilarly, pertinece monitoring and "logic monitoring acted as first-order factors, while \*post-writing monitoring' acted as the second-order factor. The results showed that the intial model achieved average but not ideal fit. In terms of asolute fit except for RMsEA, the values of allother indices did not reach the threshold; regarding incremental and parsimonious fit almost ll indices values met the fit requirements except for TLI and CN. Taking the modification indices and the relationship between the items into consideration, we set up a covariant relationship between the error-variables and revise the model into an errors-in-variables model (se ig. 3). The revised model had better fit with all indices meeting appropriate thresholds (see Table 8).

Regarding thestructure fit of the second-order CFA revised model, the standardized path coefficients showed that the factor loading of almost al items was above 0.55 the factor loding of Item13 was slightly ower ut sill within the aeptable range. Meanwhil, th

Table 4 Internal and external reliability of the scale.   

<html><body><table><tr><td>Scale</td><td>Cronbach&#x27;s a</td><td>Production-moment correlation coefficient</td></tr><tr><td> logic monitoring</td><td>0.879</td><td>.685*</td></tr><tr><td> pertinence monitoring</td><td>0.831</td><td>.691*</td></tr><tr><td>grammar use</td><td>0.891</td><td>.562*</td></tr><tr><td> lexical retrieval</td><td>0.768</td><td>.674*</td></tr><tr><td>argumentation</td><td>0.835</td><td>.538*</td></tr><tr><td> organization</td><td>0.755</td><td>.645*</td></tr><tr><td>word spelling</td><td>0.859</td><td>.687*</td></tr><tr><td> introduction and conclusion writing</td><td>0.769</td><td>.563*</td></tr><tr><td>Total scale</td><td>0.942</td><td>.889*</td></tr></table></body></html>

$$
\stackrel { * } { } = { p } < . 0 1
$$

Table 5 The result of the first-order CFA model fit indices.   

<html><body><table><tr><td rowspan="2">Fitness indices</td><td rowspan="2">Threshold</td><td colspan="2">Results</td></tr><tr><td>Initial model</td><td>Revised model</td></tr><tr><td>Absolute fit</td><td></td><td></td><td></td></tr><tr><td>x2/df</td><td>x2/df (1-3), good fit</td><td>2.571</td><td>1.917</td></tr><tr><td>RMSEA</td><td>RMSEA &lt; 0.05 good fit</td><td>0.064</td><td>0.049</td></tr><tr><td></td><td>RMSEA  0.08 average fit</td><td></td><td></td></tr><tr><td>GFI</td><td>&gt; 0.9</td><td>0.812</td><td>0.859</td></tr><tr><td>AGFI</td><td>&gt; 0.9</td><td>0.782</td><td>0.834</td></tr><tr><td> Incremental fit</td><td></td><td></td><td></td></tr><tr><td>TLI</td><td>&gt; 0.90</td><td>.830</td><td>.901</td></tr><tr><td>CFI</td><td>&gt; 0.90</td><td>.846</td><td>.912</td></tr><tr><td>IFI</td><td>&gt; 0.90</td><td>.847</td><td>.913</td></tr><tr><td> parsimonious fit</td><td></td><td></td><td></td></tr><tr><td>PGFI</td><td>&gt; 0.50</td><td>.701</td><td>.729</td></tr><tr><td>PNFI</td><td>&gt; 0.50</td><td>.702</td><td>.744</td></tr><tr><td>PCFI CN</td><td>&gt; 0.50</td><td>.769</td><td>.814</td></tr><tr><td></td><td>&gt; 200</td><td>169 (a = 0.01)</td><td>227 (a = 0.01)</td></tr></table></body></html>

Table 6 Convergent validity test result.   

<html><body><table><tr><td>Item</td><td>Factor loading</td><td>R2</td><td>1-R2</td><td>COMB-R</td><td>AVE</td></tr><tr><td>Q1</td><td>0.591</td><td>0.349</td><td>0.651</td><td></td><td></td></tr><tr><td>Q2</td><td>0.698</td><td>0.488</td><td>0.512</td><td></td><td></td></tr><tr><td>Q3</td><td>0.676</td><td>0.457</td><td>0.543</td><td>0.82</td><td>0.43</td></tr><tr><td>Q4</td><td>0.670</td><td>0.449</td><td>0.551</td><td></td><td></td></tr><tr><td>Q5</td><td>0.668</td><td>0.446</td><td>0.554</td><td></td><td></td></tr><tr><td>Q6</td><td>0.623</td><td>0.388</td><td>0.612</td><td></td><td></td></tr><tr><td>Q7</td><td>0.596</td><td>0.355</td><td>0.645</td><td></td><td></td></tr><tr><td>Q8</td><td>0.746</td><td>0.556</td><td>0.443</td><td>0.74</td><td>0.49</td></tr><tr><td>Q9</td><td>0.743</td><td>0.551</td><td>0.449</td><td></td><td></td></tr><tr><td>Q10</td><td>0.658</td><td>0.433</td><td>0.567</td><td></td><td></td></tr><tr><td>Q11</td><td>0.755</td><td>0.570</td><td>0.430</td><td>0.78</td><td>0.47</td></tr><tr><td>Q12</td><td>0.786</td><td>0.617</td><td>0.383</td><td></td><td></td></tr><tr><td>Q13</td><td>0.507</td><td>0.257</td><td>0.743</td><td></td><td></td></tr><tr><td>Q14</td><td>0.848</td><td>0.720</td><td>0.280</td><td></td><td></td></tr><tr><td>Q15</td><td>0.810</td><td>0.657</td><td>0.443</td><td>0.84</td><td>0.56</td></tr><tr><td>Q16</td><td>0.712</td><td>0.507</td><td>0.493</td><td></td><td></td></tr><tr><td>Q17</td><td>0.603</td><td>0.364</td><td>0.736</td><td></td><td></td></tr><tr><td>Q18</td><td>0.609</td><td>0.371</td><td>0.729</td><td></td><td></td></tr><tr><td>Q19</td><td>0.593</td><td>0.352</td><td>0.648</td><td></td><td></td></tr><tr><td>Q20</td><td>0.840</td><td>0.706</td><td>0.294</td><td>0.84</td><td>0.47</td></tr><tr><td>Q21</td><td>0.848</td><td>0.720</td><td>0.280</td><td></td><td></td></tr><tr><td>Q22</td><td>0.561</td><td>0.315</td><td>0.685</td><td></td><td></td></tr><tr><td>Q23</td><td>0.587</td><td>0.345</td><td>0.655</td><td></td><td></td></tr><tr><td>Q24</td><td>0.481</td><td>0.231</td><td>0.769</td><td></td><td></td></tr><tr><td>Q25</td><td>0.647</td><td>0.419</td><td>0.581</td><td></td><td></td></tr><tr><td>Q26</td><td>0.796</td><td>0.634</td><td>0.366</td><td>0.79</td><td>0.43</td></tr><tr><td>Q27</td><td>0.639</td><td>0.408</td><td>0.592</td><td></td><td></td></tr><tr><td>Q28</td><td>0.676</td><td>0.457</td><td>0.543</td><td></td><td></td></tr><tr><td>Q29</td><td>0.718</td><td>0.515</td><td>0.485</td><td></td><td></td></tr><tr><td>Q30</td><td>0.761</td><td>0.579</td><td>0.421</td><td></td><td></td></tr><tr><td>Q31</td><td>0.755</td><td>0.570</td><td>0.430</td><td>0.83</td><td>0.49</td></tr><tr><td>Q32</td><td>0.645</td><td>0.416</td><td>0.584</td><td></td><td></td></tr><tr><td>Q33</td><td>0.625</td><td>0.391</td><td>0.609</td><td></td><td></td></tr><tr><td>Q34</td><td>0.700</td><td>0.489</td><td>0.511</td><td></td><td></td></tr><tr><td>Q35</td><td>0.739</td><td>0.545</td><td>0.455</td><td></td><td></td></tr><tr><td>Q36</td><td>0.712</td><td>0.508</td><td>0.492</td><td>0.84</td><td>0.47</td></tr><tr><td>Q37</td><td>0.758</td><td>0.574</td><td>0.426</td><td></td><td></td></tr><tr><td>Q38</td><td>0.627</td><td>0.393</td><td>0.607</td><td></td><td></td></tr><tr><td>Q39</td><td>0.578</td><td>0.334</td><td>0.666</td><td></td><td></td></tr></table></body></html>

factor loadings of lexical rtrieval" and "grammar use on their higher-order factor language expressiont" were 0.944 and 0.844, respectively, and the factor loadings of "pertinence monitoring' and "logic monitoring' on their higher-order factor "post-writing mon. toring were 0.813 and .908 respectively. Both the factor loading of items on the first-order factor and that of the firt-order factor on

Table 7 The discriminant validity test result.   

<html><body><table><tr><td>VAVE</td><td>argumentation</td><td> organization</td><td>lexical retrieval</td><td>grammar use</td><td>word spelling</td><td>introduction and conclusion writing</td><td>pertinence monitoring</td><td>logic monitoring</td></tr><tr><td>argumentation</td><td>0.66</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td> organization</td><td>0.573</td><td>0.70</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>lexical retrieval</td><td>0.607</td><td>0.559</td><td>0.69</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>grammar use</td><td>0.560</td><td>0.595</td><td>0.796</td><td>0.75</td><td></td><td></td><td></td><td></td></tr><tr><td>word spelling</td><td>0.416</td><td>0.448</td><td>0.503</td><td>0.403</td><td>0.69</td><td></td><td></td><td></td></tr><tr><td>introduction and conclusion writing</td><td>0.491</td><td>0585</td><td>0.495</td><td>0.510</td><td>0.336</td><td>0.66</td><td></td><td></td></tr><tr><td> pertinence monitoring</td><td>0.540</td><td>0.437</td><td>0.500</td><td>0.376</td><td>0.524</td><td>0.521</td><td>0.71</td><td></td></tr><tr><td>logic monitoring</td><td>0.642</td><td>0.592</td><td>0.510</td><td>0.490</td><td>0.507</td><td>0.445</td><td>0.732</td><td>0.69</td></tr></table></body></html>

![](img/0c95933f5b8e3f71466a624a7788d52d9aa81e0ec9d80823a063b1bcf4fb5b00.jpg)  
Fig. 3. The second-order CFA model. PostMon repreents post-writing montorig, LogicMon reresents logic monitorig, PertinenceMon represent pertinence monitoring; LanExp represents languag expression; GramUse represents grammar use; LexRetrieval represents lexical retrieval.

the second-order factor were nearly ideal.

The reliability coefficient $( R ^ { 2 } )$ showed that the second-order factor could explain $6 6 \% 8 9 . 1 \%$ variance for the first-order factor, and the variance explained by the first-order factor for its items was $2 4 . 8 \% - 7 2 . 7 \%$ Although a few items' $R ^ { 2 }$ values were below 0.5, they were stillaccetable (Tabachnick & Fidell 2007). Both the COMB-R value of items under the first-order factor and that of the first-order factors under the higher-order factor were above 0.7, indicating that the convergent validity of items under the firs-order factor was good; at the same time, the first-order factors under their higher-order factor also showed high homogeneity.

The AVE values showed that the higher-order factors could explain $8 0 \%$ and $7 4 \%$ variance of their respective first-order factors, ndicating that the higher-order factors' attributes were well-rflected in the first-order factors. Meanwhile the first-order factors explained nearly an ideal variance of its items indicating that the items also reflected the properties of the first-order factor well

The results showed that the newly developed scale met psychometric standards well and the extracted factors were largely consistent with the tentative structure of the scale defined at the beginning of this study (see Table 9).

Table 8 The result of the second-order CFA model fit indices.   

<html><body><table><tr><td>Indices</td><td colspan="2">Results</td></tr><tr><td></td><td>Initial model</td><td>Revised model</td></tr><tr><td>Absolute fit</td><td></td><td></td></tr><tr><td>x2/df</td><td>3.158</td><td>2.003</td></tr><tr><td>RMSEA</td><td>0.075</td><td>0.051</td></tr><tr><td>GFI</td><td>.888</td><td>.932</td></tr><tr><td>AGFI</td><td>.856</td><td>.908</td></tr><tr><td>Incremental fit</td><td></td><td></td></tr><tr><td>TLI</td><td>.886</td><td>.947</td></tr><tr><td>CFI</td><td>.902</td><td>.957</td></tr><tr><td>IFI</td><td>.903</td><td>.957</td></tr><tr><td>Parsimonious fit</td><td></td><td></td></tr><tr><td>PGFI</td><td>.687</td><td>.691</td></tr><tr><td>PNFI</td><td>.743</td><td>.757</td></tr><tr><td>PCFI</td><td>.776</td><td>.789</td></tr><tr><td>CN</td><td>146</td><td>231</td></tr></table></body></html>

# 5. Discussion

Having determined the scale's factor structure, in this section we examine the extracted factors and compare them to findings from the previous researches. The implications for practice and future research were also discusse. We classfy the extracted factors into two categories: the higher-level category, including post-writing monitoring argumentation, organization, inoduction and conclusion writing and the lower level one, including language expression and word spelling.

# 5.1. The higher-level category

Among the factors in the higher-level category, post-writing monitoring explains almost one third of the variance in argumentative writing cognitive load, indicating writers engaged most in monitoring the content pertinence and logic in the post-writing phase. This resonates with Hall's (1990) reearch and Silva's (1993) review findings that L2 writing involved more revision, and that such revision was time consuming; writing did not proceed by ear'. In terms of monitoring stages and levels, previous studies (e.g, Hall 1990, Whalen & Menard, 1995) showed that writers revised more in the during-writing phase, albeit mainly focusing on the lower-level or linguistic-level category. Partly contrary to those previous findings, the writers in this study left alarge part of the reviewing and revising work on higher-level category at the post-writing stage.

This finding ads more empirical evidence that L2 writers do not allocat time equallytoall processes at any stage of composition (Roca de Larios et al, 2008). The results aso show that the phase and discourse characteristics of monitoring can be attributed to EFL learners' global management of composition constrained by their 2 proficiency. Contrary to the skilled writers who often make more effort on detaled planning during the pre-writing phase, unskilled writers make a lessglobal plan (Raimes,1985; Sasaki, 2000), and they have to subsequently make up for the limitations brought by their lck of preparation. Although thir writing processis recursive and they often go back to revise in the during-writing phase, the uncertinty brought by a lack of overall advance lanning causes them heavy cognitive load when reviewing and revising content and logic at the end. In our study, L2 writers planned les at the global level in the pre-writing phase, supported by evidence that generating ideas was not extracted as an independent factor. Notably, that writers did less overall planning did not mean that they found exploring opinions in argumentative writing to be efortles; instead, generating ideas ran throughout the process of argumentation.

Writers' cognitive load in argumentation and organization was in congruence with Wingate (2012). In her study, learners also re ported they found it diffcult to develop a position, to use resources as evidence to support this position and to structure the essay. Despite a consensus that learners face challenge i arumentation, previous studies (e.g., Miller & Pessa, 2016) took the challenges students encountered in argumentation as a point of departure, from which researchers endeavored to propose methods of scaffolding however, how writers responded to these challenges remained unclear. Conversely, this study has provided a clear picture of the specific mental effrts students invested in actual writing rather than only knowing vaguely that they perceived it to be difficult. Similarly, it i unsurprising that learners found organizing ideas difficult both in L2 writing generally (Silva, 1993) and in argu. mentative writing (Wingate, 2012). This study has contributed to research by further identifying the pecifics of writers mental effort allocation it revealed that organization occurred during writing rather than at the pre-writing phase. This is in line with Uzawa (1996)sfinding that students did not organize the generated ideas in any way before actually writing. Although learners often receive guidelines about planning and constructig outlines before writig, their mental effrt i organizing ideas was only witnessed at the during-writing stage.

Writers' cognitive load on these two dimensions could be attributed partly to the research and pedagogical tendency that over. emphasizes the lexico-grammatical features of writing (Chuang & Yan, 2022). Accordingly, writing teaching has typically been geared to improving learners' language performance while interventions targeting the specific task of argumentation are insufficient.

Table 9 The comparison between the extracted factors and the tentative structure.   

<html><body><table><tr><td colspan="2">Extracted factors</td><td>Items</td><td>Tentative structure</td></tr><tr><td colspan="2">argumentation</td><td>Q1. Q4</td><td>Argumentation</td></tr><tr><td colspan="2"></td><td>Q2. Q5 Q3. Q6</td><td>Reviewing Revising</td></tr><tr><td colspan="2"> organization</td><td>Q7. Q8</td><td>Organizing ideas</td></tr><tr><td colspan="2"></td><td>Q9</td><td>Reviewing</td></tr><tr><td colspan="2">language expression</td><td>Q10-Q13</td><td>Linguistic representation</td></tr><tr><td colspan="2"></td><td>Q14. Q15</td><td> Linguistic representation</td></tr><tr><td colspan="2"></td><td>Q16</td><td>Reviewing</td></tr><tr><td colspan="2"></td><td>Q17</td><td>Revising</td></tr><tr><td colspan="2">word spelling</td><td>Q18. Q19</td><td>Word spelling</td></tr><tr><td colspan="2"></td><td>Q20. Q22</td><td>Reviewing</td></tr><tr><td colspan="2"></td><td>Q21. Q23</td><td>Revising</td></tr><tr><td colspan="2">introduction and conclusion writing</td><td>Q24-Q28</td><td>-</td></tr><tr><td colspan="2">post-writing monitoring pertinence monitoring</td><td>Q29</td><td> Topic examining</td></tr><tr><td colspan="2"></td><td></td><td> Reviewing</td></tr><tr><td colspan="2"></td><td>Q30. Q32</td><td></td></tr><tr><td colspan="2">logic monitoring</td><td>Q31. Q33</td><td>Revising</td></tr><tr><td colspan="2"></td><td>Q34. Q36. Q38 Q35. Q37. Q39</td><td>Reviewing Revising</td></tr></table></body></html>

Unsurprisingly, learners in our study seemed to lack genre-specific knowledge and strategies about how to develop a positin effectively.

The newly emerged factor inroduction and conclusion writing indicated that students were confronted with the challenge of writing an introduction, conclusion or both. At the beginning and ending stages of writing, they alocated mental ffort both to the position being developed and the writigs rhetoric to atract readers attention. Evidence of students difficulty in writing conclusions was also found in Wingate (2012) likewise, how to introduce the topic and to present an opinion at the beginning was also abig concern. In our study, in the introduction and conclusion part, leaners showed some level of reader awareness and made eforts to make thir writing appealing to readers. However, the salient cognitie load centered on readers in these two parts could not be seen on other aspects of writing, indicating learners did not generally have reader representations in mind during writing.

# 5.2. The lower-level category

Compared with the higher-level dimensions of argumentation and organization, the lower-level category of language expression explained a larger variance, which confirmed the idea that for unskilled L2 learners, language is always their central consideration (Zamel, 1983); they often encounter language problems in English writing and there isan inadequate match between their cognitive ability and language competence (Cumming, 2001; Qu, 2017). A paradox here was that language expression was stil constrained despite the excessive emphasis on promoting L2 proficiency in writing teaching. In line with Manchon and Roca de Larios (2007) and Roca de Larios et al. (2006), who found that for FL writers, formulating content took a substantial amount of time and was often conducted at the expense of planning and revision, in our study, writers investment in language expresion spared them little working memory to cope with macro-level isues, such as argumentation and organization, and thus they also felt overloaded on those aspects. Although language constraints in L2 writing are common, EFL writers' mental eforts on language expression showed salient features in this study. For example, when retrieving words, they tended to avoid using the simple ones due to their misconception that the more complex the language used in writing, the higher their essay would be evaluated. Writers were also engaged with another language-related factor, word spelingin both during- and ost-writing phases, due totheir limited transcring proficiency. This finding provides support for ilva's (193) conclusion that  writers transcribing was laborious; it also justifies the addition of transcribing in Hayes (2012) most recent writing model, in which transcribing is considered as a sub-process of writing that competes for cognitive resources and can be a source of cognitive load.

# 5.3. Implications

The implications of the EFL-AwcLS are two-fold. Methodologically, the scal is an important supplement to current writing task complexity research. Developed specificall for measuring writing cognitive load, the EFL-AWCLS overcomes the limitations in pre vious studie relying on theoretcal evaluations or measuring overall perctions. I facilitates the validation of writig task complexty and makes it possible to explore how cognitive load mediates between task complexity and writing performance.

Pedagogically, the scale enables a quantitative evaluation of learners' cognitive load and provides an instrument for writing teachers to discover where learners experience the most cognitive load during the writing process; thus, they can gain deeper insights about learners actual mental efort distribution. In this sense, the scale can act as a stepingstone for intervening in and optimizing learners' cognitive load to facilitate their writing development. Teaching effciency would thus be improved because targeted scaffolding relating to learners greatest difficulties can be offered rather than a general support. Additionally, writers' apparentl inefficient investment in language implies that a balanced training focus on language instruction and related higher-level strategies is desirable (Roca de Larios et al., 2002). Moreover, writing expertise is proved to be related to the higher-order aspects and psychologically distinct from L2 language proficiency (Cumming, 1989), and thus it is feasibl to carry out writing strategy training independent of the constraints of language proficiency. From the learners' perspective, in the absence of a measurement instrument, their aproach to investing mental effrt in writing will only be based on habits and influenced by teachers' guidance. This scale provides them an opportunity to self-diagnose and gain better understanding of ther own writing cognitive oad distribution, and thus to promote self-regulated learning.

# 5.4. Limitations and future research

This study has several limitations. First, the construct validity of the EFL-AwCLS may be constrained by the nature and re. quirements of the writing tasks used for initial item development. The distribution of cognitive load to various writing processes may differ when the task has no time limits; meanwhile, the word limit requirement was not totally in congruence with those more proficient participants language proficiency and for those writers, the requirement might be not suficient enough to uncover their cognitive load. Second, the final structure of EFL-AWCLS was determined by CFA with revision,so the verification of the scale had an exploratory nature its stucture neds further vrificatio in utre rerch. Third, the sale's criterion-related validty should also e tested, although the content and construct validity indicators met psychological measurement standards. Another limitation is related to the cognitive load's dynamic nature. This study was cos-sectional and could only reflect students' perceived cognitive load at one point in time, however, their cognitive load i in a state of constant flux (Xie & Salvendy, 2000). Future research can explore ways to measure the dynamic change of students' cognitive load during the writing process.

# 6. Conclusion

This study developed a self-report measurement EFL-AwCLS. The findings revealed that the scale is a six-dimensional structure including argumentation, organization, language expression, word spellig, introduction and conclusion writing and post-writing monitoring The scale i proved psychometricall reliable and valid to measure EFL learners argumentative writing cognitive load. By introducing a new measure of cognitive load specificall for EFL argumentative writing, the study will hopefully create research space for related future investigations. From the methodological standpoint, the multidimensional nature of EFL-AwCLS makes it preferable to the widely used unidimensional measure of cognitive load, which seeks to evaluate learners' perceived overall mental effort. The newly developed scale allows researchers to explore the relationship between different dimensions of writing cognitive load and writing performance. From a diagnostic perspective, EFL-AwCLS can help identify the specific dimensions, to which learners usuallyallocate mental effrt highlighting those that receive too much or to littleffort. Accordingly, learners can be steered away from their habitual but inefficient allocation of cognitive resources. Nonetheless the scale should be used cautiously because it has only been preliminarily verified and some limitations remain, as discussed above. Therefore, further validation of this scale with different samples is advisable; it is hoped that EFL-AwCLS psychometric properties can be increasingly improved with further validation work.

# Declarations of interest

none.

# Data availability

Data will be made available on request.

# References

Choong, KP. (2014).ffects f tosk complexity on wrtten production (Unpublished doctoral disertation). Columbia Universt, New York.   
Chng,  2n t th  t    t w  f Writing, 56, 1-14. https://doi.org/10.1016/j.jslw.2022.100892   
ig 01).   t in     f .   of sh, 1,3./0.6018/ ijes.1.2.48331   
Denzin, N.K. (1978). The research act: A theoretical introduction to sociological methods. Engelwood Cliff, N. J. Prentice Hal.   
Flower, L., & Hayes, J. R. (1981). A cognitive process theory of writing. College Composition & Communicatio, 32(4), 365-387.   
Fomell, ., er, . 1981. n n md h e via a m r.  of   181) 39-50. https://doi.org/10.1177/002224378101800104   
aar  1)  t  r 9 1-11. https://doi.org/10.1016/j.system.2021.102524   
Hall, C. (190). Managing the complexity of revising across languages. TEs0L Quarterl, 24(1), 43-60. htps://doi.org/10.2307/3586851   
Hyes,        , e, m individual differences, and applications (pp. 1-27). New York/London: Routledge.   
Hayes, J. R. (2012). Modeling and remodeling writig. Writen Communicatio, 29(3), 369-388. htts://doi.org/10.1177/0741088312451260   
He, L, & Shi, L. (2012). Topic knowledge and ESL writing. Language Testing 29(3), 43-464. https:/o.org/10.1177/0265532212436659   
Hirvela, A. (2017). Argumentation $\&$ second language writing: Are we mising the boat? Journal of Second Language Writing, 36, 69-74. htps://doi.org/10.1016/j jslw.2017.05.002   
Joson,017.ive  xy d w tc comxit, cy, ex xty d   ys an m analysis. Journal of Second Language Writing, 37, 13-38. https:/doi.org/10.1016/j.jslw.2017.06.001

Journal of Second Language Writing, 21, 264-282. https://doi.org/10.1016/j.jsw.2012.05.011 Kaiser, H. (1974). An index of factorial simplicity. Psychometrika, 39, 31-36. https:/doi.org/10.1007/BF02291575 Kelog,  .     .    res,  e, applications (pp. 57-71). New York/London: Routledge. Kell, T. L. (1939). The selection of upper and lower groups for the validation of test items. The Jounal o Educatonal Psychology, 30(1), 17-24. Kuken  r,  (208. i  xt w     ch      n 19 48-60. https://doi.org/10.1016/j.jslw.2007.08.003 Lee, .. 2020k e d s ty fcs o w   of  ing 5011 h/./0.1016/. jslw.2020.100777 Le J .     ti essays. Journal of Second Language Writing, 33, 21-34. https://doi.org/10.1016/j.jslw.2016.06.004 Manchon, R  & Rca de Larios, . (200). n the temporal nre f plang in 1 and 2 comosing. Langge ng 27(4), 549-593. htps:/oi.org/ 10.1111/j.1467-9922.2007.00428.x Miler,     s a     n i haae student argumentative writing. TEs0L Journal, 7, 847-873. https://doi.org/10.1002/tesj.248 Paas  (r  r  -i i tst ie    of  4, 429-434. https://doi.org/10.1037/0022-0663.84.4.429 Paas . vinn,  aer,  ae, .  (203). ie  mmn  a mens to aa cotive d thory. ion Psychologist, 38(1), 63-71. https://doi.org/10.1207/S15326985EP3801_ Paas  , J (4  i   th   i .  4, 351-371. https://doi.org/10.1007/BF02213420 Qu, ..017).   iti a t fth  n 32-9/.10j.017.10.00 Rahmi,   .018i k xt, s mio el y d hr wictio isha a nga. Reading and Writing, 32, 761-786. https://doi.org/10.1007/s11145-018-9887-9 mes,  (15.k t   l 25. /./0/35868 eez, 0 i -  i  e Linguistics, 35(1), 87-92. https:/doi.org/10.1093/applin/amt039 esz, , li,  oa017.f o t xt  wg  ad isic t.  71 208-241. https://doi.org/10.1111/lang.12205 esz, , hel, &art,  (2016 n ie tk dd usin lk m, ctie -atin, d p mnts: A validation study. Studies in Second Language Acquisition, 38, 703-737. https://doi.org/10.1017/s0272263115000339 esz, A chel,   2019). ng  ang wirs sing d sion is amid mestud. de n n ngae Acquisition, 41, 605-631. oca de    0 ie    l a   i in processes. Modern Language Journal, 90(1), 100-114. https:/doi.org/10.1111/j.1540-4781.2006.00387.x ca de , ,h ph, M 00h f rst i he af tiwe. Journal of Second Language Writing, 17, 30-47. https://doi.org/10.1016/jjslw.2007.08.005 oca     a 00 ca wn  e e, e .,  n research in L2 writing (pp. 11-47). Dordrecht: Kluwer Academic Publishers. inson, . 1) k t, , d k i i   rk sc 1, 27-5. https://doi.org/10.1093/applin/22.1.27 Robinson, P. 01)  mplty cie rs, ad yls:ri rmr fr xg   LAP  .) Cognition and second language instruction (pp. 287-318). Cambridge: Cambridge University Press. uz-u,015. th  /    f   a Second Language Writing, 28, 1-19. https://doi.org/10.1016/jjslw.2015.02.001 Saaki,  0 nia  win  n y st J  n 9(3) 59291. h/o.rg 10.1016/S1060-3743(00)00028-X Saayaa . (201). s a mplex tk rely comple alig te atio of cotiv sk comlxit. he d ng nl, 1001), 31-254. (https://10.1111/modl.12313). Sila . 19.  nng  t  e  win   d is ci. 0, 27(4 57677 /. org/10.2307/3587400 Swelle,  18.ive g p fs  i  12 27-5././0106/062388900237 Tabachnick, B.G., & Fidell, L.S. (2007). Using multivariate statistics. New Jersy: Pearson Education, Inc. Tabari 1k Second Language Writing, 52, 1-14. https://doi.org/10.1016/j.jslw.2021.100814 awa,  (19. d    o win  wn n  r .  of  ng 5(3) 71-294. https://doi.org/10.1016/S1060-3743(96)90005-3 an og   8in  .st 6 doi.org/10.1080/00461520701756248 geli26.    t Ty,  adpri n.liva  ., r ok wig (pp. 107-130). Oxford: Elsevier. Whaln,  ad, . (195.1 d 2 wes tc ad si k  md eee si  ng 53, 381-418. http:/doi.org./10.1111/j.1467-1770.1995.tb00447.x. ingate, . 2012). mt lg sts drdwa e wtn i at. l f Esh for c e, 11, 45-154. htp/.g 10.1016/jjeap.2011.11.001. Xie . al   i    ingl il . ol  of iv, (3) 23-242. http://doi.org./10.1207/S153275661JCE0403_3. Writing, 55, 1-16. https://doi.org/10.1016/j.asw.2022.100690 Yon, .n  tik    w  i t,  sti ct. Journal of Second Language Writing, 54, 1-14. https://doi.org/10.1016/j.jslw.2021.100857 Zaea, . 017).  k it the   mt, lt,   y t inThe  , 101(2) 335-352. https://doi.org/10.1111/modl.12389 Zamel, V. (1982). Writing: The proces of iscovering meaning. TEs0L Quarterly, (2), 195-209. https://doi.org/10.2307/3586792 Zamel, V. (1983). The composing processes of advanced ESL students: Six case studies. TES0L Quarterl, 17(2), 165-187.

Jiah.   t t.  reeah interests include cognitive psychology, particularly the cognitive process of language learning.