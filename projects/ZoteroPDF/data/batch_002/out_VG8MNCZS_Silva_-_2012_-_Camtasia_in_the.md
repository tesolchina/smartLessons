# Camtasia in the Classroom: Student Attitudes and Preferences for Video Commentary or Microsoft Word Comments During the Revision Process

Mary Lourdes Silva

University of California, Santa Barbara, 552 University Road Santa Barbara, California 93106   
Received 19 May 2010; received in revised form 4 January 2011; accepted 1 December 2011

# Abstract

In the last ten years, libraries, individual departments, and professors have experimented with screen-capture software to develop edited tutorials, record in-class lectures via presentation software, and record think-aloud rationale for difficult problem sets. Moreover, screen-capture software has been used to provide visual/audio feedback for student writing. Currently, there is scant research on visual/audio feedback via screen-capture software in writing courses. The present study examines student perceptions and attitudes about two different modes and media of teacher feedback: Microsoft Word comments versus visual/audio commentary. The results indicate that the mode and medium of teacher feedback had an impact on students’ perceptions about the rhetorical context of the revision process and perceptions about the teacher/student relationship. Students who preferred the visual/audio modality of the teacher commentary videos cited their conversational quality, clarification of expectations, and reference to more global issues in writing. On the other hand, students who preferred the Microsoft Word comments were more apt to discuss its indexical quality in that students could easily revise surface level features or locate the “problem” sentence. The results also indicate that an either/or approach to teacher feedback is not necessary. Students articulated the relevance of using a combination approach in which Microsoft Word comments and the teacher commentary videos could be used for different elements or stages of the writing process. As instructors transition to teaching within online contexts and experiment with new technologies, it is important to examine the significance of the mode and medium of teacher feedback in student perceptions, participation, and writing practices.

$^ ©$ 2011 Elsevier Inc. All rights reserved.

Keywords: Audio feedback; Camtasia; Course management system; electronic feedback; Screen-capture software; Teacher feedback; Teacher response; Video commentary

# 1. Introduction

In recent years, as composition instructors have experimented with integrating Web resources and multi-media technologies in face-to-face composition courses, not only has student writing changed, so has the manner in which instructors respond to student writing. An experimental method for teacher feedback is the use of a screen capture program, such as Camtasia <http://www.techsmith.com/camtasia.html>, which records the draft or assignment on the monitor while the instructor records her or his voice. As an option, the software can record video via Webcam as well. As the instructor reads an essay on her computer and provides feedback, the program records all screen movements and processes. A single word, sentence, or paragraph could be highlighted while the instructor offers oral or typed feedback.

To address large units of text, an instructor could reorganize, edit, or delete text, and then open multiple documents or windows to reference the prompt, rubric, library Web site, Wikipedia, or YouTube, which provides another degree of audio and visual feedback. Moreover, the video commentary functions as a type of think-aloud that transforms the teacher role to reader (Lynn McAlpine, 1989), which underscores the values, expectations, and cognitive processes of activities such as reading, writing, critical thinking, and researching.

In recent years, digital equipment, server space, and monthly Internet services have become relatively affordable; thus, instructors have experimented with embedding audio and visual components into teacher feedback to facilitate the learning process during student revision of drafts. Audio and visual technologies have included digital audio recorders, open-source online digital recorders (e.g., the open-source software Audacity), free digital mp3 players (e.g., iTunes, Yahoo Jukebox, Amazon MP3), video production (e.g., iMovie and Windows Media), Microsoft PowerPoint, Microsoft Word Notebook (which allows users to embed audio feedback and embed tracked changes in bright colors, marginal comments encapsulated in red balloons, images, tables, graphs, and hyperlinks), Adobe Photoshop, GNU Image Manipulation Program <http://www.gimp.org/>, and Web design. Faster broadband speeds and larger servers have made it easier for instructors to transport multi-media teacher commentary to students in an expedient and timely manner.

Some professors have already experimented with screen-capture software in teacher feedback and classroom instruction, which has garnered immense interest in the academic world. Yet research in the use of screen-capture software in classroom instruction is scant. Much of the research on this topic has been conducted in the United Kingdom (UK) and Australia. Thus far, instructor response to the use of screen-capture software and student response to video tutorials and video feedback have been positive. Russell Stannard (2007), from the University of Westminster, found that video commentary sped up the time spent reading and responding to student essays, and students used the videos to revise their essays, some replaying the videos four or five times. Stannard also used Camtasia during the writing process by commenting on student plans prior to the drafting stage. In a study on the effect of screen-capture software, Stannard (2008) found that students preferred the videos to traditional forms of feedback. Similar results were found in a small-scale pilot study in the UK that compared written feedback to video feedback (Billy Brick & Jasper Holmes, 2008). At the 2006 Conference on College Composition and Communication, Scott Warnock, Ph.D. from Drexell University presented his results on a study regarding three modes of teacher feedback: a) written feedback via a rubric program, b) face-to-face conferences, and c) video feedback via Camtasia. Warnock concluded that the video feedback took the least amount of time, four essays per hour, and students preferred the video feedback second to face-to-face conferences (Fred Siegel, 2006).

Screen-capture software has also been utilized in math and science courses in conjunction with Tablet PC technology. In one study (Saroj Biswas, 2007), Camtasia recordings and PDFs of screen writing were effective in directing students’ attention to class lectures, whereas in traditional classrooms, students were distracted by transcribing lectures and copying materials from the white board or black board. In other studies on Tablet PC and Camtasia technology (Brian C. Dean, 2007; David Fisher, Phillip Cornwell, & Julia Williams, 2001; Sandra A. Yost, n.d.), Camtasia was used as an auxiliary resource or used after class, if class time expired, to record Tablet PC activity, such as elaborate mathematical problem sets. Videos were then uploaded to a course management system or Web site for students to access.

Screen-capture software in education has not been exclusive to teacher implementation. At Texas A&M University libraries, the project Let-It-V (Learning E-Resources Through Instructional Technology Videos) has developed interactive training videos that facilitate library support 24/7 (Daniel Yi Xiao, Barbara A. Pietraszewski, & Susan P. Goodwin, 2004; Christopher Cox, 2004). Screen-capture software was implemented into a similar project at Queen’s University in Ontario, Canada where tutorials introduced first-year university students to library databases (Sylvia Andrychuk & Morag Coyne, 2008). Moreover, tutorials made with screen-capture software have been designed in other departments like accounting, as exemplar models for problem solving (Ken Crofts & Carole Hunter, 2007).

Although the current research on screen-capture software in the classroom suggests that students have found tutorials or video feedback favorable, there is little empirical data regarding student use of video feedback during the drafting stage of writing. In some of the studies conducted on audio feedback (Tim Boswood & Robert H. Dwyer, 1995; Glenn C. Pearce & Jon R. Ackley, 1995; Simon Roberts, 2008), it was unclear whether only summative feedback was provided. When students actually receive feedback is essential to student revision practices. In a study of $1 2 ^ { \mathrm { t h } }$ grade high school students, Robert P. Yagelski (1995) found that students did not revise rough drafts when they only received feedback on final drafts. Students are more apt to revise if teacher feedback is provided during the drafting stage and a final grade is not assigned until all drafts have been submitted. During the drafting process, how students use the various resources of the course (e.g., video feedback, marginal feedback, tutorials, APA guides) is important to understanding the decisions students make during the revision process. Furthermore, student attitude toward a particular mode or medium of feedback is essential to understanding the way students construct the rhetorical landscape with the feedback provided to them.

# 2. Background

Traditional forms of teacher commentary have consisted of marginal (handwritten or electronic), summative, and interlineal feedback. In addition, a score or grade functions as the most abbreviated form of teacher response, which often leaves students dumbfounded when an instructor attempts to operationalize a B- or $\mathrm { C } +$ (Felicia Mitchell, 1994). Research on teacher feedback often examines instructors or students’ perspectives. In a study of both perspectives, Julie Hulme and Mark Forshaw (2009) investigated 57 tutors and 213 psychology undergraduate students. Both groups agreed that written feedback was not ideal. Furthermore, there was disagreement on what types of feedback were useful. For example, tutors found feedback on grammar and referencing to be useful, whereas students disagreed. On the contrary, in a comprehensive three-year study on the role of assessment feedback in student learning, Richard Higgins, Peter Hartley, and Alan Skelton (2002) found that students valued their tutors’ feedback. Although grades were important to students, they were motivated intrinsically to connect with the subject matter in a “deep” way. Furthermore, students commented that the most helpful feedback was presented in a timely manner and made relevant to subsequent work.

The motivation for instructors to experiment with different forms of teacher feedback is often correlated with the time it takes to read and comment on a student paper, which, according to Richard Haswell (2008), is approximately seven minutes a page. Haswell delineated, “in rough chronological order,” the history of teacher feedback, which has evolved to accommodate growing class sizes and new technologies, as: a) evaluation scales; b) peer response; c) projectors to model teacher expectations and evaluate student writing; d) checklists; e) single or group conferences for response; f) audio tapes; g) attention to praiseworthy achievements; h) student self-evaluation; i) digital feedback on student’s digital text through the use of attachments, footnotes, or hypertext (p. 333). According to empirical findings, the advantage of one method of response to another in affecting student outcomes is minimal (Haswell).

The mode and medium used for teacher feedback during the drafting stage have both affordances and limitations that shape the social dynamics between instructors and students or amongst peers. With a hard copy text and pen or pencil in hand, an instructor could cross out text, add alphanumeric text (i.e., marginal, summative, or interlineal comments), or discuss the text with the student writer during a conference. During face-to-face interactions, the hard copy text mediates dialogue and written forms of expression by all participants (e.g., a student, peer, or instructor using the text to write comments, draw, brainstorm, etc.). Due to the constraints of the 8x11 page and the white space available in conventional essays, narratives, or reports, it is not uncommon for teacher or peer feedback to extend itself into face-to-face interactions. On the other hand, electronic feedback via Microsoft Word comments or at the end of a student draft, affords the reader nearly an infinite amount of space to provide commentary. A commentary video also has similar affordances in terms of length, because a video could easily run for over an hour. However, few teachers would be willing to spend over an hour on a paper, and the file size of such a video would limit delivery options. Electronic formats and media afford instructors extensive written or oral feedback, which can, hypothetically, affect the need for face-to-face communication; however, further research is needed to determine if extensive electronic feedback affects face-to-face communication.

Interestingly, though, the ability to write extensive commentary does not necessarily correspond with higher quality feedback and higher student gains. In a study of $7 ^ { \mathrm { t h } }$ and $8 ^ { \mathrm { t h } }$ grade students, George Hillocks (1982) found that short, focused comments coupled with revision produced the most significant quality gain. In another study of two groups of developmental college students, Marilyn Ruth Sweeney (1999) concluded that students performed better revisions when they received direct comments with explicit directions for revision. What students value is feedback on global issues of content, purpose, and organization and on surface-level issues, such as syntax, vocabulary, and correctness, as opposed to feedback that negates student ideas or attempts to control the direction of the writing (Richard Straub, 1997). Moreover, students prefer an explanation as to why something in their paper is good or bad (Richard Beach, 1989).

Also, long or short commentary composed within the confines of the white space is often misunderstood by students. Chris Thaiss and Terry Myers Zawacki (2006) claimed that students fail to understand teacher comments and expectations (see also Hulme, 2009; Summer Smith, 1997; Straub, 1996), which could be one reason why students do not make substantial revisions to content and form. Melanie Sperling and Sarah Warshauer Freedman (1987) examined why high-achieving students misconstrue teacher comments even when they have been accompanied by peer response, class discussions, and teacher/student conferences. Sperling and Freedman concluded that the differences in the teacher and students’ knowledge and values about writing and revision contributed to students’ misunderstanding. A teacher may misunderstand the students’ intentions and objectives as well. David Carless (2006) suggested that tutors (and this advice can be successfully applied to instructors) could engage in assessment dialogues with students to clarify the assessment criteria and explain their feedback.

The teacher/student conference has been one method that directly addresses the problem of miscommunication between the instructor and student in that the face-to-face interaction allows teachers to elaborate on their intentions, students to offer their perspective (Carolyn R. Frank, 2001), and teachers to model revision strategies (Beach, 1989). Although face-to-face conferencing provides the most optimal method of teacher feedback (Neal Lerner, 2005) in which every student concern could be brought to the table, most writing instructors do not have the time or resources to conference. At most colleges and universities, graduate students juggle teaching freshman composition and a full-time course load. In the community college system, where adjuncts make up about $62 \%$ of the faculty (David Laurence, 2001), adjuncts fly the freeways from one district to the next, handling a 5-6 course workload. For distance learning and online courses, the face-to-face teacher/student conference is simply not feasible for students who manage daytime responsibilities like work and family.

If the face-to-face teacher/student conference is not a viable option for teachers or students, teachers have created audio recordings, which provide similar affordances in that instructors can explain the assignment, rubric, and their commentary on student writing (Thomas David Clark, 1981; Patricia Cryer & Nemeta Kaikumba, 1987; Glenn C. Pearce & Jon R. Ackley, 1995). Cheryl Mellen and Jeff Sommers (2003) added that instructors felt more comfortable offering a constructive critique orally because it was not face-to-face. Instructors may hedge or feel reluctant to critique student writing during face-to-face conferences. Students have also commented on a preference for audio feedback because it offered a personal touch and more comprehensive feedback (Stephen Merry & Paul Orsmond, 2008; Bob Rotheram, 2007). Audio feedback has also been employed in online courses to personalize feedback, create a social presence online, increase student involvement, and build a sense of community (Philip Ice, Reagan Curtis, Perry Phillips, & John Wells, 2007; Phil Ice & Jennifer Richardson, 2009).

In general, one objective of teacher feedback is for students to integrate the information in subsequent writing tasks or assignments (Margaret Jollands, Neil McCallum, & Julian Bondy, 2009; Higgins et al., 2002). When teacher feedback functions as an evaluative measure in combination with a grade, students either “bear” in mind what they need to do for subsequent work (Higgins et al.) or bypass the feedback in search for the grade or mark (Dean, 2007; Roberts, 2008; Rowe & Wood, 2008b). Teacher feedback integrated during the revision process can fulfill multiple objectives: a) initiate face-to-face or digitalized dialogue between teacher and student or amongst peers; b) address assignment expectations or praise student response; c) introduce academic discourse (Carless, 2006; Higgins et al.) or disciplinary knowledge; d) discuss the rhetorical situation; and e) inquire about student goals and prior knowledge. Moreover, the mode and medium of teacher feedback can mediate student revision practices and student perceptions. Researchers have found that students perceived audio feedback to be more personalized, which, according to self-reports, impacted student writing practices and student learning (Ice et al., 2007; Merry & Orsmond, 2008).

The use of audio recordings, however, can have logistical problems. If instructors still use analog devices, students may not have a tape recorder to listen to their feedback. Digital recordings are more convenient for students, particularly if instructors send the file by e-mail or through a course management system. Access to the content of the digital recording could prove to be difficult if students have an outdated media player or if students access the files in computer labs without audio devices. When students do access the content in the digital recording, whether or not students have their draft in hand may affect student comprehension of the teacher feedback, which may or may not include oral markers to orient students to specific sentences, paragraphs, or pages. Issues of form, such as logical arrangement, coherence, and cohesion may be difficult to underscore without detailed “visual” explanations by the instructor (e.g., “on the third page, in the first full paragraph, three lines down, when you comment on. . .”). It can be more problematic if the student does not have a draft in hand. If students do switch back and forth between the audio feedback and their paper, cognitive overload may occur due to the split-attention effect, which would result in less processing of essential information (Richard E. Mayer, 2009). The advantage of the teacher commentary videos is that students can receive simultaneous auditory and visual feedback without shifting their attention elsewhere.

The objective of the present study is to provide an ecological view of student revision activities mediated by two different forms of teacher feedback. The following questions are examined in this study:

1. Did issues of access or computer literacy impede students’ ability to obtain teacher feedback?   
2. Do students prefer Microsoft Word comments to video commentary?   
3. What attitudes and perceptions do students have about feedback in written and visual/audio modalities?

# 3. Methods

The present study draws on a multi-method, ecological approach to evaluate students’ preferences for and attitudes toward Microsoft Comments and teacher commentary videos. As the instructor and researcher, I had the emic perspective within a composition classroom for engineering majors at the University of California, Santa Barbara (UCSB) to observe students’ reactions to and attitudes about my video feedback and marginal, interlineal, and summative comments via Microsoft Word. In addition, my role as instructor also allowed me to track their development as writers. Student surveys, participation observation notes, and course management system (CMS) site statistics were used in this exploratory study. Although students were notified of the study at the beginning of the quarter, student consent was obtained after final grades were submitted and the quarter had concluded.

# 3.1. Class description

The UCSB Writing Program offers engineering majors a writing course designed to meet the genre specifications of the field and ethical issues relevant to engineers. The course is the first of two required courses for engineering majors, although a preparatory course is offered as well. Throughout the first course of the sequence, students could be asked to write essays about technology, technical reports, memos, resumes, project proposals, product re-designs, Web sites, and Microsoft PowerPoint presentations. In addition, essay topics may include ethical and moral issues of technology in history, in industrialized and non-industrialized countries, or in public and private sectors of society. Instructors are permitted to customize their curriculum and pedagogy, although specific objectives must be met for each course in the sequence.

Maximum enrollment for the course is 25. In my class, 19 students were enrolled: 16 males and 3 females. This gender imbalance is typical for courses offered by the engineering department. Most students in my course were freshman: 13 freshman and 6 sophomores. In my class, all students received a teacher commentary video produced in Camtasia for one of the two major essays assigned in the class. The first essay asked students to write a three-page response on one of the themes discussed in the introduction of the book, The Civilized Engineer by Samuel C. Florman, including the chapter titled, “The Civilized Engineer: The Concept.” The second essay was a ten-page essay on the history and technology of an invention. Students were required to locate six sources and incorporate the concepts analyzed in three articles read in class prior to the first draft. For each essay, students had to write two drafts before submitting the final draft. The first draft was peer-reviewed, and the second draft was only reviewed by me. Furthermore, both documents had to be in APA style, which created problems for students because MLA style is the predominant form taught in secondary education.

# 3.2. The screen-capture software

Available at http://www.techsmith.com for $\$ 299$ (Mac licenses are $\$ 99$ ), Camtasia is a screen-capture software that records all screen movement and activities (e.g., typing in a word-processing application, surfing the Internet, interacting with social networks or online games, or using photo editing or Web design software). Competitor products are available online as well, including open-source alternatives. However, at the time, my previous employer had informed me that Camtasia was the most effective screen-capture software for PCs (Camtasia is now available for Macintosh). Unlike key-logging and spyware technologies that work in the background without notice, Camtasia must be opened and set to record manually. A frame of the recording area flashes, along with a small red circle in the Windows toolbar. Recording can be paused, which is ideal for composition instructors who wish to read a student essay silently without recording long periods of silence. Once the recording function has been stopped, the user has the option to save or delete the recording; if saved, it is converted into a .camrec file, which can only be opened with a Camtasia product. The user then has the option to edit this file to include narration, delete large segments or minute seconds, embed Microsoft PowerPoint slides, and include zoom and pan features and customized callouts. It is ideal to edit videos if the videos are to be reused for future courses, whereas teacher commentary videos do not require editing if time is a constraint (e.g., on average, from the point of drafting a script to playing the video online, a three-minute instructional video can take about three hours total to produce—an hour a minute—whereas a teacher commentary video took approximately 15-30 minutes to produce).

# 3.3. Course management system

At the time of the study, the UCSB hosted a pilot open-course management system called Sakai, which is no longer funded. Associate Writing Professor Karen Lunsford led a grant project that studied the integration of library materials, resources, and links within Sakai (Lunsford et al., 2008). Out of the box, Sakai included basic tools that allowed instructors to create assignments, quizzes, discussion forums, and chat discussions. Sakai also included a private dropbox, which for the purpose of this study, allowed me to share teacher commentary videos with students and still meet the guidelines set by the Family Educational Rights and Privacy Act (FERPA).

Before the teacher commentary videos could be uploaded to the Sakai dropbox, the Camtasia recordings must be rendered into a shareable format, such as AVI, Windows Media, Quicktime, or Flash. Video formats such as Quicktime and Windows Media created a single file while Flash created a folder of files, of which all were needed for viewing. To view a Flash video a user must select the HTML file. I chose to render the Camtasia recordings into Flash because the files automatically played within a browser when students clicked on the HTML file to their teacher commentary video, as opposed to Quicktime or Windows Media videos, which required students to download the file first before viewing it. Moreover, Flash worked perfectly with the Sakai site statistics tool, which monitored student use of hyperlinks on the site (whether or not students watched the teacher commentary videos in their entirety cannot be determined by the site statistics tool). Because other video formats required one extra step, some students closed the “Download” window, making the decision to download the file later when they had more time. The Sakai site statistics tool did not monitor these video downloads, which made it difficult for me to determine if students actually opened the teacher commentary video files for viewing.

E-mailing teacher commentary videos to students was not an option for two reasons. First, file sizes ranged from $1 0 \ \mathrm { M B }$ to $4 0 \ \mathrm { M B }$ , which were too large for most e-mail providers. Second, e-mail providers cannot ensure student privacy, unlike a course management system. Moreover, Sakai had a maximum upload limit of $1 5 0 \ \mathrm { M B }$ (uploading multiple files at once is permissible), which presented fewer problems for uploading videos Another alternative to uploading files was the use of resources on WebDav <http://www.webdav.org>, which had no upload limits, although files larger than 1 GB often stalled my computer. Once the teacher commentary videos were uploaded to Sakai within students’ private dropboxes, a mass e-mail was sent, notifying students to go into their dropboxes within Sakai and click on the HTML file within the Flash folder to view their teacher commentary video.

# 3.4. Teacher commentary

For the first essay, half of the class received a Flash video that averaged 7 minutes in length and took 12 minutes to produce from beginning to end (i.e., on average, only 12 minutes were spent on a student draft). The other half received comments via Microsoft Office 2007. I used “Track Changes” and “New Comment,” and I included summative feedback at the essay’s conclusion. The approximate word count for the Microsoft Word comment feedback was 250-400 words. On average, it took 20 minutes to read and comment on the student drafts using Word. In attempt to homogenize the quality of the feedback in both media, I attempted to type “conversational” commentary with Microsoft Word comments and the summative feedback. For instance, my marginal feedback on a student’s paper about the technological impact of the abortion pill included the following commentary:

# Comment 3:

What was the context for creating this pill? Was it developed in the US or in another country? Was the original intention to ease world poverty because if you have poor women having babies, you exacerbate the problem and increase poverty? So talk about this context for these four individuals. Who were they? What were their credentials? Were they scientists?

# Comment 4:

Again, what country are we talking about here? Also, if you are focusing on the chemical makeup of the pill and how it’s changed, you should keep the social and historical context about the women’s movement to a minimum and then place most of your focus on the chemistry. What were the first models of the people? How were they created? What did they do to people? Talk about revenge effects here. See how that works.

In my reader-response of the student’s draft, I included numerous questions for students to make decisions about the focus of their paper; furthermore, as noted in italics above, I often included phrases that addressed the author directly, not necessarily the writing: “Again, what country are we talking about here. . ..See how that works.” The video commentary, on the other hand, was not as refined because the recordings were not scripted or edited. The objective was to conduct a form of think-aloud as I read the students’ drafts. The following feedback is a transcript from one of the teacher commentary videos. (The entire video can be viewed on YouTube at <http://www.youtube.com/watch?v=lAn0lxarTPI>; to read the entire transcript, please see Appendix C.)

[Highlighted a period outside of a quotation mark in the first paragraph]

A tiny point here. Periods always go inside the quotation marks. So, if you do this throughout the paper, make sure you proofread to change all of them.

# [Highlighted thesis in the first paragraph]

One of the things you have here is your thesis. You have here, this is your introductory paragraph, which of course you’ll have to fix that first sentence, which works fine. You’ll need an abstract, which summarizes the whole paper then you have your thesis. I can see this functioning as your thesis. You definitely want to be careful about having the thesis that is so broad where the Winchester rifle, it changed everything, and that’s your thesis, pretty much. One thing you can do to make it more specific, talking about a superior technology [highlights “super technology” in the paragraph], go ahead and be specific about what you mean by superior technology, so qualify your term here and then you can finish off the sentence. It doesn’t have to be one sentence. It can be more than one where you talk about its impact. Then we could continue with the rest of the paper.

For the second essay, the second half of the class who did not receive video commentary the first time now received a Flash video for the technology essay. The average length of the videos was 14.5 minutes, which took 20 minutes to produce. The other half of the class now received Microsoft Word comments. Again, I attempted to establish a conversational tone in Microsoft Word comments. The approximate word count was 600-750 words. On average, it took 30 minutes to read and comment on papers.

# 3.5. Survey design

At the end of the quarter, I uploaded two surveys into the Sakai CMS. The first survey (20 questions) focused primarily on students’ computer literacy and knowledge of the tools in Sakai and students’ access to the auxiliary materials I embedded within Sakai (see Appendix A for sample questions). The second survey (27 questions) focused on students’ attitudes about and access to the teacher commentary videos and instructional videos, specifically whether students accessed the videos and found the content beneficial during the revision process. Furthermore, students were surveyed on their attitudes toward Microsoft Word comments, as opposed to handwritten comments, which all students received as the primary mode of teacher response prior to my course, and whether the Microsoft Word comments were beneficial during the revision process (see Appendix B for sample questions). Students were given the option to respond to the surveys voluntarily. As a result, 19 students completed the first survey on Sakai, and 17 students completed the second survey on the teacher commentary videos.

# 3.6. CMS site stats

To track students’ access to their video commentaries and auxiliary resources, Sakai Site Stats was used to monitor student activity within Sakai. The activity report was then downloaded into a Microsoft Excel spreadsheet, organized by student name, resource accessed, and date accessed. Unfortunately, I did not learn about the tool until halfway into the quarter, after the first essay was assigned. Consequently, I narrowed my analysis to the second essay on technology, draft two, during a specific one-week period when teacher commentary videos were uploaded to student dropboxes in Sakai and the final draft deadline was one week later. I selected draft two because draft one was peer-reviewed in class and required as homework. Thus, student access to teacher commentary videos and any auxiliary materials would be completely voluntary. Because of the manner in which students accessed and distributed peer drafts (some students used their personal e-mail accounts), there was no way for me to monitor revision activities (i.e., did students wait until the night before to access comments and begin the revision process?). Site Stats can only determine access, not time on task. The present study does not provide a comprehensive description on how students revised, using the teacher commentary videos, Microsoft Word comments, or auxiliary materials; however, several questions in the two surveys distributed to students and classroom feedback on students’ revision process were attempts to address how students revised, although further research is necessary to thoroughly address this question. The objective of the present study was not only to research student access and attitudes about the teacher commentary videos, Microsoft Word comments, and the auxiliary materials, but to also improve the quality of the commentary and auxiliary materials and to increase efficiency.

# 4. Results

Survey results on students’ computer literacy and knowledge of Sakai indicated that students had varying degrees of computer literacy. First, all my students either owned a desktop computer or laptop. All students had access to the Internet on campus, in student housing, in apartment complexes, or at their parents’ homes. Furthermore, all students understood basic functional skills without a problem, such as starting a new document, saving documents, operating a computer, typing, logging onto the Internet, sending an e-mail, and uploading documents. Thus, computer literacy at the functional level was not a problem for my students; however, according to classroom testimonies about Sakai and online learning, in the first few weeks of the course, several students had difficulties navigating within Sakai, saving a document from a template, deleting comments within Microsoft Word, accepting comments within Microsoft Word, and saving in Microsoft 2003 instead of 2007 when working in the university computer lab—just to name the most common problems. The few students who used open-source software such as Linux or Open Office had difficulties locating my comments, saving a document into a readable format, and converting documents into PDFs when I could not open their files despite their efforts to create a DOC or RTF file.

Although students experienced an adjustment period at the beginning of the quarter, during the one-week revision period, none of the students communicated any difficulties accessing their Microsoft Word comments or teacher commentary videos. The major concern was whether students would bother with a technology that required extra work. Unlike Microsoft Word comments in which the reviewer’s feedback is contextualized within the student’s draft, the teacher commentary videos function like a “pull technology” in that students must access their feedback from another digital context. With teacher commentary videos, it is possible that students either have difficulties accessing these videos or simply disregard them in exchange for another mode of teacher feedback, such as a face-to-face conference. According to the Sakai Site Stats report, none of the students chose not to access the videos within their dropbox. The majority of the class viewed their teacher commentary videos within three days upon receipt. According to the student surveys $\mathrm { ( N { = } 1 7 }$ ) on video commentary (see Table 1 for results), 14 students viewed their videos in less than 3 days upon receipt of the video: 4 students stated that they watched the videos right away; 6 students viewed them in less than 24 hours; 4 students waited 1-3 days to view the videos; 2 students waited 4 days to view the videos; and 1 student claimed to never have watched it, yet provided contradictory information later in the survey, stating that he replayed parts of the video. In addition, according to the Sakai Site Stats report, he accessed his video. Of the 17 students who completed the survey, 14 viewed the videos from beginning to end: 4 students replayed the entire video twice from beginning to end, and 11 students replayed parts of the video 2-4 times, on average, after viewing the video initially in its entirety. Only 2 students did not choose to replay, only viewing their video once. Responses to the surveys corroborated with data results from Sakai Site Stats.

In summary, according to Sakai Site Stats and student surveys, during the revision period, 14 of 17 students viewed the commentary video within 3 days, which afforded $^ { 4 + }$ days for students to revise their actual papers. Whether students played the videos right before, during, or after revising their drafts and whether time of response is a significant factor in student revision practices and outcomes require further examination. The instruments used in this study did not measure actual revision activities.

Table 1 Survey Results of When Students Viewed Teacher Commentary Videos.   

<html><body><table><tr><td colspan="2">When Students Viewed the Teacher Commentary Videos.</td></tr><tr><td>Right Away</td><td>4</td></tr><tr><td>Less Than 24 Hours</td><td>6</td></tr><tr><td>1-3 Days</td><td>4</td></tr><tr><td>4+ Days</td><td>2</td></tr><tr><td>Never Watched It</td><td>1</td></tr><tr><td rowspan="3">14</td><td>Students Who Viewed Videos from Beginning to End</td></tr><tr><td>4 students replayed the entire video twice from beginning to end.</td></tr><tr><td>Number of Times Teacher Commentary Video was Replayed by Students</td></tr><tr><td>11 Students Replayed</td><td>1 Student Replayed Parts</td></tr><tr><td>Parts 2-4 Times</td><td>10 Times</td></tr></table></body></html>

Note: $_ { \mathrm { N } = 1 7 }$ . Videos were uploaded into student dropboxes one week prior to the essay deadline, and students were notified of the videos via e-mail.

# 4.1. Student responses and attitudes to video commentaries

Initial responses to the teacher commentary videos and instructional videos were positive. Most students found the teacher commentary videos far more personable than Microsoft Word comments or handwritten commentary, which they received in their primary and secondary education. Interestingly, several students found the video commentaries more personable because they assumed that I spent more time on the video commentaries than on the Microsoft Word comments, although the Microsoft Word comments, on average, took 10 minutes longer per student essay (10 pages in length). I did not inform students of the time difference in the two modes of teacher response. Students correlated time on task with caring about student performance (Anna D. Rowe & Leigh N. Wood, 2008a). In the case of the 2 to 3-minute YouTube video, I presumed that students would complain about the length of their teacher commentary videos (on average, 7 minutes for essay 1 and 14.5 minutes for essay 2). According to the video survey, 16 students stated that they would have no problem viewing a teacher commentary video that averaged 10 minutes in length, so long as it was pertinent to the revision process.

Of the 17 students who responded to the video survey, 8 outright preferred the video commentary to the Microsoft Word comments or handwritten comments. Six students found value in both Microsoft Word comments and video feedback, however, for different elements of the writing process. One student noted, “I feel like the Microsoft comment bubbles are a good quick way for small corrections and miniature comments. As a whole, for the correction and revision of a whole essay I feel like the comment bubbles just are not big enough both physically and mentally in the sense of it.” According to students, global issues in writing, such as the thesis, research question, organization, and claims and evidence, were better addressed in the video format. On the other hand, Microsoft Word comments were better at addressing “small corrections,” such as grammatical errors, punctuation, syntax, word choice, and local problems with cohesion and coherence. Some students preferred Microsoft Word feedback because it expedited the revision process for these students. They would not have to review an entire video to determine the “trouble spots,” whereas the Microsoft Word comment feature indexed the specific location that warranted attention. Also, students had the option to print their comments, although no students printed their feedback. The majority of students reported revising with two versions of their draft on the computer screen—one version contained my feedback.

Two students were indifferent to the mode and medium of feedback. Only one student downright protested against the idea of receiving video feedback and stated firmly, “Hand written or text-based assessments are vastly superior because they can be most easily referenced while revisions are being made. Text versus Microsoft Word comments isn’t that big a deal but I greatly prefer both to video.” This student also disliked the use of other technologies in the classroom, such as Google Docs as well as several tutorials that I designed. He stated that pencil and paper could get the job done faster and more efficiently.

A common theme in student responses to the video commentary was the conversational quality of the feedback. One student responded, “I felt I was talking to someone and they were explaining to me what was wrong and pointed them out on my paper.” Another student commented that the video was “like a peer edit, but verbally, which helps me understand what you want better.” Other students stated that they were aural and visual learners: “Someone needs to explain some things to me verbally and for some reason it just makes more sense that way.”

Another predominant theme present in the data was student interpretation of teacher feedback. The following responses are direct quotes from the survey on video versus Microsoft Word commentary:

Student 1: Video commentary offers the chance to make clarification of a point that otherwise could not be done with just a comment bubble. You could write something in a comment bubble but it still might be confusing. With a video, however, orally sometimes is a better way to communicate a point across, which is nice.   
Student 2: I would not have preferred a hand-written assessment, because having the professor explain to me what is needed is more clear than having to interpret the comments on paper.   
Student 3: [The video was] way more specific, and also there were more general tips for the entire paper. It gave more of an overview of things that needed to be changed rather than certain specific points, which I liked a lot better.

Students perceived that the video commentary modality afforded a degree of clarity and representation that was not evident or as effective in written modalities. Student 3 stated a near contradiction about the video commentary being both “more specific” and generalized. Contrary to previous findings that students performed better when they received direct comments with explicit directions for revision (Hillocks, 1982; Sweeney, 1999), this student valued both micro and macro level feedback (i.e., feedback that asks students to clarify or correct a specific problem versus feedback that asks students to examine options in relation to the topic, overall structure, purpose, or audience expectations). In the written modality, I had attempted to provide both levels of feedback; however, in the audio/visual modality, I did repeat macro level feedback as I moved through the draft and located further micro level issues (see Appendix C for a sample transcript or view the video at <http://www.youtube.com/watch?v=lAn0lxarTPI>). The repetition of macro level feedback as an anchor within an audio/visual modality may be essential in student revision practices.

In the video commentary survey, I asked students to describe the parts of their video that taught them something about writing. I was aware that some students may have simply enjoyed the novelty of the feedback, which would make it appear, upon first glance, that the videos were vastly superior to more traditional forms of teacher commentary. Thus, I was curious if the mode and medium of the teacher commentary illuminated different components of the writing process and if students identified fundamental differences between the Microsoft Word comments and video commentary in the ways in which they revised their essays. According to the survey, students who preferred the video commentary were more apt to discuss the rhetorical features and global issues of writing, whereas students who preferred the Microsoft Word comments emphasized the indexical quality of the comments during the revision process, which allowed students to quickly locate “errors” (I seldom corrected grammatical/punctuation errors) and apply revisions.

# 4.2. Preference for video commentary

Student 1: You said something that made me realize that there was a small group of my audience that I was totally ignoring.   
Student 2: That I sometimes write in too biased a mindset and need to look at the counter-arguments that people could possibly come up with for what I’m writing.   
Student 3: I’d have to say the more helpful parts of the video were comments that were made that helped me make more or stronger connections within my essay. I feel like if you made the same comment using a bubble it wouldn’t have had the same effect.   
Student 4: It helped me to understand that the first paragraph has too much importance for a metaphor about man and god.

# 4.3. Preference for Microsoft Word comments

Student 1: Easier to go back to and saves time.   
Student 2: The comments show exactly where a correction could be.   
Student 3: The Microsoft Word comments offered flexible time revisions; I was able to skim the revision and get the gist of what I am supposed to change.

Student 4: The Microsoft Word comment bubbles offers quickness and to the point style of making corrections for small things like grammatical errors. The video can offer this as well but it is more convenient to just make a quick comment bubble and then move on to the next thing.

In summary, the modality of teacher feedback factored into students’ attitudes and perceptions about the feedback, their writing, and their development as writers. Students who preferred the video commentary found the audio/visual and conversational properties of the feedback significant to their development during the revision process. Students who preferred the Microsoft Word comments described the revision process in more mechanical terms, in which time and location/correction of the problem were essential for the process to proceed smoothly. In the relationship between student preference and quality of revisions, there was scant evidence to indicate that one modality, video or written, had a more significant effect on the quality of student revisions.

# 5. Discussion

The results indicate that students did not have problems accessing the videos during the revision period, nor were students’ computer literacy an issue in a course that was highly dependent on several different technologies. In students’ preference for video commentary or Microsoft Word comments for teacher feedback, the results show that 8 of 17 students in my hybrid writing course preferred video commentary to Microsoft Word comments; 6 of 17 preferred the use of both forms of feedback for different elements of the writing process; 2 students were indifferent about the form of feedback; and 1 student was a bit of a Luddite and felt technologies in the classroom were unnecessary for writing development. However, this student, like others in the classroom, did find Microsoft Word comments more convenient for revision activities; yet, these students would have preferred both forms of feedback rather than Microsoft Word comments alone. Furthermore, the findings indicate that the modality of teacher feedback may factor into the development of students’ rhetorical knowledge, as students who preferred the teacher commentary videos were more apt to discuss macro level issues in their writing; however, these findings are preliminary and require further research. The present study adds to a growing body of evidence that teacher feedback in different modalities and media (e.g., audio feedback, podcasts, screen-capture video feedback, handwritten feedback, Skype conferencing) mediates different social, cognitive, and affective responses in students.

According to the survey results, students who received written comments referred to their writing as something that needed to be fixed. The placement of the feedback on the electronic page performs several functions. First, it anchors an instructor’s reader-response to a specific word, sentence, or location. Second, it creates a chronology of the reading experience that may relate directly to students’ revision activities. According to the survey results, several students commented that they used the Microsoft Word comments in this manner. Last, excessive written feedback on student writing has been shown to overwhelm students (Dana Ferris, 2003). In an electronic medium like Microsoft Word, whether the number of comment balloons, the amount of text within each balloon, or the total number of words regardless of the use of comment balloons contributes to students’ negative responses to teacher feedback deserves further attention.

The video commentary, on the other hand, afforded detailed discussion of macro level issues. Although audio commentary affords instructors similar opportunities to elaborate on macro level issues, video commentary situates the discussion within a specific frame of reference. As a graduate student, I had received audio feedback via e-mail on several occasions and it was a challenge to recall the specific context of my writing that triggered a reader-response from my professor. The ease of creating and sending audio files allows students to receive these files on mobile devices, university computer labs, and/or personal computers; however, without the actual paper in sight, students can struggle from cognitive overload as they attempt to process essential information and trigger prior knowledge. With video feedback, on the other hand, the audio data operate in tandem with the visual data. The synchronicity of the audio and visual output establishes temporal contiguity, reduces the effect of cognitive overload, and increases the opportunity for essential and generative processing (Mayer, 2009). Furthermore, as supported by the findings of this study, students perceived the feedback as dynamic because changes made to the student document were recorded. Changes included replacement of words, movement and deletion of text, and alterations to the document format. It is this altered perception about feedback as dynamic that may contribute to students’ shift in attitude about teacher feedback as conversational and about macro level issues in student writing.

The objective of this study is not to undermine the effectiveness of audio feedback, as studies like Ice et al. (2007) and studies described by Ice and Richardson (2009) have found audio feedback effective for enhancing teacher presence and increasing feelings of community within online courses; in fact, the results of this study support existing research in audio feedback, suggesting that different modalities of teacher feedback have an effect in altering students’ attitudes and perceptions about writing, revision, participation, or teacher-student relations. Pilot research in using asynchronous video from Webcam recordings also supports the preliminary findings of this study (Michael E. Griffiths & Charles R. Graham, 2009).

In the present study, it was interesting that students perceived video commentary as conversational, because the technology did not allow students to provide immediate clarifications or affirmations about their intentions (Frank, 2001). Although I attempted to make the Microsoft Word comments appear “conversational” by avoiding directives and balancing interrogatives with declarative sentences, no student commented on the Microsoft Word comments as “conversational.” Also, it would have been impossible and awkward to imitate verbatim the video commentary in Microsoft Word comments because strategies in oral and written discourse do differ. Another student expressed that “the videos were like a conference I could revisit as many times I wanted to.” In traditional teacher-student conferences, students benefit from the dialogic exchange because students can reaffirm or clarify their intentions. Within the video modality, I attempted to situate the student’s essay as part of a dialogue between the student and me and other prospective readers. The video commentaries included phrases such as, “Here, you’re saying ”, “You said but I’m not sure why you’re saying over here, or “I see what you’re trying to do here. . .You’re trying to .” The video commentary created a fiction of a dialogic exchange, which later trickled into face-to-face interactions in the classroom, where students conversed with me about comments and questions that I stated in the videos and communicated their goals or ideas for revision.

Although the video commentary was well received by most students in the class, few students had a negative response to the use of visual/audio modality for teacher feedback. One student in the class commented that he found that video commentary, in general, “insulted his intelligence.” He advocated for paper-based textbooks and believed that credible websites could operate as auxiliary resources. Another student expressed a preference for handwritten feedback because he associated the labor of handwritten comments with teacher care and quality. In one study on student preference for written or voice modality within an online learning environment, Loel Kim (2004) found that $80 \%$ of students did not recognize the fact that the same teacher provided feedback in both modalities. Students focused on paralinguistic information in the audio feedback to construct a negative impression of the teacher (e.g., a student could hear disappointment in a teacher’s comments) (Kim, 2004). Thus, the mode and medium of teacher feedback can play a significant role in student interpretation of the information.

Contrary to what the literature states about student misconceptions of teacher feedback or expectations (Carless, 2006; Thaiss & Zawacki, 2006), the students in my class did not state any problems with comprehension or clarification of the video commentary or the Microsoft Word comments. Students had one week to revise their essays after receiving their teacher commentary video. Of the 17 students who responded to the survey on video commentary, 14 students viewed the videos within 3 days. In fact, 15 of the 17 students stated that parts of the video were replayed more than twice, which suggests that students engaged with the feedback and did more than “bear” in mind what needed work or improvement. If misconceptions or confusions regarding my feedback were present, students had ample time to e-mail me or schedule a face-to-face conference. No students contacted me or requested a conference; however, this is not a reliable indicator of student comprehension because students may feel uncomfortable about conferencing with instructors about teacher commentary. In any case, within the student revisions, there was little evidence to indicate that students did not understand my feedback in the video commentaries or within the Microsoft Word platform.

At the end of the quarter, students recommended that I use a combination approach by doing teacher feedback videos and detailed Microsoft Word comments. This finding is consistent with studies in which students preferred to receive audio and written feedback (see Sue Rodway-Dyer, Elisabeth Dunne, & Matthew Newcombe, n.d.; Brian Still, 2006). Providing a summative script of the teacher commentary would double the time it takes to read and comment on a student’s draft (approximately 40 minutes). However, instructors could experiment with combining different modalities of teacher feedback when addressing both macro and micro level concerns in student writing, all the while providing feedback within a reasonable time frame. There are also opportunities for individualizing teacher feedback. Instructors could select the modality preferred by students, although students may initially select the modality with which they are most familiar.

# 5.1. Reflection on technology as a tool for teacher commentary

Access to digital materials and knowledge of various applications were, at times, an issue for students. With the Microsoft Word comments, two students who used Open Office were not able to view my feedback. I had to create a PDF of the Microsoft Word document for these students to view the marginal comments. To provide greater access to all students, I made sure to create all auxiliary materials into PDF files. (Having all the teacher feedback videos in Flash was not a problem for any students.) Also, within Microsoft Word, many students did not know how to get rid of the comments. As a result, students opened their original document, making changes in the original, while reviewing the teacher comments in a separate window. This revision practice made it impossible for students to view any punctuation corrections I made (they were in red but were very small and nearly impossible to see), which explains why students failed to address numerous punctuation problems in their revisions. Moreover, students working in two separate windows may experience a split-attention effect (Mayer, 2009) because the learner’s visual attention is split between two separate windows that may or may not be open on the screen during the revision process. In the computer lab, it was common for students to work in one window at a time, rather than have both windows visible side by side. To eliminate or reduce the split-attention effect, students could work on a large monitor and have two windows open side by side or invest in an external monitor. If students had known about the “Accept” function in Microsoft Word, they could have completed all their revisions in one digital space without managing multiple windows or monitors.

Additional technical features of Microsoft Word that created problems for students were headers, page numbers, and hanging indents. As a result, in the teacher commentary videos, I took a minute to teach students how to insert headers or page numbers, for example. On a few occasions, when students submitted the classic draft with modified margins and font size, I took a few seconds in the video to show them how to change the font to Times New Roman 12 and the margins to 1 inch. I have to admit I took pleasure transforming a 5-page draft to 4 pages. The students who attempted to modify their margins or font size never attempted it again once they received my video feedback.

The teacher commentary videos were not exempt from problems either. The audio in some parts of the videos was distorted, forcing some students to replay those parts. One student claimed that the audio did not match the visual output, which forced him to replay the video several times to understand the feedback. In the video commentary survey, whether students replayed parts because of audio problems remains undetermined because the question did not address why students replayed the video. I did not realize audio was a problem until I received the survey results. Only a few students reported audio or visual problems.

I also encountered a problem with the video technology, but only by chance while analyzing the survey data. For a conference presentation, I chose to use a student’s teacher commentary video as an example of how Camtasia works in teacher feedback. The student wrote an essay about the history of the gun. In his introduction, there was a racist subtext to the content. In his video, I addressed the racist subtext and gave him the benefit of the doubt, assuming he did not know the content had racist undertones. He was one of my best students who wanted to revise his essay again prior to the deadline. He sent the revision via e-mail and the same racist introduction was present, without a single word changed. Because I spent over a minute discussing the problem with the introduction, I interpreted the lack of revision in the introduction to be an outright resistant act to my suggestions. I contacted my supervisor, asking for her professional opinion. For the next three class days, I felt uncomfortable. In my mind, I questioned whether to advocate a student’s right to free speech or to support civil rights laws that prohibit discrimination. The following week, I mustered the courage to e-mail the student, stating that I found the introduction offensive, that he needed to address multiple audiences at the university level, and that a blatant racist description would not be acceptable at a university. He apologized, stating that the introduction was simply an attention-getter and was not intended to have a political message. He immediately changed the introduction without reservation. A couple of weeks after the awkward confrontation when the quarter was over, I played his video for consideration at a national conference. At this moment, I realized that the beginning was cropped. He had never received my commentary about the introduction. It probably was lost during the production of the video (Camtasia has several different bugs). I then realized how a technical error could quickly affect teacher-student social interactions. An easy solution to this problem would be to preview each video prior to delivery, yet that would nearly double the time it takes to comment on a student’s essay. Fortunately, the technical error only affected one student video and only one student accidentally wrote discriminatory prose; however, this small anecdote is one example by which experimentation using various media and modalities to mediate teacher-student interactions and shift student perceptions and attitudes about teacher feedback has unpredictable outcomes that require further examination.

# 6. Future research

Further research is required to determine the qualitative effect of feedback within different modalities and media in student writing. In my classroom, the video feedback had less effect on the quality of the students’ paper and the writing process. Further research is needed to understand the relationships between a specific modality or medium and the feedback provided. What effect would a specific modality or medium have on the content of the feedback? Would other instructors focus on or ignore certain elements of writing because of the affordances or limitations of a specific technology? What effect would the information in the feedback and the modality and medium have on student revision practices?

Improving student writing is not the only reason to experiment with different technologies. Instructors may want to experiment with different modalities and media because it makes their job easier and more enjoyable. Initially, I chose to do video commentary because I rode a bicycle to school and had no means to carry a large stack of papers. Also, once I got the technology part down, I was able to reduce the time it took to start and finish a student draft. Students often stated that they liked receiving their feedback electronically because they did not have to wait until class to do their homework. Student schedules varied and some students only had weekends to do their work.

The present study also has implications for hybrid classrooms and distance learning education. The use of teacher feedback videos allowed me to create a teacher presence that existed both within and outside the physical space of the classroom. If students feel as if they are in a dialogue with an instructor as a result of video commentary or other forms of audio feedback, then students may feel more of a social connection with the instructor and peers. In distance learning courses, audio feedback has shown to decrease social distance and create the feeling of being part of a real class (Ice et al., 2007), which could reduce attrition rates.

The pedagogical and methodological implications of screen-capture technology in combination with audio feedback are numerous. In a traditional lecture course with $4 0 0 +$ students, the use of video commentary is unrealistic; however, lecturers have experimented with Camtasia by recording lectures on Tablet PCs and uploading the files online for student viewing (Dean, 2006; Jaki, 2009). Wendy Maboudian and Colin S. Ward (2008) found a creative use for Camtasia by transforming a blank Microsoft Word document into a white board to instruct English as a Second Language students. Instructors could develop think-aloud videos as they problem-solve discipline-specific tasks. Instructors could spend time describing the rubric and analyze out-loud the rationale behind an A and C paper. Or, instructors could follow the lead of Jeff Hoyer (n.d.), who found another innovative use for Camtasia videos: he would project a video at the front of the classroom while he walked around the lab to monitor student comprehension and activity.

In addition to teacher commentary and online-tutorials, screen-capture technology has revolutionized the qualitative research methodologies of reading and writing research (Marion Degenhardt, 2006; Laura Elizabeth Samuels, 2006). Camtasia could record all offline and online screen activities without hindering the natural writing or reading processes of the user.

Think-aloud protocol can also be used to record explicit and tacit knowledge. Camtasia provides descriptive data, yet analysis is time-consuming; thus, Camtasia should be used with other quantitative tools of analysis (Degenhardt). As educators and researchers continue to experiment with screen-capture technologies and multimodal and multimedia contexts, it is our responsibility to meet the literacy needs of students and provide all students equal access to various learning resources.

# Acknowledgements

I would like to thank Karen Lunsford for the opportunity to work with her and her team on the UCSB Sakai project.

# Appendix A.

# Sakai Survey

Student Name:

Instructions: Respond to the questions by typing below each one. Type as much as you want. Don’t worry about page length or making the survey look pretty. When you have to select an answer from a list of options, highlight your answer in red. If you need to check off a box, insert an X inside of it. Just double-click on the box until the cursor is inside the box. You may need to right-click to “Edit Text” or “Insert Text.” I will use these surveys to improve the quality of the course. Also, after the quarter has ended and your grades have been submitted, I will ask for your permission to share your survey responses for the purpose of research.

1. Circle the links you used on a weekly basis throughout the quarter (Click on the red circles above and move them over to the links you used. If you need more, copy and paste).

2. Before you started your paper (Scientific American Summary, Civilized Engineer Essay, Technology essay, PowerPoint Presentation, Recommendation Report), which links did you use? Rank them from most helpful to least helpful. If you did not use the site at all before beginning a paper, check this box off.

3. While you were working on a paper, which links did you use? Rank them from most helpful to least helpful. I you did not use the site at all while working on a paper, check this box off.

4. Before you started a homework assignment, which links did you use? Rank them from most helpful to least helpful. If you did not use the site at all before beginning a homework assignment, check this box off.

5. While you were working on a homework assignment, which links did you use? Rank them from most helpful to least helpful. If you did not use the site at all while working on a homework assignment, check this box off. 6. In the diagram above, cross out the link or links you never used throughout the quarter. 7. Were any of the links above confusing to use? Yes No

8. Referring to question 7, which of the above links were confusing? Rank them from most confusing to least confusing.

9. Which option did you exercise most often to clarify any confusions or questions regarding any of the links?

a. I used the “Help” link   
b. I asked a classmate   
c. I asked the instructor   
d. I searched on the Internet for my answer   
e. I used a reference book for my answer

11. Where do you normally access a computer to login to Sakai? (e.g., home desktop, personal laptop, campus lab, work computer, café with your personal laptop. If you borrow a laptop, note this as well) 12. Umail is my primary email account: Yes No

13. (If no)– I set up my umail account to forward all messages to my primary email account: Yes No   
14. (If no)–On average, how many times a week did you access your umail account?

15. When you sent me an email, did you use the “Contact Professor” link above or did you reply to one of the emails I had sent earlier in the quarter?   
16. Did you save emails that I sent throughout the quarter? Yes No

17. Which were the most common ways that you read the emails? Rank the options from 0-5, 5 being the most common way (If e and f do not apply to you, then do not rank).

a. I read the entire message once   
b. I read the entire message at least two times   
c. I skimmed the email first then read the entire message later   
d. I only skimmed the email   
e. Sometimes I did not read the emails   
f. I never read the emails

18. When I sent attachments in an email, which step or steps did you follow on a regular basis (circle all that apply): a. I saved them on my computer b. I saved them onto a flash drive c. I saved them in my email account by not deleting the message d. I printed them

19. How could Sakai be modified to fit your needs as a student writer?

20. What about the course outline could be improved upon to better assist your needs as a student writer?

![](img/f491fb66b11a00668bd35aad6c2588f97e09a3737d0797258639a4ebd5deb2ba.jpg)  
See Figures A.1 and A.2.

![](img/42c20fe9e4804fda8c4f0266f5435b80ad959c8a6f09704b82ea45629702bc60.jpg)  
Fig. A.1. Sakai homepage.   
Fig. A.2. Course Outline.

# Appendix B.

Teacher Commentary Video Survey Student Name:

Instructions: Respond to the questions by typing below each one. Type as much as you want. Don’t worry about page length or making the survey look pretty. When you have to select an answer from a list of options, highlight your answer in red. If you need to check off a box, insert an X inside of it. Just double-click on the box until the cursor is inside the box. You may need to right-click to “Edit Text” or “Insert Text.” I will use these surveys to improve the quality of the course. Also, after the quarter has ended and your grades have been submitted, I will ask for your permission to share your survey responses for the purpose of research.

1. Upon receiving notification of the video commentary, how much time past before you watched it?

a. I watched it right away b. I waited less than 24 hours c. I waited 1-3 days d. I waited $^ { 4 + }$ days e. I never watched it

2. How many minutes was your video (Include the number to the right of your video):

a. Civilized Engineering video? b. Technology Impact video?

3. My Civilized Engineer video felt:

a. too long b. too short c. just right

4. My Technology Impact video felt:

a. too long b. too short c. just right

5. For your video assessments, how many minutes on average are you willing to sit through?

6. Did you watch the video from beginning to end?

7. Did you watch the video from beginning to end in one sitting or in multiple sittings (e.g., you paused it halfway through then watched the rest later)?

8. Did you replay parts of the video when you first watched it?

9. On average, how many times did you replay the entire video during the first time you watched it?

10. On average, how many times did you replay parts of the video?

11. What parts of the video did you replay?

12. Before watching the video, what did you assume I was going to focus on?

13. What did I focus on in your video assessment?

4. Describe the part or parts of the video that helped you understand something about your writing? Answer this question only if something actually helped you learn something.

15. What do you wish that I focused more on in your video assessment?

16. Would you have preferred a hand-written assessment? Explain why or why not.

7. Would you have preferred that I used Microsoft comments? Explain why or why not.

18. After you received the draft with my Microsoft comments in your dropbox, which of the following actions did you take? (Circle all that apply) a. I saved the document then made my revisions in that copy and deleted the comments as I revised the paper. b. I saved the document then pulled up my original draft and viewed the comments as I made the revisions on my version, so I had two separate word documents open on my computer and I went back and forth between the two. c. I printed the document then had it next to my computer as I revised my version.

d. I read the comments in my Dropbox then later revised my original draft without having to go back to the document with my comments.   
e. I never read the comments on the draft.

9. What did the Microsoft comments offer that the video commentary did not offer?

20. What did the video commentary offer that the Microsoft comments did not offer?   
21. In the production of the video, what could I have done differently to better assist you with your writing?   
22. With the Microsoft comments, what could I have done differently to better assist you with your writing?   
23. Had you used PowerPoint in previous courses to draft your essays or projects? (“Draft” describes the summaries,   
outlines, headings, and sub-headings you had to write in PowerPoint for your essays or projects.) Yes No

24. Which statement best describes your feelings about using PowerPoint as a drafting tool?

a. I really liked using PowerPoint to draft my essay. I will use it again.   
b. I really liked using PowerPoint to draft my essay but it was just extra work. I’d rather just type the essay in Word.   
c. Using PowerPoint to draft my essays was unnecessary work for me. I’d rather type in Word.   
d. Using PowerPoint to draft my essays confused me. I prefer to type in Word.   
e. I hated using PowerPoint to draft my essays. I would never use it again.

25. If you really liked using PowerPoint to draft your essay and organize your sources, explain why you liked it so much.

26. Which statements apply to you? Highlight all that apply. If none apply, check off this box.

a. I watched the entire APA instructional video once.   
b. I watched the entire APA instructional video more than once.   
c. I used the table of contents on the left-hand side to watch parts of the APA video once.   
d. I used the table of contents on the left-hand side to watch parts of the APA video more than once.   
e. I watched the “How to Write a Summary” video once.   
f. I watched the “How to Write a Summary” video more than once.   
g. I watched the “How to Construct PowerPoint Slides of my Sources” video once.   
h. I watched the “How to Construct PowerPoint Slides of my Sources” video more than once.

27. If there is anything else you would like to add, regarding the use of these technologies in the classroom, provide a detailed comment.

# Appendix C.

Transcript of Teacher Video Commentary

[The first minute of the video was deleted during the file conversion process. It begins with a comment about a punctuation error.]

[Highlighted a period outside of a quotation mark in the first paragraph]

A tiny point here. Periods always go inside the quotation marks. So, if you do this throughout the paper, make sure you proofread to change all of them.

[Highlighted thesis in the first paragraph]

One of the things you have here is your thesis. You have here, this is your introductory paragraph, which of course you’ll have to fix that first sentence, which works fine. You’ll need an abstract, which summarizes the whole paper then you have your thesis. I can see this functioning as your thesis. You definitely want to be careful about having the thesis that is so broad where the Winchester rifle, it changed everything, and that’s your thesis, pretty much. One thing you can do to make it more specific, talking about a superior technology [highlights “super technology” in the paragraph], go ahead and be specific about what you mean by superior technology, so qualify your term here and then you can finish off the sentence. It doesn’t have to be one sentence. It can be more than one where you talk about its impact. Then we could continue with the rest of the paper.

[Highlighted the heading “What Was the Need for Winchester Rifle?]

Make sure that all your headings are not underlined and they need to be centered.

[Highlighted the sentence, “The civil war also brought the need and want for a faster and more practical repeating rifles.”]

One of the things here you could bring into your point is about from a military perspective to have this longer range rifle that is faster. One thing that you can demonstrate is what it was like before even if you don’t have all the specifics and the facts, you can just imagine having to be closer to your enemy and the kind of devastation that would occur from that. Also, being out in the battlefield and not being able to load your weapon faster. Kind of speculate what are the possible harm. . ..what is the possible harm to having just a Colt revolver. I really like how you try to build up that context. This is where you can bring in your sources and how Edgerton talked about a particular significance and what was significant about this was the fact that they needed something in military combat because. . .I’m assuming there were a lot of lives that ended because of the previous machinery that was used. This is where you can bring in those sources—the connection between history and some of these concepts. Also, some things you could bring into effect was the cost. I don’t know if you talk about that later—the cost of the Winchester, making this longer ranged and faster rifle. I’m just trying to bring in some of the information that we’ve learned, helping you tie it in. You still have to tie in those sources into your paper. It can’t just be a history report. You have to analyze this history yourself.

# [Highlighted a figure label beneath an image]

This is definitely a lot better. You’ve labeled your images. One thing though you have to italicize the word “figure” and then you need the period and then you have “Picture of a Toggle Link,” so you’ll have to do the same for the other one [a second figure down below].

# [Highlighted the words “Springfield rifle instead”]

I’m a little lost here. You’re talking about the Springfield rifle. I thought you said before it was the Colt revolver that the military was using. Be clear. Maybe you can describe in a paragraph the kind of arsenal or equipment that was commonly found in the military at the time in the $1 9 ^ { \mathrm { t h } }$ century. Then this would make sense. Otherwise, I’m like, “Where did this thing come from? I thought it was the Colt revolver.”

# [Highlighted the end-citation “Tenner, 1996”]

This is great. I like how you try to bring in Tenner. It would definitely be more appropriate if you moved it right after you “revenge effect.” It then attributes that to Tenner [moved the citation after “revenge effect.” Trackchanges was turned on]. Even to make it more clear for the reader, you can put quotation marks right after the phrase [added quotation marks]. Then for certain you have communicated that Tenner has stated that. Then this would be unnecessary then [deleted the original citation at the end of the sentence].

[Cursor, which is a yellow circle, placed in the next paragraph]

One thing I’m curious about what were the revenge effects of this rifle. I could imagine now that you have something that’s longer range that is faster to load. What kind of accidents occurred as a result? It’s impossible that there weren’t any accidents because there’s a learning curve with any new technology, so that would be relevant. Even with a longer range, you’re more likely to accidentally kill the wrong person. That may have changed, also, warfare in of itself. This is where you could bring in Postman where it [technology] changes the whole game. Maybe it was more important to distinguish yourself from your enemy because from afar, a moving target, you can get hit. Maybe different colors were very important. You really have to think outside the box. “How did this change military combat?”

[Highlighted the words “flintlock style gun”]

One of the things that you’re really losing me here is that you’re kind of assuming that the reader already knows all these guns. You really need a paragraph after your introduction where you’re going to explain all these different guns that you’re then going to talk about in the rest of the paper. Talk about the flintlock style gun, the Colt revolver, the Springfield, then you can end with the Winchester. You would be defining your terms here. You’re just kind of using these things here all very loosely. That’s really important; otherwise, it feels like I can take any one of these sections, move them around in the paper, it would read the same way. The writing shouldn’t flow like that. Everything should have this rhyme and reason to it. The generic formula for this paper is pretty much history, present, future. That’s at its most general. And of course, people take what they want from that and go into its technological or social impact, develop the context, and things like that. That’s where it gets a little more complicated. Right now, yours is sort of random. It kind of like. . .I can see some historical progression, but I’m kind of getting lost with all these different guns. I’m not too familiar with it so that needs to be explained early on.

# [Highlighted paragraph with the heading “Winchester As An American Symbol”]

One of the things. . .a common error that most students did in their writing was to say, “Walah, it’s an American symbol. Wahlah, everybody had it.” But, how did it get there? How did it become an American symbol? How did it become synonymous with the cowboy? It was used by the military. How did it get from the battlefield to cowboy’s hands, riding on horses and gathering up the cows? You know, how did that happen? It’s common to just jump. We’ll fill in the blanks, but what is that blank? That’s very unclear. Definitely, you’ll need to explain that a little bit better, providing that context.

# [Highlighted heading “Negative Impact Guns Bring”]

I know this was an option for a lot of people’s topics. I don’t think this works with yours, talking about the negative impact of guns. Because, first you went with Winchester, now you’re just going really broad. I mean, we know the negative impact. It can kill people. Then we’re like, “No, people kill people.” Regardless, you can’t kill someone by throwing a spoon at somebody, but if you throw a bullet at someone at God knows how fast, that could kill somebody. So, the negative impact doesn’t really work with this paper. It’s sort of obvious. I think what would definitely be more interesting because this is more of a historical paper is how the Winchester technology impacted other guns or you could even keep it primarily in the history. You’re the only student doing a history. So, your whole paper is based on stuff that occurred in the past. This section, talking about this negative impact, wouldn’t be as relevant.

# [Highlighted “When Oliver Winchester”]

This works well here when you talk about Oliver Winchester but you never really talk about his industry and how that just took off and how this guy became this multi-millionaire. That was kind of disregarded altogether. I think the key is in this section here [scrolled up to an earlier part of the paper] how it went from being used by the military, which is a small percentage of the American population to now being produced to be an American symbol. That’s where the key lies [sic] is explaining that. How did that happen? Winchester himself comes into play making a lot of money. Often times, something is popular because someone was able to market it really well. Is that what Oliver Winchester did? Just savvy marketing? That’s pretty much how Bill Gates has sold all of his PCs. It’s not a superior system over a Mac. It’s really really good marketing. That’s something you really need to bring in. It’s okay if you keep it in the history. That would be fine. So, do a little more research for this paper. Otherwise, really interesting topic.

Let me see [slowly scrolled down the page]. References look good.

# [Cursor placed in paragraph, “How Has the Winchester Altered Perceptions”]

Again with altering our perceptions, you can keep it in the past when it was introduced to the military; how did it change military combat? That’s when you bring in the perception part and how it changed the whole game of fighting. Again, yeah, you can shoot it farther distances but you can also make mistakes as well, so that’s when you get revenge effects, so bring that into play. Other things is that when all of a sudden these cowboys now. . . that changed our perception of. . .not only about owning guns, but it changed the American image to the rest of the world.

# [Scrolled to the paragraph with the highlighted phrase, “When Oliver Winchester”].

Often times, an American symbol like McDonald’s, KFC. . .those kind of symbols really puts a stamp on how we’re viewed in the rest of the world. That’s maybe a nice conclusive paragraph, you know, what the Winchester did. Of this American symbol, and how we are perceived as a gun-bearing society. People are scared to death to come to America because they think they’re going to be raped and shot in the head. And we do have the worst gun record in the world. That’s a nice ending. Anyway, there’s some options as well.

Mary Lourdes Silva earned her PhD in Language, Literacy, and Composition Studies from the Gevirtz School of Education at the University of California, Santa Barbara (UCSB). She is currently an assistant professor at Ithaca College in the Writing Department. She also earned a Master of Fine Arts in Creative Writing at California State University, Fresno. Her current research examines the online navigational behaviors of second and third-year college writing students throughout the research/revision process. She also researches the pedagogical use of multimodal and multimedia technologies in her classroom. Furthermore, she is currently designing multimodal and multimedia instructional materials for instructors and students.

# References

Andrychuk, Sylvia, & Coyne, Morag (2008, February). Beyond the one-shot: Providing. course-integrated instruction for large, first-year university classes. Poster session presented at the academic libraries track of the Ontario Library Association Super Conference, Toronto, Ontario., 2008.   
Beach, Richard. (1989). Showing students how to assess: Demonstrating techniques for response in writing conferences. In Anson Chris (Ed.), Writing and response: Theory, practice, and research (pp. 127–148). Urbana, IL: National Council of Teachers of English.   
Biswas, Saroj (2007, June). Teaching courses with tablet PC: Experience and student feedback. Paper presented at the conference of the American Society for Engineering Education, Honolulu, Hawaii.   
Boswood, Tim, & Dwyer, Robert H. (1995). From marking to feedback: Audiotaped response to student writing. TESOL Journal, 5, 20–23.   
Brick, Billy, & Holmes, Jasper. (2008). Using screen capture software for student feedback: Towards a methodology. In IADIS International Conference on Cognition and Exploratory Learning in Digital Age. Germany: Freiburg. Available: http://www.iadis.net/dl/final uploads/200818C046.pdf [Accessed: March 12, 2010]   
Carless, David. (2006). Differing perceptions in the feedback process. Studies in Higher Education, 31(2), 219–233.   
Clark, Thomas David. (1981). Cassette tapes: An answer to the grading dilemma. The American Business Communication Association Bulletin, 44(2), 40–41.   
Cox, Christopher. (2004). From cameras to Camtasia: Streaming media without the stress. In Miller William, & M. Pellen Rita (Eds.), Internet reference support for distance learners (pp. 193–200). New York: The Haworth Information Press.   
Crofts, Ken, & Hunter, Carole. (2007). Using onscreen-action-capture tutorials to enhance student learning of MYOB software. Proceedings of the Second Innovation in Accounting and Corporate Governance Education Conference. 31 January-February. Hobart, Tasmania. Available: http://www.utas.edu.au/business/faculty/conference/documents/Papers/Crofts Ken and Hunter Carole 2007.pdf [Accessed: March 1, 2010].   
Cryer, Patricia, & Kaikumba, Nemeta. (1987). Audio-cassette tape as a means of giving feedback on written work. Assessment and Evaluation in Higher Education, 12(2), 148–153.   
Dean, Brian C. (2006). Beyond screen capture: Creating effective multimedia whiteboard lectures on a tablet PC. In Proceedings of the 1st Annual Workshop on the Impact of Pen Technology in Education. Available: http://www.cs.clemson.edu/∼bcdean/lscribe.pdf [March 12, 2010]   
Dean, Brian C. (2007). Dynamic homework annotation. First International Workshop on Pen-Based Learning Technologies. Available: http://www.cs.clemson.edu/∼bcdean/vgrade.pdf [Accessed: March 12, 2010].   
Degenhardt, Marion. (2006). Camtasia and Catmovie: Two digital tools for observing, documenting and analyzing writing processes of university students. In Van Waes Luuk, Leijten Mariëlle, & M. Neuwirth Chris (Eds.), Writing and digital media: Studies in writing (pp. 180–186). Amsterdam: Elsevier.   
Ferris, Dana R. (2003). Response to student writing: Implications for second-language students. Mahwah, NJ: Erlbaum.   
Fisher, David; Cornwell, Phillip. & Williams, Julia, (2001, July). Teaching dynamics using interactive tablet PC instruction software. Paper presented at the conference of the American Society for Engineering Education, Salt Lake City, Utah.   
Florman, Samuel C. (1987). The civilized engineer. New York: St. Martin’s Press.   
Frank, Carolyn R. (2001). What new things these words can do for you: A focus on one writing-project teacher and writing instruction. Journal of Literacy Research, 33(3), 467–506.   
Griffiths, Michael E., & Graham, Charles R. (2009). Using asynchronous video in online classes: Results from a pilot study. International Journal of Instructional Technology and Distance Learning, 6(3), 65–76.   
Haswell, Richard H. (2008). Teaching of writing in higher education. In Bazerman Charles (Ed.), Handbook of research on writing: History, society, school, individual, text (pp. 331–346). New York: Lawrence Erlbaum Associates.   
Higgins, Richard, Hartley, Peter, & Skelton, Alan. (2002). The conscientious consumer: Reconsidering the role of assessment feedback in student learning. Studies in Higher Education, 27(1), 53–64.   
Hillocks, George. (1982). The interaction of instruction, teacher comment, and revision in teaching the composing process. Research in the Teaching of English, 16, 261–278.   
Hoyer, Jeff (n.d.). Enhancing the “show and tell” aspects of class engagement using Camtasia, a low-cost video screen capture replay technology. Available: http://frank.mtsu.edu/∼itconf/proceed04/hoyer.pdf [Accessed: October 19, 2008].   
Hulme, Julie, & Forshaw, Mark. (2009). Effectiveness of feedback provision for undergraduate psychology students. Psychology Learning and Teaching, 8(1), 34–38.   
Ice, Philip, Curtis, Reagan, Phillips, Perry, & Wells, John. (2007). Using asynchronous audio feedback to enhance teaching presence and student sense of community. Journal of Asynchronous Learning Networks, 11(2), 3–25.   
Ice, Phil, & Richardson, Jennifer (2009). Optimizing feedback in online courses: An overview of strategies and research. The $5 ^ { t h }$ International Scientific Conference: eLearning and Software for Education, 9-10 April. Bucharest, Romania. Available: http://adlunap.ro/else2009/ journal/papers/2009/1007.1.Ice Richardson.pdf   
Jaki, Thomas. (2009). Recording lectures as a service in a service course. Journal of Statistics Education, 17(3). Available: http://www.amstat.org/ publications/jse/v17n3/jaki.html [Accessed: March 12, 2010]   
Jollands, Margaret, McCallum, Neil, & Bondy, Julian. (2009). If students want feedback why don’t they collect their assignments? In Paper presented at 20th Australasian Association for Engineering Education Conference Adelaide, Australia,. Available: http://aaee.com.au/conferences/ AAEE2009/PDF/AUTHOR/AE090060.PDF [Accessed: March 12, 2010]   
Kim, Loel. (2004). Online technologies for teaching writing: Students react to teacher response in voice and written modalities. Research in the Teaching of English, 38(3), 304–337.   
Laurence, David. (2001). The 1999 MLA survey of staffing in English and foreign language departments. ADFL Bulletin, 33(1), 56–68.   
Lerner, Neal. (2005). The teacher-student writing conference and the desire for intimacy. College English, 68(2), 186–208.   
Lunsford, Karen et al., (2008). The library and the CMS: Establishing library presence in Sakai writing course sites. Kairos: A Journal of Rhetoric, Technology, and Pedagogy, 12(2). Available: http://kairos.technorhetoric.net/12.2/binder.html?praxis/lunsford/index.html [Accessed: December 16, 2008].   
Maboudian, Wendy, & Ward, Colin S. (2008). The handheld hybrid. In T. T. Kidd, & H. Song (Eds.), Handbook of Research on Instructional Systems and Technology Volume II (pp. 641–652). Hershey: Information Science Reference.   
Mayer, Richard E. (2009). Multi-media learning $( 2 ^ { \mathrm { n d } }$ ed.). New York: Cambridge University Press.   
McAlpine, Lynn. (1989). Teacher as reader: Oral feedback on ESL student writing. TESL Canada Journal, 7(l), 62–67.   
Mellen, Cheryl, & Sommers, Jeff. (2003). Audiotaped response and the two-year-campus writing classroom: The two-sided desk, the “guy with the ax,” and the chirping birds. Teaching English in the Two-Year College, 31(1), 25–39.   
Merry, Stephen, & Orsmond, Paul (2008). Students’ attitudes to and usage of academic feedback provided via audio files. Available: http://www.bioscience.heacademy.ac.uk/journal/vol11/beej-11-3.aspx [Accessed: March 11, 2010]   
Mitchell, Felicia. (1994). Is there a text in this grade? The implicit messages of comments on student writing. Issues in Writing, 6(2), 187–195.   
Pearce, Glenn C., & Ackley, Jon R. (1995). Audiotaped feedback in business writing: An exploratory study. Business Communication Quarterly, 58(3), 31–34.   
Roberts, Simon (2008), Pod casting feedback to students: Students’ perceptions of effectiveness. Available: http://www.heacademy.ac.uk/ assets/hlst/documents/case studies/cas e125 podcasting feedback.pdf [Accessed: March 10, 2010].   
Rodway-Dyer, Sue; Dunne, Elisabeth & Newcombe, Matthew (n.d.). 0207 audio and screen visual feedback to support student learning. Available: http://repository.alt.ac.uk/641/1/ALT-C 09 proceedings 090806 web 0207.pdf [Accessed: March 10, 2010].   
Rotheram, Bob. (2007). Using an MP3 recorder to give feedback on student assignments. Educational Developments, 8(2), 7–10.   
Rowe, Anna D., & Wood, Leigh N. (2008). Student perceptions and preferences for feedback. Asian Social Science, 4(3), 78–88.   
Rowe, Anna D., & Wood, Leigh N. (2008b). What feedback do students want? Paper presented at 2007 Australian Association for Research in Education (AARE) International Education Research Conference, Fremantle, Australia. Available: www.aare.edu.au/07pap/row07086.pdf [Accessed: January 18, 2008].   
Samuels, Laura Elizabeth (2006). The effectiveness of web conferencing technology in student-teacher conferencing in the writing classroom: A study of first-year student writers. Unpublished master’s thesis, North Carolina State University.   
Siegel, Fred (2006). Scott Warnock, “Using Video Capture Software for Asynchronous A/V Writing Feedback.” CCCC 2006 Review. Available: http://wac.colostate.edu/atd/reviews/cccc2006/c24.cfm [Accessed: October 19, 2008].   
Smith, Summer. (1997). The genre of the end comment: Conventions in teacher responses to student writing. College Composition and Communication, 48(2), 249–268.   
Sperling, Melanie, & Freedman, Sarah Warshauer. (1987). A good girl writes like a good girl: Written responses to student writing. Written Communication, 4(4), 343–369.   
Stannard, Russell (2007). Using screen capture software in student feedback. The Higher Education Academy. Available: http://www.english.heacademy.ac.uk/explore/publications/casestudies/technology/camtasia.php [Accessed: October 19, 2008].   
Stannard, Russell (2008). Screen capture software for feedback in language education. Proceedings of the Second International Wireless Ready Symposium. Available: http://wirelessready.nucba.ac.jp/Stannard.pdf [Accessed: March 10, 2010].   
Still, Brian. (2006). Talking to students embedded voice commenting as a tool for critiquing student writing. Journal of Business and Technical Communication, 20(4), 460–475.   
Straub, Richard. (1996). The concept of control in teacher response: Defining the varieties of “directive” and “facilitative” commentary. College Composition and Communication, 47(2), 223–251.   
Straub, Richard. (1997). Students’ reactions to teacher comments: An exploratory study. Research in the Teaching of English, 31(1), 91–119.   
Sweeney, Marilyn Ruth. (1999). Relating revision skills to teacher commentary. Teaching English in the Two Year College, 27(2), 213.   
Thaiss, Chris, & Zawacki, Terry Myers. (2006). Engaged writers, dynamic disciplines: Research on the academic writing life. Portsmouth: Boynton/Cook.   
Xiao, Daniel Yi, Pietraszewski, Barbara A., & Goodwin, Susan P. (2004). Full stream ahead: Database instruction through online videos. Library Hi Tech, 22, 366–374.   
Yagelski, Robert P. (1995). The role of classroom context in the revision strategies of student writers. Research in the Teaching of English, 29(2), 216–338.   
Yost, Sandra A. (n.d.). Using a tablet PC to enhance instruction and productivity. Available: http://www.vcu.edu/cte/programs/instructional technology/tablet PC prog/TabletPC EnhanceInstructionProductivity.pdf [Accessed: October 18, 2008].