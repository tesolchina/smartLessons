# Navigating innovation and equity in writing assessment

# ARTICLEINFO

# ABSTRACT

Keywords:   
Writing assessment   
Validity   
Fairness   
Equity   
Writing assessment technology

The 2024 Tools & Technology forum underscores the significant role of emerging writing technologies in shaping writing assessment practices post-COviD-19, emphasizing the necessity of ensuring that these innovations uphold core principles of validity, fairness, and equity. AI-driven tools offer promising improvements but also require careful consideration to ensure that they reflect writing constructs, align with educational goals, and promote equitable assessment practices. Validity is explored through dimensions such as construct, content, and consequential validity, raising questions about how assessment tools may capture the complexity of writing and their broader impacts on educational stakeholders. Fairness in writing assessment is examined with regard to cultural responsiveness and accessibility, and how assessment tools may be designed to accommodate various student needs. Equity extends these considerations by addressing systemic inequities and promoting assessment practices that support diverse learning styles and reduce barriers for marginalized students. The reviews of three assessment tools-PERsUADE 2.0, EvaluMate, and a web application for systematic review writing--illustrate how innovations can support valid, fair, and equitable writing assessments across educational contexts. The forum emphasizes the importance of ongoing dialogue and adaptation to create inclusive and just educational experiences.

# 1. Navigating innovation and equity in writing assessment

As it should, writing asssment continually evolves, driven by developments i technology, pedagogy, and society. Each year brings new tools and methodologies and the need to enhance the efficacy and inclusivity of writing assessments. The year 2024 is no exception, presenting a critical kairotic moment for educators, researchers, and policymakers to reflect on intersections of validity, fairness, and equity in the realm of writing assessment.

This annual Tols & Technology forum comes at a time when conversations in education continue to undergo substantial post COVID 19 transformation. The pandemic aelerated the integration of digital tools in education, and artifical inelligence (AI) is increasingly being harnessed to support and asess student writing (Hutson et al., 2024; Shofiah et al., 2023; Farazouli et al., 2024). These technological advancements raise important implications regarding validity, fairness, and equity-threepillars that must undergird any writing assessment tool or tech.

# 2. Technological, social, and pedagogical shifts

The proliferation of Al-driven tols in writing asesment has dynamically altered how educators think about and evaluate student writing. From automated essay scoring systems to chatbot systems that provide real-time feedback, these technologie offer promising avenues for enhancing efficiency and amount of fedack fered in writing asssments. At the same time, the deployment of these tools necesstates a careful examination of validity, a consideration we emphasize in this forum. Such an examination involves asking questions such as: Are these tools robustly measuring the constructs they intend to asses?Do they provide meaningful fedback that aligns with curricular goals and pedagogical principles?As we consider the afordances and limitations of inovations in writing assesment, it is crucial that we examine potential opportunity structures they can enable and the limitations they can impose.

In addition to technological advancement, societal and pedagogical shifts reshape writing asssment. Increased attention to diversity, equity, and inclusion (DEI) in education has suggested that fairnessis a necessary aspect of writig asessment validit if assessments are going to emphasize and account for diverse learning, experiences, and languaging (Poe & Ellot, 2019; Randallet al. 2024). Towards those aims, we outline key dimensions of validity, fairness, and equity below, along with ways those dimensions may guide insights into the challnges and opportunities for examining possbilities and limitations of writing asessment tols and technology.

# 3. Validity: ensuring construct relevance and efficacy

Validit i the cornerstone of any assesment system, reflecting the extent to which an asessment measures what it purports to and the appropriatenes of assessment result uses (Williamson, 1993; Murphy & Yancey, 2008; Huot, 1990; Ellio, 2016a; Randall t al., 2023). In the context of writing aessment, validity has come to encompass multiple dimensins, including construct validity, content validity, and consequential validity. Example questions related to each dimension are posed below.

Construct validit refers to \*the extent to which an assessment tol conforms to a theory of writing" (Williamson, 1993; 13). For writing assessments, this involves ensuring that tasks and criteria used reflect complex and varied constructs of writig (Cushman, 2016; Elliot, 2016a; Embretson, 1983; Murphy & Yancey, 2008). As we incorporate AI and other technologies into writing assessment practices, we might consider questions such as: o Do these toos capture and reflect the richness of the writing construct? for example, do automated writing systems adequately support evaluation of writers' engagement with source material and evidence f critical interpretation, or do they tend to place disproportionate weight on conventional implementation of grammar, stylistics, and syntax?)   
Content validit, or the degreeto which assessment content is both representative of and relevant to what it intends to measure (Willamson, 1993; Hughes, 1995; iddiek, 2010), is oten discussed in relation to types f writing evaluations. It is well-accepted, for instance, that standardized test items like multiple choice questions about sentence construction bear low content validity in efforts to measure students' ability to compose texts (Yancey, 1999; Greenberg, 1992; Siddiek, 2018). o Do the tass and procese that writers take up in their use of a given tool lign with learning goals and reflect the kinds of writing situations they will encounter beyond the assessment itself?. o Are thorough content validation proceses involving educators and stakeholders in place to ensure that the tools and technologies used in a given writing assessment system are meaningful and relevant?   
Consequential validity refer to the impact f evaluations and their implementation on assessment users (Slomp et a. 2014). While we often consider consequential validity in terms of impact on students, intructors, examiners, and other educational stakeholders too, are involved, as are implications for the development of educational policies and outcomes. \*Whenever we sette on .. citria of validity, we need to consider the consequences of that choice for the types of assessments that wil be considered sound and, in turn, for the nature of intellectual work that students and teachers willbe encouraged to undertake and the discourse about educational reform that willikely be fostered (Mos, 2004; 248). Questions related to the consequential validity of specific tools for writing assessment include: o Does this too and the specific way(s) in which it is taken up perpetuate narrow and unjust (Poe & Inoue, 2016; Toth, 2018) attitudes about language diversity? o How might this tool in use perpetuate existing inequalities? o How might AI-driven assessments reinforce or disrupt linguistic biases? o What opportunity structures (Elliot, 2016; Perryman-Clark, 2016) does this tool support or deny?

# 4. Fairness: addressing bias and ensuring inclusivity

Fairness in writing assessment can also be thought of as part of ensuring that writing assessments do what they purport to do, specifically, whether every student has a chance to demonstrat their learning in a given assesment, fre from ias or discrimination. "A fair assment of writing i one that creates opportunity structures for the least advantaged, Elliot (2016) theorizes: Constraint of the writing construct is to be tolerated only to the extent to which benefits are realized for the least advantaged (3.0). Fainess in writing asssment thus deends on the consequence of assessment (Hammond, 2019; 4). This principle is particularly alient in the current educational climate, where isues of quity and social justie are athe forefront. Principles related to this conceptualizationof fairness are posed below:

Culural and Lingusic Bias: Traditional assessments ofen align with Eurocentric norms (Cushman, 2016; Elliot, 2016b; Randal et al., 2023), potentially overlooking the diverse linguistic and cultural resources of allstudents. It'scrucial to design assessments that are culturally responsive and inclusive. o Are the assessment tols and criteria cultrall responsive, recognizing and valuing diverse ways students expres their ideas and experiences? Or, "lalre Eurocentric ways of knowing and processing information being privileged over other ways of knowing and processing information?" (Randall et al., 2022)

o How do the specific affordances of a given tool or technology support or inhibit the adoption of culturall responsive approaches?

Accessility and Acommodatio: Ensuring fairnesaso means making assessments acesibl to all students through flexible and inclusive design, providing necessary supports to meet diverse needs.. o Are digital writing tools compatible with assistive technologies, ensuring they are accessible to all students? o Do assessment platforms incorporate universal design principles to accommodate a wide range of student needs?

# 5. Equity: promoting inclusive and just assessment practices

Finally, equity in writing asessment goes beyond fairness, encompassing broader pursuit of assessment systems that actively promote social justice / educational equity. As Poe and Cogan (2016) remind us, if equitability is to be valued, it must be seen. Fairness in theory cannot be an aferthought to validity or reliability. Fairness in action demands local attention in which we repeatedly question how we can achieve equitable results with les adverse impact" (15). Such effrts are related to concepts and questions posed below:

Equity involves recognizing and addressing systemic inequities that affec students educational experiences and outcomes, aiming to reduce barriers that disproportionately impact marginalized students. o Given historical digital divides in material and technological conditions (see, for instance, Banks, 2006; Hammond, 2019), how can we ensure equitable access and engagement with this tool? o How can specific asessment tools and technologies be tailored to reduce the impact of socioeconomic disparities and differential educational opportunities on assessment outcomes?   
Supporting the diverse needs and strengths of all learners means designing asessments that rcognize and value dferent forms of knowledge and expression, providing multiple pathways for students to demonstrate their abilities. o What technological strategies can be employed to provide students with multiple avenues for showcasing their skils and knowledge, honoring and accommodating their individual strengths and needs? o How can specific asessment toos and technologies incorporate features that center diverse learning styles and abilities?

# 6. Affordances and limitations in assessment technology

The thre reviews in this forum consider how such ongoing conversations about validit, fainess and equity in writing ssessment continue to inform technology design, aesibility, and implementation. In addition, the review authors focus on thre different assesment tools that span education levels, from secondary levels 6-12 (Crossley et al.), through college levels (Guo), to graduate students and experienced research writers (Agrawal t al.), inviting us to consider implications these toos may have acros a range of educational and academic contexts.

First, Scot Andrew Crossley, Perpetual Baffour, Yu Tian, Alex Franklin, Meg Benner, and Ulrich Boser review the PERsuADE 2.0 corpus, an enhanced version of its preecessor PERsUADE 1.0, which includes over 25,000annotated persuasie esays written by U. S. students in grades 6-12. PERsuADE 2.0 builds on its preeding version with the incorporation of holistic essay scores and annotation features rlated to efectivenesrtings for individual discourse elements. As Crossey et al. highlight, the ability to construct and present arguments is considered a fundamental killin educational standards such as the U.S. Common Core State Standards (CCs). Yet, as evidenced by the 2012 National Assessment of Educational Progress (NAEP) Writing Report Card, many students struggle to write argumentative texts that are evaluated as proficient (Crossey, 1). To better support student writing and assessment of that writing, PERSUADE 2.0 may offer a systematic approach.

This review underscores the 2.0 corpus's potential to enhance automated writing evaluation systems (7) and to inform classroom interventions that can foster equitable writing practices. Specificall, the demographic data included in the corpus can provide data for the examination of potential biases in writing assessment, in th effort to support fairnes in educational outcomes. As Crossley et al. emphasize the features of PERADE 2.0 may offer deer insights into the stucture of student arguments, as well as conceptions of quality related to argumentation for secondary school level writing. The affordances of the 2.0 corpus are particularly relevant for considerations of valiity, fanes, and equty in writing ssessment. Scifically, Crossles review offers a thoughtful consideration of possibilities and limitations of the PERsuADE 2.0 corpus to support more valid and fair asessment practices. As they write, while such systematic \*analyses can provide information on argument construction and eficacy that inform classroom practices (1), se. mantic \*aspects ofdiscourse make agrement for human raters dificult to obtain and may mpact validity and farness when modeling the data. Because \*[ow reliability in the human ratings may transfer into automatic writing evaluation systems developed using the PERSUADE 2.0 corpus' there are important \*bias or validity concerns" to addres in the kinds of data and "feedback such systems provide" (9). By providing detailed annotations and effectiveness scores, PERsUADE 2.0 can facilitate more nuanced analyses of argumentative elements, contributing to a better understanding of construct validity in teaching and assessing specific writing skill. Furthermore, PERUsADE 2.0 is an open source corpus, enhancing possbilities for more equitable access for users.

Next, in "EvaluMate: Using AI osupport students feedack provision in peer assessment for writing," Dr. Kai Guo of The University of Hong Kong introduces EvaluMate, an innovative Al-supported peer review system designed to enhance the quality of qualitative feedback in writing assessments. EvaluMate leverages large language models (LMs) to provide real-time, iterative feedback on the comments made by student reviewers. As Guo describes, this approach aims to address the common chllenges faced by students by guiding them in generating more thoughtful, constructive, and supportive feedback

In writing classrooms, peer feedback is a crucial element, supporting both feedback providers and recipients by fostering critical thinking, self reflection, and collaborative, process-based learning. Despite its importance, Guo explains, the effectiveness of peer feedback is sometimes limited by isues related to students' sense of how to craft purposeful, meaningful commentary for their peers, concerns relatto interpersonal relationships, and moivation eg., studets' sense of whether their fdack willoffer a meaningful contribution to their peers and/or whether it willsupport ther revisions). In efforts to help address these isues, researchers have explored various methods to enhance feedback qualit, ncluding AI technologies. However, these fforts have focused somewhat more on quantitative assessments, leaving more opportunities for the evaluation of qualitative feedback.

EvaluMate's integration into the classoom has  few important implications for validity, fairnes, and equity in writing asesment. By supporting students in effrt to provide meaningful feedack to their peer, it aims to enhance asessment processvalidity and to promote fairness by providing consistent and evaluations.

Furthermore, as Guo's review ilustrates, EvaluMate aims to support a more thoughtful and responsive peer review process. The system's feedback mechanisms can assist students as they refine comments and develop feedback practices, fostering a deeper engagement with the material and their peers. This iterative feedback loop may, then, cultivate a culture of mutual support and process-based writing delopment. EaluMate's abilit t support interity and farness in the er-review process" (9), in turn, may support more robust construct validity for student writing and feedback insofar as it centers specific, rather than generalized writing purposes, revision processes, and intentions in the feedback that they provide.

Leveraging AI to bolster the quality and impact of peer fedback, EvaluMate's focus on qualitative feedback addreses a critica need in writing assessment, one that may support students' learning experiences and success in approaching learning goals.

Closing out this year's forum is "Analysis and Recommendation System-based on PRIsMA Checklist to Write Systematic Review," by Smita Agrawal, Parita Oza, Riya Kakkar, Vishv Jetani, Jatin Undhad, Sudeep Tanwar, and Anupam Singh of Nirma University Institut f Technology in Ahmedabad, Gujarat India. Here, Agrawal et al. review a web application designed for bibliometric analysis of research papers. As these authors emphasize, the range of affordances this application carres can offer a number of pathways towards accessbility. For instance, the application employs Python and Flas, as wel as a number of generative data visualization toos in order to hance user ccessbility for a wide range of stakeholders, including students, rearchers, and academic insttions. The authors' atention to various potential uses by different users and stakeholders draw attention to the potential high degree of content validt that this web application may support, especily for supervisors asessing student rearch writing. In ddtion, asthe review authors note, the web aplicatio is designed to support transparency and securit for researchers, in effrt to support more equitable and accessible approaches to systematic review writing (21).

For student writing asessment, this analysis and recommendation tool offers a few unique possbilities: For example, it positins student witersa the first asors ofthr wor offig utut da ae  for instae, rce sction qualit. y eabling self-assesment, this feature may support more fir evaluation practices as undergraduate and graduate student writers are mentored into research publication practices.

Of course, no single assessment design or tol can, on its own, ensure farness, acesibilit, and equity. Helpfull, grawal et al. identify a few key imitations of the web appication towards these aims. For instance, they note that constraints in current visual features, surity mesures, and he application's dedency on Bibe files may limit the degre f aesbilit. y addressing these imitations, Agrawal et al. offer a thorough consideration of future opportunities in tool development to support more equitable platforms for source analysis and writing within academic and research contexts.

# 7. Closing

The current moment, characterized by advancements in AI, LMs, and technology, offers a distinct opportunity to reflect on the potential for asessment tools and technologies to support more inclusive, just assessment practices, ones that are reflective of the diverse experiences and abilites of all students. Considering asessment systems in this context allows us to explore how validity, fairness, and equity intersect with innovations in writing assessment technology.

We remain committed to the ongoing pursuit of validity, fairness, and equity, recognizing their importance in creating meaningful and just educational experiences for al students. The contributions to this year's Tools & Technology forum exemplify these values, offering reviews of three inovative tols that can support these principles in shaping the future of writing assessment.

We close with thanks to this year's reviewers and to you all for willingnessto engage with these important questions. We look forward to future submissions to help us continue to support an adaptive, inclusive, fair approach to writing assessment.

# Author note

We have no conflicts of interest to disclose.

# CRediT authorship contribution statement

Kelly Hartwell: Writing - original draft, Writing - review & editing, Laura Aull: Supervision, Writing - original draft, Writing - review & editing

# Data availability

No data was used for the research described in the article.

# References

Banks, A.J. (2006). Race, rhetoric, and technology: Searching for higher ground. Mahwah, NJ: Lawrence Erlbaum.   
Cushman, E. (2016). Decolonizing validity. Journal of Writing Assessment, 9(1).   
Elliot, N. (2016a). A theory of ethics for writing assessment. Journal of Writing Assessment, 9(1).   
Eot . 16i  t ol t o Psychological Association, and National Council on Measurement in Education]. Collge Composition and Communication, 66, 668-685.   
Embretson, S. (1983). Construct validity: Construct representation versus nomothetic span. Psychological Bulletin, 93, 179-197.   
Fali  n  d-a  024)    e   hs mt n unirit tc m prct.t n  g o,93) 36375. //.g/1080/60293.2023.2246   
Greenberg, K. (1992).Validit and reliability ssues in the direct asessment of writing. WPA: Writing Program Adminisration, 16(1-2), 7-22.   
Hammond, J.W. (2019. Mking or invisible racial gdas visie: Rce tlk in Assin ritig, 1994-2018. Assng Writing 42, ricle 100425.   
Hughes, Arthur (1995). Testing for Language Teachers. Seventh Printing. Glasgow: Universty of Cambridge. Bell & Bain, Ltd,   
Huot, B.(19. liait, vt, a listc soig W w  a  w neo . l ion nd ion , 201-213.   
Htson, J    r,  (2024). ig  n gish cotio hs aion i hrid l pct. io o Changes in Education, 1(1), 19-31.   
Moss, P. A. (2004). The meaning and consequences of "reliability. Joundl of Educational and Behavioral tatisics, 29(2), 245-249. Routledge.   
Poe, M &  . r 06. hs awg t  th  t cha a as mat al mpac. Journal of Writing Assessment, 9(1).   
Poe, M., & Ello, N. (2019). vidence f fairness Twety-five yers of reearch inassesin writing.Assing Writing, 42, Article 100418.   
Po  16   uice t s 192.   
ndall J,  , lomp,  liveri,  (2024).r validit s ike ustce.  yore etig 41(1), 203-219. /o./01177 02655322231202947   
Randal, J. m , P,  liri,  (2023.rin wt surc i  d  justic-rid, rcistvalt rmerk di  i    8.. org/10.1080/10627197.2022.2042682   
Siddiek, A.G. (2018). The impact of test content validity on language teaching and learning. Available at SsRN 3180269.   
Slo,  014rk  t i  an study. Research in the Teaching of English, 48, 276-302.   
Sho (December). Conference Psychology and Flourishing Humanity (PFH 2023) (pp. 174-193). Atlantis Press (December).   
Toth . 0 f tr    .   sic, and the advancement of opportunity, 137-170.   
Williamson193. iti  stc co  iril d  t in   si Writing assessment: Theoretical and Empirical Foundations, 1-43.   
Yancey, K B. (1999. Looking back as we loo forward: Historicizing writing asssment Colle compositio and Communication, 50(3), 483-503.

# Works referenced

Aull L. 2023). You can't write tat 8 Myths about corrct English. Cambridge University Press.htps://doi.org/10.1017/9781009231299   
r  , it,   c 209.   lt.     p 135-170). Charlotte, NC: Information Age Publishing.   
Elliot, N. (2005). On a scale: A social history of writing assessment in America. New York, NY: Lang.   
Fulcher, G. (199. Assesment in English foracademic purposes: Putting content validit in its place. Applied Lingusics, 20(2, 221-236.   
H,   . i t    tin Teaching, 7(3), 480-498.

Kell   th  ih  t t    pra administration, writing assessment justice, language ideologies, and metacognition.

You can't write that: 8 myths about correct English.

Kelly Hartwell\*,1, Laura Aull2 University of Michigan, Joint Program in English and Education and English Department, USA \* Correspondence to: c/o JPEE 610 E. University Avenue Room 4204 Ann Arbor, MI 48109-1259, USA. E-mail address: khartw@umich.edu (K. Hartwell).