# Exploring the multi-dimensional human mind: Model-based and text-based approaches

Min Kyu Kima,\*, Jinho Kimb, Ali Heidarib

a Georgia State University, 238 College of Education and Human Development, 30 Pryor St sW, Atlanta, GA 30303, USA b Georgia State University, 30 Pryor St SW, Atlanta, GA 30303, USA

# ARTICLEINFO

# ABSTRACT

Keywords:   
Mental models   
Summarization   
Knowledge structure   
Model-based   
Text-based

In this study, we conceptualize two approaches, model-based and text-based, grounded on mental models and discourse comprehension theories, to computerized summary analysis. We juxtapose the model-based approach with the text-based approach to explore shared knowledge dimensions and associated measures from both approaches and use them to examine changes in students' summaries over time. We used 108 cases in which we computed model-based and text-based measures for two versions of students' summaries (i.e., initial and final revisions), resulting in a total of 216 observations. We used correlations, Principal Components Analysis (PCA), and Linear Mixed-Effects models. This exploratory investigation suggested a shortlist of text-based measures, and the findings of the PCA demonstrated that both model-based and text-based measures explained the three-dimensional model (i.e., surface, structure, and semantic). Overall, model-based measures were better for tracking changes in the surface dimension, while textbased measures were descriptive of the structure dimension. Both approaches worked well for the semantic dimension. The tested text-based measures can serve as a cross-reference to evaluate students summaries along with the model-based measures. The current study shows the potential of using multidimensional measures to provide formative feedback on students' knowledge structure and writing styles along the three dimensions.

Scholars have explained human mind as an internall represented, structural mental model that a person develops to understand an external problem or new information while manipulating their preconceptions (e.g., Helbig, 2006; Johnson et al., 2011; JohnsonLaird, 2005; Norman, 1983; Seel, 2003). For example, theories of discourse comprehension postulate that deep comprehension emerges while learners develop a comprehensive and coherent mental model of the text (Kim & McCarthy, 2021a; Graesser t al., 1994; Kintsch, 1988). Learners leverage their prior knowledge (preconceptions) to identify and assciate essential ideas located in various parts of the text. Summarization is a pedagogical tol that helps learners develop cohesive and solid mental models f the text as well as evaluate the quality of thir understanding (Kim, 2015; Graham et al., 2013; Pinay-Dummer & Ifenthaler, 2011). Summaries re-represent a learner's mental representation of the text, composed of ideas selected from the text and the relations inferred by the learner (Kim, 2012; Johnson-Laird, 2005; McNamara & Magliano, 2009).

To asses reading comprehension, some scholars focus on extracting information from learners' mental representations of atext, While others analyze the lexical and syntactic structures of the text. We named the approach taken by the first group of scholars as the model-based approach. For example, Sugrue (1995) proposes the thre dimensions of mental model, icluding concepts, links among concepts stuated in conditions, and principles as meaning emerging from the whole knowledge structure. Scholars have articulated this three-dimensional model (Kim, 2015; Ifenthaler, 2010). The other group of scholars has focused on the lexical components of the text that can refer to dimensions of mental models (Bransford & Franks, 1972). We grouped these trends as a text-based approach. Despite the shared theoretical ground and potential overlaps, the two approaches have received scant review in relation to each other. In this current exploratory work, we juxtapose the model-based approach with the text-based approach to explore a shared-dimensional model and examine the potential to expand the interpretation of learner text comprehension.

Methodologicall, few studies have compared indice from two different approaches, imiting the potential of describing, diag. nosing, and supporting learners' knowledge construction in multiple aspects. We need to expand reliable and valid methods and measures guided by a sound asssment framework. The model-based approach analyzes a concept map elicited from a summary text. A concept map consists of concepts and relations, considered as a re-represented individual's internal mental models (Kim & McCarthy, 2021b; Clariana, 2010; Novak & Canas, 2006). We can generate descriptive indices from a concept map and comparea leaner concept map to an expert concept map. The resulting similarity measures can explain how close a learner model is to an expected expert model (Kim & McCarthy, 2021b; Kim, Clarianay et al., 2018; Ifenthaler, 2014).

In contrast, text-based approaches focus on linguistic features of learners' written responses regarding words, sentences, and paragraphs used in a document (Crossly et al., 2016; McNamara et al., 2014; Rus et al., 2013). The current natural language processing (NLP) techniques allow us to automatical analyze the lexical components of the text and their ways of organizing th text, generating many indices. In this current study, we explore various indices at multiple levels and attempt to group them into shared conceptual dimensions.

Pedagogically, a comprehensive assessment model and a broad collection of measures can enable us to accurately diagnose multidimensional learner knowledge and provide adaptable support in various aspects of individual learners' summaries, including underlying knowledge structure and writig styles (McCarthy et al., 2006). Writing a summary of the text can be a means of leaning new information and formative assessment and feedback (Jonassen et al., 1993; Kintsch, 198). However, due to the complexity and difficulty of timely reviews of learners' summaries, teachers evaluation is often lagged or almost impossible in classrooms (Dunlosky et a., 2013; Graham et al., 2013; Li et al., 2018). We view that the firs step to addressing this problem is to identify proper measures and ways to interpret them based on a sound framework.

In response to McNamara and Magliano (2009) calling for text comprehension study by scholars outside literacy studies, this current study builds on our previous study (Kim et a., 2020b) to further examine how model-based and text-based measures form a multi-dimensional framework and to investigate how measures derived from the two approaches are sensitive to changes in learners summaries in multiple dimensions. Also, the two models are expected to validate each other while explaining unique features of learner knowledge structures inferred from thir wrtten responses. To do that, we review and classfy recent automated summary evaluation tools and methods according to the two approaches. Then, we choose a too from each approach to calculateassociated measures. The following research questions guide the study:

1. How do the text-based measures explain the three-dimensional model?   
2. To what extent do the model-based and text-based measures explain changes in learner summaries?   
3. To what extent are the model-based and text-based measures correlated along the dimensions?

# 1. Literature review

# 1.1. Summary as a knowledge representation

One of the most prevailing sources people acquire new information is text (Singer & Alexander, 2017; Stevens et al., 2019) Readings (e.g., book chapters, articles, and blogs) are often required for learners to participate in complex problem-solving activities and deeper discussions. Writing summaries of what learners have read is a common practice in classrooms. Summarization is a means for learning (Km et al., 2020b; Dunlosky et l., 2013) and accessing what a learner has understood (Het al., 2009; Li et l., 2018; Sung et al., 2016; Westby et al., 2010).

Writing summaries is a knowledge-telling activity involving diverse cognitive processes (Scardamalia & Bereiter, 1986; Singer & Alexander, 2017). Summarizing requires readers to sft through large unts of text, ifferntiate essential from unimrtant idas, and then synthesize those ideas and create a new coherent text" (Dole et al., 1991, p. 24, as cited in Duke & Pearson, 2009). Learners are oriented toward and receive information from instructional materials (e.g., reading text). Their passive learning turns to active manipulation, such as underlining/highlighting parts of atext. These learning behaviors help leaners extract critical information from the reading and build connections among ideas from the text and prior knowledge. Interacting with instructors, peers, or feedback technology, learners can develop their summaries accurately, cohesively, and concisely (Chi & Wylie, 2014).

Scholars have used mental models to describe how learners construct their understanding of the text they have read (Garnham, 2001; Helbig, 2006; Johnson et al., 2011). Learners leverage their preconceptions associated with the topic to determine and connect essential ideas from the text and construct a new mental model ituated to the reading (Graesser et al., 1994; Johson-Laird, 2005). A learner write a summary, puting the text into their own words (Stevens et al., 2019). Summaries are re-represented mental models that learners have internalized as they have read (Kim, 2015; Kintsch, 1988). For this reason, we can conjecture learners' mental models by evaluating their written summary of a text (Kim, Clarianay et al., 2018; Westby et al., 2010).

# 1.2. Model-based vs. text-based approaches

The summary assessment builds on the assumption that summaries can be a means to re-represent learners' mental models of the text. Grounded on that premise, both model-based and text-ased approaches hae utilied technologiesto asses learer summaris i. e., text) with a somewhat different focus, such as elicited mental models and linguistic features of summarie, respectively. For example, natural language procesing (NLP) techniques have been widely deployed to analyze the syntactic structure of written summaries to identify individual linguistic components and relations of words featured in the text (Beamer et al., 2008; Zouaq et al. 2011). In this study, we summarize the two approaches (model-based vs. text-based) regarding associated automated summary evaluation (ASE) tools, assessment targets, theoretical dimensions, common measures, and advantages (see Table 1).

# 1.3. Model-based approach

The model-based approach utilizes a concept map--a network of concepts eicited from a written response-to evaluate an underlying mental model of the summary (Axelrod, 1976; Jonassen et al., 1993). Model-based ASEs include Automated Knowledge Visualization and Assessment (AKOVIA; Ifenthaler, 2014), Graphical Interface of Knowledge Structure (GISK; Kim, 2018), Highly Integrated Model Asessment Technology and Tools (HIMAT; Pirnay-Dummer & Ifenthaler, 2011), and Student Mental Model Analyzer for Research and Teaching (SMART, Kim et al., 2020c). These model-based toos follow these steps: (a) extracting text variables, (b) creating a concept map based on the text variables, and (c) comparing a learner model to a reference model.

More specificall, these ASEs use NLP techniques to elicit a concept map as a learner model from a summary. For example, the NLP dependency analysis enables ASEs to identify concepts and semantic relations between concepts g., \*the proteins [concet 1] n the protein coat [concept 2). NLP can rerieve text variables (e.g., concepts and semantic relations) from a summary to elict learners knowledge representations. Learner models are evaluated by comparing a reference model that a human expert ofen predetermines (Kim & McCarthy, 2021a; Coronges et a., 2007; Goldsmith & Kraiger, 1997; Kim, 2018). Such a comparison generates a variety of similarity measures, ranging from 0 (completely different) to 1 (perfectly identical), used to indicate the quality of learners summaries.

Notably, scholars in the model-based approach have posited that mental models take a multi-layered or multi-dimensional structural form (Kim, 2012, 2015; Clariana, 2010; Ifenthaler, 2014), including three dimensions: surface, structural, and semanticso-called 3 S knowledge structure dimensions (Kim, 2012). The surface dimension describes how many concepts and relations constitute a concept map. The structural dimension indicates the shape of a concept map determined by how concepts areassociated in the network (e.g., connectedness. The semantic dimension explains what specific concepts and their relations reside in a model. A learner's mental model quality can be described and evaluated in these dimensions. Scholars have demonstrated that model-based ASE tools could detect changes in learners' summary revisions (Kim et al., 2020c; Kim & McCarthy, 2021a; Clariana, 2010; Kim, 2018; Pirnay-Dummer & Ifenthaler, 2011). For example, sMART generates model-based similarity measures along the three dimensions that change as learners revise their summaries (Kim & McCarthy, 2021a, 2021b).

We use SMART because of the following advantages. First, the highest number of model-based similarity measures corresponding to the three knowledge structure dimensions are available on SMART. Second, previous studies have reported that SMART feedback on what concepts and relations learners consider in their summaries help learners develop a more appropriat and cohesive summary of the text (Kim & McCarthy, 2020a, 2021a). Lasty, MART analytics trace critical changes in leaner knowledge structure as they revise their summaries (Kim & McCarthy, 2021b).

# 1.3.1. Text-based approach

The text-based approach uses NLP-driven analytics to provide the linguistic features of learners summaries regarding the usage of words, word choices, the types of words, text coherence and cohesion, grammatical erors, etc. (Allen et al., 2019; Gao et al., 2019; Sung et al., 2016). Going beyond these word-level metrics, current NLP can analyze the content overlap between the target and the source text, redundancy, and topic releance that indicate the overall qualit of summaries (Strobl et al., 2019; Sung et al., 2016) For example, Latent Semantic Analysi (LSA) is astatistical method to determine word relations based on a hypothetical semantic space in which concepts closer to one another locate closer (Deerwester et al., 1990; Landauer & Dumais, 1997). LSA can compute word similarity within a summary and provide the semantic relation between a learner summary and a reference text (Sung et al., 2016). N-gram is a typical machine-translated linguistic property (He et al., 2009; Lin, 2004). N-gram means a sequence of N words from a given text. For example, \*performance evaluation critria" is a 3-gram. The n-gram co-occurrences between sentences or paragraphs can be used to indicate the diversity, density, or cohesion of the text. Text-based ASEs evaluate summaries on hundreds of NLP indices (Crossley et al., 2016; Kyle et al., 2018).

Table 1 Comparison of Model-Based and Text-Based Approaches.   

<html><body><table><tr><td></td><td> Model-Based</td><td>Text-Based</td></tr><tr><td>Tool</td><td>AKOVIA (Ifenthaler, 2014); GISK (Kim, 2018); HIMATT ( Pirnay-Dummer &amp; Ifenthaler, 2011); SMART (Kim &amp; McCarthy, 2021a)</td><td>Coh-Metrix (McNamara et al., 2014); SEMILAR (Rus et al., 2013); Summary Street (Wade-Stein &amp; Kintsch, 2004); Online Summary Assessment and Feedback System (Sung et al., 2016); Crowd-source summary evaluation (Li et al., 2018); ROUGE (Lin, 2004); PryEval (Ga0 et al., 2019); TAACO ( Crossley et al., 2019).</td></tr><tr><td>Assessment Target</td><td>Concept Mape</td><td>Text (word, sentence, and discourse)</td></tr><tr><td>Dimension</td><td>Surface Structure</td><td>Surface Code Textbase</td></tr><tr><td></td><td>Semantic</td><td>Situational Model</td></tr><tr><td>Key Measure</td><td>3 S Indices</td><td>LSA cosine, N-gram co-occurrence</td></tr></table></body></html>

Text-based tools include Coh-Metrix (McNamara et al., 2014), SEMILAR (Rus e al., 2013), Summary Street (Wade-Stein & Kintsch, 2004), Online Summary Assessment and Feedback System (Sung et al., 2016), crowd-source summary evaluation (Li et a., 2018) Machine-translated tools-- which typically utilize N-gram models to translate learners' written artifacts, such as evaluating the overlap of N-gram words between a learner's summary and a reference summary-include ROUGE (Lin, 2004), PryEval (Gao et al., 2019), and Tool for the Automatic Analysis of Cohesion (Tco, Crossley et al, 2019). Many of thee text-ased tool offer hundreds of indices that charactrize linguistic fatures in lerners w resonses at ifferent level, including words, sentences, and entire documents. These measures can be utlized within ASE or Automated Writing Evaluation (AWE) systems, as well as by educators, to inform learners about the diversity, density, and cohesion of their writing styles.

Similar to the 3 S knowledge structure dimensions in the model-based approach, scholars in discourse comprehension generally agree that a text comprises three principal levels: surface structure, textbase, and situational model (McNamara & Magliano, 2009). The surface level, ften refered to as the surface code, focuses on words and their syntacti associations in the text, corresponding to the surface structure in the model-based approach (Katz & Postal, 1964). The textbase level relates to the cohesion of the text, represented through propositions consisting of predicates and arguments. The levels of idea connection are reflected in argument overlaps within and across propositions (Kintsch, 1998). These overlaps can be conveyed through lins in a concept map, as described in the model-based aproach. Lastly, the situational model emerges from the construction of a cohesive and elaborated understanding of the text, based on the surface and textbase structures that correspond to the denotation and connotatio of the wrtten response (Bransford & Johnson, 1972; Gentner & Medina, 1998; Graesser et al., 1994; Kintsch & van Dijk, 1978). In the context of summarization, the situational model refers to the information primed by the context and subsequently retrieved from the text (Kintsch, 1998)

Scholars have explored these levels using tools within the text-based approach. For instance, Graesser et al. (2011, 2014) intro duced a multilel theoretical framework that encompases words and syntax, which can correspond to thesurface code, textbase, and situational model, among other categories. They examined the associations betwen these levels and text-based indices from the Coh-Metrix tol, which calculates over 200 indices of textual features (McNamara et al. 2010). According to Graesser et al. (2011), indices related to word concreteness and syntactic simplicity were categorized under the surface level. Various referential cohesion indices (e.g., word, content, noun, and argument overlap) were linked to the textbase level, while another set of cohesion indices that could explain causal relations were associated with the situational model.

In this study, we have considered the measure-level associations as proposed in the literature. However, the situational model encompasses a wide range of concepts related to deep understanding, which may involve various measures. McNamara and Magliano (2009) have argued that computational models for the situational model depend on the researcher's intention. In line with the se. mantic dimension of how learners conceptualize specificoncepts and their relations retrieved from the text within the model-based approach, we operationalized the stuational model in a summarization assgnment as the extent to which a learner's knowledge overlaps with the target knowledge model from the text. For instance, tols like ISTART (McNamara et al., 2007), Summary Street (Wade-Stein & Kintsch, 2004), and Operation ARIES (Forsyth et al., 2013) semantically comparestudents responses with the expected target text, such as a model summary.

In this study, among text-based tools, we selected TAAco because it offers a range of typical text-based indices related to the surface code (e.g., N-gram density and lexical diversity) and textbase (e.g., sentence overlap, synonym overlap, and latent semantic analysis). Similar to Coh-Metrix, TAACo also provides semantic overlap indices, such as LSA cosine match scores, which are relevant for assesing the situational model. In the following section, w will eview text-ased indice aligned with the thee dimensions shared with the model-based approach.

# 2. Methods

# 2.1. Context

# 2.1.1. SMART technology

SMART is a web-based formative assessment and feedback system that engages students in reading learning materials and summarizing their comprehension, focusing on key ideas and their propositional relationships (Kim, 2021c; Kim et al., 2023). SMART automates the analysis f linguistic data and utilizes concept maps to facilitate learners comprehension and asssment of their lels of understanding.

SMART feedback provides multimodal information. The system's automated formative assessments inform learners about the alignment of ther responses with the reference model,typically an expert's summary. Personalizd fedback messages aid learners in deciphering the concept map fedback, which graphicall reresents the expected knowledge structure, highlighting both present and absent key concepts and relationships.

Generative Al in sMART direct learners to a concept library where they can find concise definitions and examples llustrating the connections between key ideas as laid out in propositional tatements. SMART's data visulization tols uncover patterns in individual learners' revision behaviors, understanding level, and time dedicated to specific feedback components. It also provides benchmarking data against the class average, covering aspects like total study time, the number of revisions made, and achievement levels.

# 2.1.2. Setting and data sources

We gathered students' summaries of the readings from seven sections of an 8000-level master's online course in Learning Sciences, offered over six consecutive semesters from Fall 2018 to Summer 2020, including two sections in Fall 2019. On sMART, the students wrote summaries of assigned readings.

Reading texts were chapters of the textbook, each of which was 10 to 12 pages long and composed of approximately 7000 words Students read assigned readings and wrote summaries on SMART with a maximum of 300 words per reading. The word limit was set to guide students to focus on key concepts of the chapter and develop a concise but thorough summary. SMART compares astudent summary to a reference model (i., a model summary). SMART comparison generates formative asssment outcomes (.g., similarity measures) that drive multimodal feedack, including personalized feedack messages, concept maps of the expert and student model, and a list of concepts and relations to consider further (se Fig. 1). The following procedure created reference models for readings. Two doctoral students independently drafted, discussed differences in drafts, and re-wrote an ideal summary of each reading togeter. Lastly, the course instructor reviewed and approved the proposed versions. On SMART, students could refer to SMART feedback to revise their summaries as often as desired. Previous studies (Kim et al., 2020b; 2020c; Kim & McCarthy, 2021a) demonstrated that SMART feedback helped students to build more accurate and concise summaries.

As seen in Table2 62 out of114 students enrolled in the seven clases offered in the six semesters agred to participat in the study. During each semester, these students submitted summaries of the readings on SMART. It's worth noting that there were seven sum. mary assignments in the first two semesters and two summary assignments in the lattr four semester, with different numbers of revisions based on individual decisions. Therefore, the number of summary assignments and the number of revisions per summary depended on individual students and their respective semesters.

In this study, we specifically focused on summarie that were revised at least once to examine changes between the fist and the last versions of the students summaries, which we referred to as cases. Among the participants reading summary ssignments on SMART, there were a total of 115 cases that included more than one revision of their summaries. However, upon data preview, we found that 7 cases were inappropriat for analysis due to various reasons such as no edits made, copied expert examples or only a single sentence provided. onsequently, we omitted these caes, leaving us with 108cases for the current stud, resulting in a total of 216 observations for analysis (including both the first and the last versions).

![](img/226e6649878849975c850d6e6e0759748ab3e3d1f01ec54539bbcf1040bbe7b9.jpg)  
Fig. 1. SMART Feedback UI.

Table 2 Participants and Selected Cases.   

<html><body><table><tr><td>Semester</td><td>Enrollment</td><td>Participants</td><td>Cases*</td></tr><tr><td>Fall, Year 1</td><td>21</td><td>13</td><td>21</td></tr><tr><td> Spring, Year 2</td><td>20</td><td>10</td><td>21</td></tr><tr><td>Summer, Year 2</td><td>16</td><td>1</td><td>1</td></tr><tr><td>Fall, Year 2 - A</td><td>19</td><td>15</td><td>28</td></tr><tr><td>Fall, Year 2 - B</td><td>17</td><td>12</td><td>18</td></tr><tr><td> Spring, Year 3</td><td>11</td><td>6</td><td>9</td></tr><tr><td>Summer, Year 3</td><td>10</td><td>5</td><td>10</td></tr><tr><td>Total</td><td>114</td><td>62</td><td>108</td></tr><tr><td>Reading</td><td></td><td>-</td><td>Cases</td></tr><tr><td>R1</td><td></td><td></td><td>6</td></tr><tr><td>R2</td><td></td><td></td><td>10</td></tr><tr><td>R3</td><td></td><td></td><td>39</td></tr><tr><td>R4</td><td></td><td></td><td>4</td></tr><tr><td>R5</td><td></td><td></td><td>7</td></tr><tr><td>R6</td><td></td><td>-</td><td>4</td></tr><tr><td>R7</td><td></td><td></td><td>5</td></tr><tr><td>R8</td><td></td><td></td><td>33</td></tr><tr><td>Total</td><td></td><td>-</td><td>108</td></tr></table></body></html>

Note. \* The number of summary assignments and the number of revisions per summary depended on individual students and their respective semesters.

Table 3 Dimensions and Indices of the Model-Based and Text-Based Approaches.   

<html><body><table><tr><td></td><td colspan="2">Model-Based Approach</td><td colspan="2">Text-Based Approach</td></tr><tr><td>Dimension</td><td>Index</td><td>Definition</td><td>Index</td><td>Definition</td></tr><tr><td>Surface (Surface</td><td>Number of concepts</td><td>Compare the number of concepts (nodes) in two models</td><td>Lexical Diversity</td><td>Measure of lexical diversity of the ratio of unique (type) to all words (tokens)</td></tr><tr><td>Code)</td><td>Number of</td><td>Compare the number of links</td><td>Lexical Density</td><td>Lexical density indices indicate the proportion of the text that consists of content words (nouns, lexical verbs,</td></tr><tr><td></td><td>relations Density of</td><td>(edges) in two models Compare the density of the two</td><td>N-gram Density</td><td>adjectives, and adverbs derived from adjectives). Th proportion of unique (type) bigram/two-word phrases</td></tr><tr><td>Structure</td><td>graphs Average Degree</td><td>models Compare the average number of</td><td>Adjacent</td><td>or trigram/three-word phrases to all lexical bundles.. The number of words that occur at least once in the next</td></tr><tr><td>(Textbase)</td><td></td><td>degrees in two models</td><td>Sentence Overlap</td><td>one or two sentences divided by the number of words considered.</td></tr><tr><td></td><td> Mean Distance</td><td>Compare the mean distances in two models</td><td>Binary Adjacent Sentence Overlap</td><td>The number of sentences with any word overlap with the next one or two sentences divided by the number of</td></tr><tr><td></td><td>Diameter</td><td>Compare the largest geodesics in</td><td> Synonym Overlap</td><td>sentences considered. Defined as the average sentence to sentence overlap/</td></tr><tr><td></td><td></td><td>two models</td><td>Latent Semantic</td><td>similarity of synonyms. The vector space model based on word co-occurrences</td></tr><tr><td></td><td></td><td></td><td>Analysis (LSA)</td><td>within documents which establish relationships between concepts.</td></tr><tr><td></td><td></td><td></td><td>Latent Dirichlet Allocation (LDA)</td><td>Documents as mixtures of topics with related concepts having similar topic probabilities based on underlying co-</td></tr><tr><td></td><td></td><td></td><td>Word2Vec</td><td>occurrence patterns. Words co-occurring in similar contexts are represented closer, while words with dissimilar contexts are</td></tr><tr><td>Semantic</td><td>Concept</td><td>Compare semantically identical</td><td></td><td>represented farther apart in different regions of the vector space.</td></tr><tr><td>(Situational)</td><td>Matching</td><td>concepts, including contextual and principle variables</td><td>Source-Similarity</td><td>Similarity score between target text and source text (LSA, LDA, and Word2Vec).</td></tr><tr><td></td><td>Propositional Matching</td><td>Compare fully identical propositions (edges) between two</td><td></td><td></td></tr><tr><td></td><td></td><td>concept maps The proportion of key concepts that</td><td></td><td></td></tr><tr><td></td><td>Recall-C</td><td>appear in a student summary</td><td></td><td></td></tr><tr><td></td><td>Recall-P</td><td>The proportion of key relations that</td><td></td><td></td></tr></table></body></html>

Note. Defiions citd fro oss t al. 2019) Ltt icht tion LDA) ina thxtn  whch th ts in t entees have similr topic proabilities a n uderlying coccurrence tens. Worde is  simirty index that ribes the extet to which words co-occur in similar or dissimilar contexts in different regions of the vector space (Mikolov et al., 2013).

The SMART system produced the model-based measures, while we used the summaries submitted to SMART for TAACO analysis, which generated text-based measures. Its important to note that participants' demographic information was not collected; only their summaries and measures were utilized for the study.

# 2.2. Measures

# 2.2.1. Model-based measures

Table 3 summarizes model-based similarity measures and their corresponding knowledge dimensions. In a model-based approach, a concept map extracted from a student's written response represents that student's knowledge structure of the reading. We can use a concept map to generate model-related information (Coronges e al., 2007; fenthaler, 2010; Wasserman & Faust, 1994). For example, diameter is an index of a concept map, indicating the largest geodesic found in a concept map. We can compare a diameter value of a student model to an expert model's dameter. A diameter similarit takes a value between  (entrely dissimilar to 1 (identical) and is considered as a similarity measure of the structural dimension.

The measures for the semantic dimension use text variables (specific words and their inks). Concept matching and propositional matching scores are computed, using the conceptual similarity formula (Tversky, 1977):

$$
s = \frac { f ( \mathbf { A } \cap \mathbf { B } ) } { f ( \mathbf { A } \cap \mathbf { B } ) + \alpha \bullet f ( \mathbf { A } - \mathbf { B } ) + \beta \bullet f ( \mathbf { B } - \mathbf { A } ) }
$$

where the features of object A (the learner model) are weighted more heavily than object Bs (the reference model) features, with $\alpha$ taking a heavier weight than $\beta$ $\langle \alpha = 0 . 7$ and $\beta = 0 . 3$ . In contrast, Recall-P and Recall-C measures indicate the proportion of key concepts and their links observed in the learner model, respectively.

In previous studies (Kim, 2015), we performed Exploratory Factor Analysis (EFA) to test the three dimensions of mental models Results showed the presence of thre atent factors surface, structure, and semantic) and associated similarity measures. For the current study, we used 10 similarity indices suggested by Kim and McCarthy (2021b). Each similarity measure corresponds to one of the 3 S dimensions (see Table 3).

# 2.2.2. Text-based measures

We anticipate that text-based measures can be classfied into the 3S dimensions. TAACO generates hundreds of indices associated with diversty, diversity, and overlap (Cheung & Jang, 2019; Skalicky, 2019). In this exploratory study, we referred to the three theoretical dimensions (i.., surfce code, textbase, and situational model) mentiond in the literature section to select measures from among the over 200 TAACO measures. Table 3 also provides asummary of text-based measures organized into these three theoretical dimensions.

Eight measures asociated with the surface code provide insights into the lexical components and syntax of the text data. For example, Lxical density measures the roportion of content words (i.e., nouns, vers, adjectives, and adverbs) in the text. There are 24 textbase measures that describe how words are organized, overlapped, and co-occur in the tet, as proposed by Katz and Postal (1964) For instance, overlap measures explain the extent to which text components are repeated across sentences. Semantic overlap en. compasses various aspects of the relationships among concepts in the text. This includes the use of Latent Semantic Analysis (LSA), a statistical dimensionalit reduction technique that places extracted words in a multi-dimensional hypothetical semantic space It also involves Latent Dirichlet Allocation (LDA), which measures the extent to which concepts in adjacent sentences share similar topic probabilities based on underlying co-occurrence patterns, and Word2Vec, a technique that represents words in a vector-space model, positioning words with similar contextual usage closer together and those with dissimilar contexts further apart in the vector space. The situational model is assciated with thre source similarity measures, which describe how well a target text's content coverage compares to a source text. These measures are computed using the same techniques: LSA, LDA, and Word2Vec.

# 2.3. Data analysis

Previous studies have proven the 3 S dimensionality based on model-based measures (Kim, 2012, 2015; Kim & McCarthy, 2021a). Therefore, the current research focused on text-based measure. We first reviewed corrlations among pre-selected text-based measures in each dimension to determine a final set of measures for further analyses. Then, we conducted Principal Components Analysis (PCA) to reduce dimensionality and uncover the underlying structure of the text-based measures (Field et al., 2012).

Additionally, this study utilized linear mixed-efect (LME) models to examine the extent to which each measure from the model. based and text-based approaches changed from the first to the last summary versions. This analysis aimed to asses the capacit of the measures to detect changes in summaries through revisions. LMEs are wellsuited for analyzing longitudinal data with a nested structure, allowing for the control of random effects attributed o individuals and assignments within the study context (Bate t l., 2015). The LME model deployed in this study is depicted in the following equation:

$$
\begin{array} { r l } { Y _ { i j } = { } } & { { } X _ { i j } \beta + Z _ { i j } u i + \mathcal { E } _ { i j } } \end{array}
$$

where $Y _ { i j }$ represents the value of dependent variable (a summary measure) for the ith subject and the jth summary. $X _ { i j }$ represents the design matri fr fixed effcts th hae ostt effct acros il nd ssimets. n this st, tee fixed effcts reresent the predictor variable, which is abinary variable denoting the summary version (0 for the first version and 1 for the last version). The coefficient $\beta$ captures the change in the dependent variable, on average, associated with the binary change from the first to the last versions. $Z _ { i j }$ is the design matrix for random effects, including random intercepts and slopes for individual students and reading assignments, with corresponding vectors of random effects ui. $\mathcal { E } _ { i j }$ is the error term, capturing the variability in the dependent variable not explained by the model. Lastly, we ran the correlations to se if text-based measures were associated with model-based similarity measures along the three dimensions.

Table 4 Explored Text-Based Indices.   

<html><body><table><tr><td>Measures</td><td></td><td>Definition</td><td>Dimension</td></tr><tr><td>T01</td><td>lemma_ttr</td><td>The ratio of unique lemmas (types) divided by total words (tokens).</td><td> Surface</td></tr><tr><td>T02</td><td>lexical_density_tokens</td><td>Percentage of text tokens that are content words.</td><td>(Surface Code)</td></tr><tr><td>T03</td><td>lexical density_types</td><td>Percentage of lemma types that are content words.</td><td></td></tr><tr><td>T04</td><td>content_ttr</td><td>The number of unique content word (Cw)* * divided by the number of total content word</td><td></td></tr><tr><td>T05</td><td>noun_ttr</td><td>lemmas. The number of unique nouns divided by the total number of noun lemmas.</td><td></td></tr><tr><td>T06</td><td>argument_ttr</td><td>The number of unique noun and pronoun divided by the number of total noun and</td><td></td></tr><tr><td>T07</td><td>bigram_ lemma_ttr</td><td>pronoun lemmas. The number of unique bigram lemmas divided by the number of total bigram lemmas.</td><td></td></tr><tr><td>T08</td><td>trigram_lemma_ttr</td><td>The number of unique trigram lemmas divided by the number of total trigram lemmas.</td><td></td></tr><tr><td>T09</td><td>adjacent_overlap_all_sent</td><td>The number of lemma types that occur at least once in the next sentence divided by the number of types in each sentence.</td><td>Structure (Textbase)</td></tr><tr><td>T10</td><td>adjacent_overlap_binary_all_sent</td><td>The number of sentences with any lemma types that overlap with the next sentence divided by the number of sentences considered.</td><td></td></tr><tr><td>T11</td><td>adjacent_overlap_2_all_sent</td><td>The number of lemma types that occur at least once in the next two sentences divided by the number of types in each sentence.</td><td></td></tr><tr><td>T12</td><td>adjacent_overlap_binary_2_all_sent</td><td>The number of sentences with any lemma types that overlap with the next two sentences divided by the number of sentences considered.</td><td></td></tr><tr><td>T13</td><td>adjacent_overlap_cw_sent</td><td>The number of content word lemma types that occur at least once in the next sentence divided by the number of types in each sentence.</td><td></td></tr><tr><td>T14</td><td>adjacent_overlap_binary_cw_sent</td><td>The number of sentences with any content word lemma types that overlap with the next sentence divided by the number of sentences considered.</td><td></td></tr><tr><td>T15</td><td>adjacent_overlap_2_cw_sent</td><td>The number of content word lemma types that occur at least once in the next two sentences divided by the number of types in each sentence.</td><td></td></tr><tr><td>T16</td><td>adjacent_overlap_binary_2_cw_sent</td><td>The number of sentences with any content word lemma types that overlap with the next</td><td></td></tr><tr><td>T17</td><td>adjacent_overlap_noun_sent</td><td>two sentences divided by the number of sentences considered.. The number of noun lemma types that occur at least once in the next sentence divided by</td><td></td></tr><tr><td>T18</td><td>adjacent overlap_noun_sent div_seg</td><td>the number of types in each sentence. The number of noun lemma types that occur at least once in the next sentence.</td><td></td></tr><tr><td>T19</td><td>adjacent_overlap_binary_noun_sent</td><td>The number of sentences with any noun lemmas that overlap with next sentence divided by the number of sentences considered.</td><td></td></tr><tr><td>T20</td><td>adjacent_overlap_2_noun_sent</td><td>The number of noun lemma types that occur at least once in the next two sentences divided by the number of types in each sentence.</td><td></td></tr><tr><td>T21</td><td>adjacent_overlap_binary_2_noun_sent</td><td>The number of sentences with any noun lemmas that overlap with the next two sentences divided by the number of sentences considered.</td><td></td></tr><tr><td>T22</td><td>adjacent_overlap_argument_sent</td><td>The number of noun and pronoun lemma types that occur at least once in the next sentence divided by the number of types in each sentence.</td><td></td></tr><tr><td>T23</td><td>adjacent_overlap_binary_argument_sent</td><td>The number of sentences with any noun and pronoun lemma types that overlap with next sentence divided by the number of sentences considered.</td><td></td></tr><tr><td>T24</td><td>adjacent_overlap_2_argument_sent</td><td>The number of noun and pronoun lemma types that occur at least once in the next two sentences divided by the number of types in each sentence..</td><td></td></tr><tr><td>T25</td><td>adjacent_overlap_binary_2_argument_sent</td><td>The number of sentences with any noun and pronoun lemma types that overlap with the next two sentences divided by the number of sentences considered.</td><td></td></tr><tr><td>T26</td><td>syn_overlap_sent_noun</td><td>Average sentence-to-sentence overlaps of noun synonyms.</td><td></td></tr><tr><td>T27 T28</td><td>LSA_1_all_sent LSA_2_all_sent</td><td>Average latent semantic analysis cosine similarity between all adjacent sentences. Average latent semantic analysis cosine similarity between all adjacent sentences (with a</td><td></td></tr><tr><td></td><td></td><td>two-sentence span).</td><td></td></tr><tr><td>T29</td><td>LDA_1_all_sent</td><td>Average Latent Dirichlet Allocation divergence score between all adjacent sentences.</td><td></td></tr><tr><td>T30</td><td>LDA_2_all_sent</td><td>Average Latent Dirichlet Allocation divergence score between all adjacent sentences (with a two-sentence span).</td><td></td></tr><tr><td>T31</td><td>word2vec_1_all_sent</td><td>Average word2vec similarity score between all adjacent sentences..</td><td></td></tr><tr><td>T32</td><td>word2vec_2_all_sent</td><td>Average word2vec similarity score between all adjacent sentences (with a two-sentence</td><td></td></tr><tr><td>T33</td><td>source_similarity_lsa</td><td>span). Latent semantic cosine similarity between target text and source text</td><td>Semantic</td></tr><tr><td>T34</td><td>source_similarity_lda</td><td>Latent dirichlet allocation divergence score between target text and source text</td><td>(Situational</td></tr><tr><td>T35</td><td>source_similarity_word2vec</td><td>Word2vec similarity score between target text and source text.</td><td>Model)</td></tr></table></body></html>

Note. \* Tentative membership in the 3S dimension. Type-token rati (TTR) iscalculated by dividing the number of unique word lemmas in a text (types) by the ol number f words n a text (tokens).emmas the asfor f  word divted of their inlected foms so that , is and are reduced  le an sencethe boy cs a ffer olors"i tidtthe y ca e iffe color. Am nicat ouns and pronounces in sentences.

# 3. Results

# 3.1. Three-dimensional model explained by text-based measures

Regarding RQ1 (How do the text-based measures explain the three-dimensional model?), we first computed correlations among the pre-selected tex-based measures in each dimension to identify more meaningful variables. The correlation analysi for the longitudinal data was conducted on 218 observations. These observations comprised 108 cases, each including the initial and final versions of summaries written by 62 students.

The measures in the surface dimension showed a wide range of correlations, from $- 0 . 0 1$ to 0.96. Moderate $( 0 . 4 < r < 0 . 7 )$ or strong $( 0 . 7 < r < 0 . 9 $ positive correlations were found among variables T1 (lemma_ttr), T4 (content ttr), T5 (noun_ttr), T6 (argument ttr) and T7 (bigram lemma_ttr). Very high correlations $( r > 0 . 8 )$ emerged among T4, T5 and T6, which indicated potential multicollinearity problems. T4 was droped because of its high correlation withT1 as well Additionally, we decided to retain T5 ecause of the easier interpretation of the noun components in texts. Consequentl, the indices T1, T5, and T7 were selected for the next analysis.

In terms of the structural dimension, T9 (adjacent overlap_all sent) and T10 (adjacent overlap_binary all sent) exhibited signifi cant correlations with most measures, spanning a wide range of strengths. Despite a few weak correlations, we decided to include them in the final set due to their unique and extensive assciations with many other indice. However, other measure for al lemma types (T11 and T12) mainly displayed insignificant or negligible associations $( r < 0 . 2 5 )$ and exhibited higher correlations with T9 and T10, which could lead to potential multicollinearity. Consequently, we chose to exclude T11 and T12.

Content-words (CW) overlap measures encompassed T13- T16, while noun overlap measures were represented by T17 - T21. Most measures within these two groups exhibited moderate correlations with each other across the groups $( r > 0 . 5 )$ , indicating a linear relationship between the two sets of overlap variables. Notably, within each group, the noun overlap measures displayed stronger correlations compared to the CW overlap measures. Among the CW overlap measures, T13 displayed a strong corrlation $( r > 0 . 8 )$ with other measure (specificlly, T9 and T15) and was therefore excluded from further analysis. T14 exhibited strong correlations with other measures but did not raise concerns about multicollinearity $( r < 0 . 8 )$ . T15 appeared to be a unique measure, while T16 was closely related to T14 $( r = 0 . 8 6 )$ . Consequently, we chose to retain T14 and T15 for our analysis. Regarding the noun overlap measures, it's worth noting that ll measures except for T17 and T18 exhibited multicollinearity concerns when considered alongside the selected CW overlap measures (T14 and T15). Additionall, T17 and T18 displayed a srong correlation exceeding 0.8. Consequentl, we opted to retain T17 for inclusion in this study.

The argument overlap measures (T22 - T25) exhibited very high correlations $( r > 0 . 8 )$ with both CW and noun overlap measures (T23 and T25 with T14; T22 and T24 with T17). We decided to include only the CW and noun overlap measures due to concerns about potential colinearity and for the purpose of variable reduction. T26 (syn_overlap_sent noun) displayed moderate correlations $( 0 . 4 < r < 0 . 7 )$ with most of the structure measures. We decided to include T26 in the final set.

The numerical measures associated with LSA (T27 and T28) and Word2Vec (T31 and T32) demonstrated moderate correlations with most of the structural measures. In contrast, LDA measures (T29 and T30) exhibited weak associations $( 0 . 1 < r < 0 . 3 )$ . Due to the high correlation within each pair of LSA and Word2Vec measures $( r > 0 . 8 )$ , we chose one measure from each pair that displayed higher correlations across LSA and Word2Vec. This selection process resulted in T28 and T31 being chosen. In total, eight measures were selected for the structural dimension, which includes T9, T10, T14, T15, T17, T26, T28, and T31.

In terms of semantic similarity, the pairs of source similarity measures, specifically T33 and T34, as well as T34 and T35, produced acceptable corrlation values of.32 and.38, respectively. However, the relationship between T33 and T35 was nearly collinear $( \mathbf { r } = 0 . 9 3 )$ J. We selected T34 (source similarity lda) and T35 (source similarity_ word2vec) for two primary reasons: (a) this pair exhibited a stronger correlation than the others, and (b) LDA (Latent Dirichlet Allocation) can effctively explain topical similarity between the source and the student text and also word2vec considers word co-occurrence in dissimilar contexts.

Table 5 Principal Component Analysis.   

<html><body><table><tr><td rowspan="2">Index</td><td colspan="4">Component Loadings</td></tr><tr><td>Communality</td><td>Component 1</td><td>Component 2</td><td>Component 3</td></tr><tr><td>T01</td><td>0.643</td><td>0.57</td><td>0.64</td><td>0.14</td></tr><tr><td>T05</td><td>0.737</td><td>0.62</td><td>0.62</td><td>0.01</td></tr><tr><td>T07</td><td>0.568</td><td>0.53</td><td>0.64</td><td>0.16</td></tr><tr><td>T09</td><td>0.780</td><td>0.81</td><td>0.20</td><td>0.30</td></tr><tr><td>T10</td><td>0.633</td><td>0.62</td><td>0.59</td><td>0.07</td></tr><tr><td>T14</td><td>0.816</td><td>0.82</td><td>0.40</td><td>0.08</td></tr><tr><td>T15</td><td>0.633</td><td>0.79</td><td>0.10</td><td>0.21</td></tr><tr><td>T17</td><td>0.647</td><td>0.80</td><td>0.02</td><td>0.19</td></tr><tr><td>T26</td><td>0.538</td><td>0.75</td><td>0.24</td><td>0.02</td></tr><tr><td>T28</td><td>0.599</td><td>0.79</td><td>0.12</td><td>0.03</td></tr><tr><td>T31</td><td>0.787</td><td>0.76</td><td>0.19</td><td>0.37</td></tr><tr><td>T34</td><td>0.211</td><td>0.35</td><td>0.13</td><td>0.63</td></tr><tr><td>T35</td><td>0.480</td><td>0.37</td><td>0.10</td><td>0.77</td></tr></table></body></html>

Note. Number of observations $=$ 216. Rotation: None.

Regarding the semantic dimension, the pairs of source similarity measures, such as T33 and T34, and T34 and T35, generated acceptable correlation values,.32 and.38, respectively, while the relation between T33 and T35 was almost collinear $( r = 0 . 9 3 )$ . We chose T34 (source similarity lda) and T35 (source similarity word2vec) because of the two reasons: (a) the pair demonstrated a better correlation than the other and (b) LDA can explain a topical similarity between the source and the student text and word2vec consider words co-occurrence in dissimilar contexts.

We hypothesized that the three dimensions of knowledge structure, treated as components, could account for the thirteen text based measures as observed variables. To assess the dimensionality of these measures, we conducted PCA without rotation, given that the variable selection was guided by theoretical assumptions. Barlett's test of sphericity, $x ^ { 2 } \ : ( 7 8 ) = 1 9 4 5 . 7 7$ $p < 0 . 0 1$ , indicated significantly large corrlations between those measures. PCA resulted in three components, with eigenvalues over Kaiser's criterion of 1, explaining approximately $7 1 \%$ of the variance in the data. The commonalities for all variables were above 0.5, except for T34 and T35, indicating that a higher portion of the variance in each measure is accounted for by the three principal components.

As described in Table 5, the principal component analysis resulted in the three components. Component 1 represents "structure, Component 2 "surface," and omponent 3 semantic." We used the cut-ff value of 0.4 because loadings over 0.4 were expected, giver that $1 6 \%$ of the variance was explained with a sample size of 105, according to Steven (2002).

Most component loadings were positive and single, indicating that the measures contributed strongly to that component. However, three measures (T1, T5, and T7) exhibited strong negative loadings on Component 1 while also having strong loadings on Component 2. These negative loadings simply indicate a negative association with Component 1 (Burstyn, 2004). We ultimately assigned these variables to Component 2, as it explained more observed variance and aligned with our theoretical assumptions (positive relations). Additionally, T10 displayed dual loadings on both Component 1 and Component 2. Despite asignificant asociation with Component 2, we categorized it as part of Component 1 variables due to its stronger loading on Component 1 and better alignment with our theoretical assumptions.

# 3.2. Changes in model-based and text-based indices across three dimensions

Next, we examined the fixed effects of summary versions, akin to time effects, for measures from the model-based and text-based approaches in terms of how the value of each measure changes from the first to the last versions of the summaries.

As summarized in Table 6, in the surface dimension, the fixed effects of summary version were found to be statisticall significant for the two model-based measures: Number of Concepts $( \beta = 0 . 0 3 4$ $\pmb { p } = 0 . 0 2 9 ,$ and Number of Relations $( \beta = 0 . 0 3 6$ $\pmb { p = 0 . 0 2 8 } \}$ . This suggests that, for example, the similarity value for the number of concepts increased by 0.034 from the first to the last version of the summaries.

In contrast,concerning the structure dimension, only text-based measures showed changes from the firs to the last versions of the summaries, including T10 (adjacent_overlap_binary_all_sent, $\beta = 0 . 0 3 7$ A $p = 0 . 0 1 4 _ { . }$ , T14 (adjacent_overlap_binary_cw_sent, $\beta = 0 . 0 4 4$ $p = 0 . 0 1 7 )$ , T28 (LSA_2_all_sent, $\beta = 0 . 0 1 1$ $\begin{array} { r } { p = 0 . 0 4 9 , } \end{array}$ , and T31 (word2vec_1_all_sent, $\beta = 0 . 0 0 9$ $\begin{array} { r } { p = 0 . 0 0 1 } \end{array}$ . This indicates that, for example, the value for T14 (adjacent overlap_binary_cw_sent) increased by 0.044 per unit of summary version.

Table 6 Fixed Effects of Versions (Revisions).   

<html><body><table><tr><td>Descriptor</td><td></td><td></td><td>Estimate</td><td>SE</td><td>t</td><td>p</td></tr><tr><td>Surface</td><td>Model</td><td>Number of Concepts</td><td>0.034</td><td>0.016</td><td>2.192</td><td>0.029</td></tr><tr><td>(Surface Code)</td><td>-Based</td><td>Number of Relations</td><td>0.036</td><td>0.016</td><td>2.218</td><td>0.028</td></tr><tr><td></td><td></td><td>Density</td><td>0.024</td><td>0.015</td><td>1.591</td><td>0.114</td></tr><tr><td></td><td>Text</td><td>T01</td><td>0.008</td><td>0.005</td><td>1.387</td><td>0.167</td></tr><tr><td></td><td>-Based</td><td>T05</td><td>0.008</td><td>0.007</td><td>1.248</td><td>0.214</td></tr><tr><td></td><td></td><td>T07</td><td>0.002</td><td>0.004</td><td>0.604</td><td>0.547</td></tr><tr><td>Structure (Textbase)</td><td>Model</td><td>Average Degree</td><td>0.009</td><td>0.005</td><td>1.653</td><td>0.100</td></tr><tr><td></td><td>-Based</td><td> Mean Distance</td><td>0.009</td><td>0.009</td><td>1.066</td><td>0.288</td></tr><tr><td></td><td></td><td>Diameter</td><td>0.012</td><td>0.015</td><td>0.783</td><td>0.435</td></tr><tr><td></td><td>Text</td><td>T09</td><td>0.005</td><td>0.005</td><td>1.078</td><td>0.283</td></tr><tr><td></td><td>-Based</td><td>T10</td><td>0.037</td><td>0.015</td><td>2.477</td><td>0.014</td></tr><tr><td></td><td></td><td>T14</td><td>0.044</td><td>0.018</td><td>2.414</td><td>0.017</td></tr><tr><td></td><td></td><td>T15</td><td>0.008</td><td>0.006</td><td>1.559</td><td>0.121</td></tr><tr><td></td><td></td><td>T17</td><td>0.003</td><td>0.005</td><td>0.005</td><td>0.996</td></tr><tr><td></td><td></td><td>T26</td><td>0.089</td><td>0.055</td><td>1.617</td><td>0.108</td></tr><tr><td></td><td></td><td>T28</td><td>0.011</td><td>0.006</td><td>1.981</td><td>0.049</td></tr><tr><td>Semantic</td><td></td><td>T31</td><td>0.009</td><td>0.003</td><td>3.315</td><td>0.001</td></tr><tr><td></td><td>Model</td><td>Concept Matching</td><td>0.092</td><td>0.001</td><td>8.153</td><td>0.002</td></tr><tr><td>(Situational Model)</td><td>-Based</td><td> Propositional Matching</td><td>0.049</td><td>0.008</td><td>5.884</td><td>0.003</td></tr><tr><td></td><td></td><td>Recall-C</td><td>0.267</td><td>0.023</td><td>11.369</td><td>0.000</td></tr><tr><td></td><td></td><td>Recall-P</td><td>0.312</td><td>0.027</td><td>11.437</td><td>0.000</td></tr><tr><td></td><td>Text</td><td>T34</td><td>0.009</td><td>0.002</td><td>0.525</td><td>0.600</td></tr><tr><td></td><td> -Based</td><td>T35</td><td>0.001</td><td>0.004</td><td>3.258</td><td>0.001</td></tr></table></body></html>

Note. Number of observations $= 2 1 6$ $p < . 0 5$ . The measures that underwent significant changes are highlighted in bold within the table.

Among the measures of the semantic dimension, al measures except for T34 demonstrated an increase per unit of summary version. The model-based measures were Concept Matching $( \beta = 0 . 0 9 2$ $p = 0 . 0 0 2 \mathrm { ) }$ , Propositional Matching $( \beta = 0 . 0 4 9$ $p = 0 . 0 0 3 )$ , Recall-C $( \beta = 0 . 2 6 7$ $\begin{array} { r } { p = 0 . 0 0 0 \} } \end{array}$ , and Recall-P $( \beta = 0 . 3 1 2$ $\begin{array} { r } { p = 0 . 0 0 0 \mathrm { , } } \end{array}$ , while the text-based measures, T35 (source_similarity_word2vec), yielded a result of $\beta = 0 . 0 0 1$ $p = 0 . 0 0 1$ . For instance, students' T35 (source_similarity_word2vec) scores increased by 0.001 through their summary revisions.

# 3.3. Relations between model-based and text-based measures

We examined the relationships between text-based measures and their corresponding model-based measures (see Table 7). In the surface dimension, we observed either weak positive correlations $( 0 . 0 1 < r < 0 . 2 9 )$ or weak negative correlations $( - 0 . 2 9 < r < - 0 . 0 1 ) $ . No strong asociations were found. In the structure dimension, all relationships were positive but mostly weak. Notably, we found moderate correlations $( 0 . 3 0 < r < 0 . 6 9 $ ) between: T14 (adjacent_overlap_binary_cw_sent) and Density $\left( r = 0 . 3 4 1 \right)$ T28 (LSA_2_all_sent) and Average Degree $( r = 0 . 3 1 2 )$ , and T31 (word2vec_1_all_sent) and Average Degree $( r = 0 . 3 2 3 )$ . Regarding the semantic structure, T35 (source_similarity_word2vec) exhibited moderate correlations $0 . 3 0 < r < 0 . 6 9$ with all model-based semantic measures, including Concept Matching, Propositional Matching, Recall-C, and Recall-P.

# 4. Discussion

# 4.1. Findings

This study had three goals. One was to identify text-based measures to test whether the 3S knowledge dimensions fit the data obtained from students' summarie of readings in the context of technology-based online learning. The other goal was to examine how model-based and text-based measures detected changes in students' summaries toward expert-like summaries along the three dimensions. Another was to investigate how text-based measures were related to model-based measures.

First, as researchers from the model-based approach, we explored the literature on text-based asessment tols and measures, resulting in a shared conceptual framework. Subsequently, given that framework, we performed PCA using 13 selected text-based measures, unveiling three components that were significantly explained by the assciated text-based measures. These components likely coresponded to the surface, tructure, and semantic dimensions. This study underscores the existence of a multidimensional model shared across both model-based and text-based approaches, indicating specific measures aligned with each of these components.

Second, the subsequent Linear Mixed-Efect (LME) modeling tests yielded intriguing results. Model-based measures, such as the Number of Concepts and Number of Relations, in the surface dimension proved significant in detecting changes in learner summaries from the initial to the final versions. However, no text-based measure showed significant improvement in this regard.

Conversely, in the structure dimension, we observed a reverse pattern. No model-based measure demonstrated significant changes, Whereas four text-based measures exhibited an incrase from the intial to the final versions. These measures, including T10 (adjacent overlap inary all sent), T14 (adjacent overlap_binary cw sent), 28 (LSA2 all sent), and T31 (word2vec1 all sent), focus on detecting text cohesion, making them more adept at capturing structural changes in the text. They describe writing styles at the word, sentence, and document levels (McCarthy et al., 2006). Text-based measure can describe a student's writing habits but do not adhere

Table 7 Correlations with Model-Based Similarity Measures.   

<html><body><table><tr><td rowspan="2">Index</td><td colspan="4">Correlations</td></tr><tr><td>N Concepts</td><td>N Relations</td><td>Density</td><td>-</td></tr><tr><td>Surface T01</td><td>0.101</td><td>0.076</td><td>0.094</td><td></td></tr><tr><td>T05</td><td>0.013</td><td>-0.044</td><td>-0.006</td><td>1</td></tr><tr><td>T07</td><td>-0.020</td><td>0.019</td><td>-0.039</td><td>1</td></tr><tr><td>Structure</td><td>Avg. Degree</td><td> Mean Distance</td><td>Diameter</td><td>1</td></tr><tr><td>T09</td><td>0.056</td><td>0.157</td><td>0.242</td><td></td></tr><tr><td>T10</td><td>0.113</td><td>0.012</td><td>0.161</td><td></td></tr><tr><td>T14</td><td>0.266</td><td>0.152</td><td>0.341</td><td></td></tr><tr><td>T15</td><td>0.063</td><td>0.257</td><td>0.229</td><td></td></tr><tr><td>T17</td><td>0.206</td><td>0.183</td><td>0.180</td><td></td></tr><tr><td>T26</td><td>0.299</td><td>0.133</td><td>0.197</td><td>-</td></tr><tr><td>T28</td><td>0.312</td><td>0.207</td><td>0.292</td><td></td></tr><tr><td>T31</td><td>0.323</td><td>0.181</td><td>0.259</td><td>1</td></tr><tr><td>Semantic</td><td>Concept Matching</td><td>Prop. Matching</td><td>Recall-C</td><td>Recall-P</td></tr><tr><td>T34</td><td>0.244</td><td>0.214</td><td>0.116</td><td>0.188</td></tr><tr><td>T35</td><td>0.631</td><td>0.481</td><td>0.527</td><td>0.483</td></tr></table></body></html>

Note. Number of observations $=$ 216.

to a prescribed style, as exemplified in the reference model.

Both adjacent words (T10) and content words (T14)lexical words such as nouns, verbs, adjectives, adverbs, etc., which convey significant meaning in the text (Crossley et al., 2019)- tend to become more similar in two sequences of sentences when learners have improved their summaries. In other words, the levels of cohesion in the text increased. These findings were confirmed by T28 (LSA 2 all sent) and T31 (word2vec 1 all sent), which demonstrated that the content between the two sentences became more overlapped in a two-dimensional LSA or Word2Vec space, respectively. The statisical details of these two latent spaces seem to be beyond the focus of this paper. Instead, a practical interpretation is that a higher value of LSA and Word2Vec measures indicates that the two sets of sentences in the text share more semantically similar topics and content, and therefore become more aligned.

In the semantic dimension, all measures from both model-based and text-based approaches demonstrated positive changes from the initial to the final version, except for T34 (source similarit lda). One possible explanation is that sMART provides fedback infor. mation that guides learners to consider key concepts and concept-to-concept relations (i., propositions), leading to increases in measures directly associated with these concepts and relations. Additionally, it's reasonable to expect T35 (sourcesimilar. ity word2vec) to track semantic changes, as it indicates the extent to which a learners summary aligns with the content of the sources, similar to the model-based measures in this dimension. An increase in source similarity word2vec suggests conceptual similarity between two texts in terms of the words or concepts they contain (Crossley et al., 2019; Sung et al., 2016).

Lastly, correlations between model-based and text-based measures suggested potential ways to use those measures together. In the surface dimension, text-based measures could not detect the significant difference between two different versions of summaries nor showed acceptable corrlations with model-based measure. Accordingly, text-based measures can serve as supplementary information for student summaries, for example, providing the degree of using unique words in the summaries.

On the contry, text-based measures appear to be more effective in detecting changes in the structural dimension (i.e., textbase). Furthermore, some text-based measures in the structural dimension showed moderate asociations with model-based measures, such as Average Degree and Diameter. Such text-based measures are potentiall useful as indices for crafting feedback for learners to promote cohesive writing. For example, we can refer to text-based measures to provide feeback mesages that encourage students to enhance the cohesion of their text by increasing the overlap of content words across adjacent sentences and fostering connections among concepts.

The semantic-related text-based measure (T35, source similarity word2vec) was also correlated with al the model-based similrity measures. While word2vec similarity, unlike model-based indices, cannot provide specific feedback information (e.g, individual concepts and propositions), we can include T35 as part of our semantic measures to enhance measurement precision and bettr explain overall trends.

# 4.2. Implications

We can discuss the current study's implications for theory, methodology, and pedagogy. Theoretically, our study is unique in that we juxtaposed the model-based and text-based approaches through the lens of a thre-dimensional knowledge structure. The study's findings support the assumption that text-based measures can be factored in the three knowledge structure dimensions. The extant body of work has supported the existence of three-dimensional knowledge representation that is not simply articulated by the inclusion of a large amount f information (Kim, 2015; fenthaler, 2010; Sugrue, 1995). The current study's findings demonstrate the potential of the thre-dimensional model based on which we can further investgate associated measures and ways to interpret how students write summaries and develop their understanding of the texts.

Methodologicall, this study introduces text-based measures designed to identify and detect changes in the three knowledge di. mensions evident in students' summaries. While there are hundreds of text-based measures available, their significance becomes apparent when they are integrated into a measurement model tailored for a specific purpose, such as asessing the quality of summaries. Li et al. (2016) argue that many automated summary assessment (ASE) studies have produced inconclusive results due to the inclusion of an excessive number of measures that lack adequate definition within an analytical framework. Drawing upon our three-dimensional model, we have explored and recommended a concise set of empiricall validated text-based measures. ur findings reveal that model-based measures are particularly useful in detecting changes in the surface dimension, while text-based measures excel in accurately detecting changes in the structure dimension. Both model-based and text-based measures can be applied to the semantic dimension, offering a valuable cros-reference to further identify and explain semantic similarities between students summaries and an expert summary.

From a pedagogical perspective, a comprehensive assessment model, along with a wide array of validated measures, empowers us to accurately diagnose the multi-dimensional knowledge acquired by learners through their summaries. Furthermore, the current study presents new opportunitie for enhancing formative feedback in technology-enhanced learning environments like MART. By leveraging both text-based and model-based measures, we can offer versatile multimodal fedback that caters to various aspects of individual learners' summaries, encompassing their underlying knowledge structure and writing styles. For instance, we can turn to model-based measures to track students' progress in crafting summaries akin to those of experts (Fonger et al., 2018; Nadolski & Hummel, 2017), providing specific guidance on the inclusion of concepts and propositions, such as concept-to-concept relations, within a concept map. In additin, text-based measures can be tilized to offer fdack message foused on wriig styles that assist in creating summaries closely aligned with a reference model. For example, encouraging students to incorporate similar concepts across adjacent sentences can help them develop a more coherent summary and, consequently, a knowledge structure resembling that of an expert.

# 4.3. Limitations and suggestions

Admittedly, there is much work to be done. For example, changes in measures and measurement accuracy can depend on the context. For example, ths study used summaries written with approximately 200 to 300 words that students had developed utilizing SMART feedback. We need to replicate the findings with other data gathered through other ASE tools supporting summary devel. opment, such as Online Summary Assessment and Feedback System (Sung et a., 2016). Also, longer summaries containing a greater number of lexical components are likely to create a more complex structure in the texts and thus dfferent dynamics in the changes in measures along the dimensions.

The summary data used in this study were gathered from a graduate-level course in Learning Sciences. Students' writing skills and styles can vary depending on their grade levels (e.g., K12, undergraduate, or graduate level), academic disciplines, and the genre or topics of the text. Scholars have explained that different genres and disciplines influence students ways of conveying information in summarie andwhat information i in the foreground of the summaries (e.g., Goldman et al., 2016; Len et al., 2006). For example, 9th graders in a science classroom could read a more structured text that clerly defines concret concepts in cause-effect relationships, and therefore their summaries of the text could embed a more linear knowledge structure. I's worth noting that this study did not collect participants' demographic information, which could include factors such as levels of prior knowledge and gender. These background factors can interact with students' writing habits, potentially influencing the measures used in this study and the changes observed in summaries. Future research should aim to replicate these findings with a broader range of texts and text types, while also considering additional information about learners' backgrounds to better understand these interactions.

Similarl, i i imperative to explore additional text-based measures. In this study, we preselected certain TAAcO measures, and from these, we chose 13 measures during data preview. It's worth noting that a single text-based analytical tool, TAAco, generates hundreds of indices. Undoubtedly, there are other valuable measures that have not been explored in this work. Furthermore, TAACO is just one of many machine-translated tools, including ROUGE (Lin, 2004), PryEval (Gao et al., 2019), and the Too for the Automatic Assessment f Lexical Sophistication (TAALEs, Kyle et al., 2018). While these tols may compute numerous overlaing indice, their values and assessments of writing styles may differ depending on their secific text analysis functions when applied to the same written summaries. Future studies should explore a wider range of measures calculated using various text-based summary evaluation tools.

# 5. Conclusion

This study reveals that text-based measures can also be understood within the context of the three-dimensional knowledge structure that underpins the model-based approach. Both approaches tested the thee-dimensional model, showcasing it potential as a theoretical and measurement framework for summary evaluation. The significant correlations between text-based and model-based measures highlight the lexical characteristics of summaries inked to the features of mental models of the text. Future studies should further investigate the relationships between additional measures in these two approaches across the three dimensions and explore ways to utilize these measures for more adaptive and accurate assessment and feedback for individual learners.

# Compliance with Ethical Standards

A. Conflict finterest: Min Kyu Kim declares that he has no conflict of interest. Jinho Kim declares that he has no conflict ofinterest. Ali Heidari declares that he has no conflict of interest. B. Author identifying information is only on the title page, which is separate from the manuscript. C. Ethical approval: All procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or the national research committee.

# CRediT authorship contribution statement

Min Kyu Kim: Writing - review & editing, Writing - original draft, Supervision, Software, Resources, Project administration, Methodology, Formal analysis, Conceptualization. Ali Heidari: Data curation. Jinho Kim: Formal analysis.

# Declaration of Competing Interest

We declare that we have no known competing financial interests or personal relationships that could have appeared to influence th work reported in this paper.

# Data availability

The datasets used and/or analyzed during the current study are available from Dr. Min Kyu Kim on reasonable request.

# References

Bates, D., Mahr,, Bolker, B., & Wlker, . (2015). iting lir mie-ffcts mods us-ing me4. J o tisic Software, 7(1), 1-48   
Beaer,  skaya, A, & Giu,  (008, Jaary). omatic Seantic Relain atio with Mtile Bundry tio  AI (. 24829)   
rford   17.l    i   l    and Verbal Behavior, 11(6), 717-726.   
Bransford, J. D., & Franks, J. J. (1972). The abstraction of linguistic ideas. Cognitive Psychology, 2, 331-350.   
Burstyn . (2004). riil compt lys i  er inrme i oaioal hge qre.  f Ctil e, 488), 65561.   
Chng,  L  Jag H (2019 g wtin  ey cho sts: a act thry ti. ish tio te, 84), 45146   
Chi, M. T, & Wylie R. (2014). The IA framework Linking cognitive engagment to active leng outcomes. Edcatioal Pychologist 94) 219-243.   
ariana, 010iis c  litin  e ied fro e-ic d tc s Knowledge, 41-59. https://doi.org/10.1007/978-1-4419-5662-0_4.   
oronges,  , tac Valte, .. 200). l omparisonf iveastive nwork in wuations 1. Jo o Apled il Psychology, 37(9), 2097-2129.   
Crosley, . yl, ma . 016) T lfo the ic als f tet osio mi at of l, gol, d tet cohesion. Behavior Research Methods, 48(4), 1227-1237.   
Crosle, l 1 r hci . rit Methods, 51(1), 14-27.   
Derestr, ., i, . s, .. er,    Han, . (190) g  a c ay.  f the mric y Information Science, 41(6), 391-407.   
Dole .f, e . 191   t ti Research, 61(2), 239-264.   
Duke, N. K., & Pearson, P. D. (2009).Efective practice for developing reading comprehension. Jounal of Education, 189(1-2), 107-122.   
unk, ,  Mh   J.. 2013)t  wthftie  q ising directions from cognitive and educational psychology. Psychological Science in the Public Interest. 14(1), 4-58.   
Field, A., Field, Z., & Miles, J. (2012). Discovering statistics using R. Sage,.   
ger,      , ,  ,  018 s   tio,  stu learning: An example from mathematics education. Cognition and Instruction, 36(1), 30-55.   
th  r  P   i   s 13 r,  e features predict affect in : erious ouri l of Educational Data Mining, 5(1), 147-189   
Gao, . r,  , .019 al n  m r s ot . r, ad . Ps ii, . Deck ki i i, .. .018e on Lnguges d tion 32343239).  2018-1t ationa nfec  r and ation on anguage Resources Association (ELRA).   
Garnham, A. (2001). Mental models and the interpretation of anaphora. Hove: Psychology Press.   
Gentner, D., & Medina, J. (1998). Similarity and the development of rules. Cognition, 65, 263-297.   
Goldman, S R., Snow, C., & Vaughn, . (2016). ommon themes in teachg reding for understnding: esons from thee roets. Jounal of Adeet & Adult Literacy, 60(3), 255-264.   
mi r (199) f    . five Organizations (pp. 73-95). Mahwah, NJ: Lawrence Erlbaum Associates.   
Graham, S., MacArthur, C.A., & Fitzgerald, J. (Eds.). (2013). Best practices in writing instruction. NY: Guilford Press.   
Graser, C., Siger,  Traasso T. (1994). onstrting ees during nrrative text comprhension. Pclgicl Rview, 101(3), 371-395.   
Grae, C, maa ., &ch, J. (201. -erix: ng mileel ansf tt cheristc.ionl chr, 0(5) 223-234.   
Graeer, ,  , a ,  i,  kr, . 2014. i e t heristc a e e of e and discourse. The Elementary School Journal, 115(2), 210-229.   
He, Y., Hui, s. ., & Quan, T. T. (2009). Automatic sumary sesment for intlligent tutorig ystems. omputers & Ecatio, 3(3), 890899.   
Helbig, H. (2006). Knowledge representation and the semantics of natural language. Springer.   
Iflr, 1 l     n, 8 (1), 81-97.   
Ifenthaler, D. (2014). AKOVIA: Automated Knowledge Visualization and Asessment. Technology, Knowledge, and Leaning, 19(1-2), 241-248.   
Joson  Pr  r, ,  ,    011 s   mh  rreens reading text conceptualization? Technology, Instruction, Cognition and Learning, 8(3-4), 297-312.   
Johnson-Laird, P.. 2005. Mtl mdes and thoghts.  K J. lyak (d.), The bridge hook ofthinking and eon p. 185-208.Cambridge University Press.   
Erlbaum Associates.   
Katz, J. J., & Postal, P. M. (1964). An integrated theory of linguistic descriptions. Cambridge: M.I.T. Press.   
Kim, K. (2018). An automatic measure of cros-language text structures. Technology, Knowledge and Learning, 23(2), 301-314.   
m   1 course. Educational Technology Research and Development, 1-18. Technology Research and Development, 60(4), 601-622.   
m M.015.f  p i i omp rbes rtie  i thg d g.  io Py, 42, -16   
Km        ies 3 (5), 649-667.   
Kim, M Gl, C. J, Bundag . N. & Mahany, R. J. (2020. loy suped eg comphion: a dig rerch f te student ma mode! analyzer for research and teaching (SMART) technology. Interactive Learning Environments, 1-25.   
Kim,M Hri,   crh,  020) R ohion an mel md dme  cros-ltio f ms and  ases ue gf t  ,  .) f t, 4onf t g Sciences (ICLS) 2020 (pp. 561-564). Nashville, Tennessee: International Society of the Learning Sciences.   
Kim   0     J. n Aalst  o,  Brn (d.), Pdgs of th 17 nio onfe of th n cie -CL 2023. 98689. nrel, ada International Society of the Learning Sciences.   
Kim, M & Mcahy,  . (2020). Sy wting as a prcesf building a od mnl model: goal ind to decribe kwlee stte change. M.Greli, .  .) o 1. Th  f th  i, 4o onef h  c 2020p 74-81). Nashville, Tennessee: International Society of the Learning Sciences.   
m,M r, . 2021a)mig sar witin hh omative fak in lan omt. l of omue Assisted Learning, 37(3), 684-704.   
Kim, M. &cahy, . (2021b). ig grap talit s goal inex tos stets metl mode ste dlmet rn smar witing Educational Technology Research and Development (ETRD), 69, 971-1002.   
Kintsch, W. (198). The role of knowledge indiscourse comprehension: A construction-integration model. Psychological Review, 95(2), 163-182.   
Kintsch, W. (1998). Comprehension: A paradigm for cognition. Cambridge University Press.   
Kintsch, W., & van Dijk, T. A. (1978). Toward a model of text comprehension and production. Psychological Review, 85, 363-394.   
Kyle K Ce  r,  (018) T  for te mc asi f ex hstic ): vi. r , 503) 1030-1046.   
ndaer   19    he     o, Psychological Review, 104(2), 211.   
Le .    JJ  06g        i narrative and expository texts. Behavior Research Methods, 38(4), 616-627.   
Li H ai,r . 016)   t  g  ,hi  ., P  t International Conference on Educational Data Mining (pp. 430-435). Raleigh, NC: EDM Society.   
Lin, C.Y. (2004) RG: A Packg for Aomatic alatio f marie. In Txt Smariztion Brches t, 7481. Bareoa a:Aiationfor Computational Linguistics.   
cary,  t  00 i st  -i. .f . P the 19th annual Florida Artificial Inteligence Research Society International Conference (pp.764-770). Melbourne Beach, FL:AAAIPress.   
Mcamara ., Gaer, ., Mcary,  & Cai, .2014). me ti f tex d dre wh -i. mbr ert res.   
McNamara, D. S., & Magliano, J. (209. Toward a comprehensive model of comprehension. Pychology of Leaming and Motivtion, 51, 297-384.   
camara, . were, M ary, M &aer,  (2010). -Merix tr igusti fe f esio.se roes, 474) 292-330. strategies. Reading Comprehension Strategies: Theories, Interventions, and Technologies, 397-421.   
r     013     t. 1310, 4546.   
ki  017tii r  m .     ,86, 1368-1379.   
Norman, D. (1983). Some oservations on mental models. In D. Gentner, & A. L Stevens (Eds.), Mental models (p. 7-14). Hilsdale, NJ: Erlbaum.   
ovak, J.D ,  J. 006) Th thy din  ms    st th.  i o  h i  1-31   
Pirayer,   er, 011).    h io:  m  io   n reading comprehension tasks. Instructional Science, 39(6), 901-919. the association for computational linguistics: system demonstrations (pp. 163-168).   
Scardamalia, M., & Bereiter, C. (1986). Helping students become better writers. School Administrator, 42(4), 16-26.   
Seel, N. M. (2003). Model-centered learning and instruction. Technology, Instruction, Cognition, and Learning, 1(1), 59-85.   
Singer, d, 1  r  th tia  6) 1007-1041.   
Salcky, .19t ar s  o    iv ic,  c . e d Cognition, 11(3), 499-525.   
Stees , Park, ., Vagh . (2019.  e f izing and m id inerion fo strgig rer in G 3 trough 12 1978-2016 Remedial and Special Education, 40(3), 131-149.   
Steven, J. P. (2002). Applied multivariate statistics for the social sciences (4th ed.). Hillsdale, NJ: Erlbaum. Computers & Education, 131, 33-48.   
Sugrue, B. (1995).A thery-based framework for asssing domain-specific problem-solving ailit. dcational Meeent sses and Practie.   
01f       t graders: The LSA-based technique. Computers & Education, 95, 1-18.   
Wade-Stein, D., & Kintsch, E. (2004). Summary Stret: Interactive computer support for writig. Cognition and Instructio, 22(3), 33-362.   
Waseman, ., & Faust, K (1994). cil ntwork analysis: Methods and plication (l.8). ambridge, United Kingdm: ambrige niverty Pres.   
Westby, C., Culata, B., Lawrece, B., & Hll-enyon, K (2010). Sumarizing expository texs. Toics in Language Dorders, 30(4), 275-287.   
Zouaq, A., Gasevic, D., & Hatala, M. (2011). Towards open ontology learning and filtering. Information Systems, 36(7), 1064-1108.