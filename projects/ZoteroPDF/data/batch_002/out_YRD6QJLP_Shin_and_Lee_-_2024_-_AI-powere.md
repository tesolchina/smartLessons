# AI-powered automated item generation for language testing

Dongkwang Shin, and Jang Ho Lee

Although automated item generation has gained a considerable amount of attention in a variety of felds, it is still a relatively new technology in ELT contexts. Therefore, the present article aims to provide an accessible introduction to this powerful resource for language teachers based on a review of the available research. Particularly, it will give a brief introduction to diferent types of automated item generation approaches, provide a summary of previous ELT studies on this technology, and introduce three diferent AI-powered tools, along with practical tips for ELT practitioners. We conclude by calling for more empirical research on automated item generation from the ELT community and encouraging language teachers to take an interest in this technology themselves.

Key words: AI, assessment, automated item generation, ChatGPT, language testing

Although the idea of using computer programmes to develop testing items was frst conceived of more than 50 years ago (Bormuth 1969), it is only recently that the potential of automated item generation (AIG) has been explored and discussed in depth in education (Circi, Hicks, and Sikali 2023). One of the most evident advantages associated with utilising AIG lies in its capacity to produce rapidly a substantial quantity of test items (Gierl, Lai, and Turner 2012). This feature may be particularly appealing to educators and organisations challenged by content demands surpassing their capacity to generate items conventionally by relying on a few experts in test item development. Furthermore, AIG could produce items that bear a resemblance in appearance but necessitate distinct cognitive processes to arrive at divergent responses. This could mitigate the impact of item recall among test-takers (Pugh et al. 2020).

In the language-learning domain, using AI-powered technologies for AIG for ELT practitioners has begun attracting researchers’ attention (e.g.,

Kohnke, Moorhouse, and Zou 2023). However, no proper introduction for teaching practitioners has yet been provided. Therefore, the present article aims to review recent research on AI-powered AIG, introduce AI-powered tools, including ChatGPT, which could be employed to generate language testing items. Finally, this paper seeks to provide useful tips for ELT practitioners.

Types of automated item generation approaches

Throughout the twentieth and twenty-frst centuries, the quest to automate test item generation has been evolving, beginning with Bormuth’s proposal in 1969, as mentioned above. Kurdi et al. (2020) classifed AIG approaches into three categories: template-based, rule-based, and statistical. The frst relies on predefned templates or models to generate test items following established structures, but they may be limited in diversifying item types and adapting to new ones. Rule-based methods use linguistic properties to annotate sentences and generate items with predefned rules, but these are also circumscribed when introducing new item types or modifcations. In contrast, statistical approaches, which use machine learning and advanced language models, learn item generation rules from extensive test item data, thereby allowing for a wider range of item types and formats. They also produce more natural-sounding items than template and rule-based approaches.

Deep learning has also been applied to AIG. For example, von Davier (2018) used a type of recurrent neural network (RNN) deep learning technique, called a long short-term memory (LSTM) network, to generate items that measure non-cognitive aspects (e.g., emotions, attitudes). More recently, AIG using ChatGPT, which was developed based on the transformer network method (Radford et al. 2019) by utilising massive data, has been attracting attention. This method’s value lies in its integration of contextual information, resulting in more authentic text output, especially in larger contexts. Based on the transformer-based deep language modelling approach, Attali et al. (2022) developed an interactive reading task, which showed the potential of employing the AIG approach in building more complex assessment items, demonstrating the remarkable evolution of this approach.

ELT research on AI-powered automated item generation

In this section, we review recent studies that focus on the potential of AI-powered AIG in the ELT context.

Kohnke, Moorhouse, and Zou (2023) noted that language teachers could use ChatGPT to generate reading comprehension questions to support reading tasks. To this end, they showed a list of comprehension and expansion questions generated by ChatGPT based on Lewis Carroll’s book Alice’s Adventures in Wonderland. Although Kohnke et al. did not outline any advantages or disadvantages of employing ChatGPT in this way, it is nonetheless noteworthy that they highlighted the use of ChatGPT not just as a supplemental language-learning tool, but also as a language teacher’s test-building tool.

Shin (2023) tested the potential of various AI tools in creating English reading testing items for the College Scholastic Ability Test (CSAT) (i.e., the Korean SAT). In the secondary-level teacher training programme, ten in-service teachers underwent the item development process using various tools (ChatGPT, CopyAI, Google Translate etc.), with the author’s guidance. Overall, the participating teachers found using AI tools to generate testing items was both efcient and convenient. The author, however, pointed out the need to carefully draft the prompts for each question type (e.g., identifying a character’s mood, inferring an appropriate phrase from blanks).

More recently, Shin and Lee (2023) conducted a blind test by which 50 pre- and in-service English teachers in the Republic of Korea were asked to evaluate the reading passages and testing items extracted from the English test of CSAT and those generated by ChatGPT. It was found that there was no signifcant diference in the sampled teachers’ perception of the naturalness of fow and expressions in the reading passages, whereas the participants rated CSAT higher in terms of the attractiveness of multiple-choice options and the overall completion level of the testing item (see Gierl and Lai 2013 for a similar discussion of the problem related to distractor quality in AIG-generated multiple-choice questions). Based on such results, Shin and Lee suggested that English teachers should carefully review and revise ChatGPT-generated testing items before using them with their learners.

The review of the above-mentioned studies has started to reveal the potential of AI-powered AIG in the ELT domain. In light of these studies, the following section will introduce three extant AI tools that could be employed for AIG.

This section will introduce three AI-powered tools that do not require programming knowledge and can therefore be used conveniently for AIG.

The frst tool is ChatGPT, which was frst trained with demo data that served as an AI assistant, before being retrained by humans, ranking their satisfaction with the model’s output on that data, and then being developed to produce answers as close to human-like as possible through an optimisation process called reinforcement learning. Currently, ChatGPT has evolved to ChatGPT Plus, based on GPT-4, which is a multimodal model that can also recognise and generate images. As ChatGPT Plus is a paid version, the free version based on GPT-3.5 can be used for the purpose of generating testing items.

As demonstrated in Shin (2023), it is important to identify a model testing item from extant tests when employing ChatGPT to generate testing items. The user should then input the target model item along with the prompt (e.g., Create a new passage with a diferent topic and make new multiple choices as follows). Once the prompt and model item are entered, ChatGPT generates the testing item on a par with the model item. The user can then examine the generated item in terms of the naturalness of the passage’s expressions and fow, as well as the overall quality of the testing component. They may then change the wording of the previously entered prompt to generate another testing item, if not satisfed with the initial one.

The second tool, which is similar to ChatGPT in terms of item generation, is Perplexity AI, an advanced AI chat-based conversational search engine that harnesses the power of natural language processing (NLP) and machine learning. Perplexity AI leverages the capabilities of OpenAI’s

GPT-3.0 and a large language model to provide accurate and reliable answers to user queries. Perplexity AI provides the websites and resources associated with the search, through which users can explore the information and verify the answers. By using a Bing-based search system and internet access, Perplexity AI also ensures that the answers it provides are based on real-world sources (Garg 2023).

The third tool is genQue, an educational content generation platform that harnesses the combined power of ChatGPT and Artifcial Society’s (2023) proprietary technology. With genQue, users can create educational texts and questions while benefting from a comprehensive suite of 17 features, including text generation, automated essay scoring, and grammar correction. However, genQue difers from both ChatGPT and Perplexity AI in that it does not function as a search engine. It is also notable that, unlike ChatGPT and Perplexity AI, genQue is specifcally designed to facilitate the development of CSAT English testing items. It accomplishes this by ofering a passage generation tool tailored for the reading test and provides templates for 12 distinct English testing item types found in the CSAT English test.

When using genQue’s Passage Generator, the user must manually enter the frst 100 or more characters of a passage when creating a new passage, which can be problematic for some users. The difculty of the text can be adjusted to one of fve levels depending on the readability index and vocabulary level. The user can also choose between a non-fction article and a fctional story, and a passage length of 50, 150, or 250 words. Once the passage is generated, the user can utilise the item generator function to generate testing items across fve levels of difculty.

Each of the three AI-powered tools introduced in the previous section has its own idiosyncrasies. Therefore, we will attempt to ofer practical tips for language teachers in what follows.

To begin with, genQue allows the difculty of the passage and questions to be fne-tuned across fve levels. This feature ensures that the generated questions are tailored to the specifc profciency levels and learning needs of the users. Furthermore, genQue’s item generator function further provides an explanation alongside the correct response and an estimated distraction level for the answer options. The latter feature enables users to assess the quality of the options and their potential to divert attention from the correct answer. By ofering insight into the distraction level, the generator aids users in improving the item’s quality.

Because of its design limitations, however, genQue is restricted to only generating English reading section items for the CSAT English test, meaning that it cannot generate other types of testing items. Furthermore, the passage length is constrained to approximately 250 words, which makes generating long passages unfeasible. Additionally, the difculty level of the passages generated by genQue does not exceed the CEFR B2 level, thereby limiting its ability to produce passages at a higher C1 or C2 profciency level. Nevertheless, such level adjustments may be more ‘exquisite’ than anything ChatGPT and Perplexity AI can ofer. With them, the difculty of a passage or testing item can only be roughly controlled by an additional prompt

containing modifers such as ‘harder’ or ‘easier’, or by specifying a target grade or age and an absolute criterion in a language such as CEFR.

However, ChatGPT and Perplexity AI are more fexible than genQue in terms of generating other types of testing items because they could be directed via conversation. Below illustrates an example of a listening test item generated by ChatGPT.

# Prompt and model listening testing item:

Make a new MCQ on a diferent situation in the following question format: Listen to the conversation and choose something that both people didn’t pack.

A: I can’t wait to get to the beach.   
B: I’m excited, too. Do we have everything we need?   
A: I have sunscreen and a towel.   
B: I have some chips and cold water.   
A: Are you going to swim in the ocean this time?   
B: Maybe, but I like lying on the beach more than swimming. A: You should try swimming with me. It is fun!

1. Sunblock 2. Towel 3. Sunglasses 4. Snacks 5. Water

# Testing item generated by ChatGPT:

Listen to the conversation and choose something that both people didn’t pack.

A: The festival this weekend is going to be amazing. B: I know! Have we packed everything we might need? A: I’ve got our tickets and a portable charger. B: I packed some snacks and water bottles. A: Do you think it’ll be cold in the evening? B: Possibly, but I’m bringing a blanket just in case. A: Great idea! I can’t wait to see the live bands.

1. Tickets 2. Portable charger 3. Snacks 4. Water bottles 5. Sunscreen

When the same prompt shown above was entered into Perplexity AI, it created a dialogue but did not generate fve choices. However, with the additional prompt ‘Make 5 choices’, Perplexity AI produced the same result as ChatGPT. This type of chat-based instruction is fexible in that it allows the user to try diferent types of tasks. In other words, if the AI fails to accomplish a task, the user could revise a prompt so that the AI could produce the item they aimed to generate.

Another important point concerns the importance of phrasing in drafting prompts. For example, our experience revealed that, when the word ‘passage’ was frst included in the prompt ‘Create a new passage with a diferent topic and make new multiple choices as follows’ to generate a new reading passage and testing item, ChatGPT failed to do this, even though it had worked well in generating other testing items. However, after replacing ‘passage’ with ‘text’, ChatGPT correctly generated the reading passage and testing item. We have observed a similar issue with Perplexity AI in terms of prompt understandability. Therefore, the user should keep such inconsistency and instability in mind when drafting prompts on ChatGPT and Perplexity AI. Drafting prompts is less of an issue when using genQue, as it is equipped with a question generation template.

In this article, we introduced three diferent AI-powered tools for AIG, highlighted their features, and provided some pedagogical tips for ELT practitioners. We concur that AIG will be very useful for test developers, including teachers required to generate testing items (Kurdi et al. 2020). It is also expected that AI-powered tools for AIG will be a main source of continuous testing and large test providers (Circi et al. 2023), namely by allowing users to build a massive repository of test items at a much lower cost, a faster pace, and with a minimum of human oversight. However, the current state of AI technology means that human experts would still play a vital role in improving the quality of AI-generated items. Nevertheless, we believe that language teachers will eventually need to be trained on how to generate language testing items using AI technologies, including the ones introduced above. We also suggest that more research will be necessary on how such testing items may be perceived by language learners, teachers, and other stakeholders. Finally, research on AIG’s actual usefulness likewise seems imperative. Through these eforts, we expect AIG to be a game changer in language assessment and curriculum design.

Final version received March 2024

# References

Artifcial Society. 2023. genQue, accessed 18 May 2023, https://www.genque.ai/. Attali, Y., A. Runge, G. T. LaFlair, K. Yancey, S. Goodwin, Y. Park, and A. A. von Davier. 2022. ‘The Interactive Reading Task: Transformer-Based Automatic Item Generation.’ Frontiers in Artifcial Intelligence 5:903077. https://doi.org/10.3389/frai.2022.903077 Bormuth, J. 1969. On the Theory of Achievement Test Items. Chicago: University of Chicago Press. Circi, R., J. Hicks, and E. Sikali. 2023. ‘Automatic Item Generation: Foundations and Machine Learning-Based Approaches for Assessments.’ Frontier in Education 8:858273. https://doi.org/10.3389/feduc.2023.858273 Garg, S. 2023. ‘What is Perplexity AI and How to Use It.’ Allthings.how, accessed 2 March 2024, https:// allthings.how/what-is-perplexity-ai-and-how-to-use-it/. Gierl, M. J., and H. Lai. 2013. ‘Evaluating the Quality of Medical Multiple-Choice Items Created with Automated Processes.’ Medical Education 47(7):726–33. https://doi.org/10.1111/medu.12202 Gierl, M. J., H. Lai, and S. R. Turner. 2012. ‘Using Automatic Item Generation to Create Multiple-Choice Test Items.’ Medical Education 46(8):757–65. https:// doi.org/10.1111/j.1365-2923.2012.04289.x Kohnke, L., B. L. Moorhouse, and D. Zou. 2023. ‘ChatGPT for Language Teaching and

Learning.’ RELC Journal 54(2):537–50. https://doi.   
org/10.1177/00336882231162868   
Kurdi, G., J. Leo, B. Parsia, U. Sattler, and S. Al-Emari. 2020. ‘A Systematic Review of Automatic Question Generation for Educational Purposes.’ International Journal of Artifcial Intelligence in Education 30:121– 204. https://link.springer.com/article/10.1007/   
s40593-019-00186-y   
Pugh, D., A. De Champlain, M. Gierl, H. Lai, and C. Touchie. 2020. ‘Can Automated Item Generation be Used to Develop High Quality MCQs that Assess   
Application of Knowledge?.’ Research and Practice   
in Technology Enhanced Learning 15:12. https://doi.   
org/10.1186/s41039-020-00134-8   
Radford, A., J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. 2019. ‘Language Models are Unsupervised Multitask Learners.’ OpenAI Blog 1(8):9. https://   
d4mucfpksywv.cloudfront.net/better-language-models/ language_models_are_unsupervised_multitask_   
learners.pdf   
Shin, D. 2023. ‘A Case Study on English Test Item   
Development Training for Secondary School Teachers Using AI Tools: Focusing on ChatGPT.’ Language   
Research 59(1):21–42. https://doi.org/10.30961/   
lr.2023.59.1.21   
Shin, D., and J. H. Lee. 2023. ‘Can ChatGPT Make   
Reading Comprehension Testing Items on Par with

# The authors