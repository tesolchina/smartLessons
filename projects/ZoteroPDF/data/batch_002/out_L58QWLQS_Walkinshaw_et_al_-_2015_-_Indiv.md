# Individual Consultations: Academic Writing Outcomes for International Students

Ian Walkinshaw1 , Todd Milford2 , and Keri Freeman1

# Abstract

Responding to calls for research into measurable English language outcomes from individual language support consultations at universities, this study investigated the effect of individual consultations (ICs) on the academic writing skills and lexicogrammatical competence of students who speak English as an additional language (EAL). Attendance by 31 EAL students at ICs was recorded, and samples of their academic writing texts before and after a 9-month interval were compared. Participants’ academic writing skills were rated, and lexico-grammatical irregularities were quantified. No statistically significant positive shifts manifested, due to the relatively short research period and limited participant uptake, but there were encouraging predictors of future shifts given continued utilization of the service. First, although a Wilcoxon signed-rank test showed no association between attendance at ICs and shifts in academic writing ability, a Spearman’s rho calculation suggested a tentative relationship to positive pre–post shifts in three academic writing sub-skills: Task Fulfillment, Grammar, and Vocabulary. Second, instances of four common lexico-grammatical irregularities (subject/verb, wrong word, plural/ singular, and punctuation) declined at post-testing. Although only regular, sustained attendance would produce statistically significant shifts, there is a potential association between participants’ use of ICs and improved academic writing skills/lexicogrammatical competence.

# Keywords

academic language and learning, academic writing, English as an additional language, individual consultations

# Introduction

Recent years have seen a drive to implement positive changes in English language support and development practice in Australian higher education. This has been spurred by Birrell, Hawthorne, and Richardson’s (2006) report evaluating Australia’s General Skilled Migration Categories, which noted that “many international students enter and exit Australian courses at IELTS levels far below the published guidelines” (p. 107). Another study by Birrell (2006) reported that one in three international students did not achieve an International English Language Testing System (IELTS) score of 6.0 (typically required for entry to tertiary institutions) by the end of their university programs. These studies have placed the onus for ensuring adequate language skills on institutions, forcing universities to reappraise their language support practices. Their findings underline the necessity for appropriate academic and linguistic support programs (Barrett-Lennard, Dunworth, & Harris, 2011; Birrell, 2006; Briguglio & Howe, 2006) for students who speak English as an additional language (EAL).

One language support resource often employed at Australian institutions is individual academic writing consultations (Arkoudis & Starfield, 2007; Stevenson & Kokkinn, 2009; Wilson, Collins, Couchman, & Li, 2011). Academic language and learning (ALL) advisors support students’ academic writing through targeted one-to-one tutoring (Chanock, 2000) to assist students to gain sufficient linguistic and academic skills to become independent learners. The specific practices vary between universities in terms of location and context (e.g., within a library or faculty or in a dedicated ALL unit or as part of a general support resource), the duration of sessions, the nomenclature (appointment, tutorial, session, drop-in), how students access the service (selfselect, referral, or both), and how frequently they can access it (limited or unlimited; Stevenson & Kokkinn, 2009). Nevertheless, a commonality is that all such services manage a myriad of language and learning needs that are specific to individual students and which call for a targeted individual response rather than a prepared lesson (Chanock, 2002).

Research has shown that the pedagogic attributes of individual consultations (ICs) differ considerably from those of classroom teaching. For example, the flexibility of ICs allows teachers to focus on each student’s specific language and academic skill needs (Chanock, 2000). Consultations are task driven, provide immediate formative feedback, and allow students to learn, query, and experiment without judgment by their peers, often increasing their engagement in the consultations (Ewart, 2009). Chanock (2000) argues that as well as improving students’ academic writing and linguistic skills, ICs may also positively influence the broader teaching processes within universities: Their informal and confidential nature allows ALL advisors a more granular insight into students’ academic and linguistic needs, which may (if disseminated) translate to a more targeted approach to classroom teaching (cf. Shin, 2006).

The focus of the current research is a free one-to-one English language consultation resource established by Griffith University in 2007 to provide in-degree English language support to EAL students and to encourage independence in improving their English language skills. The service provides weekly $4 5 \mathrm { - m i n }$ consultations on all five Griffith University campuses and is accessed by several thousand students each semester with an exponential year-on-year increase. According to an internal report on Griffith’s English Language Enhancement Strategy (GELES; Griffith University, 2012), $9 1 . 1 \%$ of users expressed satisfaction with the service. Yet satisfaction does not necessarily impact English language proficiency, as we shall see.

A primary issue for EAL students in Australia is the struggle to assimilate to Western academic writing conventions (Snow Andrade, 2006). Part of this is insufficient language proficiency (Coxhead & Byrd, 2007), but other issues have emerged as well. First, the educational backgrounds of many EAL students differ from the Australian tertiary education context. The issue is not necessarily one of ignorance: EAL students frequently have a generic grasp of English for academic purposes, but are erroneously conforming to known rules of good writing, which are not appropriate to the particular discipline (Pardoe, 2000). Second, students often encounter a lack of information about the writing, formatting, or referencing requirements of their discipline (Chanock, 2007). Unfortunately, support in this area may not be given by discipline academics (Murray, 2010), who tend to see it as outside their purview (Arkoudis & Starfield, 2007; Zhu, 2004). Discipline academics also tend to provide summative rather than formative feedback, focusing on content not linguistic competence—hence the need for EAL students’ academic writing skills to be honed through targeted individual instruction with a non-threatening, knowledgeable partner (Chanock, 2000) to increase their likelihood of academic success (Bretag, 2007).

# Research Into Individual Consultation Services

Despite the call for institutions to monitor the effectiveness of language support strategies such as ICs (Hirsh, 2007), this area has so far been under-explored, particularly in the Australian context (Baik & Greig, 2009; Harryba, Guilfoyle, & Knight, 2011; Hirsh, 2007). We will outline some of the research done into the effectiveness of IC resources, beginning with worldwide findings and then narrowing to the Australian higher education context.

Williams (2004) explored the impact of ICs on the quality of second language writers’ draft writing at a U.S. university, recording and analyzing five advisor–student interactions. Both pre- and post-intervention texts were studied to determine the extent of the revisions made by the writers after advisor feedback. The holistic grades for pre- and postintervention drafts were also compared to determine whether the interventions had positive assessment outcomes. Williams’s (2004) study revealed that the interventions encouraged small-scale revisions of sentence-level problems but systemic text-level problems were less likely to be revised. Also, issues explicitly raised by the advisor were more likely to be revised than those implicitly treated. In addition, student reaction to feedback was salient: Students who wrote down advisors’ comments about an issue were more likely to revise the error than students who resisted or ignored suggestions. Crucially, text revision did not always lead to higher assessment outcomes.

In the U.K. context, a quantitative investigation of a university’s writing center was conducted by Yeats, Reddy, Wheeler, Senior, and Murray (2010) to determine the impact of attendance at ICs with advanced peer tutors on students’ achievement and academic progression. They compared the academic records of 806 first-year students with their writing center attendance and found that regular writing center attendees recorded significantly higher levels of achievement (measured by recorded grades) than non-attendees. A higher (though not statistically significant) number of attendees progressed to a more advanced course than non-attendees. Notably, this service suffered from poor uptake (only 45 of the 806 participants attended the writing center), perhaps because it was staffed by peer tutors and not ALL specialists.

In Australia, Huijser, Kimmins, and Galligan (2008) evaluated the perceived effectiveness of ICs in mathematics and academic writing with 55 students over a 2-week period through student questionnaires and tutor logs. ICs were perceived to have assisted the students to overcome inaccessible and troublesome cognitive schemata (“stuck places”), as well as providing a non-threatening emotional environment for risk taking and error recovery. However, Huijser et al.’s (2008) focus was limited to perceptions of the service rather than presenting measurable outcomes of its effectiveness. In another Australian study, Woodward-Kron (2007) analyzed a recorded IC with an EAL graduate student to probe student learning processes. She found that the interaction largely involved negotiation of surface-level errors, that is, linguistic forms and text organization. Content was also addressed to a certain extent, despite a faculty academic’s advice to “fix the grammar [but] don’t touch the content” (p. 255). Wilson et al. (2011) carried out a similar study with a mature-age international student, concluding that ICs increased the student’s confidence, command of discipline-specific language, use of academic literacy metalanguage, and technical competencies such as structuring and paragraphing. These studies, despite their limited generalizability, illuminate some core benefits of ICs. Again, however, they do not measure outcomes in terms of developing academic writing skills.

Chanock (2002) points out that evaluating the effectiveness of ICs is a challenging task. First, ALL work is constructed and positioned differently in different universities and often sits outside their mainstream teaching evaluation processes. So ALL departments often need to design and implement their own evaluation mechanisms, reducing comparability between institutions. Also, assessing what students have learned and identifying how ALL instructors contributed to that learning is challenging (Stevenson & Kokkinn, 2009) because of all the other sources of language input that second language (L2) learners are exposed to in a target culture.

Although the studies above provide some insight into ICs as a source of linguistic and academic support for EAL students, empirical research is needed to quantitatively measure linguistic development over time (Jun, Ramirez, & Cumming, 2010; Stevenson & Kokkinn, 2009; Williams, 2004). Also, research to date has largely focused on student satisfaction (Arkoudis & Starfield, 2007), rather than evaluating the link to long-term student learning. The current study takes an initial step in addressing these issues by longitudinally examining some of the structural, lexical, and syntactic shifts in EAL students’ academic writing and attempting to identify an association with attendance at ICs.

The preliminary and initiatory study outlined in this article recorded participants’ attendance at ICs and analyzed their self-selected samples of academic writing pre- and post-intervention (a 9-month interval). The study investigated how students’ academic writing ability might be influenced by participation in ICs, as measured via attendance. Our research questions were as follows:

Research Question 1: Does an association exist between EAL students’ participation in ICs and shifts in their academic writing skills and lexico-grammatical competence? Research Question 2: Is there a relationship between the extent of participants’ utilization of this resource and the degree of shift in academic writing skills and lexicogrammatical competence?

We anticipated that there would be a relationship between student participation in ICs and shifts in writing ability and lexico-grammatical competence. Specifically, we hypothesized the following:

Hypothesis 1a: Students who attended ICs during the research period would increase their academic writing ability and lexico-grammatical competence.   
Hypothesis 1b: There would be no difference in development across all four rating criteria for academic writing ability (Task Fulfillment, Coherence and Cohesion, Grammar and Vocabulary).   
Hypothesis 2: There would be a relationship between the number of ICs attended during the research period and increases in academic writing ability and lexico-grammatical competence.

# Data Collection Method and Its Rationale

The effectiveness of the IC resource was measured through written text analysis to illuminate criterion indicators of language ability (Bachman, 1990). The two facets to the current analysis were (a) rating written texts according to established criteria to measure participants’ academic writing skills and (b) identifying and quantifying specific lexico-grammatical irregularities. The text types included in the sample were reports and essays. All other text types were rejected, as were texts which had been written collaboratively. Ethical clearance was obtained from the Griffith University Office of Ethics.

Pre- and post-testing were carried out over a 9-month research period to identify longitudinal shifts in academic writing proficiency and lexico-grammatical skills. The analyzed texts were all grade-bearing, high-stakes assessment items from the participants’ discipline courses. The specified minimum length was 1,000 words. In longer texts, lexicogrammatical irregularities were counted from only a 1,000- word section to ensure overall comparability when quantifying error density. Analysis was confined to the central portion of these longer texts rather than the beginning or the end because perusal of pre-intervention texts suggested a lower density of errors at the beginning of texts and higher density toward the end. Quantifying irregularities in these portions would therefore be misrepresentative of the overall text. Note that this system applied only to the quantification of grammatical irregularities: Overall academic skill ratings were based on the entire text irrespective of length.

# Rating Academic Writing Sub-Skills

Two raters separately gave each writing sample a score out of 20, that is, a score out of five for each of four criteria: Task Fulfillment, Coherence and Cohesion, Grammar, and Vocabulary. A simple product–moment correlation between the two raters’ scores, determined to be an adequate measure of the level of agreement, was calculated as $r = . 8 9$ , $p < . 0 0 0$ , indicating a high level of agreement between raters. The four rating criteria were broadly based on the assessment criteria for the IELTS Writing Task 2 (Task Response, Coherence and Cohesion, Lexical Resource, and Grammatical Range and Accuracy; Shaw, 2002). Ratings for these four criteria were calculated using a scale of English language proficiency based on IELTS Writing Task 2 band descriptors, though employing five assessment bands rather than the nine bands used in IELTS descriptors. The assessment bands were limited to five to simplify post-rating quantitative analysis, and also because a higher number of bands produces significantly different relative mean scores in some rating instruments (cf. Dawes, 2008). The descriptor was analytical rather than holistic, requiring raters to focus judgments on specific writing features or skills. This ensured reasonable agreement between raters such that a reliable score could be formulated from dual ratings of a text. It also allowed particular areas of language ability to be profiled, which was useful for the analysis of participants whose language skills were developing at varying rates.

The Task Fulfillment criterion was based on (a) the depth of knowledge about the issue/s being discussed, and the quality and relevance of the ideas presented; (b) whether a clear and consistent argument was maintained throughout the writing sample; and (c) whether academic conventions (e.g., referencing, citation) were followed appropriately. The Coherence and Cohesion criterion was based on (a) whether the structure of the introduction and conclusion was appropriate for an academic text; (b) whether each body paragraph contained a clear topic sentence and one main idea; (c) whether ideas were developed, supported, and logically grouped in paragraphs; and (d) whether there was appropriate use of cohesive devices. The Grammar criterion encompassed (a) accurate and appropriate application of grammar, density of lexico-grammatical irregularities, and communicative effect; (b) whether there was a range of sentence structures; and (c) appropriate punctuation. Finally, the Vocabulary criterion consisted of (a) conveyance of precise meaning and tone through choice of words; (b) the application of a range of appropriate vocabulary, including discipline-specific terms; and (c) accuracy of spelling.

# Quantifying Lexico-Grammatical Irregularities

To afford a more granular insight into participants’ lexicogrammatical development, a taxonomy of 10 frequently occurring lexico-grammatical irregularities was constructed:

Subject/verb agreement (The topic of gun control are   
controversial in the U.S.A.)   
Verb form (Australians enjoy to have democratic   
freedom.)   
Tense (The assignment is submitted yesterday.)   
Word order (The exam one hour will be.) Wrong word used (The most of citizens are in favour of euthanasia rights.)   
Word form (Privately educate is expensive in most countries.)   
Count/non-count nouns (There are too many informations in this summary.)   
Plural/singular (There are three reason why smoking should be banned.)   
Article (We have to take exam at the end of the course.)   
Punctuation (india will host a united nations conference in july.)

Although other lexico-grammatical irregularities occur in the sample, the taxonomy was limited to those listed above as a broader analysis would have been outside the scope of this research. Instances of the 10 irregularities were quantified in each pre- and post-intervention written text. They were identified by the first rater and then verified or eliminated by the second rater. After this calibration, pre–post results were compared to illuminate shifts in the occurrence of each irregularity.

# Participants

The research was carried out on a convenience sample of 31 EAL students from a variety of nationalities and pathways of entry to the university. The two criteria for inclusion were (a) that participants were EAL students in their first semester of study at the university, with an IELTS score of ${ < } 7$ or one or more sub-scores (Reading, Writing, Speaking, or Listening) of $< 6 . 5$ , and (b) that they were currently enrolled in a compulsory one-semester English language enhancement course (Fenton-Smith, Humphreys, Walkinshaw, Michael, & Lobo, 2015) at the university. Although attendance and engagement in courses of this type tend to fluctuate (Lobo & Gurney, 2013), the attendance pattern of the 31 participants in this study was relatively uniform: Their mean participation score for their English language enhancement course (encompassing attendance, engagement, and preparation) was 7.52 out of 10 (standard deviation $[ S D ] = 1 . 0 5 )$ , and the mode was 8, indicating generally high attendance and engagement in the course. This means that in principle the participants were being exposed to a broadly similar amount of English language support at their institution, and we tentatively posit that any additional shifts in their academic writing skills or lexico-grammatical competence were at least partly attributable to their attendance at ICs.

We should note here the issue of attrition. Despite having 98 volunteers, only 31 usable written texts were received. This issue is common in longitudinal studies of this type. Craven’s (2012) study of EAL students’ English language proficiency tested only 40 participants. O’Loughlin and Arkoudis’s (2009) study of EAL students at Australian universities had a sample of 63. Humphreys et al.’s (2012) study yielded 51 participants. Fortunately the current sample $\overset { \cdot } { n } =$ 31) is sufficient for correlational research and to potentially reach statistical significance $( p < . 0 5$ ; Dornyei, 2007). To determine an appropriate power level for this study, a sample size calculation for the first research question (i.e., based upon the potential association between students’ participation in ICs and shifts in their academic writing ability and lexico-grammatical competence) was calculated as $n = 1 8$ with an effect size of 0.7, power of 0.8, and alpha at .05 (Machin, Campbell, Fayers, & Pinol, 1997).

# Procedure for Data Collection and Analysis

Volunteers were solicited through the distribution of flyers at English language enhancement lectures. Anonymity was ensured, reducing potential discrimination against non-participants. Subsequently, writing samples were obtained from participants at two points: the beginning of semester 1 (March) and the end of semester 2 (November). Participants’ attendance at ICs was recorded during the data-collection period. Participants were advised that they could attend consultations as often or as rarely as they chose, and received no additional compensation for attendance. The development of academic writing skill and lexico-grammatical competence, and its tentative association with attendance at ICs, was measured by comparing pre- and post-testing academic writing ratings and the density of lexico-grammatical irregularities with the number of ICs attended over the research period.

# Results

This section addresses the two research questions posed earlier, that is, whether an association exists between IC attendance and shifts in academic writing/lexico-grammatical competence, and whether there is a relationship between extent of IC utilization and degree of shift. Before addressing these research questions, though, the first variable to consider is participant uptake of the resource, due to its impact on other findings. Table 1 presents the number of participants in this study and their median level of attendance. The highest number of sessions attended was 20 $\lceil n = 1 \rceil$ ), and the lowest was 0 $( n = 6 )$ ; $4 8 . 1 \%$ ${ \mathit { n } } = 1 5$ ) attended four or more sessions, but only $1 6 . 1 \%$ ${ \mathrm { \Delta } n = 5 }$ ) attended 10 or more sessions.

Because most participants only attended a small number of consultations, a measurable shift in any aspect of their academic writing competence was not expected. These normally occur over a much longer period; a 0.5 band improvement in IELTS may require 4 to 6 months of contact instruction (Birrell et al., 2006). In the present study, even ten 45-min ICs would equate to only $7 . 5 \mathrm { h r }$ of contact instruction. Hence, rather than use statistical significance as the sole benchmark for success, we illuminate positive patterns, which emerge after a limited period of IC attendance and which might predict future significant shifts in academic writing competence.

Table 1. Attendance at ICs Over the Research Period.   

<html><body><table><tr><td>Number of participants</td><td>Median attendance</td><td>Range</td></tr><tr><td>31</td><td>3.00</td><td>20</td></tr></table></body></html>

The ratings for each of the four academic sub-skills of Task Fulfillment, Coherence and Cohesion, Grammar, and Vocabulary are presented in Table 2. Table 2 offers mean values (as well as $S D$ , range, and median) for the pre- and posttesting ratings of each academic sub-skill as well as a total value (i.e., the sum of each sub-skill).

All four ratings declined from pre- to post-testing. The “Discussion” section will explore this finding in greater detail.

Results from the raters’ identification of the 10 common lexico-grammatical irregularities are presented in Table 3. Again, Table 3 offers mean values (as well as $S D$ , range, and median) for the pre- and post-testing ratings of each of the 10 irregularities.

Of note, more variability between pre- and post-testing measures is evident in these data with six irregularity areas increasing between pre- and post-testing (i.e., verb form, tense, word order, word form, count/non-count, and article) and four irregularity areas declining (i.e., subject/verb, wrong word, plural/singular, and punctuation). We will expand on this pattern in the “Discussion” section.

Sample text ratings were balanced against participants’ attendance at ICs to shed light on the two research questions. The first research question asked whether an association existed between EAL students’ participation in ICs and shifts in academic writing ability/lexico-grammatical competence. To investigate this question, the academic writing sub-skill ratings of participants who attended at least one session $\overset { \cdot } { n } =$ 25) were compared. Because these data did not meet normalcy assumptions for parametric analysis, the question of shift in writing ability and IC attendance was addressed with a Wilcoxon signed-rank test—a test based on the differences between scores in the two conditions being compared. In this case, participant ratings pre- and post-intervention were compared. The Wilcoxon signed-rank test tests the null hypothesis that there would be no difference (i.e., improvement) in the ratings of students’ written samples between pre- and post-testing. The differences in the academic subskills are computed as positive numbers. Data from the tabulated results were entered into PASW SPSS 18 (IBM SPSS, 2010). No significant difference was found for participants whose ratings in the four sub-skills increased after attending at least one IC $( T = 9 , p = . 5 5 , r = - . 2 7 )$ .

A Wilcoxon signed-rank test was also used to further analyze each of the four academic sub-skills ratings in turn to test the null hypothesis that there would be no positive shift in the ratings of participants’ written samples pre- and post-intervention. This time, using a Wilcoxon signed-rank test with a Bonferroni correction for inflated type 1 error (i.e., $p = . 0 5 / 4 = . 0 1 2 5 )$ , no significant difference was found for participants whose written sample showed a higher rating for Task Fulfillment $( T = 8 , p = . 6 1 , r = - . 0 0 3 )$ , Coherence and Cohesion ( $T = 9$ , $p = . 2 5$ , $r = - . 1 7 _ { . }$ ), Grammar $( T = 5$ , $p = . 0 2$ , $r = - . 3 3 ,$ , and Vocabulary $( T = 7 , p = . 0 5$ , $r = - . 2 7 ,$ after attending at least one consultation. With adjustments, no support was found for rejecting the null hypothesis. However, both Grammar and Vocabulary approached statistical significance with reasonable effect sizes, suggesting an association—though no definite statistical link at this stage—between attendance at ICs and shifts in these two sub-skills. We will expand on this finding in the “Discussion” section.

Table 2. Academic Sub Skills Identified by Analysis of Written Submissions (Pre- and Post-Testing).   

<html><body><table><tr><td></td><td colspan="2"></td><td colspan="2">Coherence and</td><td colspan="2"></td><td colspan="2"></td><td colspan="2">Total</td></tr><tr><td></td><td colspan="2">Task Fulfillment</td><td colspan="2">Cohesion</td><td colspan="2">Grammar</td><td colspan="2">Vocabulary</td><td colspan="2"></td></tr><tr><td>Sub-skill</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td></tr><tr><td>N</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td></tr><tr><td>M</td><td>3.35</td><td>3.29</td><td>3.17</td><td>3.12</td><td>2.86</td><td>2.54</td><td>2.89</td><td>2.56</td><td>12.28</td><td>11.55</td></tr><tr><td>SD</td><td>0.77</td><td>0.73</td><td>0.70</td><td>0.69</td><td>0.78</td><td>0.75</td><td>0.72</td><td>0.76</td><td>2.69</td><td>2.66</td></tr><tr><td>Range</td><td>3.00</td><td>2.75</td><td>2.75</td><td>3.00</td><td>3.00</td><td>2.75</td><td>3.25</td><td>2.75</td><td>11.75</td><td>10.25</td></tr><tr><td>Median</td><td>3.50</td><td>3.25</td><td>3.25</td><td>3.25</td><td>3.00</td><td>2.50</td><td>3.00</td><td>2.50</td><td>12.5</td><td>11.25</td></tr></table></body></html>

Table 3. Lexico-Grammatical Irregularities Identified by Analysis of Written Submissions (Pre- and Post-Testing).   

<html><body><table><tr><td></td><td colspan="2">Subject/verb</td><td colspan="2">Verb form</td><td colspan="2">Tense</td><td colspan="2">Word order</td><td colspan="2">Wrong word</td><td colspan="2">Word form</td><td colspan="2">Count</td><td colspan="2">Plural/ singular</td><td colspan="2">Article</td><td colspan="2">Punctuation</td></tr><tr><td>Error</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td><td>Pre</td><td>Post</td></tr><tr><td>N</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td></td><td></td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td>31</td><td></td><td>31</td><td></td><td>31</td></tr><tr><td>M</td><td>4.03</td><td>3.41</td><td>8.45</td><td>11.97</td><td>8.39</td><td>9.00</td><td>2.32</td><td>3.93</td><td>31 23.0</td><td>31 22.8</td><td>6.48</td><td>9.38</td><td>0.97</td><td>1.48</td><td>9.81</td><td>8.9</td><td>31 19.5</td><td>22.54</td><td>31 26.4</td><td>23.4</td></tr><tr><td>SD</td><td>3.44</td><td>2.43</td><td>6.89</td><td>7.46</td><td>5.99</td><td>6.80</td><td>2.19</td><td>2.59</td><td>11.59</td><td>10.48</td><td>4.40</td><td>6.43</td><td>2.02</td><td>1.43</td><td>7.56</td><td>9.44</td><td>10.4</td><td>10.9</td><td>18.36</td><td>10.4</td></tr><tr><td>Range</td><td>14.0</td><td>8.0</td><td>30.0</td><td>29.0</td><td>22.0</td><td>25.0</td><td>7.0</td><td>10.0</td><td>41.0</td><td>40.0</td><td>15.0</td><td>15.0</td><td>22.0</td><td>5.0</td><td>36.0</td><td>47.0</td><td>44.0.</td><td>39.0</td><td>90.0</td><td>51.0</td></tr><tr><td>Median</td><td>3.00</td><td>3.00</td><td>6.00</td><td>11.00</td><td>7.00</td><td>8.00</td><td>2.00</td><td>4.00</td><td>21.00</td><td>22.00</td><td>6.00</td><td>8.00</td><td>0.00</td><td>1.00</td><td>8.00</td><td>6.00</td><td>19.00</td><td></td><td>20.00 22.00</td><td>23.00</td></tr></table></body></html>

Table 4. Attendance Record for Research Participants Over Study Period.   

<html><body><table><tr><td>Academic sub-skill</td><td>rho</td><td>p</td></tr><tr><td>Task Fulfillment</td><td>.033</td><td>.86</td></tr><tr><td>Coherence and Cohesion</td><td>-.080</td><td>.67</td></tr><tr><td>Grammar</td><td>.140</td><td>.44</td></tr><tr><td>Vocabulary</td><td>.058</td><td>.76</td></tr><tr><td>Total</td><td>.048</td><td>.79</td></tr></table></body></html>

In addition, Spearman’s rho was used to measure the strength of the relationship between attendance at ICs and shifts in sub-skill ratings pre- and post-intervention, again with a Bonferroni correction (i.e., $p < . 0 1$ ). The findings are presented in Table 4. In all, no significant increasing or decreasing relationships were uncovered between any of these paired variables.

Lack of statistical significance notwithstanding, some potentially interesting patterns emerged. The majority of the relationships were positive and offered some evidence that the more consultations participants attended, the more their academic writing sub-skill ratings improved. The pattern is not particularly strong except in the Grammar sub-skill, but may predict a positive shift in written language proficiency given continued use of this resource over a sustained period of time. The “Discussion” section will further unpack this finding.

The second research question investigated the relationship between the utilization of ICs and the degree of shift in academic writing skills/lexico-grammatical competence. To address this, we again employed a Wilcoxon signed-rank test to explore each of the common irregularities in turn. The null hypothesis was that there would be no increase or decline in the number of grammatical irregularities in participants’ written samples pre- and post-intervention. A Wilcoxon signed-rank test with a Bonferroni correction for inflated type 1 error revealed no significant difference after attending at least one consultation for the following irregularities: subject/verb ( $T = 1 0$ , $p = . 8 6$ , $r = - . 0 2 )$ , tense $( T = 1 0 , p = . 8 5 , r$ $= - . 0 3 )$ , wrong word $T = 1 1$ , $p = . 6 5$ , $r = - . 0 6 )$ , count/noncount $T = 5$ , $p = . 3 1$ , $r = - . 1 4 )$ ), plural/singular ( $T = 1 1$ , $p =$ .31, $r = - . 1 4 ,$ ), article $T = 1 2 , p = . 7 9$ , $r = - . 0 3$ ), and punctuation $( T = 1 1 , p = . 6 6$ , $r = - . 0 6 )$ ). Instances of three irregularity types increased post-intervention: verb form $( T = 6 , p = . 0 4$ , $r = - . 2 9 )$ , word order $T = 5$ , $p = . 0 2$ , $r = - . 2 4 )$ ), and word form $( T = 8 , p = . 0 9$ , $r = - . 2 4 )$ ), and would actually be significant but for the post hoc adjustment to alpha. Again, no support was found for rejecting the null hypothesis for any of these tests in a direction that indicated improvement.

# Discussion

We now attempt to interpret these findings. Before doing so, though, we need to reiterate that there was only a small degree of shift in language proficiency over 9 months, partly because of participants’ relatively low uptake of the service. Indeed, the low uptake is an important finding in itself: Although several thousand IC sessions are conducted at Griffith University each semester, each individual normally only attends a few sessions. Given the considerable time-ontask normally required to demonstrably increase language proficiency, it is unsurprising that only a limited shift occurred during the research period. That said, several positive patterns emerged in the data, potentially pointing to a more notable shift in linguistic competence after a longer investigation period and with sustained participant utilization of the service. We will examine the patterns here.

# Pattern 1: Irregularity Areas Declined at Post-Test

Four lexico-grammatical items that participants frequently used erroneously in the initial phase were less evident at post-test: subject/verb, wrong word, plural/singular, and punctuation (Table 3). Interestingly, these items all require little or no morphological modification (e.g., changing a verb form) to produce an appropriate variant. Conversely, several of the six irregularity areas that increased in frequency over the research period did call for morphological modification: verb form, word form, and tense. Given the number of variables in play in second language acquisition, it would be simplistic to argue that the items which participants did not need to modify morphologically were more easily processed and used than those they did. At this early stage, we merely speculate that less cognitive processing was required for one than the other (Ellis, 1985, 2008), helping to accelerate processing and appropriate production of the item. Future research will investigate this phenomenon in more detail.

# Pattern 2: Sub-Skill Ratings Approached Significance Pre- and Post-Intervention

Two sub-skill ratings—Grammar and Vocabulary— approached statistical significance between pre- and posttesting, tentatively implying that a perceptible shift in lexico-grammatical competence took place over the research period. This encouraging result may reflect the greater focus of ICs on lexico-grammar than on other academic skills (Freeman, 2011; Moussu, 2013), potentially accelerating lexico-grammatical intake. Other variables in play include the targeted nature of IC services, their individual focus, and the provision of immediate feedback on learner output (Atwood, Turnbull, & Carpendale, 2010). Though not exceptional, this finding could foreshadow an eventual statistically significant shift in lexico-grammatical competence among the sample.

# Pattern 3: Positive Association Between IC Attendance and Sub-Skill Rating Shifts

The third positive pattern comes from analysis of the relationship between the extent of attendance at ICs and shifts in sub-skill ratings (Table 4). None of the ratings approached statistical significance, yet all except Coherence and Cohesion shifted in a positive direction—particularly Grammar, which as we mentioned before is a frequent focus of IC sessions (Freeman, 2011). While a positive result for only one sub-skill could be dismissed as a random occurrence, the apparently similar positive shift in three of the four sub-skill ratings indicates a possible association between extent of attendance at ICs and shifts in sub-skills. Given that these shifts occurred despite limited attendance at ICs, it is reasonable to speculate that more pronounced shifts might be discernable after sustained attendance over a longer time frame.

# Concluding Remarks

In Australia’s results-focused higher education sector (Stevenson & Kokkinn, 2009), it would be desirable to demonstrate that IC services measurably affect EAL students’ academic writing skills. The current research contributes by identifying a likely association between EAL students’ attendance at ICs and gains in their academic writing skills and lexico-grammatical competence. While attending a small number of consultations makes only limited difference to an EAL student’s writing ability, there are predictors that regular attendance over a sustained period will reap benefits.

As we mentioned earlier, this is an initiatory study, intended more to guide future enquiry than to propose robust findings itself. In future, longitudinal research may investigate what degree of positive shift is most likely to occur over time, and in which particular aspects of academic writing. To compensate for our necessarily brief and incomplete taxonomy of lexico-grammatical irregularities, we also propose that future research explore the potential effect of ICs on other such lexico-grammatical irregularities in EAL students’ academic writing. In the meantime, we can report that even if the EAL students in this study have not yet advanced very far, the preliminary data indicate that they have taken initial steps in the right direction.

# Acknowledgments

The authors are grateful to Margaret Brigg, Simon Howell, Pamela Humphreys, and Ian Johnson for their assistance. Thanks also to the anonymous reviewers for their constructive feedback.

# Declaration of Conflicting Interests

The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.

# Funding

The author(s) disclosed receipt of the following financial support for the research and/or authorship of this article: The Office of the Deputy Vice Chancellor (Academic) at Griffith University, Professor Sue Spence.

# References

Arkoudis, S., & Starfield, S. (2007). In-course English language development and support. Canberra: Australian Education International.   
Atwood, S., Turnbull, W., & Carpendale, J. I. M. (2010). The construction of knowledge in classroom talk. Journal of the Learning Sciences, 19, 358-402. doi:10.1080/10508406.2010 .481013   
Bachman, L. F. (1990). Fundamental considerations in language testing. Oxford, UK: Oxford University Press.   
Baik, C., & Greig, J. (2009). Improving the academic outcomes of undergraduate ESL students: The case for discipline-based academic skills programs. Higher Education Research & Development, 28, 400-416. doi:10.1080/07294360903067005   
Barrett-Lennard, S., Dunworth, K., & Harris, A. (2011). The good practice principles: Silver bullet or starter gun? Journal of Academic Language and Learning, 5(2), A99-A106. Retrieved from http:// www.journal.aall.org.au/index.php/jall/article/view/166/112   
Birrell, B. (2006). Implications of low English standards among overseas students at Australian universities. People and Place, 14(4), 53-64. Retrieved from http://search.informit.com.au/doc umentSummary;dn=332480460154352;res=IELHSS   
Birrell, B., Hawthorne, L., & Richardson, S. (2006). Evaluation of the general skilled migration categories. Canberra, Australia: Department of Immigration and Multicultural Affairs. Retrieved from http://www.immi.gov.au/media/publications/ research/gsm-report/index.htm   
Bretag, T. (2007). The emperor’s new clothes: Yes, there is a link between English language competence and academic standards. People and Place, 15(1), 13-21. Retrieved from http:// search.informit.com.au/documentSummary;dn=33486548047 5401;res=IELHSS   
Briguglio, C., & Howe, J. (2006). Critical perspectives: Students’ expectations of difficulties they may face in undertaking their degree. Research and Development in Higher Education, 29, 50-56.   
Chanock, K. (2000, September 21-23). Evaluating one-to-one support for academic literacy: From institution to documentation. Paper presented at the Australian Council for Adult Literacy International Conference, Lens on Literacy, Fremantle, Australia.   
Chanock, K. (2002). Problems and possibilities in evaluating oneto-one language and academic skills teaching. In J. Webb & P. McLean (Eds.), Academic skills advising: Evaluation for program improvement and accountability (pp. 199-221). Melbourne, Australia: Victorian Language and Learning Network.   
Chanock, K. (2007). Valuing individual consultations as input into other modes of teaching. Journal of Academic Language and Learning, 1(1), A1-A9. Retrieved from http://www.journal. aall.org.au/index.php/jall/article/view/1/4   
Coxhead, A., & Byrd, P. (2007). Preparing writing teachers to teach the vocabulary and grammar of academic prose. Journal jslw.2007.07.002   
Craven, E. (2012). The quest for IELTS 7.0: Investigating English language proficiency of international students in Australian universities (IELTS Research Reports Vol. 13). Retrieved from http://www.ielts.org/researchers/research/volume_13. aspx   
Dawes, J. (2008). Do data characteristics change according to the number of scale points used? An experiment using 5-point, 7-point and 10-point scales. International Journal of Market Research, 50, 61-77. Retrieved from http://ssrn.com/ abstract=2013613   
Dornyei, Z. (2007). Research methods in applied linguistics. Oxford, UK: Oxford University Press.   
Ellis, R. (1985). Understanding second language acquisition. Oxford, UK: Oxford University Press.   
Ellis, R. (2008). The study of second language acquisition (2nd ed.). Oxford, UK: Oxford University Press.   
Ewart, D. E. (2009). L2 writing conferences: Investigating teacher talk. Journal of Second Language Writing, 18, 251-269. doi:10.1016/j.jslw.2009.06.002   
Fenton-Smith, B., Humphreys, P., Walkinshaw, I., Michael, R., & Lobo, A. (2015). Implementing a university-wide credit-bearing English language enhancement program: Issues emerging from practice. Studies in Higher Education. Advance online publication. doi:10.1080/03075079.2015.1052736   
Freeman, K. (2011). Investigating the teaching strategies used in one-on-one academic writing consultations (Master’s thesis). Griffith University, Brisbane, Australia.   
Griffith University. (2012). Report on GELES outcomes and activities 2012. Nathan, Australia: Griffith University.   
Harryba, S. A., Guilfoyle, A., & Knight, S. (2011, February 1-2). Staff perspectives on the role of English proficiency in providing support service. In Proceedings of the 20th Annual Teaching Learning Forum. Perth, Australia: Edith Cowan University. Retrieved from http://otl.curtin.edu.au/tlf/tlf2011/ refereed/contents-refereed.html   
Hirsh, D. (2007). English language, academic support and academic outcomes: A discussion paper. University of Sydney Papers in TESOL, 2, 193-211.   
Huijser, H., Kimmins, L., & Galligan, L. (2008). Evaluating individual teaching on the road to embedding academic skills. Journal of Academic Language and Learning, 2(1), 23-38. Retrieved from http://www.journal.aall.org.au/index.php/jall/ article/view/61/54   
Humphreys, P., Haugh, M., Fenton-Smith, B., Lobo, A., Michael, R., & Walkinshaw, I. (2012). Tracking international students’ English proficiency over the first semester of undergraduate study. Canberra: IELTS Australia.   
IBM SPSS. (2010). PASW STATISTICS 18.0 command syntax reference. Chicago, IL: SPSS.   
Jun, S. W., Ramirez, G., & Cumming, A. (2010). Tutoring adolescents in literacy: A meta-analysis. McGill Journal of Education, 45, 219-238. Retrieved from http://mje.mcgill.ca/ article/view/4770   
Lobo, A., & Gurney, L. (2013). An investigation of the links between international students’ expectations and reality in the English Language Enhancement Course. Journal of Further and Higher Education, 38, 730-754. doi:10.1080/03098 77X.2013.817002   
Machin, D., Campbell, M., Fayers, P., & Pinol, A. (1997). Sample size tables for clinical studies (2nd ed.). Oxford, UK: Blackwell Science.   
Moussu, L. (2013). Let’s talk! ESL students’ needs and writing centre philosophy. TESL Canada Journal, 30(2), 55-68.   
Murray, N. (2010). Considerations in the post-enrolment assessment of English language proficiency: Reflections from the Australian context. Language Assessment Quarterly, 7, 343- 358. doi:10.1080/15434303.2010.484516   
O’Loughlin, K., & Arkoudis, S. (2009). Investigating IELTS exit score gains in higher education (IELTS Research Reports Vol. 10, No. 3). Retrieved from http://www.ielts.org/researchers/ research/volume_10.aspx   
Pardoe, S. (2000). A question of attribution: The indeterminacy of “learning from experience.” In M. Lea & B. Stierer (Eds.), Student writing in higher education: New contexts (pp. 125- 146). Buckingham, UK: SRHE and Open University Press.   
Shaw, S. D. (2002). IELTS writing: Revising assessment criteria and scales (Phase 2). Research Notes, 10, 10-13. Retrieved from http://www.cambridgeenglish.org/Images/23124-researchnotes-10.pdf   
Shin, S. J. (2006). Learning to teach writing through tutoring and journal writing. Teachers and Teaching: Theory and Practice, 12, 325-345. doi:10.1080/13450600500467621   
Snow Andrade, M. (2006). International students in Englishspeaking universities. Journal of Research in International Education, 5, 131-154. doi:10.1177/1475240906065589   
Stevenson, M. D., & Kokkinn, B. A. (2009). Evaluating one-toone sessions of academic language and learning. Journal of Academic Language and Learning, 3(2), A36-A50. Retrieved from http://journal.aall.org.au/index.php/jall/article/ view/86/66   
Williams, J. (2004). Tutoring and revision: Second language writers in the writing centre. Journal of Second Language Writing, 13, 173-201. doi:10.1016/j.jslw.2004.04.009   
Wilson, K., Collins, G., Couchman, J., & Li, L. (2011). Co-constructing academic literacy: Examining teacherstudent discourse in a one-to-one consultation. Journal of Academic Language and Learning, 5(1), A139-A153. Retrieved from http://www.journal.aall.org.au/index.php/jall/ article/view/138/103   
Woodward-Kron, R. (2007). Negotiating meanings and scaffolding learning: Writing support for non-English speaking background postgraduate students. Higher Education Research & Development, 26, 253-268. doi:10.1080/07294360701494286   
Yeats, R., Reddy, P., Wheeler, A., Senior, C., & Murray, J. (2010). What a difference a writing centre makes: A small scale study. Education & Training, 52, 499-507. doi:10.1108/00400911011068450   
Zhu, W. (2004). Faculty views on the importance of writing, the nature of academic writing, and teaching and responding to writing in the disciplines. Journal of Second Language Writing, 13, 29-48. doi:10.1016/j.jslw.2004.04.004

# Author Biographies

Ian Walkinshaw is a lecturer in English at the School of Languages and Linguistics, Griffith University, in Queensland, Australia.

Todd Milford works in educational psychology and leadership studies at the University of Victoria in Canada.

Keri Freeman is a doctoral candidate in the School of Education and Professional Studies, Griffith University, in Queensland, Australia.