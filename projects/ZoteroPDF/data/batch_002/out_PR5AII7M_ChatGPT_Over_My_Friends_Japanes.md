# ChatGPT Over My Friends: Japanese English-as-a-ForeignLanguage Learners’ Preferences for Editing and Proofreading Strategies

# Todd J Allen

Faculty of Foreign Language Studies, Graduate School of Foreign Language Education and Research, Kansai University, Japan

# Atsushi Mizumoto

Faculty of Foreign Language Studies, Graduate School of Foreign Language Education and Research, Kansai University, Japan

# Abstract

This study contrasts 33 Japanese English-as-a-foreign-language learners’ experiences using writing groups and artificial intelligence technology (ChatGPT-3.5) for editing and proofreading academic writing assignments. Recent advancements in artificial intelligence technology have significantly influenced academic writing, teaching and learning, prompting researchers to explore the practical application of these technologies in educational settings. In this study, participants edited and proofread their writing in class by using a writing group and experimenting with ChatGPT. After each activity, participants answered a questionnaire about their experiences. In the final questionnaire, participants compared their experiences of each activity and reported on which one they preferred. The results indicate that students mostly prefer using artificial intelligence technology for editing and proofreading. However, they acknowledge some value in writing groups. They assert that technology provides effective feedback, improving clarity and cohesion in their writing. Additionally, the findings reveal preferences for specific prompts to enhance their writing. Overall, this research demonstrates how teachers can use artificial intelligence in language classrooms to improve writing and practices, while emphasizing the importance of ethical implementation.

# Keywords

Artificial intelligence, ChatGPT, academic writing, student preferences, writing groups, English-as a-foreign-language writing development

# Introduction

Artificial intelligence (AI) technology, especially with the advent of ChatGPT and similar technologies, has emerged as a prominent topic in both broader society and specifically in the field of English-language teaching (Hockly, 2023). Borenstein and Howard (2021: 61) have suggested that the ‘creation and deployment of AI is changing our lives and communities in countless ways’, and the higher education space is no exception (Barrot, 2023). One such technology is ChatGPT, a large language model (LLM) developed by OpenAI that can understand and generate text conversationally, simulating chatbased conversations with humans that can be applied in various contexts and respond to prompts and queries and provide feedback.

Teachers and researchers are currently grappling with the implications of integrating such technologies in the classroom, discovering both positive and negative uses (e.g. AlAfnan et al., 2023; Dwivedi et al., 2023) across several fields including applied linguistics (e.g. Godwin-Jones, 2022; Kohnke et al., 2023). Some researchers are exploring questions regarding the suitability of AI technology for student use and engaging in discussions about the ethical implications of integrating such technologies into higher education contexts (e.g. Kasneci et al., 2023). However, despite these discussions, there remains a notable gap in our understanding of ChatGPT’s practical implementation and effectiveness within the foreign-language classroom. Although ethical considerations are important, it is equally essential to conduct research that examines the actual use and impact of such technologies in language learning environments. The limited research in this specific area underscores the urgency and significance of further investigation to inform best practices and pedagogical approaches in second language teaching.

In the context of English-as-a-foreign-language (EFL) classrooms, researchers have underscored the importance of integrating proofreading and editing activities into students’ overall writing development and practice (e.g. Naghdipour, 2022). However, there has been much debate on the value of explicit error correction, such as grammar and other forms of feedback, in the language learning classroom (e.g. Ferris, 1999; Truscott, 1996). Thus, questions persist about how to instruct and develop students’ editing and proofreading skills (Godwin-Jones, 2022). Limited current studies show that AI interventions may be effective in developing these skills (Barrot, 2023). Furthermore, there is a noticeable gap in the literature regarding the exploration of student experiences and preferences concerning editing and proofreading activities and various technologies (Schmidt-Fajlik, 2023). This study seeks to address this gap by examining Japanese EFL students’ preferences for traditional activities, such as writing groups, and emerging AI technologies for editing and proofreading academic writing. By examining students’ willingness and confidence in adopting both methods during the final stages of the writing process, this research aims to shed light on practical and effective strategies for enhancing writing skills in the EFL context.

Understanding students’ attitudes and preferences towards different teaching methods is vital for effective language education. This study’s findings can inform educators on integrating traditional and technological approaches to enhance students’ writing skills. Additionally, by identifying challenges in adopting AI technology for writing instruction, this research provides practical recommendations for educators and policymakers.

# Strategies Used in Classes for Editing and Proofreading

Previously, scholars have explored different methods to develop students’ writing through various proofreading and editing activities, namely, consciousness-raising checklists and worksheets, explicit feedback and peer-review activities (Ferris, 2003; Lee, 2015; Yu and Hu, 2017). Error correction exercises that ask students to identify errors, such as grammar, vocabulary or punctuation mistakes, are often used. This activity develops students’ abilities to detect and correct errors in their writing (Hedge, 2005). In addition, self-editing checklists or rubrics have been identified as valuable tools that guide students through a structured framework for reviewing their work (Cogie et al., 1999). Another more social approach is peer editing/reviewing, where students exchange their written work with a classmate and provide constructive feedback on areas that need improvement (often using a checklist). This collaborative activity allows students to learn from each other, develop analytical skills to find writing errors and refine their editing skills (Hedge, 2005).

Teachers employ peer writing groups as an effective strategy to enhance students’ editing and proofreading skills (Allen, 2019). These small groups bring students together to share their written work and engage in constructive feedback sessions. The focus within these groups spans various aspects of the writing process, such as editing and proofreading. This collaborative approach cultivates a supportive environment, enabling students to benefit from multiple perspectives on their writing and gain valuable insights for improvement (Allen, 2019).

From the perspective of undergraduate EFL learners, writing groups, not only aid in developing critical thinking skills and error identification, but also encourage selfcorrection (Zemach and Rumisek, 2003) and foster social activities through discussions about writing experiences (Aitchison, 2009; Fegan, 2016). By participating in writing groups, students benefit from receiving feedback and developing their editing skills by evaluating and suggesting improvements in their peers’ work (Fegan, 2016). However, some challenges include the potential to focus on form over content, limited opportunities for authentic writing practice, and the time-consuming nature of traditional proofreading tasks (Ferris, 2003; Hedge, 2005). Overall, these benefits and challenges can be strengthened and mitigated using technology in the classroom for editing and proofreading, which are explored in the next section.

# Technology-Assisted Editing and Proofreading in the Classroom

Teachers enhance EFL writing classes by promoting technology use. This includes basic software such as Microsoft Word, which have spell-checkers and grammar tools (John and Woll, 2020). Moreover, advanced tools like Grammarly (Schmidt-Fajlik, 2023) enable self-editing with sophisticated grammar, spelling and style checks. Collaborative platforms like Google Docs support real-time peer editing and feedback exchange (Hoang and Hoang, 2022). Teachers also provide digital feedback using features like track changes and comments (AbuSeileek, 2013; Shintani and Aubrey, 2016) to explicitly address student errors and enhance awareness.

Recently, scholars and educators have shown growing interest in implementing advanced LLMs, including ChatGPT, across diverse educational settings (Barrot, 2023; Schmidt-Fajlik, 2023). In their commentary on LLMs across different contexts,

Kasneci et al. (2023) highlighted several key affordances of incorporating ChatGPT into education, including its potential to enhance all five language skills: reading, writing, listening, speaking and culture. By ‘culture’, we refer to the broader socio-cultural aspects inherent in language learning, such as understanding cultural nuances, social norms, customs and traditions associated with the target language, which can apply to developing second-language (L2) writing.

Mizumoto and Eguchi’s (2023) recent research explored the potential of integrating ChatGPT into L2 writing settings, particularly focusing on its effectiveness in automated essay scoring (AES). Their study, analysing 12,100 essays, revealed a notable level of accuracy and reliability in AES using a generative pre-trained transformer (GPT-3 text-davinci-003 model), offering valuable support for human evaluations. Additionally, the analysis indicated that leveraging linguistic features could further enhance scoring accuracy. The authors suggest that ChatGPT’s capabilities make it a viable tool for providing automated corrective feedback, presenting a valuable resource for classroom teachers.

Similarly, in their review of technology which aimed to examine the advantages and challenges of incorporating ChatGPT into L2 writing environments, Barrot (2023) found that ChatGPT can offer insights into diverse aspects of writing, encompassing word choice, coherence, structure, language style and grammar. Regarding L2 writing, editing and proofreading, ChatGPT can be helpful if systematically implemented and supervised by teachers in the classroom. For example, ‘ChatGPT’s editing feature to improve language style, vocabulary, and grammar of their final draft’ (Barrot, 2023: 5) can be used as an effective tool for students in the writing process.

Although ChatGPT offers positive affordances, its use for editing and proofreading may lead to overreliance on technology, hindering critical thinking and language awareness (Barrot, 2023). Technology access challenges, potential biases and AI hallucinations (i.e. generating outputs that are nonsensical) require careful intervention and supervision (Kasneci et al., 2023). Educators should promote efficient editing with technology while ensuring a balanced approach to develop language and information technology (IT) literacy skills (Barrot, 2023), while also acknowledging the potential disparities in resource access (Allen, 2021).

Despite the growing interest in AI-assisted language teaching, research is fragmented, primarily focusing on traditional or technology-based methods. Comparative studies are limited, especially in assessing technologies like ChatGPT against traditional methods like writing groups. Studies that examine ChatGPT’s real-world implementation and use in classrooms are also limited. Additionally, comprehension of students’ experiences and preferences when using AI technology versus traditional methods for editing and proofreading academic writing assignments is needed.

In response to this identified gap in the literature, the following research questions are proposed:

1) How do students’ experiences differ when using writing groups compared to AI technology for editing and proofreading academic writing assignments?   
2) What are students’ preferences for editing and proofreading academic writing assignments?

In the next section, we detail how we address these research questions.

# Methodology

# Participants and Course

Thirty-three first-year undergraduate English-language majors (11 males, 22 females, aged 18–19) participated in the study. Of the participants, 16 had intermediate and 17 had upper-intermediate English proficiency, and all used a Common European Framework of Reference (CEFR) B1-level academic writing textbook. Participants undertook a TOEFL Institutional Testing Program (ITP) test upon entering the university and were assigned a level of proficiency for their language courses. The participants were purposively selected (Creswell and Creswell, 2018), as they were developing academic writing skills, including proofreading and editing, through a one-year course.

Students were enrolled in an academic writing class that aimed to develop fundamental writing skills in English. This is a year-long course divided into two semesters. In the first semester, students complete various assignments and activities focused on developing their paragraph-writing skills. Students focus on developing their essay-writing skills in the latter half of the year. During the first semester, students are expected to complete three writing assignments: (a) a narrative paragraph; (b) a descriptive paragraph; and (c) an opinion paragraph. In this study, we focused only on (a) and (b). For each task, students are instructed through a process approach to writing. This approach involves instructed prewriting, drafting, revising, editing and publishing. Table 1 outlines the descriptions for each assessment.1 Although students chose their own topics, assignments follow a consistent structure. This includes organizing topic sentences, developing supporting information, using transition words, and concluding the writing effectively. However, the content covered may vary slightly (e.g. describing a logo or a product).

For four weeks per assignment, students worked on their writing, including proofreading and editing in class. Writing groups were introduced for the narrative paragraph, whereas ChatGPT with specific prompts was introduced for the descriptive paragraph.

Table 1. Assessment descriptions presented to students.   

<html><body><table><tr><td>Assessment</td><td colspan="2">Description</td></tr><tr><td rowspan="2">Narrative Paragraph</td><td>!)</td><td>Choose between the two following scenarios:. Share a personal or someone else&#x27;s experience of overcoming a challenge, such as trying something new or facing a difficult situation like learning a language, playing a challenging game, or traveling to a new</td></tr><tr><td>2)</td><td>place. Narrate a story highlighting the assistance of a person or animal in helping someone overcome a physical or emotional challenge,. whether it&#x27;s a teacher aiding a friend with public speaking fears or a dog</td></tr><tr><td rowspan="3">Descriptive Paragraph</td><td></td><td>supporting a disabled veteran in facing their fears.. Choose between assignment I or assignment 2 below:.</td></tr><tr><td>!)</td><td>Describe a favourite business, for example, a store, a hotel, a company, or a restaurant. The business should be a specific one that you like for</td></tr><tr><td>2)</td><td>specific reasons. Describe one of your favourite products or logos. The product could be something you bought recently. The logo could be one that appeals to you for specific reasons and has made you like a specific brand.</td></tr></table></body></html>

# Procedure

As mentioned above, for each paragraph assignment, students were given four weeks to complete and submit the final version of the task. The fourth week of class included proofreading and editing activities to help improve the final version. For the narrative paragraph, students self-assigned groups.2 Then students were introduced to writing groups and their function and purpose. Students then used a standard checklist (as featured in the textbook) to guide them in proofreading and editing each other’s writing. For example, Figures 1 and 2 show the checklist used in the writing groups.

For the descriptive paragraphs, students were first introduced to ChatGPT. This involved explaining what ChatGPT is and how it is used (e.g. specific prompts). We also discussed ethical implications (e.g. plagiarism). Students registered a free account and were given a list of specific prompts (see Appendix A) to edit and proofread their paragraphs. Once students completed editing and proofreading their paragraphs using ChatGPT, they sent the chat screen to the instructor. After each activity (i.e. writing group and ChatGPT), they were then given access to the questionnaire and asked to reflect on their experiences. Overall, students completed one questionnaire for each assessment item and a comparative questionnaire, which are explored below.

# Instrument

Questionnaires have been widely used in applied linguistics, including those studies that have investigated writing development (e.g. Allen, 2019), L2 writing instruction (e.g. Cheng, 2004) and perceptions on various forms of feedback (e.g. Lee, 2015). These studies show that effective survey and questionnaire design can yield significant insights into participants’ experiences, processes, preferences and language use. Thus, the questionnaire was considered an appropriate instrument to obtain participants’ insights and

# Your Own Writing

![](img/ab875c78db388f3984aee077411c36aa9e2880dee26690f9e6073ba9afef4de9.jpg)  
Figure 1. Checklist for proofreading narrative paragraph assignment (Ward, 2012: 21).

![](img/ed97762c1ee5ca5d039f66a6592afc456433a5f752037cda3b9e25ffc1ee2f71.jpg)  
Figure 2. Checklist for editing narrative paragraph assignment (Ward, 2012: 25).

preferences about their experiences in a writing group and their use of ChatGPT to edit and proofread their work. Each questionnaire was available in Japanese and English.

For all questionnaire items, participants were required to respond to statements using a 5-point Likert scale of strong agreement or strong disagreement. Participants provided their informed consent prior to responding to any questionnaire items.

The first questionnaire contained 10 items (Appendix B) probing the participants’ feelings about the feedback they both provided and received within their writing group. These items were intended to capture their overall experiences and perceptions of their writing group interactions.

For the second questionnaire, participants were asked about their experiences using ChatGPT and how they felt when using it for proofreading and editing their work. It included 13 items (Appendix C). A further set of 10 items (Appendix D) in this questionnaire asked participants to compare their experiences using ChatGPT with their experiences in the writing group.

# Analyses

Questionnaire 1 featured 10 items that sought to ascertain the benefits participants found in the writing group. Each item was rated using a 5-point scale. The Cronbach’s alpha value, indicative of internal consistency across these 10 items, was confirmed at a robust .91. This high value implies that respondents’ answers were consistent. Consequently, we proceeded to analyse the descriptive statistics and scrutinize the distribution of our data.

One of the questionnaire items was: ‘How helpful was the writing group in enhancing your narrative paragraph writing skills?’ Given the potential of this question to serve as a dependent variable, we adopted the other nine items as predictors, which allowed us to gauge the influence of each. During this analysis, we noticed the skewed distribution of responses. To counteract this and accurately establish the importance of the variables, we utilized a machine-learning method known as the random forest. Random forest is a technique used for predictive modelling and behaviour analysis and is built on decision trees. It contains many decision trees representing a distinct instance of the classification of data input into the random forest (see Mizumoto, 2023 for details). The decision to apply the random forest model exclusively to the writing group responses was based on the presence of a specific item in Questionnaire 1 that could serve as a suitable dependent variable, directly assessing the perceived usefulness of the writing group strategy. In contrast, Questionnaire 2 (ChatGPT) did not include a directly comparable item that could serve as a clear outcome variable for the ChatGPT strategy. Given this difference in the questionnaire structures, we focused the random forest analysis on the writing group responses to investigate the factors influencing students’ perceptions of the writing group’s effectiveness.

Following this, we analysed Questionnaire 2, which was focused on participants’ overall experiences with ChatGPT as a proofreading and editing tool. Mirroring Questionnaire 1, we established a high internal consistency with a Cronbach’s alpha value of .96 across the 13 items. Subsequently, we delved into descriptive statistics and scrutinized the data distribution.

Drawing from the data, we inspected the response patterns from 10 different perspectives to ascertain which approach – ChatGPT or writing groups – participants found more effective for editing and proofreading tasks. We also accommodated for responses indicating equal effectiveness of both approaches (i.e. ChatGPT and writing groups) or the lack of effectiveness of either (‘Neither’).

All data analyses and visualizations were conducted using R version 4.2.3. To ensure the reproducibility and transparency of the data analysis process, the data, the prompt and the R code used in the study have been made accessible on the Open Science Framework (OSF) (https://osf.io/kqphy/).

# Results and Discussion

# Writing Groups (Questionnaire 1)

Upon analysing the descriptive statistics (see the online supplementary material on the OSF (https://osf.io/kqphy/) for further details) and assessing the distribution of the data, we discovered that the responses of most participants leaned towards a value of 3 or higher. Figure 3 represents the response distribution for the 10 items in Questionnaire 1 (see Appendix B for each item), with answers ranging from 1 to 5. In this figure, the sum of 1 (Strongly disagree) and 2 (Disagree) is presented on the left, 3 (Neutral) is centred and the combined percentage of 4 (Agree) and 5 (Strongly agree) is displayed on the right. These results clearly indicate that the participants held a highly positive view of their involvement in the writing group.

Figure 4 presents a measure of variable importance, indicating how the target variable – ‘How helpful was the writing group in enhancing your narrative paragraph writing skills?’ (‘Overall Group Useful’ in Figure 3) – was predicted by the other nine predictor variables (i.e. questionnaire items), using random forest. Since random forest incorporates random numbers in its analysis, results can change with each iteration, but it was found that the model typically explains approximately $50 \%$ of the variance.

As shown in Figure 4, learners who responded positively to the ‘Confident’ item (Did the writing group help you feel more confident about your writing abilities?) are also likely to rate the ‘Overall Group Useful’ highly. Similar trends were observed among learners who selected high scores for items such as ‘Writing Skill Up’ (Did the writing group help you improve your academic writing skills?), ‘Useful Feedback’ (Did you find the feedback from your peers useful and informative?) and ‘Easy to Incorporate’ (Did you find it easy to incorporate the feedback you received into your writing?).

![](img/c5a9aa65a59d155448f3f9c85e3ee7575bc2db368b94a3f55504db6f9b5d107b.jpg)  
Figure 3. Responses to Questionnaire 1 (Writing Groups). Note: $n = 3 3$ . Response 1 represents ‘Strongly Disagree’, 2 represents ‘Disagree’, 3 represents ‘Neutral’, 4 represents ‘Agree’ and 5 represents ‘Strongly Agree.’ See Appendix B for each item.

![](img/a2d73bf5642f7b6c325f75bb9740be40d214996f2161c3ce9320d5b276025ae1.jpg)  
Figure 4. Variable importance in predicting overall writing group usefulness.

Two items with low importance, ‘Comfortable’ (Did you feel comfortable sharing your writing with the group?) and ‘Same Person’ (Did you feel that you were receiving feedback primarily from the same person?), are logically considered to have a minimal connection as they do not directly relate to writing skill improvement.

Overall, students reported that they felt comfortable sharing their assignments with one another and found it useful to give and receive feedback from one another. Although the benefits and perceptions of group work and peer review are debated in L2 research (e.g. Morgan, 2023), the results demonstrate that students enjoyed working in groups to get feedback on their writing. Upon analysing and reflecting on participants’ responses, mainly focusing on questions 6 and 8 in the questionnaire, it becomes evident that there is a need for greater specificity. Moreover, participants found it challenging to integrate the feedback into their editing and proofreading process, primarily due to a lack of knowledge or skills to effectively enhance the writing, both on their part and their partner’s.

# ChatGPT (Questionnaire 2)

Before delving into the results of Questionnaire 2 (ChatGPT), it is important to examine the participants’ baseline use of various tools and resources. At the time of the survey in May 2023, a substantial majority, 25 out of 33 students $( 7 5 . 7 6 \% )$ , had never used ChatGPT. The participants reported the use of several other tools and resources, with responses allowing for duplication. These included Google Translate $( 3 0 . 3 0 \% )$ , DeepL $( 2 7 . 2 7 \% )$ , Google’s search engine $( 1 2 . 1 2 \% )$ , online dictionaries $( 9 . 0 9 \% )$ and translation apps on smartphones $( 6 . 0 6 \% )$ . Interestingly, four students $( 1 2 . 1 2 \%$ ) reported that they did not use any of these resources and tools in their writing.

Figure 5 shows the results from Questionnaire 2, which was comprised of 13 items examining participants’ experiences and perceptions of using ChatGPT as a proofreading and editing tool. The figure shows that a significant majority of participants chose 4 (Agree) and 5 (Strongly Agree), demonstrating a positive response to the feedback provided by ChatGPT.

For Item 13 (Confidence), where a score of 3 (Neutral) was a prevalent response, accounting for $3 3 \%$ of replies, the question asked for a self-assessment of participants’ confidence in implementing the suggestions made by ChatGPT. The question posed was: ‘I feel confident in my ability to apply the suggestions provided by ChatGPT to enhance the quality of topic sentences, body sentences, concluding sentences, and transitions in my future writing.’ Therefore, this item was not a measure of ChatGPT’s usefulness as a proofreading and editing tool but rather an indicator of the learners’ self-efficacy or perceived proficiency.

These findings indicate that learners consider ChatGPT’s feedback beneficial and directly applicable to their writing tasks, affirming its effectiveness as a proofreading and editing tool. Although confidence levels were comparable to the writing group, most participants found ChatGPT’s corrections accurate, reliable and easy to integrate into their work.

![](img/4d211b821ccf07f38bf551049490acabd18294525a8befdf8007c7a3160aa57f.jpg)  
Figure 5. Responses to Questionnaire 2 (ChatGPT). Note: $n = 3 3$ . Response 1 represents ‘Strongly Disagree’, 2 represents ‘Disagree’, 3 represents ‘Neutral’, 4 represents ‘Agree’ and 5 represents ‘Strongly Agree.’ See Appendix C for each item.

AI feedback generated in the study results from precise prompts (Fuchs, 2023; Wu, 2017). Clear directions to ChatGPT ensure that the feedback aligns with the intention of the exercise. As Javan et al. (2023) noted, prompt engineering is crucial for maximizing ChatGPT’s potential in specialized fields. Although participants did not create prompts, they used specific directions tailored to their assignments for optimized learning (Fuchs, 2023). This suggests that teachers should carefully create prompts to provide accurate information that aligns with the task’s intended purpose (Fuchs, 2023). However, this may require additional planning time.

One potential drawback is students’ dependency on technology, hindering skill development (Fuchs, 2023). Using specific prompts, such as summarizing corrections, can address this issue and raise awareness of errors in grammar, word choice, spelling and structure (Barrot, 2023), as seen in Figure 6. This is likely why most participants preferred prompts summarizing changes in a table along with overall ratings and suggestions.

The results presented in Figures 3 and 5 clearly demonstrate that learners have responded favourably to both writing groups and ChatGPT as methods for proofreading and editing. This provokes an intriguing question: do learners have a preference between writing groups and ChatGPT? The following subsection explores a more comparative analysis to address this question.

![](img/3e994d111f921f8736dd16ad197b5af922f048e1f30b10785d6f079babf35bbd.jpg)

# Anonymous

![](img/d422d91b034afd1ac1317bc66e16cb32314ad4777548194951f9f42aaebd99ba.jpg)  
Figure 6. Example of ChatGPT feedback for listing changes in a table.

# Writing Groups versus ChatGPT

Figure 7 offers a summarized analysis of a questionnaire (Appendix D) that surveyed respondents across 10 distinct aspects, which included 9 varied perspectives and ‘overall satisfaction’. The aim was to identify which category—ChatGPT, Writing Groups, both or neither—resonated the most with each participant.

![](img/7c4c38bf7bd2fadbbb9f2998c0339bd513eab2aac9c2df236616a199d6ff5984.jpg)  
Figure 7. Comparative evaluation of editing and proofreading: Writing groups versus ChatGPT. Note: $n = 3 3$ .

More than half of the learners selected ChatGPT, a choice that substantially outnumbered the responses in the ‘Both’ category. Particularly surprising was the minimal number of learners who stated that ‘Writing Groups are more effective’, a position that contradicts the initial impression given by the results of Figure 3.

Despite acknowledging the value of writing groups for proofreading and editing, learners find AI-based feedback, particularly from ChatGPT, more beneficial. They reported that ChatGPT offers clear and specific feedback for editing and proofreading. Additionally, the results indicated that ChatGPT’s responsiveness and feedback quality surpassed the writing group, providing an overall advantage in feedback type and speed.

Scholars have suggested that students prefer and prioritize feedback from an authoritative figure such as their teacher (Nelson and Carson, 1998; Paulus, 1999) but stress that ‘a combination of peer and instructor feedback in undergraduate writing courses plays a crucial role in improving students’ writing’ (Alharbi and Alqefari, 2022: 114). In this case, ChatGPT serves a dual role by providing authoritative insights into students’ writing, distinct from language learners in writing groups. It also delivers feedback that learners can comprehend, akin to the peer review process that occurs in writing groups $\mathrm { Y u }$ and Lee, 2016). This may explain why participants reported that ChatGPT offered more effective feedback as they may not trust their peer reviewer to have the same abilities or knowledge as the AI.

Although some students preferred ChatGPT over the writing group, others valued the collaborative peer review process, as shown in Figure 7. The results indicate that students found benefits in collaborative writing with their peers. Previous research supports the positive impact of writing groups and peer review on writing skills and learning (Yu and Lee, 2016; Zemach and Rumisek, 2003). To further illustrate this, when problems arose using ChatGPT, it was observed in the classroom that they grouped themselves together to solve problems. This suggests that although they used ChatGPT to edit and proofread their work, they used writing groups to solve technical and other writing-related problems. Related research has suggested that AI technology may diminish social aspects of learning (Yu and Lee, 2016). To address these challenges, universities should ensure that natural language processing (NLP) models serve as supplements, not replacements for human interaction, encouraging collaboration in the classroom. We suggest ways to combine both ChatGPT and writing groups to enhance editing and proofreading practices, fostering social and collaborative learning, and address potential drawbacks associated with AI technology in education.

Lastly, while exploring ChatGPT’s positive impact, we recognize academic and ethical concerns, including potential plagiarism (Fuchs, 2023). Transparent instruction is crucial. In our study, students used specific prompts and shared their chat screen with the classroom teacher, preventing plagiarism. Additional solutions like AI detectors may be required in the future.

# Concluding Remarks

This study demonstrated students’ preferences for using writing groups and AI tools to improve the quality of their written assignments through guided (and explicit) editing and proofreading activities. The findings indicate that although writing groups were deemed practical for improving writing, ChatGPT emerged as the preferred option due to its provision of specific, timely and easily implementable feedback. However, it is essential to acknowledge the study’s limitations.

Firstly, the research is confined to a single specific context with a limited number of participants, potentially limiting the generalizability of the results. In addition, not distinguishing between the two writing genres (i.e. narrative and descriptive) may influence the study’s outcome. Secondly, an in-depth analysis of the types of feedback students received from both ChatGPT and writing groups (e.g. grammatical, lexical and structural suggestions) should be conducted. Thirdly, a theoretical basis for developing the questionnaire items is needed. Emphasis was placed on the practical aspects of comparing the two activities, and the items were designed to capture students’ experiences and preferences. Future research should address this limitation by grounding the questionnaire development process in a well-established theoretical framework and further confirming the validity of the conceptualized variables. Despite these limitations, this research addresses a significant gap in the literature by demonstrating practical and guided implementation of these tools in EFL writing classes.

# Funding

The authors received no financial support for the research, authorship and/or publication of this article.

# ORCID iD

Todd J. Allen

https://orcid.org/0000-0002-4958-3193

# Notes

1. For the narrative paragraph, 28 participants opted for option 1, whereas only 5 chose option 2. For the descriptive paragraph, 21 participants favoured option 1, whereas 12 opted for option 2.

2. Students were allowed to assign groups themselves. Previous research has suggested (e.g. Myers, 2012) that students are motivated and engage groupwork more effectively when they can self-assign groups.

References   
AbuSeileek AF (2013) Using track changes and word processor to provide corrective feedback to learners in writing. Journal of Computer Assisted Learning 29(4): 319–333.   
Aitchison C (2009) Writing groups for doctoral education. Studies in Higher Education 34(8): 905–916.   
AlAfnan MA, Dishari S, Jovic M, et al. (2023) ChatGPT as an educational tool: Opportunities, challenges, and recommendations for communication, business writing, and composition courses. Journal of Artificial Intelligence and Technology 3(2): 60–68.   
Alharbi MA and Alqefari AN (2022) Students’ uptake and perspectives on teacher and peer feedback on written assignments. Learning and Teaching in Higher Education: Gulf Perspectives 18(2): 107–118.   
Allen TJ (2019) Facilitating graduate student and faculty member writing groups: experiences from a university in Japan. Higher Education Research & Development 38(3): 435–449.   
Allen TJ (2021) Infrastructure, literacy and communication: The challenges of emergency remote teaching in a university in Japan. In: Chen J (ed.) Emergency Remote Teaching and Beyond. New York: Springer.   
Barrot JS (2023) Using ChatGPT for second language writing: Pitfalls and potentials. Assessing Writing 57: 100745.   
Borenstein J and Howard A (2021) Emerging challenges in AI and the need for AI ethics education. AI and Ethics 1: 61–65.   
Cheng YS (2004) A measure of second language writing anxiety: Scale development and preliminary validation. Journal of Second Language Writing 13(4): 313–335.   
Cogie J, Strain K and Lorinskas S (1999) Avoiding the proofreading trap: The value of the error correction process. The Writing Center Journal 19(2): 7–32.   
Creswell JW and Creswell JD (2018) Research Design: Qualitative, Quantitative, and Mixed Methods Approaches. 5th edn. London: SAGE.   
Dwivedi YK, Kshetri N, Hughes L, et al. (2023) ‘So what if ChatGPT wrote it?’ Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management 71: 102642.   
Fegan S (2016) When shutting up brings us together: Several affordances of a scholarly writing group. Journal of Academic Language and Learning 10(2): A20–A31.   
Ferris D (1999) The case for grammar correction in L2 writing classes: A response to Truscott (1996). Journal of Second Language Writing 8(1): 1–11.   
Ferris D (2003) Response to Student Writing. Mahwah: Lawrence Erlbaum.   
Fuchs K (2023) Exploring the opportunities and challenges of NLP models in higher education: Is Chat GPT a blessing or a curse? Frontiers in Education 8.   
Godwin-Jones R (2022) Partnering with AI: Intelligent writing assistance and instructed language learning. Language Learning & Technology 26: 5–24.   
Hedge T (2005) Writing. Oxford: Oxford University Press.   
Hoang DTN and Hoang T (2022) Enhancing EFL students’ academic writing skills in online learning via Google Docs-based collaboration: A mixed-methods study. Computer Assisted Language Learning: 1–23.   
Hockly N (2023) Artificial intelligence in English language teaching: The good, the bad and the ugly. RELC Journal 54(2): 445–451.   
Javan R, Kim T, Mostaghni N, et al. (2023) ChatGPT’s potential role in interventional radiology. CardioVascular and Interventional Radiology 46: 821–822.

John P and Woll N (2020) Using grammar checkers in an ESL context: An investigation of automatic corrective feedback. Calico Journal 37(2): 193–196.

Kasneci E, Sessler K, Küchemann S, et al. (2023) ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences 103: 102274.   
Kohnke L, Moorhouse BL and Zou D (2023) ChatGPT for language teaching and learning. RELC Journal 54: 537–550.   
Lee MK (2015) Peer feedback in second language writing: Investigating junior secondary students’ perspectives on inter-feedback and intra-feedback. System 55: 1–10.   
Mizumoto A (2023) Calculating the relative importance of multiple regression predictor variables using dominance analysis and random forests. Language Learning 73(1): 161–196.   
Mizumoto A and Eguchi M (2023) Exploring the potential of using an AI language model for automated essay scoring. Research Methods in Applied Linguistics 2(2): 100050.   
Morgan E (2023) Japanese Tertiary students’ perceptions of group work with explicit scaffolding. Language Learning in Higher Education 13(2): 371–391.   
Myers SA (2012) Students’ perceptions of classroom group work as a function of group member selection. Communication Teacher 26(1): 50–64.   
Naghdipour B (2022) ICT-enabled informal learning in EFL writing. Journal of Second Language Writing 56: 100893.   
Nelson GL and Carson JG (1998) ESL Students’ perceptions of effectiveness in peer response groups. Journal of Second Language Writing 7(2): 113–131.   
Paulus TM (1999) The effect of peer and teacher feedback on student writing. Journal of Second Language Writing 8(3): 265–289.   
Schmidt-Fajlik R (2023) ChatGPT as a grammar checker for Japanese English language learners: A comparison with Grammarly and ProWritingAid. AsiaCALL Online Journal 14(1): 105–119.   
Shintani N and Aubrey S (2016) The effectiveness of synchronous and asynchronous written corrective feedback on grammatical accuracy in a computer-mediated environment. The Modern Language Journal 100(1): 296–319.   
Truscott J (1996) The case against grammar correction in L2 writing classes. Language Learning 46(2): 327–369.   
Ward C (2012) Focus on Writing 3. Pearson.   
Wu JY (2017) The indirect relationship of media multitasking self-efficacy on learning performance within the personal learning environment: Implications from the mechanism of perceived attention problems and self-regulation strategies. Computers & Education 106: 56–72.   
$\mathrm { Y u } \mathrm { s }$ and Hu G (2017) Understanding university students’ peer feedback practices in EFL writing: Insights from a case study. Assessing Writing 33: 25–35.   
$\mathrm { Y u } \mathrm { s }$ and Lee I (2016) Peer feedback in second language writing (2005–2014). Language Teaching 49(4): 461–493.   
Zemach DE and Rumisek LA (2003) Academic Writing from Paragraph to Essay. New York: Macmillan.

# Appendix A

# Prompts Students Used During the ChatGPT Editing and Proofreading Activity

1) Deconstruct my descriptive paragraph highlighting the topic sentence, body sentences, concluding sentence(s) and transition words.   
2) Overall, rate my descriptive paragraph out of 15 and give me suggestions to make it better.

3) Is my topic sentence appropriate for a descriptive paragraph?   
4) Are the body sentences appropriate for a descriptive paragraph?   
5) Are the body sentences coherent and logical for a descriptive paragraph?   
6) Are the transition words appropriate for a descriptive paragraph?   
7) Is my concluding sentence(s) appropriate for a descriptive paragraph?   
8) Proofread and edit my paragraph.   
9) List the changes you made to my paragraph in a table.

# Appendix B

# Questionnaire 1 (Writing Groups)

1) How helpful was the writing group in improving your narrative paragraph writing skills? (Overall Group Useful)   
2) Did you feel comfortable sharing your writing with the group? (Comfortable)   
3) Did you find the feedback from your peers useful and informative? (Useful Feedback)   
4) Did the writing group help you improve your academic writing skills? (Writing Skill Up)   
5) Did the writing group help you feel more confident about your writing abilities? (Confident)   
6) Using the scale below, did you feel that the feedback you received was specific and detailed enough to help you improve your writing? (Specific)   
7) Did you feel that the feedback you received was constructive and supportive? (Constructive)   
8) Did you find it easy to incorporate the feedback you received into your writing? (Easy to Incorporate)   
9) Did you feel that you were receiving feedback primarily from the same person? (Same Person)   
0) Did you find it helpful to read and provide feedback on the writing of other students in the group? (Evaluative)

# Appendix C

# Questionnaire 2 (ChatGPT)

1) Using the scale below, did you feel that the feedback you received from ChatGPT was specific and detailed enough to help you improve your writing? (GPT01: Specificity)   
2) Did you feel that the feedback you received from ChatGPT was constructive and supportive? (GPT02: Constructiveness)   
3) Did you feel that ChatGPT’s feedback was easy to incorporate into your writing? (GPT03: Usability)   
4) Overall, did you feel that ChatGPT provided clear and understandable suggestions for improving your descriptive paragraph? (GPT04: Clarity) 5) Did you feel that ChatGPT contributed to improving the overall quality of your descriptive writing? (GPT05: Improvement)   
6) Did you feel that the grammar and punctuation suggestions provided by ChatGPT for your descriptive writing were accurate? (GPT06: Accuracy)   
7) Did you feel that the grammar and punctuation suggestions provided by ChatGPT for your descriptive writing were helpful? (GPT07: Helpfulness)   
8) Did you feel that ChatGPT assisted you in developing a clear and engaging topic sentence for your paragraph? (GPT08: Topic-Sentence)   
9) Did you feel that ChatGPT helped you develop well-structured and coherent body sentences that support your topic in the paragraph? (GPT09: Coherence)   
10) Did you feel that ChatGPT suggested effective concluding sentences that effectively summarized your paragraph? (GPT10: Conclusions)   
11) Did you feel that ChatGPT suggested effective transition words and phrases to help with the smooth flow and cohesion between sentences in your paragraph? (GPT11: Transitions)   
12) Overall, I am satisfied with the editing and proofreading assistance provided by ChatGPT for your descriptive paragraph. (GPT12: Satisfaction)   
13) I feel confident in my ability to apply the suggestions provided by ChatGPT to enhance the quality of topic sentences, body sentences, concluding sentences, and transitions in your future writing. (GPT13: Confidence)

# Appendix D

# Comparative Evaluation of Editing and Proofreading: Writing Groups versus ChatGPT

1) When it comes to editing and proofreading your descriptive paragraph, which option did you find more effective? (Which01: Effectiveness)   
2) Which option better helped improve the clarity and coherence of your paragraph writing? (Which02: Clarity)   
3) Which option addressed your specific editing and proofreading needs for your writing more effectively? (Which03: Specificity)   
4) When it comes to providing suggestions for grammar and punctuation improvements, which option did you find more satisfactory? (Which04: Satisfaction)   
5) Which option are you more likely to use again in the future for editing and proofreading purposes? (Which05: Future-Use)   
6) Which option contributed more to improving the overall quality of your writing? (Which06: Quality)   
7) Which option was more responsive and prompt in providing editing and proofreading assistance for your writing? (Which07: Responsiveness)   
8) Which option provided you with more valuable insights and suggestions for improving your paragraph writing? (Which08: Insights)   
9) Which option better adapted to your writing style and preferences during the editing and proofreading process? (Which09: Adaptability)   
10) Overall, which option left you more satisfied with the editing and proofreading experience for your paragraph writing? (Which10: Overall-Satisfaction)