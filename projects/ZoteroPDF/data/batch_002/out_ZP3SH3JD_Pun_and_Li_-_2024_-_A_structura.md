# A structural equation investigation of linguistic features as indices of writing quality in assessed secondary-level EMI learners’ scientific reports

Jack Pun \*,1 , Wangyin Kenneth Li 2

Department of English, City University of Hong Kong, 18 Tat Hong Avenue, Kowloon Tong, Hong Kong

# A R T I C L E I N F O

# A B S T R A C T

Keywords:   
L2 writing   
Lexical complexity   
Syntactic complexity   
Scientific report writing   
EMI   
Secondary-level learners

While inquiry into the relationship between linguistic features and L2 writing quality has been a long-standing line of research, little scholarly attention has been drawn to the predictive value of linguistic features in assessing the writing quality of English-medium scientific report writing. This study adds to the existing literature by examining the relation of lexical and syntactic complexity to writing quality, based on 106 scientific reports composed by Hong Kong Chinese learners of English in EMI secondary schools. Natural language processing tools were employed to extract computational indices of linguistic complexity features, followed by the use of a structural equation modeling (SEM) approach to investigate their predictive power. The validity of the anticipated construct was confirmed based upon several goodness-of-fit criteria. The SEM analysis indicated that writing quality was predicted by lexical sophistication (i.e., text-based complexity: word range and academic words; psycholinguistic complexity: word familiarity and age-ofacquisition ratings), lexical diversity (i.e., MTLD and VocD), and syntactic complexity (i.e., mean length of sentence and dependent clauses per T-unit). However, the relation of lexical di versity and syntactic complexity to writing quality was mediated by lexical sophistication. Im plications for scientific report writing assessment and pedagogy in EMI educational contexts are discussed.

# 1. Introduction

There is no doubt that effective written communication in English is a critical and versatile skill (Graham, 2006), which plays an important role in learners’ academic success in a plethora of educational contexts throughout the world (Weigle, 2002). As a process to creating meaning (Murray, 1980), written production entails the appropriate and meaningful deployment of linguistic resources to serve various communicative goals (Hyland, 2009; Ravid & Tolchinsky, 2002). Over the past few decades, therefore, an important question that has drawn considerable attention from second language (L2) writing researchers concerns the extent to which linguistic features can serve as valid and reliable indicators of L2 writing quality (e.g., Guo et al., 2013; Kim & Crossley, 2016, 2018; Peng et al., 2023). Research evidence has accumulated to suggest that higher-rated L2 writing products tend to contain more sophisticated vocabulary (e.g., Kyle & Crossley, 2016; Maamuujav et al., 2021; Qin & Uccelli, 2016) and more complex syntactic structures (e.g., Lee et al., 2021; Lu, 2011; Ortega, 2003). Collectively, this substantial body of research has generated important insights into what constitutes a good piece of writing with respect to linguistic features, which in turn, can inform decisions in L2 writing assessment and pedagogy.

However, it is imperative to note that the majority of prior L2 writing studies have predominantly focused on argumentative essays (e.g., Kim & Crossley, 2018; Laufer & Nation, 1995; Vogelin ¨ et al., 2019) and narratives (e.g., Bi & Jiang, 2020; De Wilde, 2023; Qin & Uccelli, 2016; Zhang, 2022), with scientific reports rather underexplored (see Sanchez-P ´ ´erez, 2021 for an exception). As evidenced in previous research, different genres with different communicative purposes would impact writers’ composing processes and elicit different uses of lexicon and syntax (e.g., Guo et al., 2013; Qin & Uccelli, 2016; Yoon & Polio, 2017). Scientific report writing, an academic text genre which is central to science and engineering study (Parkinson, 2017), appears to have its unique linguistic features and structures that encode specialized knowledge of the scientific community (Fang, 2005; Halliday & Martin, 1993). Further, while considerable research effort has gone into examining L2 writing in English as a second language (ESL) contexts, there has been a paucity of empirical research on writing in English as a medium of instruction (EMI) settings where the English language is used ‘to teach academic subjects other than English itself’ (Macaro, 2018, p.1). There remains a pressing need for studies examining student writing in EMI contexts, as writing has been repeatedly noted as a skill that many EMI students struggle with (e.g., Evans & Morrison, 2011; McKinley & Rose, 2022).

In this light, the current study set out to examine how a set of linguistic features are associated with the writing quality of scientific reports composed by Hong Kong Chinese learners of English in EMI secondary schools. Moreover, within the theoretical framework of linguistic complexity, it aims to unpack the interrelationships of lexical and syntactic complexity in assessing writing quality of secondary-level learners. The empirical evidence yielded may provide useful pedagogical information to guide assessment and pedagogy on scientific report writing. It may also inform EMI policy and curriculum design by raising stakeholders’ awareness of the importance of addressing students’ linguistic needs, which has been a repeated call in EMI contexts (e.g., Doiz & Lasagabaster, 2020; Pun, 2019; Rose et al., 2020).

# 2. Literature review

While a considerable number of L2 writing studies have examined the links between syntactic measures and writing quality (e.g., Bi & Jiang, 2020; Peng et al., 2024), researchers have highlighted the importance of investigating lexical characteristics of L2 writing to capture learners’ writing proficiency (Skehan, 2009). Given that vocabulary and syntax are two fundamental linguistic resources for academic writing (Coxhead & Byrd, 2007), this study looked at both lexical and syntactic measures, with the aim of painting a fuller picture of the relationship between linguistic features and L2 writing quality in scientific reports. The subsequent sections provide an overview of previous research on the relations of L2 writing quality with lexical and syntactic measures. However, it should be noted that the aim of the literature review is not to map the entire landscape of literature on this topic. Given that this study centers on scientific reports composed by EMI secondary-level learners, the focus will be on studies involving L2 secondary-level learners.

# 2.1. Lexical complexity and L2 writing quality

As an umbrella term, lexical complexity is typically conceptualized as a multidimensional construct that subsumes three related yet distinct components—lexical density, diversity and sophistication (Read, 2000). Lexical density refers to the percentage of content words (e.g., verbs, nouns, and adjectives) in a particular text (Ure, 1971). A text with a high incidence of content words arguably contains a greater amount of information as compared to a text with high incidence of functional words (e.g., articles, prepositions, and conjunctions). This study decided not to take lexical density into account, as research evidence has accumulated to suggest that lexical density indices fall short of qualifying as effective indicators of L2 writing quality either for college students (e.g., Engber, 1995; Zhang, 2022) or secondary-level learners (e.g., Linnarud, 1986, Zhang & Ouyang, 2023).

As another sub-component in characterizing lexical complexity, lexical diversity refers to ‘the range of different words in a text’ (McCarthy & Jarvis, 2010, p.1). It has traditionally been measured by computing the Type-Token Ratio (TTR), that is, the ratio of different words (types) to the total number of words (tokens) in a given written sample (Johnson, 1944). However, the TTR has been called into question for being sensitive to text length effects, with longer texts being associated with lower lexical diversity scores (e.g., Malvern et al., 2004; McCarthy & Jarvis, 2007). In an attempt to address this issue, researchers have developed more robust measures that can somewhat circumvent text length effects, including the measure of textual lexical diversity (MTLD; McCarthy & Jarvis, 2010), VocD (Malvern et al., 2004), and moving average TTR (MATTR; Covington & McFall, 2010). A number of studies using these more valid measures have examined the relation of lexical diversity to L2 writing quality for college students or researchers (e.g., Crossley et al., 2015; Engber, 1995; Peng et al., 2023), showing that higher-rated texts tend to have a greater variety of words. A few studies have examined the role of lexical diversity in L2 secondary-level learners’ writing performance. Lee et al.’s (2021) analysis of examination scripts written by secondary-level learners in Hong Kong revealed that compositions with a higher lexical diversity value tended to receive a higher score. Likewise, in a secondary school context in Switzerland, Vogelin ¨ et al. (2019) explored the relationship between lexical features of Year 11 learners’ argumentative essays and teacher judgements of text quality. Their findings showed that lexical diversity was one of the significant predictors of writing proficiency. Consistent results were also observed in the case of compositions produced by students from a Japanese secondary school (Yasuda, 2024), narrative writing tasks composed by Belgian secondary-level learners (De Wilde, 2023), and essays written by Chinese secondary-level learners (Zhang & Ouyang, 2023). The results from existing studies suggest that lexical diversity, a measure considered to be included in this study, may be a robust indicator of writing quality.

An additional aspect of lexical complexity is lexical sophistication which broadly refers to the proportion of relatively sophisticated, advanced, or difficult vocabulary in a language sample (Read, 2000). In line with previous research (e.g., Kyle & Crossley, 2015; Peng et al., 2023), this study conceives of there being two facets of lexical sophistication: text-based complexity and psycholinguistic complexity. Text-based complexity captures the objective, linguistic properties of words represented in a text, whereas psycholin guistic complexity denotes the degree to which linguistic properties of words in a text influence recognition and cognitive processing (Balota et al., 2007). Traditionally, the evaluation of text-based complexity has been predominantly based on corpus-based frequency measures, such that L2 writing of higher quality tends to exhibit significantly fewer high-frequency words (Laufer & Nation, 1995). A few studies have reported that word frequency could significantly predict the L2 writing proficiency of secondary-level learners (Crossley & McNamara, 2012; De Wilde, 2023).

Beyond word frequency, different indices to gauge text-based complexity have been proposed: word range, academic words, and technical words. While word frequency concerns the incidence of words within a reference corpus, word range refers to the number of texts in which words occur across a certain corpus (Kyle & Crossley, 2015). Word range has been shown to have a stronger explanatory power in human judgments of writing quality than word frequency (Kyle & Crossley, 2015), and there is emerging evidence that word range can be an effective quality indicator of secondary-level learners’ writing (Vogelin ¨ et al., 2019). The use of academic words—words that frequently occur in written academic texts (Coxhead, 2000)—also serves as a proxy for measuring text-based complexity. Its positive association with L2 writing quality has been observed in the analysis of examination scripts by secondary-level learners (Lee et al., 2021). As scientific report writing is an academic text genre (Parkinson, 2017), it is possible that secondary-school learners’ scientific reports with higher ratings may contain a greater number of academic words. The use of technical words—words that are primarily found in a specialized domain (Chung & Nation, 2004)—is another indicator that has received scholarly attention. In a Spanish EMI context, Sanchez-P ´ ´erez (2021) analyzed the linguistic features represented in college students’ scientific reports and found that technical words significantly contributed to writing quality. Given that Sanchez-P´ ´erez’s study focuses on college students, it remains unknown whether the predictive role of technical words can be applicable in the case of secondary-level learners.

Psycholinguistic complexity—a facet of lexical sophistication—has typically been measured using five indices derived from native speakers’ online processing of words on psycholinguistic tasks: namely word familiarity, imaginability, concreteness, meaningfulness, and age of acquisition (henceforth AoA) (see Kyle & Crossley, 2015). Only a few studies have examined the psycholinguistic proxies in terms of their relationship to L2 writing proficiency and their results were somewhat mixed. AoA has been found to be positively related to secondary-level learners’ writing scores in narrative essays (De Wilde, 2023; Zhang & Ouyang, 2023) and analytical compositions (Maamuujav et al., 2021). These findings show that higher-rated writing tends to include words with higher AoA values, that is, words that are learned at a later age, and therefore likely pose a higher demand on lexical processing and retrieval. However, AoA failed to result as a significant factor contributing to quality of examination scripts written by secondary-level learners in Hong Kong (Crossley & McNamara, 2012). Crossley and Mcnamara instead found word familiarity and meaningfulness to be significantly indicative of learners’ writing proficiency, showing that more proficient writers likely use less familiar words and words with fewer associations. While a positive relationship between word imageability and L2 writing quality was demonstrated in De Wilde (2023) and Maamuujav et al. (2021), this association was not observed in the studies by Crossley and McNamara (2012) or Zhang and Ouyang (2023). The discrepancies of the results might be due to the differences in specific task genres and learner groups.

# 2.2. Syntactic complexity and L2 writing quality

Syntactic complexity denotes the variety and degree of sophistication of grammatical forms and structures used in language production (Lu, 2011). There is a consensus among researchers that syntactic complexity is a multidimensional construct capturing the complexity of structures at multiple syntactic levels (e.g., Lu, 2017; Norris & Ortega, 2009). Syntactic complexity has been historically gauged by the mean length of clause, sentence, and T-unit (see Ortega, 2003). Along with the three indicesLu (2010) incorporated 11 additional large-grained complexity indices that were selected from a large-scale research synthesis of L2 writing studies (Wolfe-Quintero et al., 1998). Based on specific syntactic constructs that the 14 complexity measures tap into, Lu classified them into five categories: length of production (mean length of clause, sentence, and T-unit), subordination (clauses per T-unit, complex T-units per T-unit, dependent clauses per clause, and dependent clauses per T-unit), coordination (coordinate phrases per clause, coordinate phrases per T-unit, and T-units per sentence), sentence complexity (clauses per sentence), and phrasal structures (verb phrases per T-unit, complex nominals per clause, and complex nominals per T-unit). Apart from the aforementioned measures, researchers have proposed a plethora of fine-grained clausal (e.g., nominal subjects per clause) and phrasal (e.g., dependents per nominal) complexity indices (see Biber et al., 2011 and Kyle & Crossley, 2018).

A substantial amount of research has looked at the relation of syntactic complexity to the rated quality of different types of adult college students’ writing, including examination scripts of high-stakes tests like the Test of English as a Foreign Language (TOEFL) (e. g., Guo et al., 2013; Kyle & Crossley, 2018), and narrative or argumentative writing for specific courses (e.g., Yoon & Polio, 2017; Peng et al., 2024). Only a handful of studies have examined the relationship between syntactic complexity and secondary-level learners’ writing quality. In their analysis of narrative essays written by Chinese L2 secondary-level learners, for example, Bi and Jiang (2020) found that mean length of sentence, clauses per T-unit (a proxy of subordination), and complex nominals per clause were significant quality indicators. The positive relation of mean length of sentence and subordination (i.e., dependent clause ratio) to L2 writing quality was also observed in Martínez’s (2018) study involving Spanish secondary-level learners. However, clauses per T-unit did not demonstrate a significant relationship with L2 writing quality of Chinese secondary-level learners’ narrative and argumentative essays (Qin & Uccellie, 2016) and examination scripts (Lee et al., 2021). A possible reason for the mixed results might be due to the dif ferences in learners’ language proficiency. While focal learners in studies by Bi and Jang (2020) and Martínez (2018) are both of low and intermediate English proficiency, those in the study of Lee et al. (2021) are secondary school graduates aged 17–18 who may have higher English proficiency, and a substantial proportion of those in Qin & Uccellie (2016) are at the upper-intermediate level. Re searchers have contended that subordination (i.e., the act of linking clauses hierarchically within a sentence) can be an effective indicator of written complexification for beginners and intermediate learners (e.g., Lu & Ai, 2015; Norris & Ortega, 2009). Moreover, indices related to complex nominals were found to be indicative of the quality of argumentative essays among Japanese secondary-level learners (Yasuda, 2024), and examination scripts of Chinese secondary-level learners (Lee et al., 2021). In a cross-genre study, Zhang and Ouyang (2023) found that syntactic features predicting Chinese secondary-level learners’ essay scores varied considerably across different task genres: narrative writing, argumentative writing, and continuation writing. Their results highlight the need to explore the predictive power of syntactic features over L2 writing of different genres (e.g., scientific reports).

# 2.3. Research gap

While existing research literature has provided useful insights into the relationship between linguistic features and L2 writing quality, there are some limitations pertaining to this line of research. First, the predictive power of linguistic features, to the best of our knowledge, has rarely been examined with relation to holistic ratings of writing quality in scientific reports—an important academic text genre (Parkinson, 2017). The only exception is Sanchez-P ´ ´erez’s (2021) study which looked at the links between a few lexico-syntactic features and the quality of scientific reports written by Spanish learners in an EMI university. The results revealed that the use of technical words and passive voice predicted the ratings of scientific report writing. Notably, however, this study focused on relatively advanced college-level students. Although adolescent students seem to constitute the majority of L2 learners, the linguistic features in their writings have been reportedly understudied in L2 writing research (e.g., Belcher, 2013; Harklau, 2011; Silva, 2013). Given the crucial role of writing in L2 development (Williams, 2012), further research is clearly warranted to provide a richer un derstanding of L2 writing composed by learners of different proficiency levels. Further, the focal variables of analysis in Sanchez-P ´ ´erez’s (2021) study were limited in number, making it hard to provide a fine-grained understanding of the relationship between linguistic features and writing quality in scientific reports. Secondly, among the handful of studies that examined linguistic features of secondary-level learners’ writing as reviewed in the preceding text, none of them have used a single model to investigate the interrelationships of lexical and syntactic complexity in assessing writing quality. Thirdly, this line of inquiry has witnessed a paucity of research on writing in EMI settings, where many EMI students reportedly struggle with academic writing (e.g., Evans & Morrison, 2011; McKinley & Rose, 2022). Accordingly, the current study aims to examine the predictive power of a set of linguistic complexity measures on the writing quality of scientific reports composed by learners in EMI secondary schools in Hong Kong (henceforth HK). This study is guided by the following research question: To what extent can indices of lexical and syntactic complexity predict the quality of secondary-level EMI learners’ scientific report writing?

# 3. Methodology

# 3.1. Context, participants, and dataset

HK, the site of the current study, is a former British colony reverted to a special administrative region of China. While Cantonese is the mother tongue of most of the population in HK, English is one of the official languages and is regarded with high status (Tsui, 2004). Under the fine-tuning medium-of-instruction policy (Education Bureau, 2009), local secondary schools in HK have the autonomy to deliver classes in either EMI, partial-English-Medium (i.e., a few subjects conducted in EMI), or Chinese-Medium-Instruction. The data for this study was obtained from a larger government-funded research project that examined the effect of a genre-based approach on scientific writing among secondary-school learners in EMI programmes. The dataset comprised a total of 111 untimed, typed scientific reports in English composed by Chinese learners of English from seven different secondary schools in HK, all of which are all EMI schools where almost all academic subjects (e.g., Science) are delivered in English. An important learning element in these learners’ science classes is performing scientific investigations such as laboratory experiments (The Cur riculum Development Council, 2017). As a typical type of assessment, therefore, they need to write up scientific reports where they introduce readers to the research topic, elaborate on all the procedures during their experimental investigations, and present as well as discuss their results. As reported in a background survey, the 111 learners (60 females, 49 males, and 2 unreported) speak either Cantonese $( n = 9 2 )$ ) or Mandarin $( n = 1 9 )$ ) as their first language, with ages ranging from 13 to 15 years old $\mathbf { \mathit { M } } = 1 3 . 6 9$ , $S D = 0 . 7 4 \mathrm { \Omega }$ ). According to their teachers’ assessments, their English language proficiency is roughly at the intermediate level as per the Common European Framework of Reference (CEFR).

All learners participated in a 12-week intervention programme during which they were explicitly taught, among others, how to formulate research questions and use keywords to search online sources, standards for citations and referencing, as well as macro- and micro-structure of scientific reports (i.e., Introduction-Method-Results-Discussion-Conclusion genre). They were required to complete their scientific investigations on pre-determined topics by Week 8 of the intervention, and then spend the remaining four weeks writing up their scientific reports. In scientific report writing, they were instructed to adhere to the structure of Introduction-Method-ResultsDiscussion-Conclusion and encouraged to gather useful information by searching online resources (e.g., books, journal articles, websites) (see Appendix A for the detailed instructions). Five scientific reports were excluded, either because they contained too few or too many words, leaving 106 reports with a total of 58,706-word tokens for analysis. The remaining reports ranged from 382 to 996 words in text length $( M = 5 5 3 . 8 3$ , $S D = 2 8 6 . 6 7 \mathrm { \Omega }$ ). They were written based on the learners’ investigation conducted in response to one of the following authentic scientific issues: the influence of multitasking $( n = 3 4 )$ ), the effects of music $\left( n = 3 9 \right)$ ), and the relationship between laughter and mood $( n = 3 3$ ).

# 3.2. Scoring rubric and human rating

The scoring rubric (see Appendix B) used to rate the scientific reports underwent piloting, revision, and adaption by the authors based on a marking grid of scientific reports compiled by the authors’ institution. It consists of three components: organization, content, and language, each of which is graded on a seven-point scale, ranging from zero as the lowest to seven as the highest. The holistic rating of each report equates to the average score of the three sub-components. Each report was independently rated by two trained research assistants who hold a Master’s degree in Applied Linguistics using the scoring rubric. Prior to their scoring, rater training was provided to improve the consistency between raters. Following previous research (e.g., Kim & Crossley, 2018), two ratings were averaged if they differed by one point or less. If there was a discrepancy of more than one point, the second author would score the report and then the final score was the average of the two closest ratings. For 89 out of the 106 scientific reports $( 8 3 . 9 6 \%$ ), the first two raters’ scores differed by one point or less.

# 3.3. Measures and computational tools

# 3.3.1. Lexical complexity

Lexical complexity was measured through its two related yet distinct sub-constructs: lexical diversity and lexical sophistication (Read, 2000; see Table 1 for the selected measures). Given that the scientific reports collected in the present study vary somewhat greatly in text length, MTLD and VocD were used as indices of lexical diversity since they appear to be the best available measurements that are less susceptible to text lengths (McCarthy & Jarvis, 2010). The MTLD and VocD indices were extracted from Coh-Metrix (McNamara et al., 2014).

Regarding lexical sophistication, Kyle et al.’s (2018) Tool for the Automatic Analysis of Lexical Sophistication 2.0 (TAALES) was employed to generate a range of indices covering theoretically important aspects related to lexical sophistication. The

included measures were word range, academic words, academic formulas, and psycholinguistic word properties—age of acqui sition, word familiarity, concreteness, imageability, and meaningfulness. Given that scientific report writing is an academic text genre (Parkinson, 2017), the word range values (raw scores) were generated based on the Corpus of Contemporary American English (COCA; Davies, 2008) academic sub-corpus. The metrics of academic words were calculated based on the entire Academic Word List (AWL) and each of the 10 sublists (see Coxhead, 2000). The indices pertaining to academic formulas were generated based on the entire Academic Formulas List (AFL), the core AFL, the spoken AFL and the written AFL (see Simpson-Vlach & Ellis, 2010). The parameters of psycholinguistic word properties were deduced from the Medical Research Council (MRC) psycholinguistic database (Coltheart, 1981) and two newly collected databases (Brysbaert et al., 2014; Kuperman et al., 2012). The indices regarding word range and psycho linguistic word properties were computed for all words, content words, and function words, respectively. In addition to the aforementioned lexical measures, the frequency of technical vocabulary (e.g., cortex, brainstem, and dopamine)—words that pertain to a specialized domain (Chung & Nation, 2004)—was also included as they seem to be distinctive lexical features in scientific and technical writing (Sanchez-P ´ ´erez, 2021).

The word classification for each scientific report was independently completed by the second author and a research assistant using a digital dictionary, reaching an acceptable level of inter-rater agreement $( 8 5 . 2 \% )$ . All disagreements were discussed until a consensus was reached.

3.3.2. Syntactic complexity The scientific reports were input into Lu’s (2010) L2 Syntactical Complexity Analyzer (L2SCA) to generate a comprehensiv   
Table 1 Selected measures of lexical complexity.   

<html><body><table><tr><td>Measure</td><td>Definition</td></tr><tr><td>MTLD</td><td>Measure of textual lexical diversity</td></tr><tr><td>VocD</td><td>Measure of vocabulary diversity</td></tr><tr><td>Word range</td><td>Range of words based on COCA academic sub-corpus</td></tr><tr><td>Academic words</td><td>Frequency of words that appear in AWL</td></tr><tr><td>Academic formulas</td><td>Frequency of formulas that appear in AFL</td></tr><tr><td>Age of acquisition</td><td>Earliness/lateness of words being acquired</td></tr><tr><td>Word familiarity</td><td>Degree of familiarity with words</td></tr><tr><td>Word concreteness</td><td>Degree of concreteness of words</td></tr><tr><td>Word imageability</td><td>Degree of imageability of words</td></tr><tr><td>Word meaningfulness</td><td>Number of associations of words</td></tr><tr><td>Technical words</td><td>Frequency of technical words</td></tr></table></body></html>

measurement with 14 large-grained indices of syntactic complexity (see Table 2). These measures capture five dimensions of syntactic complexity: length of production unit, sentence complexity, subordination, coordination, and phrasal sophistication (see Lu, 2010 for a detailed description). Previous research has revealed the acceptable reliability of this computational tool to analyze the syntactic complexity of texts written by low-proficiency learners (e.g., Chau ˆ & Bult´e, 2023; Jiang et al., 2019). Following researchers’ guidelines on learner data preprocessing (Chau ˆ & Bult´e, 2023; Lu, 2014), run-on sentences, misspellings and spacing were corrected prior to L2SCA analysis to increase the reliability of the syntactic analyses. In-text citations and reference lists were also deleted. Along with the 14 large-grained indices, passive voice was also included because it appears to be one of the distinctive syntactic features of scientific writing (Parkinson, 2017). The frequency of passive voice was generated by the tool Passive Voice Detector (https://datayze.com/ passive-voice-detector) which can automatically detect passive voice in a block of text.

# 3.4. Statistical analysis

While NLP tools have been widely employed in L2 writing research (e.g., Kyle & Crossley, 2016; Lee et al., 2021), it seems unrealistic for these tools to achieve $1 0 0 ~ \%$ accuracy in generating indices of language features (Lu, 2010). Researchers have proposed structural equation modeling (SEM) as a method to accommodate the potentially misleading effects of measurement errors—inaccuracies or discrepancies that may potentially exist in the output generated by NLP tools in this case (e.g., Hancock & Schoonen, 2015). As a multivariate statistical analysis technique, SEM can examine the relationships between observable variables and latent constructs of interest while accounting for and separating the effects of potential measurement errors (In’nami & Koizumi, 2011; Kline, 2018). Using Amos 26.0 and SPSS 26.0, therefore, this study utilized the structural equation modeling approach (Collier, 2020; Kline, 2023) to construct a hypothesized structural model and analyze the quantitative data. The normal distribution of all the focal indices was checked through the inspection of skewness and kurtosis values. Based on guidelines in Social Science Research (Cameron, 2004), a variable was deemed normal in univariate distribution if both kurtosis and skewness values fell between $- 2$ and $+ 2$ .

# 4. Results

# 4.1. Descriptive statistics and correlations

The descriptive statistics of the focal indices that significantly correlated with writing scores for scientific reports are shown in Table 3. Among the lexical complexity measures, seven indices (i.e., word range (content words), academic words (all list), word familiarity (all words), word imageability (all words), age-of-acquisition ratings (content words), VocD, and MTLD) showed significant correlations with writing scores and therefore were chosen for confirmatory factor analysis (CFA). Likewise, regarding the 14 syntactic complexity indices, three measures (i.e., mean length of sentence, dependent clauses per T-unit, and passive voice) that significantly contributed to writing scores were selected for inclusion in the CFA. A correlation analysis was also conducted between writing scores and text length—a measure that may influence human judgments of writing quality (e.g., Qin & Uccelli, 2016). However, text length fell short of being an indicator included in the CFA given its insignificant correlation with writing scores $( r = 0 . 1 6 1$ , $p = 0 . 0 9 3 \mathrm { ) }$ . Another justification for excluding text length is that the indices included in the subsequent CFA were standardized (i.e., as either normalized frequency or ratios) and therefore any potential confounding effect of text length can be minimized (see Zhang, Lu, et al., 2022 for a similar approach).

# 4.2. Confirmatory factor analysis (CFA) and structural equation modeling (SEM)

As suggested by Kline (2018), the construct validity of a latent variable in CFA entails sufficient correlations among indicator measures. Measures of word imageability and passive voice were therefore excluded as their correlations with other indicators of the same hypothesized latent variable did not reach 0.30 (Tabachnick & Fidell, 2019). Since two indices (i.e., word range and word fa miliarity) exhibited negative correlations with writing scores, their values were multiplied by $^ { - 1 }$ in order to generate a positive factor loading. Prior to CFA, the dataset was evaluated using Keiser-Meyer-Olkin (KMO) measure of sampling adequacy and Bartlett’s test of sphericity. Assumptions of CFA were satisfied by the significant value of Bartlett’s test of sphericity $( p < 0 . 0 0 1 )$ ) and a KMO value of 0.81 which exceeded the cut-off point of 0.60 proposed by Tabachnick and Fidell (2019). Based on statistical and theoretical scrutiny, the latent constructs were hypothesized: (a) text-based complexity informed by word range and academic words which were moderately correlated $\textcircled { r h o } = 0 . 4 0$ , $p < 0 . 0 0 1 $ ); (b) psycholinguistic complexity informed by word familiarity and age-of-acquisition ratings which were strongly correlated $( r = 0 . 6 9$ , $p < 0 . 0 0 1 $ ); (c) lexical diversity informed by MTLD and VocD which were strongly correlated $( r =$ 0.60, $p < 0 . 0 0 1 $ ), and (d) syntactic complexity informed by mean length of sentence and dependent clauses per T-unit which were moderately correlated $( r h o = 0 . 4 1$ , $p < 0 . 0 0 1 $ ) (see Fig. 1 for the initial hypothesized model). The overall model was assessed using several fit indices: Chi-square/degree of freedom (χ2/df), root mean square error of approximation (RMSEA), standardized root mean square residual (SRMR), comparative fix index (CFI), Tucker-Lewis index (TLI), incremental fit index (IFI), normed fit index (NFI), and Parsimony normed fit index (PNFI) (see Collier, 2020 and Kline, 2023).

Table 2 Selected measures of syntactic complexity.   

<html><body><table><tr><td>Measures</td><td>Definition</td></tr><tr><td> Mean length of sentence</td><td>Number of words/number of sentences</td></tr><tr><td>Mean length of clause</td><td>Number of words/number of clauses</td></tr><tr><td>Mean length of T-unit.</td><td>Number of words//number of T-units</td></tr><tr><td>Sentence complexity ratio</td><td>Number of clauses/number of sentences</td></tr><tr><td>T-unit complexity ratio</td><td>Number of clauses/number of T-units</td></tr><tr><td>Dependent clause ratio.</td><td>Number of dependent clauses/number of clauses</td></tr><tr><td>Dependent clauses per T-unit</td><td>Number of dependent clauses/number of T-units</td></tr><tr><td>Complex T-unit ratio.</td><td>Number of complex T-unit/number of T-units.</td></tr><tr><td>Coordinate phrases per T-unit</td><td>Number of coordinate phrases/number of T-units</td></tr><tr><td>Coordinate phrases per clause</td><td>Number of coordinate phrases/number of clauses</td></tr><tr><td>Sentence coordination ratio</td><td>Number of T-units/number of sentences</td></tr><tr><td>Complex nominals per T-unit</td><td>Number of complex nominals/number of T-units</td></tr><tr><td>Complex nominals per clause</td><td>Number of complex nominals/number of clauses</td></tr><tr><td>Verb phrases per T-unit</td><td>Number of complex verb phrases/number of T-units</td></tr><tr><td>Passive voice</td><td>Frequency of passive voice</td></tr></table></body></html>

Table 3 Descriptive statistics and correlations between the indices selected for CFA and writing scores.   

<html><body><table><tr><td>Index</td><td>Category</td><td>Mean (SD)</td><td>Correlation (r or rho)</td></tr><tr><td>Word range (content words)</td><td>Lexical sophistication</td><td>0.61 (0.06)</td><td>rho = -0.39***</td></tr><tr><td>Academic words (all list)</td><td>Lexical sophistication</td><td>0.14 (0.03)</td><td>rho = 0.41***</td></tr><tr><td>Word familiarity (all words)</td><td>Lexical sophistication</td><td>593.59 (5.89)</td><td>r = -0.46****</td></tr><tr><td>Word imageability (all words)</td><td>Lexical sophistication</td><td>329.96 (10.03)</td><td>r = -0.28*</td></tr><tr><td>Age-of-acquisition (content words)</td><td>Lexical sophistication</td><td>5.16 (0.37)</td><td>r = 0.35***</td></tr><tr><td>VocD</td><td> Lexical diversity</td><td>56.24 (15.65)</td><td>r = 0.29*</td></tr><tr><td>MTLD</td><td>Lexical diversity</td><td>51.89 (14.36)</td><td>r = 0.31**</td></tr><tr><td>Mean length of sentence</td><td>Syntactic complexity</td><td>17.35 (3.40)</td><td>r = 0.33***</td></tr><tr><td>Dependent clauses per T-unit</td><td>Syntactic complexity</td><td>0.73 (0.29)</td><td>rho = 0.30**</td></tr><tr><td>Passive voice</td><td>Syntactic complexity</td><td>0.29 (0.09)</td><td>r = 0.34***</td></tr></table></body></html>

Note: $^ { * } p < 0 . 0 5$ ; $^ { * * } p < 0 . 0 0 5$ ; $^ { \ast \ast \ast } p < 0 . 0 0 1$ . Pearson’s correlations $^ \mathrm { \textregistered }$ were reported for indices that were normally distributed, while Spearman’s rank order correlations (rho) were reported for those indices that violated normal distribution.

The results of CFA indicated a satisfactory model fit: $\chi 2 / \mathrm { d f } { = } 2 . 8 9$ ; RMSEA $_ { \it - 0 . 0 3 }$ ; SRMR $_ { = 0 . 0 6 }$ ; $\mathrm { C F I } { = } 0 . 9 4$ ; $\mathrm { T L I } { = } 0 . 9 3$ ; $\scriptstyle { \mathrm { I F I } } = 0 . 9 4 ;$ $\mathrm { N F I } { = } 0 . 9 5$ ; $\mathrm { P N F I } { = } 0 . 6 1$ (see Table 4 for the recommended values). Following Collier’s (2020) guidelines on CFA, factor loading for every indicator, average variance extracted (AVE) for constructs, and each construct’s square root of AVE values were then derived to assess convergent and discriminant validity. As presented in Table 5, the factor loadings for each indicator were higher than the threshold of 0.60, and all AWE values exceeded the recommended value of 0.50 (Kline, 2023), indicating acceptable convergent validity. The evaluation of discriminant validity entails the comparison between the square roots of AVE and inter-factor correlation coefficients. It was found that text-based complexity and psycholinguistic complexity were strongly correlated, as indicated by a covariance of 0.78, which might challenge discriminant validity. Given that text-based complexity and psycholinguistic complexity both tap into lexical sophistication (see Kyle & Crossley, 2015 and Peng et al., 2023), a modification was made to the model by including a second-order hypothesized latent variable lexical sophistication as a unitary construct. The second-order model demon strated a satisfactory fit based on the following indices: $\chi 2 / \mathrm { d f } { = } 2 . 0 7$ ; RMSEA $_ { \it - 0 . 0 2 }$ ; SRMR $_ { - 0 . 0 5 }$ ; $\mathrm { C F I } { = } 0 . 9 9$ ; $\mathrm { T L I } { = } 0 . 9 9$ ; $\mathrm { I F I } { = } 0 . 9 8$ ; $\mathrm { N F I } { = } 0 . 9 3$ ; $\mathrm { P N F I } { = } 0 . 6 4$ , with acceptable convergent and discriminant validity (see Table 6).

![](img/c746b1de2764efadaa8b23141f3de369d5be35cb0b6223dac40ecfeccd3482e4.jpg)  
Fig. 1. The initial hypothesized model.

The existence of potential mediation effects was ascertained if the variables satisfied the mediational assumptions proposed by Baron and Kenny (1986): (a) an independent variable alone predicted the dependent variable—lexical diversity $( R ^ { 2 } { = } 0 . 1 0 6$ , $\beta = 0 . 2 4 7$ , $p < 0 . 0 5 )$ and syntactic complexity $( R ^ { 2 } = 0 . 1 1 7$ , $\beta = 0 . 2 7 6$ , $p < 0 . 0 5$ ) each predicted writing quality alone, (b) the mediator alone predicted the dependent variable—lexical sophistication predicted writing quality $\scriptstyle ( R ^ { 2 } = 0 . 1 9 6$ , $\beta = 0 . 3 9 8$ , $p < 0 . 0 1 { \dot { . } }$ , and (c) an independent variable alone predicted the mediator—lexical diversity $( R ^ { 2 } = 0 . 1 5 6$ , $\beta = 0 . 3 3 6$ , $p < 0 . 0 5 )$ and syntactic complexity $( R ^ { 2 } =$ 0.168, $\beta = 0 . 3 7 3$ , $p < 0 . 0 5$ ) each predicted lexical sophistication. A mediation analysis of the Sobel test revealed that lexical so phistication significantly mediated the relationship between lexical diversity and writing quality $( p = 0 . 0 3 1 )$ ), as well as the relationship between syntactic complexity and writing quality $( p = 0 . 0 0 7 )$ ). Given the good fit of the modified model, we thus considered it as the final model which is presented in Fig. 2 with standardized parameter estimates (see Table 7 for standardized regression weights).

# 5. Discussion

# 5.1. The predictive relationship between lexical complexity and writing quality

As an important sub-component in characterizing lexical complexity, the latent variable lexical diversity that subsumes MTLD and VocD was found to be a significant predictor of the quality of scientific reports in this study. This result is in line with a handful of studies that furnished evidence for the predictive power of lexical diversity on the assessed writing quality of texts produced by secondary-level learners (e.g., De Wilde, 2023; Lee et al., 2021; Yasuda, 2024). The significant role of lexical diversity was expected as producing a greater variety of words intrinsically reflects learners’ ability to access more diverse and complex lexical choices as well as the greater complexity of their mental lexicon (Crossley et al., 2011), which would be unsurprisingly favored by human judgments of perceived writing proficiency. However, it is important to note that the predictive power of lexical diversity on writing quality was significantly mediated by lexical sophistication. This suggests that it is not merely the sheer variety of words used in writing that positively affected the writing quality. Instead, writing quality was predicted by a greater variety of words containing sophisticated lexical items in the scientific reports.

As for lexical sophistication, the latent variable text-based complexity informed by word range and academic words resulted as one of the robust indicators of writing quality. The significant bearings of word range on writing quality suggests that higher quality scientific reports tend to contain words that occur in fewer contexts, that is, more specific and domain-appropriate words. This result concurs with the study of Vogelin ¨ et al. (2019) that analyzed the argumentative essays produced by secondary-level learners in Switzerland. As the other indicator, the predictive effect of academic words is somewhat expected in that scientific report writing is of an academic genre (Parkinson, 2017). This result echoes the findings from previous research on different types of academic writing (e.g., Lee et al., 2021; Zhang, 2022). However, it runs counter to Kyle and Crossley’s (2015) finding that the use of academic words had a negligible impact on judgments of L2 lexical proficiency. The conflicting results may stem from differences in the written samples used. While this study targeted an academic text genre, the lexical proficiency corpus used in Kyle and Crossley’s study was composed of undergraduate free writing. It seems plausible that academic words may not play a role in less formal writing. Unexpectedly, while the use of technical words was included as a criterion in the scoring rubric, it did not exert a significant influence on the writing scores. This differed from Sanchez-P ´ ´erez’s (2021) study which revealed that the use of technical words predicted the ratings of scientific reports written by Spanish EMI college students. One possible explanation for the insignificant role of technical words observed in this study could be the relative absence of such terminology in the students’ writing. Specifically, 85 out of 106 scientific reports $( 8 0 . 1 9 \% )$ did not demonstrate any use of technical words. The topics chosen for the scientific reports were derived from authentic scientific issues, which might be relatively unfamiliar to the learners. Therefore, it stands to reason that they may not have been exposed to the specific technical vocabulary associated with these topics.

As another constituent that features lexical complexity, psycholinguistic complexity measured by word familiarity and age-ofacquisition ratings was demonstrated to be significantly indicative of higher standards of writing quality. These findings lend support to previous findings which suggest that higher-rated writing is likely to contain less familiar words (e.g., Guo et al., 2013; Kyle & Crossley, 2015; Peng et al., 2023), and words that are acquired at a later age (De Wilde, 2023; Jung et al., 2015). The stronger correlation between word familiarity and writing scores $( r = - 0 . 4 6 )$ , compared to that of age-of-acquisition ratings $( r = 0 . 3 5 )$ ), seemingly echoes the findings of Crossley and McNamara (2012) that word familiarity scores may serve as a better quality indicator.

Table 4 Model fit indices.   

<html><body><table><tr><td>Index</td><td>x2/df</td><td>RMSEA</td><td>SRMR</td><td>CFI</td><td>TLI</td><td>IFI</td><td>NFL</td><td>PNFI</td></tr><tr><td>Adequate fit</td><td>&lt;3</td><td>&lt; 0.08</td><td>&lt;0.05/0.05-0.09</td><td>&gt; 0.90</td><td>&gt; 0.90</td><td>&gt; 0.90</td><td>&gt; 0.90</td><td>&gt;0.50</td></tr><tr><td>First-order model fit</td><td>2.89</td><td>0.03</td><td>0.06</td><td>0.94</td><td>0.93</td><td>0.94</td><td>0.95</td><td>0.61</td></tr><tr><td>Final model fit</td><td>2.07</td><td>0.02</td><td>0.05</td><td>0.99</td><td>0.99</td><td>0.98</td><td>0.93</td><td>0.64</td></tr></table></body></html>

Table 5 Convergent and discriminant validity of the first-order model.   

<html><body><table><tr><td>Construct</td><td>Factor loading</td><td>AVE</td><td colspan="5">The square root of AVE and correlation coefficient matrix</td></tr><tr><td></td><td></td><td></td><td>WQ</td><td>LD</td><td>TC</td><td>PC</td><td>sc</td></tr><tr><td>WQ</td><td>0.77-0.91</td><td>0.72</td><td>0.85</td><td>0.46</td><td>0.41</td><td>0.47</td><td>0.42</td></tr><tr><td>LD</td><td>0.76-0.79</td><td>0.61</td><td>0.46</td><td>0.78</td><td>0.29</td><td>0.27</td><td>0.28</td></tr><tr><td>TC</td><td>0.64-0.86</td><td>0.58</td><td>0.41</td><td>0.29</td><td>0.76</td><td>0.78</td><td>0.27</td></tr><tr><td>PC</td><td>0.800.87</td><td>0.69</td><td>0.47</td><td>0.27</td><td>0.78</td><td>0.83</td><td>0.28</td></tr><tr><td>Sc</td><td>0.69-0.74</td><td>0.51</td><td>0.42</td><td>0.28</td><td>0.27</td><td>0.28</td><td>0.71</td></tr></table></body></html>

Note: The square root of AVE is presented along the diagonal line in bold. WQ $=$ writing quality; LD=lexical diversity; TC=text-based complexity; $\mathrm { P C = }$ psycholinguistic complexity; ${ \mathrm { S C } } { = } 1$ syntactic complexity.

Table 6 Convergent and discriminant validity of the second-order model.   

<html><body><table><tr><td>Construct</td><td>Factor loading</td><td>AVE</td><td colspan="4">Square root of AVE and correlation coefficient matrix</td></tr><tr><td></td><td></td><td></td><td>WQ</td><td>LD</td><td>LS</td><td>Sc</td></tr><tr><td>WQ</td><td>0.770.92</td><td>0.72</td><td>0.85</td><td>0.49</td><td>0.52</td><td>0.47</td></tr><tr><td>LD</td><td>0.760.79</td><td>0.61</td><td>0.49</td><td>0.78</td><td>0.31</td><td>0.33</td></tr><tr><td>LS</td><td>0.730.82</td><td>0.60</td><td>0.52</td><td>0.31</td><td>0.77</td><td>0.25</td></tr><tr><td>Sc</td><td>0.690.74</td><td>0.51</td><td>0.47</td><td>0.33</td><td>0.25</td><td>0.71</td></tr></table></body></html>

Note: The square root of AVE is presented along the diagonal line in bold. WQ $=$ writing quality; LD $\stackrel { } { = }$ lexical diversity; TC $=$ text-based complexity; $\mathrm { P C = }$ psycholinguistic complexity; ${ \mathrm { S C } } { = } 1$ syntactic complexity.

![](img/0997cb41853d1ab6ebc70617aee9d5e5471ba0c5c0ddbd694c1a7a0eee0cbcff.jpg)  
Fig. 2. The final model.

Table 7 Standardized regression weights.   

<html><body><table><tr><td></td><td></td><td></td><td>Estimate</td><td>p</td></tr><tr><td>Writing quality</td><td>&lt;</td><td>Lexical sophistication</td><td>0.398</td><td>0.014</td></tr><tr><td>Writing quality</td><td>&lt;</td><td>Lexical diversity</td><td>0.247</td><td>0.046</td></tr><tr><td>Writing quality</td><td>&lt;</td><td>Syntactic complexity</td><td>0.276</td><td>0.043</td></tr></table></body></html>

# 5.2. The predictive relationship between syntactic complexity and writing quality

Concerning syntactic complexity, the SEM indicated that the mean length of sentence and dependent clauses per T-unit were sig nificant indicators of perceived writing quality. The result for mean length of sentence is generally aligned with previous research which showed that more proficient L2 secondary-level learner writers tend to produce longer sentences (Bi & Jiang, 2020; Lahuerta Martínez, 2018). The finding with regard to dependent clauses per T-unit, a proxy for the amount of subordination, is in accord with previous findings revealing that subordination-related measures were significantly indicative of L2 writing quality (e.g., Grant & Ginther, 2000; Khushik & Huhta, 2020; Lahuerta Martínez, 2018). However, it contrasts with Biber et al. (2011) who found that clausal subordination correlated with proficiency in the spoken register, whereas phrasal-level complexity was a more robust quality indicator in academic writing. A possible explanation for the conflicting results may be that this study targeted secondary-level learners who are of relatively low proficiency. Subordination has been considered as an effective indicator of written complex ification for beginners and intermediate learners, whereas at the advanced levels of proficiency, the predictive role of subordination may be subdued by phrasal elaboration (e.g., Lu & Ai, 2015; Norris & Ortega, 2009; Ortega, 2003).

Notably, while syntactic complexity exhibited predictive effects on writing quality, their relationship was significantly mediated by lexical sophistication. The mediational effect seems to suggest that the raters were more likely to associate higher-quality writing with longer sentences and/or a greater number of dependent clauses containing sophisticated words rather than such syntactic features per se. The finding with regard to the interactive relationship between syntactic complexity and lexical sophistication is in line of those observed by two previous studies that targeted college students: complex nominals per clause mediated by lexical decision times (Zhang, Lu, et al., 2022), and mean length of clause mediated by lexical decision times (Kim & Crossley, 2018). Expanding on the previous research, this study showed that the finding may be applicable to the writing of secondary-level learners. Taken together, these results arguably provide empirical support for the claim of Construction Grammar that lexis and syntax appear to be interconnected and mutually influencing aspects of language, without a clear-cut distinction (Goldberg, 2006).

# 5.3. Implications for writing assessment and pedagogy

The findings of this study hold some implications for scientific report writing assessment and pedagogy in EMI contexts. The identified pool of useful linguistic features may contribute to the empirically-driven development of a nuanced writing rubric for assessing the quality of secondary-level learners’ scientific reports (Knoch, 2011). For instance, the use of academic words and domain-specific vocabulary (lexical features indicated by word range) to communicate scientific knowledge can be explicitly mentioned in the assessment criteria of a rubric or integrated into automatic text scoring tools. Explicit inclusion of the research-informed linguistic complexity features in the writing rubric has the potential to clarify teacher expectations, improve sci entific report writing assessment, and enhance the feedback given to learners to facilitate learning and teaching, which attends to some extent the repeated calls to address students’ linguistic needs in EMI contexts (e.g., Doiz & Lasagabaster, 2020; Pun, 2019; Rose et al., 2020). It may be also advisable for teachers to share the writing rubric with learners to make the rubric elements and high qualities of a scientific report transparent, which in turn can potentially improve learner writing performance (e.g., Reddy & Andrade, 2010; Wang, 2014). Moreover, the results concerning the interrelationships between lexical complexity and syntactic complexity in predicting writing quality (i.e., the mediating effect of lexical sophistication on the relation of lexical diversity and syntactic complexity to writing quality) could serve as empirical evidence for establishing the weights of different components in both automated scoring models and traditional school assessments. In light of the mediational role of lexical sophistication, a writing rubric may state that proficient writers employ not solely complex grammatical structures or a wide variety of words, but rather such linguistic features containing sophisticated vocabulary.

From the perspective of writing instruction, the identified linguistic features that significantly contributed to the quality of sci entific reports may warrant consistent pedagogical attention from science teachers and language teachers in EMI contexts. Teachers may foster learners’ language awareness by providing them with annotated high-rating scientific reports with explanations and comments on sophisticated language forms and grammatical structures. Alternatively, teachers may develop instructional activities involving a corpus-based orientation (Campoy et al., 2010), wherein secondary-level learners are encouraged to explore the use of complex lexical items and sentence structures in scientific reports composed by expert writers. Such data-driven learning for writing (Crosthwaite, 2019) can help secondary-level students understand the language conventions of scientific reports, and raise their awareness of linguistic complexity in academic and scientific written discourse.

# 6. Conclusion

The current study used an SEM approach to examine the factor structure and construct validity of lexical and syntactic complexity, and to unpack their predictive effects on the quality of scientific reports written by EMI secondary-level learners. However, this study inevitably suffers from some limitations that need to be acknowledged and should be addressed in future research. Firstly, the study did not satisfactorily examine features of coherence and cohesion—two related but distinct constructs that play an important role in writing (Kim & Crossley, 2018)—represented in the learners’ scientific reports. Future research is encouraged to investigate their interrelationship with linguistic complexity in assessing secondary-level learners’ writing quality. Secondly, the study solely focused on the role of linguistic complexity in writing assessment. Further research may usefully delve into secondary-level learners’ written performance using the comprehensive evaluation framework CAF (complexity, accuracy, and fluency)—‘a standard practice for assessing L2 written production’ (Abdi Tabari et al., 2023, p.1). Thirdly, the SEM model constructed in this study did not control for the potential effect of topic on the relationship between linguistic complexity and writing quality. Future research with a sufficient sample size for each of the different writing topics would merit consideration in generating a more robust SEM model of the shared features in predicting the quality of scientific reports across diverse topics. Apart from addressing the above-mentioned limitations, there are some future directions to expand this line of inquiry. Given that scientific report writing follows the Introduction-Method-Results-Discussion-Conclusion format (Hyland, 1996), it would be useful to explore whether the predictive power of different linguistic indices may vary across the part-genres. Moreover, as scientific report writing likely integrates visual components such as diagrams and tables, there is also a need for subsequent research to examine the multimodal features in learners’ scientific reports and their impact on L2 writing quality (see Liu & Lo, 2024 for a call for multimodal assessment in EMI contexts).

# Ethics statement

The studies involving human participants were reviewed and approved by City University of Hong Kong. Written informed consent to participate in this study was provided by the participants’ legal guardian/next of kin.

# CRediT authorship contribution statement

Wangyin Kenneth Li: Writing – review & editing, Writing – original draft, Visualization, Methodology, Formal analysis, Data curation. Jack Pun: Writing – review & editing, Writing – original draft, Visualization, Resources, Project administration, Investi gation, Funding acquisition, Formal analysis, Data curation, Conceptualization.

# Declaration of Competing Interest

The authors declare no conflict of interest.

# Acknowledgements

The work described in this paper was fully supported by the Hong Kong General Research Fund (GRF) (Grant number: 11603521). We would like to express our gratitude to three anonymous reviewers and Dr. Stefanie Wind (the associate editor of Assessing Writing) for their constructive comments and suggestions, which helped us improve the content of this manuscript. However, any errors that remain are our own.

# Appendix A. Instructions for scientific report writing

If scientists do research and find something useful or interesting, they will want to share their findings with other scientists. This is not only to inform other scientists of what they have found but also to give other scientists a chance to check their results, sometimes by repeating the same experiments to see if they get the same results. By now, you should have drafted your hypotheses/research questions, completed your data collection, found some definitions and other information about your topic as well as information about previous research.

Now it’s time to write your scientific report! The report will explain what you were researching, how you did the research, what the results were and what you think the results mean. The purpose of such a report is not only to share your findings, but also to convince readers that your research is valid, in other words to convince them that the research was carried out properly and that your results can be trusted. Please use the structure of Introduction-Methods-Results-Discussion-Conclusion (see below) we have gone through in our previous lessons. In previous lessons, you have learnt how to write up each section. As a reminder, what content is likely included in each section is also shown below. You do not need to strictly follow the suggested content in each section. Feel free to modify them according to your own scientific research. When you write your scientific report, you are also expected to use online resources such as books, journal articles, websites, and news reports to support your ideas and discuss your findings. Good luck!

Title Introduction

Provide background information on your research topic based on information from online resources.

$\bullet$ Outline the objectives of your scientific research.   
$\bullet$ Clearly state the research question or hypothesis.   
$\bullet$ Explain the significance of your research topic.

Method

$\bullet$ Describe the materials and equipment used in your scientific research.   
$\bullet$ Give information about your participants.   
$\bullet$ Detail the procedures followed during data collection and experiments.   
$\bullet$ Discuss how data was analyzed.

Results

Present the findings of your scientific research in a clear and organized manner. $\bullet$ Use tables, graphs, and figures to illustrate the results. $\bullet$ Avoid interpretation at this stage; simply report the data.

Discussion

$\bullet$ Interpret the results in the context of the research question or hypothesis.   
$\bullet$ Discuss any unexpected results.   
$\bullet$ Discuss your results in light of information from online resources.   
$\bullet$ Address the implications of the results and their significance.

Conclusion

Summarize the key findings of your scientific research. $\bullet$ Suggest potential areas for further research.

Appendix B. Scoring rubric for scientific reports   

<html><body><table><tr><td></td><td>Organization (/7)</td><td>Content (/7)</td><td>Language (/7)</td></tr><tr><td>(7/6.5/6)</td><td>Able to present</td><td>Able to introduce and.</td><td>Able to express ideas in.</td></tr><tr><td></td><td> information in a</td><td> develop ideas</td><td>accurate English with</td></tr><tr><td></td><td> clearly organized,</td><td>clearly, effectively</td><td>few errors (of grammar,)</td></tr><tr><td></td><td>coherent and</td><td>and in an interesting</td><td>vocabulary), using</td></tr><tr><td></td><td>cohesive way, using</td><td>way, following</td><td> appropriate language</td></tr><tr><td></td><td>effective signposting</td><td>scientific</td><td>forms and an</td></tr><tr><td></td><td>with all expected</td><td>conventions,</td><td>appropriate range of</td></tr><tr><td></td><td>sections of the</td><td>referring to relevant</td><td> academic and technical vocabulary for</td></tr><tr><td></td><td>report present and</td><td>theory and</td><td>the different sections of.</td></tr><tr><td></td><td> in a logical</td><td>supporting claims</td><td>the report.</td></tr><tr><td></td><td>sequence.</td><td>appropriately.</td><td></td></tr><tr><td>(5.5/5/4.5)</td><td>Able to present</td><td>Able to introduce and</td><td>Able to express ideas in.</td></tr><tr><td></td><td> information in a</td><td>develop ideas</td><td>accurate English with</td></tr><tr><td></td><td>mostly clearly</td><td>clearly, effectively</td><td> some errors, using</td></tr><tr><td></td><td>organized, coherent</td><td> and in an interesting</td><td> mostly appropriate</td></tr><tr><td></td><td>and cohesive way,</td><td>way most of the time.</td><td> language forms and a</td></tr><tr><td></td><td>using some</td><td>Mostly follow</td><td>mostly appropriate</td></tr><tr><td></td><td>signposting with all</td><td>scientific</td><td>range of academic and technical</td></tr><tr><td></td><td>expected sections</td><td>conventions, refers to</td><td>vocabulary for the</td></tr></table></body></html>

<html><body><table><tr><td></td><td>Organization (/7)</td><td>Content (/7)</td><td>Language (/7)</td></tr><tr><td></td><td>of the report present and in a logical sequence.</td><td>relevant theory where necessary and supports claims appropriately. Able to introduce and</td><td>different sections of the report.</td></tr><tr><td></td><td>information in a somewhat organized way, with most of the expected sections of the report present and in a logical sequence.</td><td>develop ideas clearly, effectively and in an interesting way some of the time. May follow scientific conventions, refer to relevant theory where necessary and support claims appropriately</td><td>somewhat accurate English with some errors, using mostly appropriate language forms and a mostly appropriate range of academic and technical vocabulary for the different sections of the report.</td></tr><tr><td></td><td>to present information in a somewhat organized way, with most of the expected sections of the report present and in a logical sequence.</td><td>the student is able to introduce and develop ideas clearly, effectively and in an interesting way. May not follow scientific conventions, refer to relevant theory where necessary nor</td><td>student is able to express ideas in somewhat accurate English with some errors, using mostly appropriate language forms and a mostly appropriate range of academic and technical vocabulary for</td></tr><tr><td></td><td>information in a somewhat organized way. Important sections of the report are missing.</td><td>and develop ideas clearly, effectively and in an interesting way. Does not adequately follow scientific conventions to support claims.</td><td>Unable to express ideas in somewhat accurate English with some errors, using mostly appropriate language forms. The report is difficult to understand because of problems with language use.</td></tr></table></body></html>

# Data Availability

Data will be made available on request.

# References

Abdi Tabari, M., Bui, G., & Wang, Y. (2023). The effects of topic familiarity on emotionality and linguistic complexity in EAP writing. Language Teaching Research, 28 (4), 1616–1634.   
Balota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler, B., Loftis, B., … Treiman, R. (2007). The English lexicon project. Behavior Research Methods, 39, 445–459.   
Baron, R. M., & Kenny, D. A. (1986). The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology, 51(6), 1173–1182.   
Belcher, D. (2013). The scope of L2 writing: Why we need a wider lens. Journal of Second Language Writing, 22(4), 438–439.   
Bi, P., & Jiang, J. (2020). Syntactic complexity in assessing young adolescent EFL learners’ writings: Syntactic elaboration and diversity. System, 91. Article 102248.   
Biber, D., Gray, B., & Poonpon, K. (2011). Should we use characteristics of conversation to measure grammatical complexity in L2 writing development? TESOL Quarterly, 45(1), 5–35.   
Brysbaert, M., Warriner, A. B., & Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911.   
Cameron, A. C. (2004). Kurtosis. In In. M. S. Lewis-Beck, A. Bryman, & T. F. Liao (Eds.), The SAGE encyclopedia of Social Science research methods (pp. 544–546). Sage.   
Campoy, M. C., Ma, C., & Bell´es Fortuno, ˜ B. (2010). Corpus-based approaches to English language teaching. Continuum.   
Chˆau, Q. H., & Bult´e, B. (2023). A comparison of automated and manual analyses of syntactic complexity in L2 English writing. International Journal of Corpus Linguistics, 28(2), 232–362.   
Chung, T. M., & Nation, P. (2004). Identifying technical vocabulary. System, 32(2), 251–263.   
Collier, J. (2020). Applied structural equation modeling using AMOS: Basic to advanced techniques. Routledge.   
Coltheart, M. (1981). The MRC psycholinguistic database. The Quarterly Journal of Experimental Psychology Section A, 33(4), 497–505.   
Covington, M. A., & McFall, J. D. (2010). Cutting the Gordian knot: The moving-average type–token ratio (MATTR). Journal of Quantitative Linguistics, 17(2), 94–100.   
Coxhead, A. (2000). A new academic word list. TESOL Quarterly, 34(2), 213–238.   
Coxhead, A., & Byrd, P. (2007). Preparing writing teachers to teach the vocabulary and grammar of academic prose. Journal of Second Language Writing, 16(3), 129–147.   
Crossley, S., & McNamara, D. S. (2012). Predicting second language writing proficiency: The roles of cohesion and linguistic sophistication. Journal of Research in Reading, 35(2), 115–135.   
Crossley, S., Salsbury, T., & McNamara, D. S. (2015). Assessing lexical proficiency using analytic ratings: A case for collocation accuracy. Applied Linguistics, 36(5), 570–590.   
Crossley, S., Salsbury, T., McNamara, D. S., & Jarvis, S. (2011). Predicting lexical proficiency in language learner texts using computational indices. Language Testing, 28(4), 561–580.   
Crosthwaite, P. (2019). Data-driven learning for the next generation corpora and DDL for pre-tertiary learners. Routledge.   
Davies, M. (2008). Corpus of contemporary American English (COCA). 〈http://corpus.byu.edu/coca/〉.   
De Wilde, V. (2023). Lexical characteristics of young L2 English learners’ narrative writing at the start of formal instruction. Journal of Second Language Writing, 59. Article 100960.   
Doiz, A., & Lasagabaster, D. (2020). Dealing with language issues in English-medium instruction at university: A comprehensive approach. International Journal of Bilingual Education and Bilingualism, 23(3), 257–262.   
Education Bureau. (2009). Fine-tuning the medium of instruction for secondary shcools (Education Bureau Circular No. 6/2009). Hong Kong: Government Printer.   
Engber, C. A. (1995). The relationship of lexical proficiency to the quality of ESL compositions. Journal of Second Language Writing, 4(2), 139–155.   
Evans, S., & Morrison, B. (2011). Meeting the challenges of English-medium higher education: The first-year experience in Hong Kong. English for Specific Purposes, 30 (3), 198–208.   
Fang, Z. (2005). Scientific literacy: A systematic functional linguistics perspective. Science Education, 89(2), 335–347.   
Goldberg, A. E. (2006). Constructions at work: The nature of generalization in language. Oxford University Press.   
Graham, S. (2006). Writing. In P. Alexander, & P. Winne (Eds.), Handbook of educational psychology (pp. 457–478). Erlbaum.   
Grant, L., & Ginther, A. (2000). Using computer-tagged linguistic features to descirbe L2 writing differences. Journal of Second Language Writing, 9(2), 123–145.   
Guo, L., Crossley, S., & McNamara, D. S. (2013). Predicting human judgments of essay quality in both integrated and independent second language writing samples: A comparison study. Assessing Writing, 18(3), 218–238.   
Halliday, M. A. K., & Martin, J. R. (1993). Writing science: Literacy and discursive power. University of Pittsburgh Press.   
Hancock, G. R., & Schoonen, R. (2015). Structural equation modeling: Possibilities for language learning researchers. Language Learning, 65(S1), 160–184.   
Harklau, L. (2011). Commentary: Adolescent L2 writing research as an emerging field. Journal of Second Language Writing, 20(3), 227–230.   
Hyland, K. (1996). Talking to the academy: Forms of hedging in science research articles. Written Communicaiton, 13(2), 251–281.   
Hyland, K. (2009). Teaching and researching writing (2nd ed).). Routledge.   
In’nami, Y., & Koizumi, R. (2011). Structural equation modeling in language testing and learning research: A review. Language Assessment Quarterly, 8(3), 250–276.   
Jiang, J., Bi, P., & Liu, H. (2019). Syntactic complexity development in the writings of EFL learners: Insights from a dependency syntactically-annotated corpus. Journal of Second Language Writing, 46. Article 100666.   
Johnson, W. (1944). Studies in language behavior: A program of research. Psychological Monographs, 56(2), 1–15.   
Jung, Y., Crossley, S. A., & McNamara, D. S. (2015). Linguistic features in MELAB writing performances (Working Paper No. 2015-05). Retrieved from Cambridge Michigan Language Assessments website: http://www.cambridgemichigan.org/wp-content/uploads/2015/04/CWP-2015-05.pdf.   
Khushik, G. A., & Huhta, A. (2020). Investigating syntactic complexity in EFL learners’ writing across common European framework of reference levels A1, A2, and B1. Applied Linguistics, 41(4), 506–532.   
Kim, M., & Crossley, S. (2018). Modeling second language writing quality: A structural equation investigation of lexical, syntactic, and cohesive features in sourcebased and independent writing. Assessing Writing, 37, 39–56.   
Kline, R. B. (2018). Principles and practice of structural equation modeling. Canadian Studies in Population.   
Kline, R. B. (2023). Principles and practice of structural equation modeling (5th ed).). Guilford Publications.   
Knoch, U. (2011). Rating scales for diagnostic assessment of writing: What should they look like and where should the criteria come from? Assessing Writing, 16(2), 81–96.   
Kuperman, V., Stadthagen-Gonzalez, H., & Brysbaert, M. (2012). Age-of-acquisition ratings for 30,000 English words. Behavior Research Methods, 44, 978–990.   
Kyle, K., & Crossley, S. (2015). Automatically assessing lexical sophistication: Indices, tools, findings, and application. TESOL Quarterly, 49(4), 757–786.   
Kyle, K., & Crossley, S. (2016). The relationship between lexical sophistication and independent and source-based writing. Journal of Second Language Writing, 34, 12–24.   
Kyle, K., Crossley, S., & Berger, C. (2018). The tool for the automatic analysis of lexical sophistication (TAALES): Version 2.0. Behavior Research Methods, 50, 1030–1046.   
Kyle, K., & Crossley, S. (2018). Measuring syntactic complexity in L2 writing using fine-grained clausal and phrasal indices. The Modern Language Journal, 102(2), 333–349.   
Laufer, B., & Nation, P. (1995). Vocabulary size and use: Lexical richness in L2 written production. Applied Linguistics, 16(3), 307–322.   
Lee, C., Ge, H., & Chung, E. (2021). What linguistic features distinguish and predict L2 writing quality? A study of examination scripts written by adolescent Chinese learners of English in Hong Kong. System, 97. Article 102461.   
Linnarud, M. (1986). Lexis in composition: A performance analysis of Swedish learners’ written English. CWK Gleerup.   
Liu, J. E., & Lo, Y. Y. (2024). Multimodality in CLIL assessment: Implications for teacher assessment literacy. Journal of Multilingual and Multicultural Development, 1–19.   
Lu, X. (2010). Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4), 474–496.   
Lu, X. (2011). A corpus-based evaluation of syntactic complexity measures as indices of college-level ESL writers’ language development. TESOL Quarterly, 45(1), 36–62.   
Lu, X. (2014). Computational methods for corpus annotation and analysis. Springer.   
Lu, X. (2017). Automated measurement of syntactic complexity in corpus-based L2 writing research and implications for writing assessment. Language Testing, 34(4), 493–511.   
Lu, X., & Ai, H. (2015). Syntactic complexity in college-level English writing: Differences among writers with diverse L1 backgrounds. Journal of Second Language Writing, 29, 16–27.   
Maamuujav, U., Olson, C. B., & Chung, H. (2021). Syntactic and lexical features of adolescent L2 students’ academic writing. Journal of Second Language Writing, 53. Article 100822   
Macaro, E. (2018). English Medium Instruction: Content and language in policy and practice. Oxford University Press   
Malvern, D., Richards, B., Chipere, N., & Dur´an, P. (2004). Lexical diversity and language development: Quantification and assessment. Palgrave Macmillan.   
Martínez, A. C. L. (2018). Analysis of syntactic complexity in secondary education EFL writers at different proficiency levels. Assessing Writing, 35, 1–11.   
McCarthy, M., & Jarvis, S. (2007). vocd: A theoretical and empirical evaluation. Language Testing, 24(4), 459–488.   
McCarthy, M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Behavior Research Methods, 42(2), 381–392.   
McKinley, J., & Rose, H. (2022). English language teaching and English-medium instruction: Putting research into practice. Journal of English-Medium Instruction, 1(1), 85–104.   
McNamara, D. S., Graesser, A. C., McCarthy, M., & Cai, Z. (2014). Automated evaluation of text and discourse with Coh-Metrix. Cambridge University Press.   
Murray, D. M. (1980). Writing as process: How writing finds its own meaning. Eight Approaches to Teaching Composition, 3–20.   
Norris, J. M., & Ortega, L. (2009). Towards an organic approach to investigating CAF in instructed SLA: The case of complexity. Applied Linguistics, 30(4), 555–578.   
Ortega, L. (2003). Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college-level L2 writing. Applied Linguistics, 24(4), 492–518.   
Parkinson, J. (2017). The student laboratory report genre: A genre analysis. English for Specific Purposes, 45, 1–13.   
Peng, Y., Sun, J., Quan, J., Wang, Y., Lv, C., & Zhang, H. (2023). Predicting Chinese EFL learners’ human-rated writing quality in argumentative writing through multidimensional computational indices of lexical complexity. Assessing Writing, 56. Article 100722.   
Peng, Y., Zheng, Y., Sun, J., Jiang, Y., Lin, J., & Zhang, H. (2024). Modeling relationships among large-grained, fine-grained absolute syntactic complexity and assessed L2 writing quality: An SEM approach. Assessing Writing, 61. Article 100875.   
Pun, J. (2019). Salient language features in explanation texts that students encounter in secondary school chemistry textbooks. Journal of English for Academic Purposes, 42. Article 100781.   
Qin, W., & Uccelli, P. (2016). Same language, different functions: A cross-genre analysis of Chinese EFL learners’ writing performance. Journal of Second Language Writing, 33, 3–17.   
Ravid, D., & Tolchinsky, L. (2002). Developing linguistic literacy: A compehensive model. Journal of Child Language, 29(2), 417–447.   
Read, J. (2000). Assessing vocabulary. Cambridge University Press.   
Rose, H., Curle, S., Aizawa, I., & Thompson, G. (2020). What drives success in English medium taught courses? The interplay between language proficiency, academic skills, and motivation. Studies in Higher Education, 45(11), 2149–2161.   
S´anchez-P´erez, M. (2021). Predicting content proficiency through disciplinary-literacy variables in English-medium writing. System, 97. Article 102463.   
Silva, T. (2013). Second language writing: Talking points. Journal of Second Language Writing, 22, 432–434.   
Simpson-Vlach, R., & Ellis, N. (2010). An academic formulas list: New methods in phraseology research. Applied Linguistics, 31(4), 487–512.   
Skehan, P. (2009). Modelling second language performance: Integrating complexity, accuracy, fluency, and lexis. Applied Linguistics, 30(4), 510–532.   
Tabachnick, B. G., & Fidell, L. S. (2019). Using multavariate statistics (7th ed). Pearson.   
The Curriculum Development Council. (2017). Science education: Key learning aera curriculum guide (Primary 1-Secondary 6). The Education Bureau.   
Tsui, A. B. (2004). Medium of instruction in Hong Kong: One country, two systems, whose language? In J. Tollefson, & A. B. M. Tsui (Eds.), Medium of instruction policies: Which agenda? Whose agenda? (pp. 97–116). Lawrence Erlbaum.   
Ure, J. (1971). Lexical density and register differentiation. In G. Perren, & J. L. M. Trim (Eds.), Applications of linguistics (pp. 443–452). Cambridge University Press.   
Vogelin, ¨ C., Jansen, T., Keller, S. D., Machts, N., & Moller, ¨ J. (2019). The influence of lexical features on teacher judgements of ESL argumentative essays. Assessing Writing, 39, 50–63.   
Weigle, S. C. (2002). Assessing writing. Cambridge University Press.   
Williams, J. (2012). The potential role(s) of writing in second language development. Journal of Second Language Writing, 21(4), 321–331.   
Wolfe-Quintero, K., Inagaki, S., & Kim, H.-Y. (1998). Second language development in writing: Measures of fluency, accuracy, and complexity. University of Hawaii Press.   
Yasuda, S. (2024). Does “more complexity” equal “better writing”? Investigating the relationship between form-based complexity and meaning-based complexity in high school EFL learners’ argumentative writing. Assessing Writing, 61. Article 100867.   
Yoon, H., & Polio, C. (2017). The linguistic development of students of English as a second language in two written genres. TESOL Quarterly, 51(2), 275–301.   
Zhang, X. (2022). The relationship between lexical use and L2 writing quality: A case of two genres. International Journal of Applied Linguistics, 32(3), 371–396.   
Zhang, X., Lu, X., & Li, W. (2022). Beyond differences: Assessing effects of shared linguistic features on L2 writing quality of two genres. Applied Linguistics, 43(1), 168–195.   
Zhang, Y., & Ouyang, J. (2023). Linguistic complexity as the predictor of EFL independent and integrated writing quality. Assessing Writing, 56. Article 100727.

Jack Pun is an associate professor at the Department of English, City University of Hong Kong, with a DPhil in Education (Applied Linguistics and Science Education) from the University of Oxford. His research focuses on English as a medium of instruction (EMI), second/foreign language acquisition, sociolinguistics, and systemic functional linguistics.