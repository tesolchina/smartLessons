# Graduate students' use of ChatGPT for academic text revision: Behavioral, cognitive, and affective engagement

Svetlana Koltovskaiaa,1, Payam Rahmatib,2, Hooman Saeli c,\*,3

a Northeastern State University, Tahlequah, OK, USA b Oklahoma State University, Stillwater, OK, USA c The University of Tennessee, Knoxville, TN, USA

# ARTICLEINFO

# ABSTRACT

Keywords:   
ChatGPT   
Artificial Intelligence (AI)   
L2 writing   
Learner engagement

Amidst the growing hype surrounding ChatGPT and its use for writing purposes, numerous studies have emerged, but there has been limited emphasis on investigating how individual students utilize it for text revision. Against this backdrop, the present study examined how six Iranian graduate ESL students from STEM fields engaged with ChatGPT when revising their academic research proposals. The study employed a tripartite multidimensional framework encompassing behavioral, cognitive, and affective aspects of engagement. Behavioral engagement was analyzed by studying students' texts and their screencasts using ChatGPT for text revision. Cognitive and affective engagement were evaluated through students' comments during stimulated recall of the screencasts, along with semi-structured interviews and follow-up surveys. The findings show that participants used ChatGPT behaviorally for lower-order concerns, relying on a training prompt. While they generally noticed and comprehended the feedback, they expressed doubts regarding its accuracy whenever necessary, implying a reasonably high level of cognitive engagement. Affective engagement revealed high satisfaction, with participants' favoring ChatGPT for paraphrasing to enhance professionalism in writing. The research offers insights for educators on how to effectively integrate ChatGPT into L2 writing classrooms.

# 1. Introduction

Advancements in artificial intelligence (Al) have led to the emergence of diverse writig ssistance tools. These Al-powered tools encompass automated writing evaluation (AWE) systems like Criterion, automated written corrective feedback (AwCF) tools like Grammarly, machine translation platforms, such as Google Translate, and automatic text generation systems, such as OpenAI's ChatGPT (Godwin-Jones, 2022). Among these tols, the launch of ChatGPT on November 30, 2022, tirred excitement and garnered attention due to its versatile capabilities (Cotton et al., 2023). These capabilitie include generating written content in a human-like manner, composing creative works, providing coherent responses to diverse queries, summarizing textual content, and writing computer codes, among others (OpenAI, 2023). In the writing context, research has shown that ChatGPT can be a valuable tol for students in crafting argumentativ esays, facilitating pre-writing tass like outlining, assting during the revision phase for argu. mentative content and language, and promoting rflection after the writing proces (Su et al., 2023). Additionally, some researchers have emphasized ChatGPT's utilit in research-related tass, such as data analysis and processing, compiling information, and serving as a research assistant throughout the research process (Casal & Kessler, 2023).

Despite the aforementioned advantages, academic discussions have raised some ethical concerns linked to the incorporation of ChatGPT within writing clasrooms (Pack & Maloney, 2023a). Among these concerns is the prospect of students reliance on ChatGPT for content generation (Brrot, 2023), pottiall dsregarding proper attribution (Jarrah et l., 2023; Su et al., 2023). Concns about the generation of biased information have also emerged (Godwin-Jones, 2022; Kohnke et al., 2023), which could potentiall result in students' incorporating inaccurate information into their writing. To address some of these concerns, earlier studies have focused on the extent to which ChatGPT-generated texts can be identified by plagiarism detection tools (e.g., Taloni et al., 2023). Additionall, investigations have been conducted to determine the ability of experts, including journal editors, to distinguish between human-written and ChatGPT-generated texts (Casal & Kesser, 2023). These studies offr valuable insights regarding minimizing unethical uses of ChatGPT (Cotton et al., 2023).

While much of the recent research has centered around the ethical aspects of ChatGPT (Pack & Maloney, 2023b), ittl attention has been paid to comprehending how students engage with ChatGPT behaviorally, cognitively and afectively during the writing proces. Additionall, unlike previous AWE systems, ChatGPT provides feedback on a wide range of ssues and fosters an interactive envi. ronment because of its dialogic nature. Focusing on behavioral, cognitive, and affective engagement in this context is crucial for understanding the tools affordances and limitations, as well a the various factors that can aid or hinder students efectie utilization of i (Zhang, 2017). This, in turn, can inform the development of more efective instructional strategies in L2 writing. Therefore, this study employed a multifaceted framework of learner engagement to explore how six ESL graduate students engaged with ChatGPT when revising their texts.

# 2. Literature review

# 2.1. AI-powered tools for writing

Recent attention has been paid to AI-powered toos for writing, including AWE systems (e.g, Criterion and Pigai) and AwCF tools (e.g., Grammarly) which are designed to offr writing-related asstance and assessment (Godwin-Jones, 2022). Iitilly, debates revolved around whether incorporating these tols into writing classrooms was advisable. However, it has become evident that these tools are here to stay (Weige, 2013). Therefore, the discourse has quickly transitioned from \*should we' to how can we effectively harness these ools for the benefit of students (Chen & Cheng, 2008). These studies have yielded valuable insights into how AWE and AWCF tools should be employed and how they are perceived by students and educators (e.g., Ranalli et al, 2017). While studies on AWE and F pren conlicting ults readin thir useulss th i a conseus tht efetivetilization f th oos can e achieved if students and teachers receive proper training (Ranall e l., 2017). Additionall, their efectiveness may be heightened when utilized alongside other feedback forms like teacher or peer feedback (Zhang & Hyland, 2022). Besides, both teachers and students generally tend to perceive these tools positively (e. g., Li, 2021).

With the advent of more advanced AI-powered tools the potential for writing support offred by ChatGPT may surpass that by AWE and AwCF (Guo & Wang, 2023). Unlike AWE and AwCF, which faced criticism for their focus on local aspects of writing and their inability to ad stdent i tackling global aspts of writing (Chen & Cheng, 2008), ChatGT offrs ssistance on wide array f isue. This includes commenting on grammar, vocabulary, and syntax, as wellas explaining complex concepts, adjusting writing style and tone, offein fdback, generating dierse essa topics, and crating outlines (Barrot, 2023; Bishop, 2023). Khnke t al. (2023) states that ChatGPT stands out as one of the most advanced AI-powered chatbots. This is because ChatGPT is asubstantial language model, which is a machine-learning system that independently learns from data and has the capacity to generate advanced and seemingly intelligent text once it has been trained on an extensive dataset of written material (OpenAl, 2023). Because of this as Cotton et al. (2023) notes, fascination with ChatGPT surged shortly after its launch.

Given its advanced nature and diverse capabilities, ChatGPT holds the potential to aid students in enhancing their writing skill by functioning as a versatil writing asstant (Pack & Maloney, 2023a). Nevertheles there has been an mphasis on addressing the tool's limitations and the possiity for its misuse Jarrah t al., 2023; Su t al., 2023), thus aracting rearch to its caacit to contribute to plagiarism. Taloni et al. (2023) determined that GPT-4.0 could rephrase abstracts of published cientifi article in ways that evade AI detection, particularly when humanized through platorms like Undetectable.AI (htps://undetectable.ai/). Similarly, Casal and Kessler (2023) discovered that ChatGPT/AI's proficiency in crafting research abstracts isof such high quality that even experienced reviewers and linguists have limited capability to differentiate its abstracts from those created by humans.

While the above concerns about ChatGPT's (mis)use by students are valid (Pack & Maloney, 2023b), another area worth inves tigating is the actual manner in which students incorporate ChatGPT in their writing. Despite the increasing volume of research on ChatGPT and learner engagement in L2 contexts (Lo et al., 2024), there remains a dearth of empirical evidence regarding students engagement with this ool, specifically feedback provided by it, for tasks like textrevision. This line of inquiry can also shed further light on the ethicality of learners use of ChatGPT during the writing proces by providing evidence on whether, why, and how they utilize ChatGPT for text revision.

# 2.2. Learner engagement with AI-generated feedback

A substantial body of research has ben conducted using a multidimensional framework of learner engagement. This framework, initilly introduced by Ellis (2010) in the context of oral and witten corrective feedback encompases three dimensions (behavioral cognitiv, and affective), along with sub-constructs within each dimension (e.g., Zhang & Hyland, 2022). These imensions have been adopted and adapted in the present study:

Behavioral egagement relates to the revision operations, revision strategies, and overall time spent on revisions: Whether leaners incorporate ChatGPT-generated feedback in their draft revisions, how they do it (e.g., copying and pasting paraphrased sentences) and how much time they spend during this process.   
Cognitive engagement pertains to the presence and depth of participants' processing of ChatGPT's fedback: Whether learners notice and comprehend ChatGPT-generated feedback, and whether they employ any cognitive (e.g., reasoning) and/or metacognitive operations (e.g., planning) to facilitate this processing.   
Affective engagement encompasses participants emotional reactions to feedback: How learners perceive ChatGPT's feedback upon receiving it and what overall attitudinal responses they show toward ChatGPT.

This framework has been employed to examine how students engage with different forms of feedback, such as teacher feedback (Han, 2017; Han & Hyland, 2015; Koltovskaia & Mahapatra, 2022), peer feedback (Yu et al., 2019; Zhang et al., 2023), automated feedback (Koltovskaia, 2020; Saeli et al., 2023; Zhang, 2017; 2020; hang & Xu, 2022), and multiple feedback types (Tian & Zhou, 2020; Zhang & Hyland, 2018; 2022). The widespread utilization of thi framework by several scholars can be attributed to itsaility to provide a more profound comprehension of how students engage with feedback provided by different sources. Additionall, it unveils the factors that can either faclitate or impede the integration of feedback (Zhang, 2017). Scholarly attention has also focused on learners engagement with feedback delivered by AWE systems in both EFL and ESL contexts, such as Pigai (Zhang, 2017; 2020; Zhang & Xu, 2022) and E-rater (Saeli et a., 2023), as well as AwCF tools, such as Grammarly (Koltovskaia, 2020). dditionall, studies have examined how students engage with automated feedback in conjunction with teacher feedback and/or peer feedback (Tian & Zhou, 2020; Zhang & Hyland, 2018, 2022). In general, studies investigating learner engagement with automated feedback delivered by different AI-powered tools suggest that learner engagement is an intricate, non-linear, and ever-evolving process (Zhang & Hyland, 2022).

Following the inception of ChatGPT, research has started to explore learner engagement with this too in L2 contexts (Lo et l. 2024). In their systematic review of 72 empirical studies, Lo et al. highlight the patterns of student engagement with ChatGPT in various educational setings. They conclude that students behaviorally engage with ChatGPT, but this engagement might lead to undesirable learning outcomes, such as plagiarism. Cognitively, according to the authors, ChatGPT can enhance students' under. standing of learning activities, although it can hinder their critical and independent thinking. Affectively, as Lo et al. show, students hold varying degreesof enthusiasm and trust toward ChatGPT in the classroom. The author conclude that leaners' engagement with ChatGPT is significantly underexplored, which might negatively affect the implementation of this tool in the classoom. In another systematic review, Yang and Li (2024) conclude that the role of ChatGPT in the L2 classroom nees further investigation. Althoughthe authors point to ChatGPT's abilit to enhance engagement with learning activities, as mainly evidenced through survey-based studies, they callfor the examination of this merit in mixed-methods studies. Similarly, Wang and Wang (2024) report that L2 learners engagement with ChatGPT, as a language learning asstance tol, is highly individualized and variable, ultimately depending on learners' basic psychological needs. Though not directly related to L2 writing and feedback, these studies provide important insights into the non-linear and dynamic nature of learners' engagement with ChatGPT.

ChatGPT has been widely hailed as an available source of immediate feedack to students (Escalante et al., 2023; Lo e al., 2024; Shi & Aryadoust, 2024; Xia0 & Zhi, 2023; Yang & Li, 2024; Zhang & Tur, 2024). Although emerging research suggests that human-generated feedback may be more ccurate than its ChatGPT-generated counterpart (Stesset al., 2024), the understanding of ChatGPT-generated feedback's efficacy is still i it infancy. As Zhang and Hyland (2022) postulate, the investigation of feedback efficacy can be achieved by fousing on the ultimate usrs of feedback, i, ners, and what thy do with fdback after rcevin it i.e., engagement with feedack. Within L2 writing contexts, the extremely limited research on th effectiveness of ChatGPT-generated feedback and leners peretion of i has yielded mixed reults. alante e al. (2023), for example, ntce that leners likly engage with writing tutors more profoundly than with ChatGPT, although they may find ChatGPT-generated feedback clear and specific. Albeit gleaned from a quasi-experimental study, Escalante et al.'s results highlight the importance of examining learners' individualized engagement with ChatGPT-generated feedback in carefully-designed case studies.

# 2.3. The present study

Our rationale for conducting this study was threefold. First, with the increasing popularity of ChatGPT as atol offering diverse writing assistance (Guo & Wang, 2023), it becomes crucial to investigate how students, the ultimate decision-makers in utilizing feedback, engage with ChatGPT's suggestions when revising thir texts. This knowledge holds the potential to \*unlock" the benefits associated with feedback (hang & Hyland, 2018, p. 90), examine the efectivnes of feedback in a lener-centered fashion (hang & Hyland, 2022), and ultimatel fster learning (Zhang & Xu, 2022). Understanding patterns of engagement with feedback can also help L2 writing instructors learn more about learners (un)ethical use of ChatGPT by providing valuable insights into whether, why, and how learners employ revision operations, utilize (meta)cognitive strategies in using Chat-GPT feedack, and perceive this feedack attitudinall. Second, unlike AWE systems, such as Grammarly, ChatGPT offers a dialogic environment where learners can interact with this too, seek fback, and, if nesary, ask follow-up question from it. Thee intractive properties makeChatG a uniquely valuable tool in the L2 writing process Third, ChatGPT ofrs feedback on a variet of issues in student writing, ranging from genre-related considerations to sentence-level ssues. This wider range of feedback distinguishes ChatGPT from other toos (e.g, Grammarly) and underscores the importance of understanding how students handle ChatGPT-generated feedback. Attis point, we are not aware of any empirical research applying a multidimensional learner engagement framework to understanding the working of ChatGPT-generated feedack. Motivated by this, we addressd the following research question, aiming to provide pedagogical implications for L2 writing instructors:

1. How do graduate ESL students engage behaviorall, cognitively, and affctively with ChatGPT when revising their academic research proposals?

# 3. Methods

# 3.1. Context and participants

The research was conducted at a large Midwestern universty in the US. ix students, pursuing graduate degrees in STEM, vol. unteered to participate in the study by signing the IRB-approved consent forms. All participants were from Iran with Persian as their first languag. According tothe language proficiency criteria for admissionsto graduate programs at the participants universt, their proficiency levels were upper-intermediate or advanced. Notably, the participants had limited prior exposure to ChatGPT for editing their wrtten work. Nevertheles, few mentioned using ChatGPT occasionall for coding tasks, and a few brought up their use of other AI-powered writing tols, like Grammarly, as a feedback source. Table 1 displays participant profiles, including gender, age, degre, field of study, L2 proficiency, and prior experience using AI tools for writing.

# 3.2. Workshop and data collection

Before data collction, the participants ook part in a workshop titled Maximizing Writing with ChatGPT, designed to introduce ChatGPT as a resource to asst them in improving their writing. The 50-minute workshop was conducted in English by the second author, who was a then-Ph.D. candidate in Applied Linguistics. The workshop first showcased ChatGPT's afordances, with the participants' experimenting with various prompts to explore is capabilities. Then, the workhop emphasized the application of ChatGPT in writing, where the participants were provided a handout developed by the authors (Appendix A) to incorporate prompts concerning grammar, clarity, organization, and any other relevant aspects. The discussion then shifted towards addressing academic integrity, followed by a question-and-answer session. The data collection comprised three phases which have been summarized in Fig. 1:

In the initial phase, the participants were engaged in revising their research proposals assgned by their graduate supervisors during their academic studies. The proposals differed in length, writing stage, and topic, depending on the participants' majors. Before documenting their revisions, the participants viewed a step-by-step video tutorial on using QuickTime Player to record their computer screens. They all watched this tutorial at ther convenience before starting the revision process ach participant revised their research proposal at home to enhance the authenticity of the revision process. The purpose of the screen recording was to capture the par. ticipants' behavioral engagement with ChatGPT-generated feedback during the revision process. In addition to the participants screencasts, their original drafts and drafts after using ChatG were collected for analys. It is important to note that the participants employed the freely accessible ChatGPT-3.5 Turbo version for their revisions.

In the second phase of the data collection, the participants underwent stimulated recall (SR) and semi-structured interviews, both of which were carred out in English. We followed the guidelines by Gassand Mackey (2000 regarding conducting delayed SRs within $4 8 \mathrm { ~ h ~ }$ of the revision process to maintain the freshness of participants' thought processes. The participants who had completed their individual revision processes contacted the second author toschedule a meeting for this phase. During the R sessions, each participant viewed the entirety of their scren recording, which acted as a stimulus, and responded to various questions designed to elicit information regarding their cognition and emotions. The participants then had a semi-structured interview, where they were further questioned about their reactions and atitudes toward ChatGPT's feedback. ee Appendix B for the recall and interview questions. The combined SR and interview sessions lasted approximately an hour and a half per participant and were audio recorded.

Table 1 Participants' profiles.   

<html><body><table><tr><td>#</td><td>Names *</td><td>Gender</td><td>Age*</td><td>Degree</td><td>Field of Study</td><td>L2 Proficiency</td><td>Prior experience with AI tools.</td></tr><tr><td>1</td><td>Farah</td><td>Female</td><td>32</td><td>Doctorate</td><td>Civil Engineering</td><td>Advanced</td><td>Grammarly</td></tr><tr><td>2</td><td>Sara</td><td>Female</td><td>32</td><td>Doctorate</td><td>Electrical and Computer Engineering</td><td>Upper-intermediate</td><td>N/A</td></tr><tr><td>3</td><td>Tara</td><td>Female</td><td>33</td><td>Master&#x27;s</td><td>Business</td><td>Upper-intermediate</td><td>N/A</td></tr><tr><td>4</td><td>Amir</td><td> Male</td><td>33</td><td>Doctorate</td><td>Civil Engineering</td><td>Advanced</td><td>Grammarly</td></tr><tr><td>5</td><td>Farid</td><td>Male</td><td>36</td><td>Doctorate</td><td>Geography</td><td>Upper-intermediate</td><td>N/A</td></tr><tr><td>6</td><td>Milad</td><td>Male</td><td>32</td><td>Doctorate</td><td>Electrical Engineering</td><td>Upper-intermediate</td><td>Grammarly</td></tr></table></body></html>

\*All names are pseudonyms \*During the data collection phase (April, 2023)

![](img/91daf1aedca53ea5506af22db465fed40d6ebcca7e522401456f3b409636c1eb.jpg)  
Fig. 1. The three phases of data collection.   
Fig. 2. Example analysis of Milad's revision operations and strategies.

In the last phase, a Google Forms survey was distributed to the participants. This was done three weeksafter the completion of the data collection. During these three weeks, the participants had been utilizing ChatGPT independently for various writing assgnments. This survey aimed to collect additional insights into the participants' prolonged engagement with ChatGPT, and the questions were formulated based on the concerns participants raised during the SRs and interviews about using ChatGPT in writing. The survey comprised a total of seven open-ended questions, delving into the participants usage patterns of ChatGPT in writing, their preferred prompts, their overall impressions of ChatGPT, their perspectives on plagiarism, advice for educators and students on utilizing ChatGPT for writing, suggestions for developers, and any additional information they wished to share (Appendix C).

# 3.3. Data analysis

The data analysis was conducted in two parts. The first part was focused on examining participants' screencasts and texts to explore their behavioral engagement. The second part dealt with analyzing participants' recall and interview transcripts as well as survey responses to assess their cognitive and affective engagement.

# 3.3.1. Part one: Behavioral engagement

For behavioral engagement, the main data source was the screencasts. In case the screencasts did not provide a clear view of the revision operations, the participants text efore and after the revisions were consuled for refeence. The analysis of the crencasts involved four phases.

The first phase, which spanned aproximately two weeks, entailed organizing data in a Google spreadsheet, where the authors set up six tabs, each corresponding to a specific participant. Within these tabs, the authors created five columns: creencast timestamps, original text, ChatGPT's suggestions, revision operations, and revision strategies (Fig. 2). Then, the authors documented the participants original sentence, noting erors and ChatGPT's suggestions in the second and third columns, respectively, while also including the corresponding timestamps in the firt column. These timestamps served as a reference, faciliating quick rtrieval of the suggestions. Notably, there were instances where ChatGPT provided asingle suggestion for multipl errors within the same sentence as seen in the following example: "In the first sentence, impressive development and improvement could be replaced with significant advancements, and qualities' should be changed to qualit." In such cases, these suggestions were entered as two separate entries. Additionall, the authors reviewed the screencasts to determine the prompts used for text revision. The analysis showed that all six participants consistently used a pecific prompt from the workshop handout: \*I'm alitte unsure about my grammar and punctuation in this essay. Can you help me identify any errors and sugest corrections?" Two participant, Farah and Amir, also employed different prompts. Amir asked, "Can you help me mprove my word choice and clrity inth following essy," whil Farah's prompt was, I've written a rough draft of my essay but I know it needs some work. Can you help me identify any areas that need improvement?".

<html><body><table><tr><td rowspan="2">Video timestamp</td><td rowspan="2">Original text</td><td colspan="3">ChatGPT&#x27;s comment</td><td rowspan="2">Revision Operations</td><td rowspan="2">Revision Strategies</td></tr><tr><td>comment</td><td> error category</td><td>accuracy</td></tr><tr><td></td><td>4:08It contains X-rays, gammaIn the second sentence, rays, infrared, ultraviolet,add a comma after THz (Terahertz), radio waves, microwaves, and visible light.</td><td>&quot;X-rays&quot; to separate the items in the list.</td><td>mechanics</td><td>inaccurate</td><td></td><td>dissmissesNo revision stratgy is observed as he dismisses the feedback.</td></tr></table></body></html>

The second phase, which lasted three weeks, encompassed analyzing ChatGPT's suggestions and inputting the data into the same spreadsheet o analyze the feeback, the authors conducted independent reviews of the errors identified by ChatGPT, classfying them according to a taxonomy adapted from Han and Hyland (2015), as well as ChatGPT's own error categorization (Appendix D). For example, in response to a prompt by one participant, ChatGPT offered a comprehensive comment covering four global aspects of writing, namely, clarit,coherence, evidence, and proofreading. This feedack was coded under the category of higher-order concerns and was labeled as "clarity, coherence, evidence, and prooreading." Overall, we coded ChatGP's comments into 1) lower-order concerns which included six error categories (i.e., definite articles, singular-plural nouns, subject-verb agreement, verb tense, spelling, and mechanics, including punctuation and capitalization), and 2) higher-order concerns which encompassed two error categories (i.., style-related recommendations and feedback on clarity, coherence, evidence, and proofreading) (see Appendix D.) During this proes, the authors coded the aforementioned eor categories individuall, compared their coding schemes, and resolved any disagreements. Additionally, the authors individually evaluated the accuracy of ChatGPT's suggestions, acknowledging that Al-powered tool can be prone to errors (Chapelle et al, 2015). If the comment was deemed accurate, it was labeled as "accurate" conversely, i i as n accurate, th labenccurate ws assned. feward, the athrs convnd  ompare thei allie or the accuracy of feedback instance, addressed any aresf diagrement through discssion, and adjusted the tallies as needed.Given the qualitative nature of the research, the authors conducted basic calculations, aimed at determining the count of eror categories, the quantity of feedack ech participant reeied the accuracy of the suggestions, and the umber faccetd and dismise suggetions.

The third phase, which took approximately two weeks, involved examining the participants' revision operations and strategies (Fig. 2). To identify revision operations, the authors individually analyzed the screencasts to check whether the participants imple. mented ChatGPT's suggestions. They recognized two revision operations: \*accept," which denoted cases where the participants corrected errors based on ChatGPT's suggestions, and "\*dismiss' which indicated instances where no response to ChatGPT's sugges. tions was observed. In this stage, the authors also consulted the participants' original texts and revised texts to examine whether ChatGPT's suggestions were implemented. This was particularly important for cases where the screencasts did not clearly demonstrate the employed revision operations. To identify revision strategies, each author documented the actions the participants made during revision. For instance, when a participant inserted ChatGT's suggestion into their text by copying and pasting, the authors documented his action in the corresponding column dedicated to revision strategies. Following this the authors reconvened to review their notes and tallies. During the meeting, they compared the revision strategies utilized by the six participants and identified two revision strategies: copying and pasting ChatGPT's suggestions and manually typing the suggestions into their text.

In the final phase, which lasted ess than an hour, the authors calculated both the time spent on revisions by each participant, using the length of the screencasts, and the word count of each research proposal.

# 3.3.2. Part two: Cognitive and affective engagement

To asses cognitive and affective engagement, an analysis was conducted on the participants SR and interview transcripts, s wel as survey responses. This part of the data analys encompassed four phases. In the firt phase, the audio recordings of the combined SR and interview sessions were transcribed using a paid online transcription program, Trint (ww.trint.com). These transcriptions were then cross-checked for accuracy against the original recordings.

In the second phase, thetranscripts were organized in a Google spreadsheet, with each participant's having their own tab. The data were structured as follows: the first column contained the interview questions, and the second column included the participant re sponses. The third and fourth columns captured insights intocognitive and affective engagement. The cognitive engagement column was further divided into four sub-columns: noticing, understanding, cognitive operations, and metacognitive operations. The affctive engagement column was divided into two sub-columns: immediate emotional reactions and attitudinal responses. The survey data, extracted from Google Forms, were similarly entered into the same spreadshet for each participant below the SR and interview data.

In the third phase, each author individuall examined the recorded data in the spreadsheet. As they reviewed each participant's responses, they made notes in the respective columns and sub-columns for cognitie and affective engagement. For example, in the cognitive engagement column, under the sub-column "noticing, the term "noticed" was recorded if the participant identified the error. It is important to note that all erors were noticed by the participants because they addressed them individuall, either by accepting or rejecting them In the \*understanding" sub-column, the authors noted "understood if the participants indicated in the SRs that they understood the error and how to addres it, or "did not understand" if they did not For the "cognitive operations" subcolumn, which refers to the mental processes involved in acquiring and processing information, any cognitive operations the participants mentioned during the SRs were noted. Metacognitive operations" which involve awareness and control over one's cognitive processes were also noted. For affective engagement, each author noted the participants' immediate emotional reactions upon receiving fedback mentioned during the SRs in the \*immediate emotional reactions" sub-column, and a similar approach was taken for overallattitudinal responses brought up in the SRs, interviews, and surveys in the "attitudinal responses" sub-column.

In the fourth phase, when the authors reconvened, they compared thir notes, engaged in discussions, and adjusted the final version of the notes. They then established a separate tab for cognitive engagement with its four sub-columns, grouping similar notes along with participants quotes into corresponding categories for the entire dataset. Overall it was noted that all participants noticed errors and understood the majorit of them, but at times did not understand ChatGPT's feedback on mechanics, as some of those comments were inaccurate. For cognitive operations, five main strategies were identified among all participants: 1) reading and re-reading the provided suggestions to ensure comprehensive understanding, 2) comparing the suggested paraphrasing, words, and phrases with their original writing to understand the alterations, 3) reasoning, 4) applying knowledge from other sources of feedback, and 5) applying discipline-specific and genre-specific knowledge. For metacognitive operations, three main strategies were utilized by the participants 1) addressing comments sequentially, 2) displaying both ChatGPT's responses and one's own text side-by-side, and 3) using the search box in Word to quickly locate the sentence flgged by ChatGPT. A similar tab was created for affective engagement for the entire dataset, grouping similar notes under two sub-columns. For the "immediate emotional reactions" sub-column, two major reactions were idntified: satisfaction and dissatsfaction with the feedack For overallattitudinal responses, a positive attitude was noted among allparticipants, except for a few perceived limitations. Representative quotes were then chosen to illstrate both cognitive and affective engagement.

# 4. Findings

# 4.1. Behavioral engagement

Behavioral engagement encompasses the revision operations and strategies used for text revision, as well as the total time spent on the revision proces. Table 2 displays the revision operations undertaken by the participants i response to ChatGPT's suggestions. In the table, the acceptance (A) and dismisal (D) tallies for each individual stand as ollows: Farah acceted 17 comments and dismissed 6, Farid acetd 11 and dismissed2, and Sara acceted 20 whiledismissing 4. Mild accepted6 and disssed 11, while Tara accepted 14 comments but dismissed 16. Amir accepted 16 comments but dismissed 12.

Table 2 shows that the participants primarily received feedback on style, and the majorit of such feedback was accepted. During the SR, Amir, for example, elaborated on the reasons behind his choice to accept feedback related to style:

I pplied the second comment by relacing "depending on where i exis and forms, we can cal it bad or good ozone" with ChatGPT's suggestion ozone ca be classfied as ether beficil r hul deending on whei exsts and foms. It atuall es make it more clear to the audience or the reader who reads the paper. It makes it more professional.

During the SRs, the participants who rected comments on style consistently mentioned a pattern. They explained that they either found ChatGPTs word choic irrelevant to their feld of study or simply preferred their own wording over the suggestions. This reveals their inclination to rely on their own judgment and preferences in such instances. Here is an example from Farah's SR, llustrating her response to the rejection of a style-related suggestion:

Inthe third sentence, used subsequently" and it says changed to s a result. " But I ddnt lik it because I used "as result" more than ten times in this draft.

The suggestions that received the highest number of dismissals were primarily linked to mechanics, specifically punctuation and capitalization. The participants dismisse ChatGPT's comments on mechanics because they either already had the appropriat commas and capitalizations i place or deemed the suggested commas as unnecessary. For example, Tara dismissed a comma in the following instance because it was irrelevant (Table 3):

Table 2 also indicates that Farah was the sole participant to receive feedback on global aspects of writing, specifically on clarity, flow, evidence, and prooreading Farah, however, dismissed these comments, explaining that they were not rlevant to the abstract of her research proposal. She expressed that her abstract was already concise, and that she had no intention of adding any extra infor. mation to it.

Table 2 Participants' revision operations in response to ChatGPT's suggestions.   

<html><body><table><tr><td colspan="2"></td><td colspan="2">Farah</td><td colspan="2">Farid</td><td colspan="2">Milad</td><td colspan="2">Sara</td><td colspan="2">Tara</td><td colspan="2">Amir</td></tr><tr><td></td><td></td><td>A</td><td>D</td><td>A</td><td>D</td><td>A</td><td>D</td><td>A</td><td>D</td><td>A D</td><td></td><td>A</td><td>D</td></tr><tr><td rowspan="3">LOCs</td><td>Determiners (articles) Singular-plural</td><td>1a</td><td></td><td></td><td></td><td></td><td></td><td>8a</td><td></td><td>2a</td><td></td><td></td><td></td></tr><tr><td>Subject-verb agreement</td><td></td><td>2ab</td><td>1a</td><td></td><td></td><td>1b 4b</td><td>2a 2ab</td><td>1a</td><td>1a</td><td></td><td></td><td></td></tr><tr><td>Verb Tense</td><td></td><td></td><td></td><td></td><td></td><td>1b</td><td>1a</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="8">HOCs</td><td>Spelling Mechanics (punctuation &amp; capitalization)</td><td>1a</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Style (neither a or b)</td><td>2a 13</td><td>1b 2</td><td>1a</td><td>1a 1</td><td>1a</td><td>2b</td><td>6a</td><td>2b</td><td></td><td>9b</td><td>2b</td><td>7b</td></tr><tr><td>Clarity, Flow, Evidence, &amp; Proofreading</td><td>1</td><td></td><td>9</td><td></td><td>5</td><td>3</td><td>1</td><td>1</td><td>14 3</td><td></td><td>14</td><td>5</td></tr><tr><td>(neither a or b)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Total</td><td>17</td><td>6</td><td>11 13.</td><td>2</td><td>6</td><td>11</td><td>20</td><td>4</td><td>14</td><td>15</td><td>16</td><td>12</td></tr><tr><td>Overall Total</td><td>23. 1296</td><td></td><td>1505</td><td></td><td>17. 872</td><td></td><td>24. 971</td><td></td><td>29. 959</td><td></td><td>28. 2139</td><td></td></tr><tr><td>Word Count Time spent on revision</td><td>41 mins. 15 s</td><td></td><td></td><td>43 mins. 33 s</td><td>26 mins.</td><td></td><td>33 mins.</td><td></td><td>1 h</td><td></td><td>43 mins. 48 s</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>56 s</td><td></td><td>8 s</td><td></td><td>21 mins. 12 s</td><td></td><td></td><td></td></tr></table></body></html>

Note: A - accept; D - dismiss; a - accurate; b - inaccurate LOCs - lower-order concerns HOCs - higher -order concerns

Table 3 Tara's comment regarding a dismissed comma.   

<html><body><table><tr><td>Video Time Stamp</td><td>Tara&#x27;s Original Text</td><td>ChatGPT&#x27;s comment</td><td>Tara&#x27;s SR</td></tr><tr><td>4:49</td><td>It is unfortunate that despite the existence of a water supply crisis in the country and the importance of operating desalination plants on a commercial scale, little research has been done in this field in our country.</td><td>In the third sentence of the abstract, consider adding a comma after &quot;country&quot; to improve the flow of the sentence.</td><td>I think it doesn&#x27;t need a comma here. It does not make sense to add one.</td></tr></table></body></html>

It i noteworthy that out f the 62 comments provided by ChatGPT on local aspects of writing to all participants, 31 were accurate, with 25 being accepted and 6 dismissed, while 31 were inaccurat, with only 1 being accepted and 30dismissed. This sugests that half of the ChatGPT-generated feedback was inaccurate. Nevertheless the participants' abilit todiscen this inaccuracy is demonstrated by their revision operations, which included the dismissal of such fedback. Moreover, out of the 71 styl-related comments, 56 were accepted while 15 were dismissed. Additionally, the single comment addressing the globalapects of writing was ismissed. Overal the participants collectively received 134 comments from ChatGPT, with 87 being accepted and 47 being dismissed.

Regarding revision strategies, the participants showed a clear preference for manually typing the suggested corrctions from ChatGPT, especially when addressng particular words or phrases. In contrast, when it came to incorporating ChatGPT's paraphrasing suggestions for parts or entirety of sentences, the participants ofte favored copying and pasting the suggestions directly into their text. Fig. 3illustrates Tara's paper, featuring a sentence in which she employed ChatGPT's suggestion by copying and pasting it. None of the participants sought additional fedback from source ike dictionarie r other individualsto improve their writig. This suggests that ChatGPT was deemed sufficient as the sole feedback source.

In terms of the time spent on revisions, Tara invested the most time, devoting one hour and $2 1 \mathrm { m i n }$ , which aligns with her receiving the highest number of comments (29). Conversely, Milad spent approximately $2 7 \mathrm { { m i n } }$ the shortest duration among the participants (Table 2). These extended revision periods were probably motivated by the participants' desire to improve their writing, as their research proposals were intended for thir supervisors. Additionall, some participants took longer times when locating the specific sentences to which ChatGPT's comments referred.

# 4.2. Cognitive engagement

Cognitive engagement refers to the extent of participants' cognitive interaction with ChatGPT's suggestions, including their ability to notice and understand erors use cognitive operations to evaluate suggestions and make informed revision decisions, and employ metacognitie operations to regulate their mental fforts. Regarding noticing and comprehending errors in their writing, the participants noticed all therrors as they adressed them individually and undersood how t rectify the majority of the flagged errors For example, during the SR, Farah showcased her abilit to notice and understand a peling eror that ChatGPT had pointed out, as shown in (Table 4):

In addition to recognizing and comprehending errors, the participants identified and understood inaccuracies in ChatGPT's grammar suggestions. For example, Tara received feedback that inaccurately suggested changing a singular verb into a plural. She recognized this inaccuracy and dismissed the suggestion. In her SR, she stated the following (Table 5):

The participants also observed inaccuracies in ChatGPT's suggestions concerning mechanics, specifically with commas that were either already appropriately placed or not necessary in the given context These inaccuracies created a minor challenge to the par ticipants, as they intily stggled toundrstand the rror untilthey relizd th feack was incorrect. An illsrtionof this ca b seen during Amir's SR (Table 6):

Regarding cognitive operations, the participants employed various approaches to analyze ChatGPT's suggestions and decide on the appropriate revisions. One such strategy that allparticipants use involved carefull reading and re-reading the provided suggestions to ensure a comprehensive understanding. Tara's comment during the SR exemplifies this process:

a water supply crisis in the country and the importance of operating desalination plants on a commercial scale, little research has been done in this field in our country. -Thus, this research is one of the first pioneering projects to address this issue. Therefore, this research is one of the pioneering research projects in this field. In this study, the appropriate solar desalination systems for agricultural purposes were selected through information gathered from experts and commentators. Then, the effective factors and indices for commercializing this technology were determined. Finally, these factors were assessed and graded in terms of their importance for commercializing solar desalination and the methods for commercializing solar desalination were prioritized using a developed questionnaire and the AHP technique through analysis of the gathered data in

Fig. 3. Tara's text with an integrated ChatGPT's suggestion on revision.

Table 4 Farah's comment on noticing and understanding of the error.   

<html><body><table><tr><td>Video Time Stamp</td><td>Farah&#x27;s Original Text</td><td>ChatGPT&#x27;s comment</td><td>Farah&#x27;s SR</td></tr><tr><td>25:15</td><td>Cracks are dangerous for concrete durability because they provide outside chemicals with a direct patch to rebars and concrete matrix.</td><td>In the third paragraph, &quot;patch&#x27; should be &quot;path.&quot;</td><td>(Reads the comment) I mistakenly typed &quot;past&#x27; as &quot;patch,&quot; and the system detected it. And I accepted the correction.</td></tr></table></body></html>

Table 5 Tara's Comment on Recognizing the Incorrect Feedback.   

<html><body><table><tr><td>Video Time Stamp</td><td>Tara&#x27;s Original Text</td><td>ChatGPT&#x27;s comment</td><td>Tara&#x27;s SR</td></tr><tr><td>2:39</td><td>The crisis of water and energy is a crucial issue in Iran.</td><td>In the first sentence of the abstract, consider using &quot;are crucial instead of &quot;is a crucial.&quot; This is because &quot;water and energy&quot; are two separate issues that are both important.</td><td>It told me to change &quot;is&#x27; to &quot;are, but I knew that the word &quot;crisis&quot; is not plural, so I ignored the suggestion and didn&#x27;t make the change.</td></tr></table></body></html>

Table 6 Amir's comment on confusion over incorrect feedback.   

<html><body><table><tr><td>Video Time Stamp</td><td>Amir&#x27;s Original Text</td><td>ChatGPT&#x27;s comment</td><td>Amir&#x27;s SR</td></tr><tr><td>5:20</td><td>Ozone, an odorless, colorless, and short- lived gas, is a natural part of the atmosphere.</td><td>In your introduction, consider adding a comma after &quot;colorless&quot; for better flow.</td><td>I used a comma and it again asked me to include a comma. Why? So, sometimes it failed to make good suggestions.</td></tr></table></body></html>

I was reading and thinking, thinking, and thinking about it sentences or sugestions to be sure if I was right or ChatGPT was better or more accurate. And only then I made some changes in my writing.

Another trategy employed by all participants was to compare the suggested paraphrasing, words, and phrases with their original writing to gain a beter understanding of the alterations that were made. Farah's comment during the SR corroborates this finding:

I was reading my original and reading the corrected version together to understand what changed.

All participants also engaged in reasoning to make an informed decision about whether to accept or reject the feedback. Sara's comment exemplifies this strategy:

I am thinking that ChatGPT's sentence sounds more professional to me than mine, so I accepting the correction.

Two participants, Farah and Milad, also applied knowledge from previous comments provided by alternative feedback sources, such as teachers or AWE. For example, Farah recalled her advisor giving asimilar suggestion to ChatGPT's, so she decided to accept ChatGPT's suggestion (Table 7):

Similarly, in the SR, Milad noted that in his previous revisions using Grammarly, the tool would occasionally suggest adding an Oxford comma. Therefore, when ChatGPT recommended adding the Oxford comma, Milad recognized the need to include one:

Because we always use a comma before 'and.' This is actually something Grammarly suggests to me sometimes.

Finally, two participants, Farid and Milad, leveraged their domain knowledge to select appropriate revision operations. They criticall asessed the ChatGPT-generated feedback, making informed decisions based on their comprehension of the subject matter and their fields language conventions. Consequently, they sometimes reected ChatGPT's suggestions and retained their own domain

Table 7 Farah's comment on accepting feedback based on similar advice from her advisor.   

<html><body><table><tr><td>Video Time Stamp</td><td>Farah&#x27;s Original Text.</td><td>ChatGPT&#x27;s comment</td><td>Farah&#x27;s SR</td></tr><tr><td>34:18</td><td>The -XRF Chloride map shows that a high amount of chloride is penetrated through the larger crack; however, near the tighter crack, there is less. chloride penetration.</td><td>In the eighth sentence, change &quot;u-XRF Chloride map&quot; to &quot;u-XRF chloride map&quot; for consistency in capitalization.</td><td>It says that I shouldn&#x27;t use Chloride map&#x27; with a capitalized &#x27;C.&#x27; I always thought I should capitalize the &#x27;C&#x27;in Chloride. But my advisor had previously advised me not to do that, and ChatGPT found it as well.</td></tr></table></body></html>

specific language. Farid's comment exemplifies this approach:

I chose to keep the original vrsion as Ibelie implement is more commonly used in this context. It's not that ChatGPT's sugestion is wrong, but I consider my word choice to be more suitable.

Regarding metacognitie operations, the participants SRs indicated that they al approached the revision processin a similar manner: they read every comment as they were presented. By employing this metacognitive strategy, the participants were able to carefully consider each feedback point, aiming for more substantial improvements in their proposals.

Another metacognitive operation used by the majority of the participants involved having both ChatGPT's responses and their own text displayed side-by-side on a scren. This rrangement alowed them to convenientlycompare the suggestions with thir original text and make necessary changes (Fig. 4).

Lastly, ChatGPT's suggestions often referred to specific sentences using phrases like \*in sentence one" or \*in sentence two," or sometimes provided general references like \*in your introduction." Since the proposals contained many sentences, it became chal. lenging and time-consuming for the participants t ocate the exact sentence ChatGPT referred to. To expedite the proces, Amir used the "search'" icon on the left side of the Word document to enter keywords to locate the sentences needing correction (Fig. 5):

# 4.3. Affective engagement

Affective engagement encompases the participants emotional reactions trigered by ChatGPT's suggestions, as well as the general attitudes they held towards ChatGPT. Two immediate emotional reactions were identified: satisfaction and dissatisfaction with the feedback received. Concerning satisfaction with ChatGPr's suggestions, allparticipants expressed strong satisfaction with the sug. gestions on style, particularly paraphrasing, finding them helpful in making their writing more "professional, clear," and "native. like." For example, during the interview, Tara mentioned the following:

I think the way ChatGPT phrased it is more native-like or in an American style. The translation of my sentence from Persian to English was word-for-word, but ChatGPT's suggestion feels more natural and native-like.

Notably, Tara did not engage in any Persian-to-English translation in her writing. Rather, she intended to convey that her English appeared to resemble a translation from Persian to English, as opposed to a more native-like writing style. Therefore, she found ChatGPT's suggestions to be more native-sounding, which she appreciated and favored.

Additionally, the participants expressed satisfaction with the suggestions on word choice, as some of them learned new words during this process. For example, Farid shared his enthusiasm, saying this:

I was excited whenI saw ChatGPT's comment I had never en come acrossthe phrase \*thermal behavior" that ChatGPT sugested. The great thing about this comment isthat ChatGPT identified that the entire paragraph discusses the thermal properties of concrete."

Furthermore, Farah emphasized the value in ChatGPT's suggestions on transition words. She acknowledged the challenges many tudents face when transitioning among ideas and appreciated ChatGPT's feedback on this particular area.

Regarding dissatisfaction with the received feedback all participants expressed discontent with some suggestions on mechanics.

![](img/410e024cf269eed07b1a37447688b517b9f084e3e70652d05cee553299f8124c.jpg)  
Fig. 4. Farah's computer screen during text revision.

![](img/49a51de663ed2989d32e888247b7ffa8aebf9dd19bb65b29309a224df7f75780.jpg)  
Fig. 5. Amir's approach to locating sentences flagged by ChatGPT.

They found some of those suggestions \*useless" and "redundant' because either the sugested commas and capitalizations were already present or they were unnecessary for that particular sentence Farah and Farid also added that certain word choice suggestions were not helpful as they consisted of unspecifi terminology not commonly used in their respective fields. Therefore, they preferred to adhere to their own word choices that were relevant to the context. Moreover, Milad found ChatGPT's word suggestions unnecessarily sophisticated and opted to use simpler language in his writing. Tara expressed her frustration with ChatGPT's use of sentence numbers like \*in the seventh sentence" or \*in the second sentence" as locating these exact sentences in her text was time-consuming and difficult. Insted, Tara stated that he would prefer ChatGPT to provide fullsentences and point out the specific ssues instead of using numbered sentences.

As for overal atitudes toward ChatGPT, the participants generally expressed positive feelings and were impresed with ChatGPT as a writing assistance tool. For instance, Milad expressed his sense of awe by stating the following:

I'm really happy that I am living in the cutting-edge era, and I am living in the ChatGPT era. It's amazing.

The participants used terms like "user-friendly, fre," "powerful, \*amazing," and innovative," among other positiveattributes, to describe ChatGPT. The participants believed that ChatGPT sigificantly improved the quality of their writen work, lending it a more \*professional\* touch. Some rated ChatGPT between 8 to 9 out of 10 for it usefulness and compared it to Grammarly, acknowledging that it surpasses Grammarly in providing useful suggestions for paraphrasing, reconsidering word choice, improving clarity, brainstorming, and changing writing style. They also noted that, unlike Grammarly, ChatGPT is versatile and can assist with tasks like Python scripting, offering cooking recipes, and generating interview questions-functions they noticed while experimenting with ChatGPT during the workshop. All participants expresed their intentionto continue utilizing ChatGPT for writing, anticipating that it will continue to advance with time. They specificlly highlighted it value for non-native English speakers who face challenges when writing in English. Milad expressed this sentiment, saying this:

I love ChatGPT because, as you know, composing a paper as a foreign student can be overwhelming. You have to check grammar, vocabulary, tites, conclusions, and many other things. If I had known about ChatGPT in the past it would have ben a tremendous help for me in writing my papers.

Despite their positie attitudes, the participants also recognized certain limitations in ChatGT, such as occasional inaccuracies in suggestions on mechanics, and emphasized the need to be critical of is feedback. Farah noted that although ChatGPT was helpful, it was essential to remain aware of its nature as a \*robot" and validate it suggestions regularly. The participants also highlighted the importance of providing clear prompts to obtain more accurate feedback. In addition, Farah expressed securit-related concerns regarding the privacy of the text she inputs into the tol, wondering what happens with her textual data. Milad also mentioned that ChatGPT's limitation is is inability to handle tables, which was inconvenient for him when working on a paper that included tables. Amir expressed broader concerns about ChatGPT and is implications for the future. While he was highly impressed with ChatGPT, Amir also fared that it could become dangerous tool, potntiall capable of replacing him. Hearticulated these concens, saying this.

I think sometimes it could replace me, take my job and write papers and do research instead of me.

Regarding the evolving attitudes towards ChatGPT over time due to extended usage, the overall sentiments seemed to remain largely positive. As shown by the open-ended survey findings, the participants continued to actively employ the ool for paraphrasing and grammar correction, as indicated by the prompts they used, such as "paraphrase," "check grammar," "rewrite," "polish," and "improve. They all xpressed appreciation for ChatGPT's abilit to improve ther sentence flow, grammar, and paraphrasing, therey elevating the professionalism, native-likeness and efficiency of their writing. For example, Farid compared ChatGPT to a "writing buddy" that improves writing quality, saves time during editing, and aids him in overcoming writer's block.

Although the participants expressed a highly positive view of ChatGPT as an AI tool, they also highlighted several ethical con. siderations and limitations, offering implications for both users and tool developers. All partipants stessed the importance of using ChatGPT responsibly and maintaining ethical writing practices. They cautioned against employing the tool for academic tasks like paper and assinment writing or coding, as this could be considered plagiarism and might not promote the development of students writing skill. For example, Milad expressed the belief that students should independently compose their texts and use ChatGPT for revising their text. Similarl, Sara suggested that students should use ChatGPT to check their writing and generate ideas rather than relying on it to generate entire pieces of writing, thus emphasizing the value of preserving one's voice in writing. Tara also believed that citing ChatGPT use was unnecessary for rephrasing but essential when seeking specic information. Amir noted that due o the AI generated nature of ChatGPT's paraphrases and suggestions, the term \*plagiarism" could not be applicable. He advocated for the introduction of a fresh label to describe paraphrasing generated by AI tools.

Generall, the participants proposed several suggestions to enhance ChatGPT, with a primary focus on improving corrections on lower-order concerns. Farid also proposed the addition of a feature that can detect and alert users about potential plagiarism, demonstrating his awareness of thical considerations and desire to promote responsible writing practices. Sara suggested enabling the tool to support other formats like PDFs, granting users more flexibility and convenience in making changes to their papers. Amir identified an issue in ChatG, wherein it occasionally incororated informationfrom a previous question into its resonse, even when his current query was unrelated to the previous one. He referred to this as a bug" and recommended improving this aspect of the feature. Overall the participants maintained a positive outlook on ChatGPT as a powerful witing support tool but also advocated for responsible and thoughtful usage.

# 5. Discussion and conclusion

Grounded in the tripartite conceptual framework of learner engagement (Han & Hyland, 2015; Zhang & Hyland, 2018), the present study revealed the complexity of six Iranian students' engagement with ChatGPT-generated feedback while revising their research proposals and provided new insights into how these graduate students engaged with this fedback behaviorall, cognitively, and affectively. To our best knowledge, the present study was the first attempt to provide empiricl evidence on learners individualized patterns of engagement with ChatGPT-generated feedback. Our research addressed the need to explore the eficacy of ChatGPT as a tool which can foster learner engagement (Yang & Li, 2024). The findings pointed to the dynamic nature of learner engagement and showed that the participants primarily used ChatGPT to addres their perceived needs in L2 writing, thereby using the tool in a personalizd fashion (Wang & Wang, 2024). Behavioraly, the participants accepted most of ChatGPTs suggestions, particularly those related to style, such as word choice and paraphrasing, to improve their papers. They reected sugestions that were inaccurate, especialy those related to mechanics such as punctuation, demonstrating their critical thinking by recognizing these inaccuracies. They also chose not to consult external sources for improving their papers. This could be attibuted to the participants primary focus on addressing minor isues in their writing, and the feedack they obtained from ChatGPT may have bee sufficiently satisfactory to deter them from seeking guidance from other sources. On average, the participants spent about $4 0 \mathrm { { m i n } }$ revising their research proposals. Whil the time spent may indicat high level of engagement during the revision process it is notworth that the participants found it challenging to locate some sentences flagged by ChatGPT, thus adding extra time to their revisions.

Cognitively, the participants demonstrated the ability to notice and understand most of the suggestions, although occasional confusion arose from inaccurate feedback, which the participants recognized and rejected. Overall, the participants showed a relatively high level of cognitive engagement, as evidenced by their use of cognitive and metacognitive operations. All participants employed cognitive strategies, such as carefully reading and re-reading the sugestions, comparing the suggested corrections to thir original sentences, using reasoning, and drawing upon prior knowledge from other fedback sources as wel as knowledge of subjectspecific and language-related norms. The identified metacognitive strategies included addressing comments individually, placing proposals and ChatGPT's suggestions side-by-side on acomputer screen to locate and addresscomments more eficientl, and using the search box in Word to quickly locat sentences flagged by ChatGPT. Our results paint a diffrent picture than that i Lo et al. (2024). In fact, while our analysis reveals high levels of cognitive engagement, as evidenced by the participants' employing various cognitive and metacognitive strategies, Lo et al.'s review lists lack of critical thinking (e.g., by simply copying and pasting ChatGPT-generated feedback into one's writing) as a pitfall in utilizing ChatGPT among L2 learners.

Affectively, the participants' immediate reactions upon receiving feedback were primarily positie, although they experienced some negative feeings. The participants were mostly satisfied with the feedback, especially regarding word choice and paraphrasing, noting that such feedback made their writing more professional and native-like. Contrastively, negative reactions ocurred when the participants ecountered occasionall naccurate fedback (Steiss et al., 2024), particularly on punctuation and capitalization. They were puzzled by this fedback, as they had already correctl applied these punctuation marks and capitalizations. Overal, the participants reported positive attitudes toward ChatGPT and were impressed with it. Additionall, after weeks of use, they maintained similar levels of satisfaction, though they brought up some caution regarding ethical considerations. Our results add to the limited literature on L2 leaners' varying levels f enthusiasm about using ChatGPT (Lo et al., 2024), some of whommight stil prefer in-person interactions with and feedback from teachers/tutors (Escalante et al., 2023). Although enthusiasm among our participants, as first-time user of ChatGPT for writing, can be ascribed to this tool's novelty, as also reported by Uddin et al. (2023), our participating learners reported high levels of interes in using ChatGPT for writing-related purposes, even afer three wees of independent use.

Our study builds upon and contributes to previous research which has utilized a learner engagement framework to investigate the ways in which students use AWE and AI-generated feedback (e.g., Koltovskaia, 2020; Saeli et al., 2023; Zhang, 2017; 2020). In doing so, we examined the engagement patterns of six ESL graduate students who interacted with a new AlI-powered chatbot tool, ChatGPT. As a result, we presented novel empirical evidence on how ESL students may employ ChatGPT for the purpose of revising research proposals-a common graduate-level genre. Consistent with the existing research on AI-powered tools (e.g., Zhang, 2024), which demonstrte that students generall have a positive view of automated feedback despite its occasional inaccuracies, our study also highlights that the participants, on the whole, held favorable opinions about ChatGT and considered it a valuable source of fedback. Additionall, the present study shows that our participants, students with upper-intermediate and advanced proficiencies, were capable f criticall ealuating the feedback, thereby identifying and disregarding inaccuracies. This finding aligns with Koltovskaia (2020) who found that advanced language proficiency can enable students to critically ases automated feedack, thus investing more time in its evaluation and carefully incorporating it into their work.

Moreover, our findings indicate that the participants employed ChatGPT in a responsible manner, utilizing it exclusively for the purpose of revising their work rather than using it as a tool for generating text. This trend persisted over time, with the participants using ChatGPT for rephrasing their sentences to achieve a more "native-like and \*professonal quality. Unlike concerns expressed by some researchers about the potential for students to misuse ChatGPT for composing their entire texts (.g., Cotton et al., 2023; Kohnke et al., 2023; Lo et al., 2024), our study did not yield evidence supporting this notion. This might potentially tem from th fct that our participants were graduate students. As we noticed, each participant posssed their individual writing styles, was well-versed in crafting papers within their respective felds which demanded specalized vocabulary, and demonstrated a grasp of ethical writing and responsible use of AI tools. Although ChatGPT-generated feedback was readily available to them (Shi & Aryadoust, 2024; Xiao & Zhi 2023; Zhang & Tur, 2024), i ppeared that the participants employed ChatGPT responsibly, viewing it solely as a tol to improve the quality of their texts.

Overall, the participants exhibited an understanding of the tool's ethical implications. They highlighted the importance of creating their own content independentl, viewing ChatGPT as a\*writing buddy" or supplementary tool for addressing grammatical isues and paraphrasing. This tendency could be linked to the workshop provided to participants before they began using ChatGPT for text revision. This workshop not only encompassed the mechanics of utilizing ChatGPT but also delved into the ethical aspect associated with its use. This suggests that meaningful training (e.g., workhops) could potentially lead students to employ the tool in a more ethical manner. This aligns with the perspectives of Godwin-Jones (2022) and Kohnke et al. (2023) who emphasize the importance of providing training in ChatGPT usage. These researchers propose that such training is integral to digital literacy, and that students should acquire an understanding of the too's capabilities and limitations to optimize its benefits and mitigate its adverse outcomes.

In addition to providing training tostudents on ChatGPT's capabilities and limitations and addressing ethical considerations when using it (Barrot, 2023; Su et a., 2023), we agree with Wang and Wang (2024), in that learner engagement with ChatGPT is highly individualized. Therefore, we propose several strategies to enhance behavioral, cognitive, and affective engagement with ChatGPT in L2 classrooms:

1. For optimal behavioral engagement, students should be provided with examples of effective prompts that yield the best outcomes and suggestions.   
2. Concerning cognitive engagement, students should be mindful that ChatGPT-generated feedback might lack accuracy, and that they should criticall asses te feedback ffered by ChatGPT rather than acceting it unequivocally (Koltovskaia, 2020; u et al., 2023).   
3. In terms of affctive engagement, to prevent students from feeling disheartened, because of assuming that fedack from ChatGPT might be incorrect, and developing a sense of distrust in this tol, it becomes essential to underscore the educational potentials embedded in encountering inaccurate feedback. Such experiences can, in turn, serve as catalysts for nurtring critical thinking abilitie, i.e., cognitive engagement. Additionall, i i crucial to highlight the supplementary nature of ChatGPT's role in the writing proces. Students should view ChatGPT as a "writing buddy," as mentioned by one participant, available to offer assistance whenever they require support. This approach can foster a positive outlook towards the tool.

Our study is not devoid of limitations. Our research was conducted outside the L2 writing classoom context, and thus, future studies should be classoom-oriented. Such studies could yield intriguing insights as students' assgnments would beevaluated by ther instructors, and the presence of grading incentives might influence their revision behaviors, potentiall leading to different interactions with ChatGPT. To enhance the scope and applicability of this line of research, future investigations should consider diversfying the participant pool. This could encompas students at various edcational levels, within different contextual settings, and across diverse writing genres. By doing so, a more comprehensive understanding of how students from varied backgrounds engage with and benefit from AI toos like ChatGPT could be gained. Lastly, more longitudinal engagement studies can trace how students interact with AI fedback over time. Such research could provide more profound insights into the strategies students use to address ChatGPT-generated feedback on their writing. These studies can also explore how students utilize these tols in earlier and/or later stages of writing.

# Ethics approval

This study was performed i line with the principles of the Declaration of Helsinki. Approval was granted by The Oklahoma State Universty Institutional Review Board (IRB) (02/22/2023/ Application Number: IRB-23-83). Application Number: IRB-23-83. PI: Payam Rahmati, Review Level: Exempt.

# Consent to participate

Informed consent was obtained from all individual participants included in the study.

# Funding

No funding was received to assist with the preparation of this manuscript.

# CRediT authorship contribution statement

Svetlana Koltovskaia: Writing - review & editing, Writing - original draft, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Hooman Saeli: Writing - review & editing, Writing  original draft, Methodology, Formal analysis, Data curation, Conceptualization. Payam Rahmati: Writing - review & editing, Writing - original draft, Methodology, Formal analysis, Data curation, Conceptualization.

# Declaration of Competing Interest

The authors have no relevant financial or non-financial interests to disclose.

# Data availability

The data that has been used is confidential.

# Appendix A. Supporting information

Supplementary data associated with this article can be found in the online version at doi:10.1016/j jslw.2024.101130.

# References

Barrt, J.. 2023. i  for od ana witing ls and nil.sin ing 57 ile 100745. hp/i.or0.1016/. asw.2023.100745   
ishop L 2023). tr  his par Wht Ch m or io, h, d wtin  117. /g/102139/43898   
Casl Je, ih  frh t a Methods in Applied Linguistics, 2(3), Article 100068. https://doi.org/10.1016/j.rmal.2023.100068   
Chaell,  o, ,  . 015.t mns or tic  g d win ti. e esi (3, 5-405. https://doi.org/10.1177/02655322145653   
Che .   t ne. Language Learning & Technology, 12(2), 94-112. https://doi.org/10125/44145.   
Cotton, D on,   hiay, J. 03.ting an chig n c intit  the ef h.is i ion and Teaching International, 1-12. https://doi.org/10.1080/14703297.2023.2190148 org/stable/44488131).   
e P     i Technology in Higher Education, 20, 57. https://doi.org/10.1186/s41239-023-00425-2   
Gas S. M. Mackey, A (2000. tuted rell mthodgy in scod angge rerch.Mwah, NJ Lawence Eam Assciates, Publishrs, org/10125/73474.   
Guo, K  Wag, . (2023).  st i  to brc it n Chs tl o urt tcr eak i  wtig. ion nd nomaion Technologies. https://doi.org/10.1007/s10639-023-12146-0   
Han, . (217n  e mde  e ad r   w rie  9 13-142 ht/o. 10.1016/j.system.2017.07.003   
Hn 5. tie i 30(4)), 31-44. https://doi.org/10.1016/j.jslw.2015.08.002   
Jarrah,  Wt,    . 2023). i  n a wtin i ( f of plgris d t te ain Jo Communication and Media Technologies, 13(4), Article e202346. https:/doi.org/10.30935/ojcmt/13572   
Kohnke, L, Moorhse,  L, & Zou,  (2023). ChatGT for languag tchng and leng, 368822311628 RELC Jounal. htp://doi.rg/10.1177 00336882231162868.   
olka    ti  4 Article 100450. https://doi.org/10.1016/j.asw.2020.100450   
Kolkai . 2).t t h r-d r  rrtie fakc dh L l, 18 (2), 286-315. https://doi.org/10.29140/jaltcall.v18n2.519   
Li Z. (2021). hr   wtinin ( t-e wi  Pto mio, d  t 9 1-14 https://doi.org/10.1016/j.system.2021.102505   
Lo, C.K, Hw, K F, & Jong, M.. Y. 2024). The ilece f ChatGT on studet engagment A ystematic review and fur rerch agda. uers Education, Article 105100.   
OpenAI (2023). ChatGPT (August 3 version) [Large Language Model].https:/chat.openai.com.   
Pack   3) ii   s l. https://doi.org/10.1002/tesq.3253   
Pack,  e,  3)   i rts  a ie  ishh Technology, 23(2), 4-24. https://doi.org/10.56297/BUKA4060/VRRO1747   
Ranali, , Lin , hkh-ane, 2017.tig in for fotie at of on aa witin stg the accuracy and usefulness of fedback as part of argument-based validation. Educational Pychology, 37(1), 8-25. htps://doi.og/10.1080/ 01443410.2015.1136407   
Saei  ,    i   f to Writing, 9(3). (https://scholarsarchive.byu.edu/journalrw/vol9/iss2/1).   
Shi, H., & Aryadoust, V. (2024). A systematic review of AI-based automated written feedback research. ReCALL, 36(2), 187-209.   
Steis, J, Tat , G, C , rt, , W , o,. 2024).  the qt  n n fa f st wting Learning and Instruction, 91, Article 101894.   
Su . i  3.   tie  l 075.101 asw.2023.100752   
Tani   3t   e 14 https://doi.org/10.1038/s41433-023-02678-7   
Tin     k  x 144st 91. https://doi.org/10.1016/j.system.2020.102247.   
d, . Jrt,  i,  h,  2023).  Ch to i d ion d p y  an trnn. Sustainability, 15(9), 7121. https://doi.org/10.3390/su15097121   
Wang, X, & Wang, . (2024). Expig h  r eet wt e ag mod: self-dtin thry pertie ng and Motivation, 87, Article 102014.   
Weigle, .C. (2013. ngih s ad  wing and atd y atin. M h, J. s ds.) Hoo f  y evaluation: Current applications and new directions (pp. 36-54). New York: Routledge.   
Xio, ., & hi,  2023). n x ud f  r e f C fr lag g s: Eriee d to. es 3, 2.   
Yng  2024).C f2  a ct t 124A103351//.0101t0241035   
Yu, .      01       er t Evaluation in Higher Education, 44(1), 50-65. https://doi.org/10.1080/02602938.2018.1467879 for Academic Purposes, 64, Article 101255. https://doi.org/10.1016/j.jeap.2023.101255   
Zhang, P., & Tur, G. (2024). A systematic review of ChatGPT use in K-12 education. European Jounal of Education, 59(2), Article e12599.   
Zhang,2020. wh wigio  k  wtg  ptd si.in 3tie 100439. https://doi.org/10.1016/j.asw.2019.100439   
Zhang, . (2017). tnt et wt ompterd feak:  e sudy.  Jol, 71(3) 317-328. htp://i.g/0.1093/t/c089   
hag        1 0.1.1016 j.asw.2021.100586   
Zhang,, d, 018tt e wt tcr a d fckn2 wtin  g 36, 90-10. /./0.106/. asw.2018.02.004   
hang  0h i y  t ha Journal of Multilingual and Multicultural Development, 1-14. https:/doi.org/10.1080/01434632.2022.2102175.   
ang, 2024   in   i win.     e, 6, 1153-1160.

e s  sh     thy tst    Ph. Appied sic rtt  in r it.  h  rstangae learning, L2 writing, and L2 assessment.

Paya Ri h s t  sn te .   t pronunciation.

Homan  e t s  tit   st a has published and presented on writte corctive fdback, oral corrctive fdback, Persian sociolinguistics, and Persian pragmatics.