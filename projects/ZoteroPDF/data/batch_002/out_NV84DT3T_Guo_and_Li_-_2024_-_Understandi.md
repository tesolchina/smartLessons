# Understanding EFL students' use of self-made AI chatbots as personalized writing assistance tools: A mixed methods study

Kai Guoa, Danling Lib,

a The University of Hong Kong, Hong Kong, China b The Chinese University of Hong Kong, Shenzhen, Guangdong, China

# ARTICLEINFO

# ABSTRACT

Keywords:   
Artificial intelligence   
Chatbots   
Retrieval augmented generation   
EFL writing   
Personalized learning

This study aimed to explore English as a foreign language (EFL) students' use of self-made retrieval augmented generation (RAG) chatbots to enhance their learning to write. In the study, 69 Chinese undergraduate students participated in a workshop focused on creating chatbots, using Poe, that can assist with their writing processes. Multiple data sources were collected, including chatbots built by students, essays students wrote using their chatbots, students' responses to pre- and post-workshop questionnaires, and written reflections. The findings revealed that students developed chatbots for various purposes, such as assisting with idea generation, producing writing outlines, and identifying grammatical and spelling errors. Students made various requests, including assistance, customization, and translation, during their interactions with chatbots. Moreover, the use of self-made chatbots had a positive impact on students' writing motivation. It resulted in clearer writing goals, increased writing confidence, reinforced writing beliefs, and a more positive attitude towards writing. This study contributes to a deeper understanding of chatbots as pedagogical tools that enhance personalized language learning for students. By leveraging self-made chatbots, students can receive tailored support for their specific writing needs, leading to improved motivation.

# 1. Introduction

Personalized learning is an educational approach that tailors instruction and learning experiences to meet the individual needs, interests, and abilitie of each student (Kasnja-Milicevic et al., 2011). It recognizes that students have unique learning styles, preferences, and strengths, and aims to provide customized learning pathways to maximize their educational outcomes. Personalized learning goes beyond a one-size-fits-allapproach and takes into account factors such as students' prior knowledge, learning pace, and specific learning goals. It may involve utilizing adaptive technologies (Slavuj et al., 2017), individualized goal-setting (Jansen et al., 2024), flexible grouping (McGillicuddy, 2021), andferentiatd instruction (Boelens et al., 2018) to create a more personalized and tailored learning experience for each student. Research has shown that personalized learning approaches can significantly enhance student engagement, motivation, and achievement by ensuring that learning experiences are relevant, meaningful, and aligned with each individual learner's needs (Barker, 2007; Chen & Chung, 2008; Huang, Yu, et al., 2023; Perez-Segura et al., 2022). Notably, studies have delved into the use of various educational technologie to support personalized language learning among students (Chen & Chung, 2008; Hsu e l., 2013; Ko, 2022). Furthermore, as suggested by Xie et al. (2019, the emerence f artificial intllignce (I)

technologies has opened up new avenues and expanded possibilities for technology-enhanced personalized learning.

Chatbots have been used as effective toos in language education, ffring students the opportunity to practice language skills at their convenience, receive tailored feedback, and engage in a stres-fre and interactive learning environment (Fryer et al., 2020, Guo et al., 2024; Jeon, Le, & Choi, 2023). These chatbots have shown potential in boosting students learning motivation, engagement, and outcomes (Fathi et al., 2024; Jeon, 2021; Kim & Su, 2024; Kwon et al., 2023; Liu et al., 2022; 0u et al., 2024; Zhang et a., 2023a). Additionall, they can allviate teachers' workload, allowing them to focus on other aspects of teaching. While researchers and teachers have developed customized chatbots to support students leaning (e.g., Guo et al., 2022; Shin et al., 2024), these solutions may not address each student's unique needs given their diverse learning habits, language proficiency levels, and individual challenges. To address this, empowering students to create their own chatbots for personalized learning has gained attention. Student-developed chatbots have the potential to offer a more tailored and individualized learning experience. By creating their chatbots, students can align the chatbot's functionality with their personal lerning preferences and goals. Allowing students to take an active role in chatbot creation would potentially enable them to addres their unique larning needs, surpassing the limitations often associated with standardized, one-size-fit-all solutions. Additioall, students' elf-made chatbots could foster a sense of ownership and engagement in the lening process. When students actively participate in building their chatbots, there i a likelihood that they would become more invested in the learning experience. This increased involvement may have the potential to enhance students motivation and consistency in engaging with the chatbot, which could ultimately lead to improved learning outcomes (Gode e al., 2019).

Recent advancements in large language models (LMs) have le to the development of chatbot creation platforms like Poe, enabling users to build powerful and personalized chatbots for various purposes. Particularly, rerieval augmented generation (RAG) techniques that combine retrieval and generation methods offer chatbot developers opportunities to tailor chatbots to meet individual users specific needs (Cai et al., 2022). These technological advancements provide students with the opportunity to construct personalized chatbots that can augment their language learning. For example, by incorporating retrieval mechanisms, student-developed chatbots could retrieve accurate and up-to-date information from reliable sources such as language textboos and educational websites. This would ensure that the content provided by the chatbot is reliable and relevant, enhancing the learning experience. Furthermore, RAG techniques willenable chatbots to generate responses that consider the context of the conversation. Students can potentiall use these techniques to train their chatbots to produce contextually appropriate content.

This study aims to investigate the potential of students' self-made RAG chatbots in supporting their learning to write. Specifically, we examine the impact of this innovative learning approach on students writing motivation and writing processes. The findings from this study wil significantly contribute to our understanding of chatbots' ability to enhance language learning for students. By embracing the possbilitie offered by students self-made RAG chatbots, our aim is to promote personalized leaning, improve learner autonomy, and foster self-regulated learning in language education.

# 2. Literature review

# 2.1. Chatbot-supported writing learning in higher education

The integration of chatbots in the writing classroom in higher education has been the subjec of numerous studies. For example, in a study by Kilickaya (2020), university students' perspectives on using a chatbot named Replika for writing asistance were examined. The findings revealed that students held a positieattitude towards the chatbot due to its perceived usefulness and prompt responses. Lin and Chang (2020) used achatbot named DD to ssist teachers in delivering writing instructions. The chatbot's role was to support university students in developing thesis statments for ther argumentative esay outlines. The reults indicated improved performance in essay outlining and a positiv student attitude towards the chatbot's assistance. Guo et al. (2023) integrated an argumentative chatbot into debate activities to support Chinese university students' preparation for debates and their larning of English argu. mentative writing. The findings demonstrated that engaging in chatbot-assted debates had a positive impact on students' English argumentative writing skill. It is worth noting that while there are positive findings about the us f chatbots in writing intruction, some studies have reported negative effects. For example, Zhang et al. (2023b) developed a chatbot for self-regulated training on logical fllacies in EFL agmentative writing, but the reults showed areduction in students writin elfefficacy, suggesting potential limitations or challenges associated with chatbot-based instruction.

Since its release in November 2022, ChatGPT, an LLM-empowered chatbot, has sparked a growing body of research exploring its potential in the writing classroom. For example, Song and Song (2023) conducted a study to evaluate the impact of ChatGPT assistance on Chinese undergraduate students English writing skils. The results indicated significant improvements in various aspects of writing, such as organization, coherence, grammar, and vocabulary, among students who received ChatGPT-assted instruction compared to those who received traditional instruction. In another investigation, Guo and Wang (2024) examined ChatGPT's potential in supporting EFL teachers feedback on students writing. The study uncovered that ChatGPT and teachers tended to employ different types of feedback when evaluating students writing. The findings also revealed that ChatGPT generated a considerably larger amount of feedback than teachers. While teachers primarily focused on content-related and language-related issues, ChatGPT distributed its attention relatively evenly among content, organization, and language aspects. However, concerns have been raised about the limi tations of ChatGPT, both in terms of its writing performance and it integration into the writing classroom. For instance, Zhou et al. (2023) conducted a study comparing the writing performance of ChatGPT and Chinese intermediate English learners. The findings revealed that while ChatGPT exhibited superior performance in terms of narrtivity, word concreteness and referential cohesion compared to human writers it displayed weaker performance in terms of syntactic simplicity and deep cohesion. Escalante et al.

(2023) explored the learning outcomes and perceptions of university EFL students who received writing feedback generated by ChatGPT. The results indicated that, compared to feedback from human tutors, the use of ChatGPT feedback did not yield significant differences in learning outcomes. Furthermore, the study revealed that students were nearly equally divided in their preference for ChatGPT-generated or human-generated feedback.

The use of LLM-based chatbots not only inluences students' writing skill but also has an impact on their writing motivation (Song & Song, 2023; Yan, 2023). Writing motivation plays a crucial role in driving students to actively participate in writing activities and maintain their dedication to writing tasks, ultimately leading to improved outcomes (Bruning & Horn, 2000). It can be influenced by both external social and situational factors, as well as intenal cogntive and affective factors (Ling e l., 2021). Previous studies on writing motivation have explored various aspects, with considerable research focusing on selfefficacy, which refers to individuals confidence in their bility to accomplish specific tasks (Bandura, 1997). Achievement goal theory (Elliot & Church, 1997; Pintrich, 2000) has also been employed to study writing motivation. Furthermore, research has examined the impact of students' beliefs regarding the key aspects of effctive writing (Wright et al., 2019). Considering the use of LM-based technologies, the influence on students writing motivation should be acknowledged. For instance, LLM-based technologie could afect student writers selfefficacy as they may feel more onfident in ther ailit to gnerate high-quality content with AI asstance. Converely, f Ignerated content is perceived as superior or more appealing to audiences, students may experience a decline in confidence, leading to self-doubt about their writing abilitie. LLM-based technologies also have the potential to shape writing goals by offering new possbilities and challenges. On one hand, students may set higher goals by envisioning the use of LLM-based technologies to create more sophisticated and innovative content. On the other hand, students might fee overwhelmed or intimidated by the capabilities of LLM-based technologies, causing a shift in their goals. The complex relationship between LLM-based technologies and writing motivation emphasizes the ongoing need for research.

# 2.2. Using customized chatbots for personalized learning

Using chatbots as learning companions has emerged as a popular pedagogical approach for personalized learning. While many studies have adopted commercial chatbots like ChatGPT (e.g., Su et al., 2023) and Alexa (e.g., Dizon, 2020) to support students language learning, some researchers have developed customized chatbots talored to specific educational purposes. These chatbots are often designed and developed by the researchers themselves. For example, Guo et al. (2022) developed a chatbot named Argumate, designed to assis students in constructing arguments and writing argumentative essays. Argumate provides two important scafflds: suggesting ideas to support students' opinions and exposing them to opposing ideas. Zhang et al. (2023b) created a chatbot for out-of-class self-reulated training on ogical fallaies in EFL argumentative writing. Liu et l. (2022) develoed an A-enabled chatbot with a basic understanding of 157 books, enabling it to provide book discussions and socialaffective cues to facilitate students engagement in book talks and sustain their interest in reading. Hsu et al. (2023) developed an interactive chatbot system named TPBOT (rOEIC Practice Chatbot) for EFL learners to overcome their ear of speaking English and practice poken English with chatbots at any time. Fathi et al. (2024) employed an AI-mediated chatbot clled Andy English Chatbot, which was designed to aid language learners in practicing their peaking skills. This chatbot simulated conversations with native English speakers and provided feedback and corrections on EFL learners' pronunciation, grammar, and vocabulary.

Furthermore, the availability of chatbot creation platorms, such as Danbee Al and Google's Dialogflow, has made i easier for researchers to develop customized chatbots without requiring complex programming skill. For instance, Jeon (2023) created chatbots using Dialogflow for mediating dynamic ssessment to support students' vocabulary learning. Kwon et al. (2023) developed a chatbot using Dialogflow to enhance students' writing practice. Kim and Su (2024) emphasized the importance of considering learners' levels and requirements, as well as ensuring user-friediness and cost-fre access when deigning chatbots. To addres thee factors, they employed the Danbee AI platform, which allowed them to create chatbots with a predetermined conversation flow encompassing text-based and voice-enabled interactions. In addition to researcher-created chatbots, some studies have explored the use of teacher-created customized chatbots in the language classroom. For instance, Shin et al. (2024) demonstrated an innovative racticeof building atalored and task-based chatbot to provide corrective feedback. Teachers utilized options like action and parameters' and define prompts in the chatbot building platorm, Dialogflow. The study showed that teachers customized chatbots could sucesfully offer corretive fdack when students made non-targt utterances and elicit learner uptake. However, it is important to note that the chatbots created in these studies were largely retrieval-based, with predefined outputs rather than generative.

With the emergence of generative Al, it is now possible to create more powerful and intlligent chatbots to support student learning. Furthermore, the latest advancements in chatbot creation platorms like Poe have eliminated the need for technical expertise. Users can now create chatbots simply by writing prompts, making the processaccessible to a wider range of individuals This means that not only rearchers and teachers, but also students themselves, can take on the role f chatbot creators. Students, being intimately aware of their own learning nees and preferences, can now actively participate in designing and developing chatbots that cater specifically to their requirements. This empowerment of students to become chatbot creators may not only foster a sense of ownership and agency in thir learning journey but also enable them to personalize their learning experiences to a greater extent (Godhe et al. 2019). By actively engaging in thecreation process students can tailor their chatbots to align with their individual learning goals and enhance the effectiveness of their educational interactions.

# 2.3. Retrieval augmented generation (RAG)

The emergence of LLMs, such as ChatGPT, has brought significant advancements in various fields, including education (Bahroun et al., 2023; Baido-Anu & Ansah, 2023; Chiu, 2023). Generative AI technologies have the potential to transform learning experiences by providing personalized and interactive toos for students. However, LLMs face challenges such as lack of contextual information, outdated knowledge, and untraceable reasoning processes (Huang, Yu, et al., 2023; Kandpal et a., 2023). These limitations highlight the impracticality of deploying LLMs as black-box solutions in real-world production environments without implementing additional safeguards (Gao et al., 2023). Consequently, these shortcomings can impede the effctivenessof LM-based chatbots in supporting students writing learning. For example, the lack f contextual information means that chatbots may struggle to understand the specific context and nuances of a writing task, leading to potential inaccuracies or inappropriate suggestions. Additionall, chatbots may rely on outdated knowledge, as their training data has a cutoff date nd may not incorporate the latest information or developments. This can result i incorrect or obsolete recommendations being provided to students. Furthermore, the untraceable reasoning processes of chatbots pose a challenge. Students may receive suggestions or feedback from chatbots, but they are ofen unable to trace the underlying reasoning or understand the logic behind the suggestions. This lack of transparency can hinder students' abilit to learn and improve their writing skills effectively.

To address these imitations, researchers have introduced RAG, a promising solution that incorporates knowledge from external databases (Lewis et al., 2020). RAG involves two steps: an initial retrieval phase where the LMs query an external data source to gather relevant information, followed by the generation phase where the retrieved evidence informs the text generation proces Gao et al., 2023. This approach significantly enhances the accuracy and relevance of the output by grounding responses in retrieved evidence. RAG's dynamic rerieval of information from knowledge bases during the inference phase helps mitigate isues like generating factually incorrect content, often referred to as hallcinations (Shuster t al., 2021). This integration of RAG into LLMs has gained rapid adoption, becoming a pivotal technology in refining chatbot capabilities and making LMs more practical for real-world applications (Cai et al., 202; Kim et al., 2020). This holds great promise for enhancing students' learning experiences by providing more accurate and contextually relevant support. However, despite the increasing number of studies exploring the potential of LLMs in enhancing teaching and learning since the launch of ChatGPT in November 2022 (for reviews, se Lo, 2023; Montene. gro-Rueda et al., 2023; Rahman & Watanobe, 2023), limited research has thus far explored the application of RAG-enabled chatbots in educational settings.

In the current Aldriven era, it iscrucial for students o acquire the skills and knowledge necessary to effectively utilize Al tech. nologies in their learning and future carers. With the introduction of RAG techniques and the availabilit of AI-empowered chatbot creation platforms like Poe, students now have the opportunity to create their own chatbots tailored to their individual learning needs, facilitating personalized learning experiences. However, there istill a lack of understanding regarding how students create RAG chatbots and how the utilization of such chatbots influences their learning. Consequently, this study aims to bridge this gap by investigating students creation and use of self-made RAG chatbots in the context of EF writig. Specifially, the study sees to address the following research questions (RQs):

RQ1: What purposes do students have when they create their own RAG chatbots to support their writing? RQ2: How do students engage with their self-made RAG chatbots during the writing process? RQ3: How does writing with self-made RAG chatbots impact students' writing motivation?

# 3. Methods

# 3.1. Participants and context

The study was conducted at a comprehensive university in China that employs English as the medium of instruction. A total of 69 undergraduate students, comprising 42 males and 27 females, participated in the study. The age range of the participants was between 18 and 23, with a mean age of 20.03 $\mathrm { \Delta } \mathrm { S D } = 1 . 0 5 \mathrm { \Delta }$ . All participants demonstrated proficiency in English, with their proficiency levels ranging from B2 to C1 according to the Common European Framework of Reference for Languages. These students came from Science, Technology, Engineering, and Mathematics (STEM) disciplines. All participants were enrolled in atechnical English course instructed by the second author, which included a module on technical proposal writing. One of the primary teaching objectives of this module was to enhance the students' argumentation and argumentative writing sils. Argumentative writing holds significant value for undergraduate students. I is a crucial kill that helps them develop critcal thinking and effective communication ailitie. In January 2024, as part of a module sesion, a workshop was conducted to instruct students on creating chatbots to enhance their argumentative writing skil. Further detals about the workshop can be found in ection 3.2. Prior to ther involvement in the study, informed consent was obtained from all participants.

# 3.2.  Chatbot creation workshop

# 3.2.1. Poe, a chatbot creation platform

A 1.5-h long workshop on chatbot development and application was designed to equip students with the necessary skills tocreate a chatbot for enhancing argumentative writing using the Poe platform. Poe, an acronym for Platform for Open Exploration, serves as a platorm that offers users access to various Al-enabled chatbots, including ChatGPT, GPT-4, Claude-instant, Google-PaLM, etc. Moreover, users have the opportunity to create their own chatbots on Poe. Fig. 1 ilustrates the proces of creating a chatbot on the platform. Users can generate their own chatbot by providing information such as prompts, greeting messages, and chatbot bios. This input allows users to customize the behavior and characteristics of the chatbot according to their preferences and requirements.

Importantly, the platform includes a knowledge base feature that enables users to incorporate custom knowledge,shaping the chat bot's responses and facilitating the use of RAG techniques.

# 3.2.2. A five-phase workshop design

As presented in Fig. 2, the workshop comprised five phases. In the first phase (nees idenication), the students were given a worksheet to reflect on their experiences with writing argumentative essays. The worksheet prompted them to think about the challenges they typicall faced during the writing process. Itincluded specific questions and prompts to stimulate self-reflection and self-assesment. The students were encouraged to consider various aspect of argumentative writing, such as generating ideas, organizing thoughts, using evidence, structuring paragraphs, and addresing counterarguments. In addition to the workshee, explicit instructions were provided on how to approach the self-asessment process. The students were encouraged to draw upon their past experiences, feedback from teachers or peers, and their own obserations to gain insights into their writing chllenges. The instructor offered guidance and clarification when needed to ensure students understood the purpose of the selfassessment and how it would inform thir chatbot creation. Through this process the students were able to identify their specific needs and chllenges in argu. mentative writing, which served as the foundation for creating personalized chatbots talored to address those need.

In the second phase teacher instruction), the instructor provided a demonstration of the chatbot creation proces on Poe. The focal point of this demonstration was prompt engineering training, which encompassed three steps: (1) Students were guided to select a persona for their chatbot, defining the desired character and formulating prompts using the second person perspective; (2) Students received instructions on crafing prompts that would guide the LLM; (3) Students explored the customizatio of the knowledge base, allowing them to incorporate relevant files and resources into their chatbots. y covering these thre stes, the instructor equipped the students with the necessary skills to create effective and personalized chatbots on Poe.

In the third phase (chatbot creation), students were tasked with developing their own chatbots to address the challenges they identified in the first phase. They designed and input information, including the chatbot's name, prompt, greeting message, and objective. Additionall, they uploaded knowledge base files and incorporated relevant resources into their chatbots. This phase empowered the students to personalize their chatbots and talor them to asist with their specific needs in writing argumentative essays.

The fourth phase (chatbot implementation) centered around practice. Students used their personalized chatbots to asist them in an argumentative writing task, adapted from GRE's Analyze an Isue' task (designed to evaluate test takers' argumentative writing abilities; se https://www.ets.org/gre/revised general/prepare/analytical writing/ssue/). The chosen writing task was in line with the objectives of the course module where the research was conducted. Students were required to write an esay of around 300 words, discussing which view in an issue aligned with their own position and explained their reasoning for the position they take. The issue was:

![](img/c94fc91f814a834abb8be82f14de65eb6ddad2aa203bb91f4336b7dea7167370.jpg)  
Fig. 1. Creating a chatbot on Poe.

![](img/59162bb0e634cee202e4403c42e50c8ae6f09bd5d30d112c447f516c13f5b6a1.jpg)  
Fig. 2. Workshop procedures.

Some ele blieve iis fte sry, evesirable foltc lders t wthod nomation frm he public. thers beliee that the public has a right to be fully informed.

The students were given a time limit of $3 0 \mathrm { m i n }$ to complete the writing task. They were required to annotate the sections of theit essay where they received assistance from their chatbots.

In the final stage (student reflection), ater completing the essay, the students were tasked with writing a reflection on thir elf made-chatbot-assted writing process They were provided with a reflection prompt consisting of two questions: (1) In what apects can your elf-made chatbot asst you in completing the argumentative writing task? (2) What do you think are the advantages of your self-made chatbots compared to existing chatbots (e.g., ChatGPT) in enhancing your argumentative writing? In response to the second prompt question, the students were encouraged to reflect on their prior experiences with LLM-based chatbots as a tol for supporting their argumentative writing. It is important to note that throughout the module sessions preceding the study, the students had received instruction on using LLM-based chatbots as a resource during their argumentative writing proces. Therefore, when addressing the second prompt question, the students drew upon their familiarity with these chatbots and incorporated their past experiences into their written reflections.

# 3.2.3. Ethical considerations

In the workshop, students were guided tocreate their own RAG chatbots. This involved the uploading of various files and resources to enhance the functionality and content generation capabilities of their chatbots. We recognize that such activities raise ethical considerations that need to be addressed. One of the primary ethical concerns is ensuring the appropriate use of copyrighted material. Students were explicitly instructed on the importance of respecting intellectual property rights and were advised to only upload files and resources that they had the legal right to use. They were provided with guidelines and examples to help them understand what constitutes fair use and how to avoid copyright infringement.

Another ethical concern pertains to privacy and data protection. To mitigate this, students were informed about the importance of not uploading any personal or sensitive information, both their own and others', to their chatbots. They were educated about the potential riss asociated with sharing personal data and were instructed to use anonymized or ictional examples whenever possible. Additionally, students were advised to only use publicly available or properly licensed resources to avoid unintentionally sharing

confidential or proprietary information.

During the workshop, regular discussions were held to addres any ethical concerns or questions raised by the students. They were encouraged to ask for clarification or guidance if they were uncertain about the appropriatenes of certain files or resources. The instructor provided ongoing support and feedback, ensuring that students understood and adhered to ethical guidelines throughout the process.

By incorporating these ethical considerations and providing clear instructions, we aimed to promote responsible use of resources and foster an ethical learning environment for the students.

# 3.3. Data collection and analysis

Multiple data sources were collected in this study to address the thee RQs, including students completed workheets,self-made chatbots, chat histories with chatbots, argumentative essays, written reflections, and questionnaire responses (see Table 1).

3.3.1. For addressing RQ1 (students' purposes for creating chatbots)

To answer RQ1, we collected the students' self-identified challenges in argumentative writing in the initial phase (needs identfi cation) of the workshop and the details of their slf-made chatbots, including chatbot names, objectives, prompts, and knowledge base files. These two datasets were used to understand the students' purposes for creating chatbots to tackle their challenges in argumentative writing.

We carried out a content analysis (Dornyei, 2007; Evans, 2017) to classify the expected caffoldings provided by the students self-made chatbots, which were then categorized, in accordance with Su et al's (2023) proposed proces-based approach to examining LLM-based-chatbot-assted argumentative writing, into four stages: preparatio, editin, profreding, and reflection. To ensure coding reliabilit, the first and second authors studied the coding scheme together and then coded 18students' worksheets and chatbot information independently ( $2 5 \%$ of the data). The inter-coder agreement was over $9 5 \%$ . The two coders discussed their coding until they reached final agreement. Then, the two authors each coded half of the remaining data independently.

Finally, for each chatbot creation purpose, the number of students who created a chatbot for that particular purpose was recorded. If a chatbot was created for two or more purposes, the number of students who created the chatbot was counted separately for each purpose. Additionally, the percentage of students was calculated based on the total of the 69 students.

3.3.2. For addressing RQ2 (students' engagement with their chatbots in the writing process)

To investigate RQ2, we examined the chat histories between the students and their chatbots to gain insights into their interactions during the writing process. The analysis was conducted at the level of single-turn conversations, where each exchange between a student's message and their chatbot's response was treated as a unit of analysis. In total, we identified 364 conversation units.

Informed by Han et al.'s (2023) study on student-ChatGPT dialogue patterns, we identified three main categories of requests which ;tudents employed when seeking help and engaging with their chatbots. These categories were as follows:

(1) Requestfor asstance: obtaining assistance to addres students' challenges during the writing process; this category included three subcategories, namely request for information, request for evaluation, and request for revision; for their definitions, see Appendix A.   
(2) Request for customization: pursuing tailored responses aligned with students' requirements; this category included four sub categories, namely customization of response scope, customization of response format, customization of response credibility, and customization of response relevance; for their definitions, see Appendix A.   
(3) Request for translation: seeking translation services, usually involving translating Chinese into English.

Considering the dialogic nature of the chat histories the analysis of each conversation unit incorporated the context to identify the underlying request. For instance, student 67's inqury, \*What grammatical errors did you correct?Could you highlight the detail by using the bold font?", was categorized as a request for customization of response format, rather than a request for revision. This determination arose from the student's intention for their chatbot to tailor the previous response, transforming an improved error-free essay into one with annotated grammatical corrections.

Table 1 Data sources and purposes.   

<html><body><table><tr><td>Data source</td><td>Purpose</td><td>Related RQ</td></tr><tr><td>1. Student worksheets (self-identified challenges in argumentative writing)</td><td>To understand the students&#x27; challenges in argumentative writing and their purposes for creating chatbots</td><td>RQ1</td></tr><tr><td>2. Students&#x27; self-made chatbots (prompts and knowledge base files)</td><td>To understand the students&#x27; purposes for creating their own chatbots and how RAG techniques were adopted in their chatbot creation processes</td><td>RQ1</td></tr><tr><td>3. Chat histories</td><td>To understand the students&#x27; interaction with their self-made chatbots during the writing process</td><td>RQ2</td></tr><tr><td>4. Essays (with students&#x27; annotations on how chatbots assisted in their writing)</td><td>To investigate the students&#x27; use of their self-made chatbots for argumentative writing</td><td>RQ2</td></tr><tr><td>5. Written reflections</td><td>To examine the students&#x27; perceptions of and engagement with their self-made chatbots</td><td>RQ2</td></tr><tr><td>6. Questionnaire responses (pre- and post-workshop)</td><td>To examine the students&#x27; changes in their writing motivation</td><td>RQ3</td></tr></table></body></html>

Within each conversation unit, one or multiple requests were observed, and we documented a total of 398 requests. To ilustrate, student 64 made a statement, \*Please check the logic of the fllowing paragraph, point out what problems exist, and what can be improved." Two requests were discerned: a request for evaluation (i.e., "check the logic" and "point out what problems exist") and a request for revision (i.e., "what can be improved").

To ensure coding reliability, the first and second authors studied the coding scheme together and then coded 91 single-turn conversations independently $2 5 \%$ of the data). The inter-coder agreement was over $9 2 \%$ . The two coders discussed their coding until they reached final agreement. Then, the two authors each coded half of the remaining data independently.

During the analysis of the chat histories, we complemented the findings with the students' annotated argumentative essays. In these essays, students were instructed to highlight the sections where their chatbots provided support. The essays served as supplementary data, allowing us to corroborate and provide further explanations for our findings. In addition to the esays, we also referred to the students' written reflections collected during the final phase f the workshop. These reflections provided insights into the students thoughts and experiences with their chatbot-assted writing process By triangulating these ifferent data sources, we aimed to gain a comprehensive understanding of the students' engagement with their chatbots.

3.3.3. For addressing RQ3 (impact of using self-made RAG chatbots on students' writing motivation)

To address RQ3, participants were required to complete pre- and post-workshop questionnaire see Appendix B) to examine any changes in their writing motivation resulting from the chatbot-assted writing experience. The questionnaire was adapted from Ling et al. (2021). We employed Ling et al's (2021) instrument because it was developed specificall for asessing the self judgment of undergraduate students regarding their writing motivation. Since our study focused on undergraduate students, we found this instrument to be particularly relevant to our research context.

The questionnaire consisted of four components, which assessed different aspects of writing motivation: (1) writing goals (the aims or objectives individuals have regarding their writing); (2) writin confidence (or slf-eficy) (individuals perception of their capability to accomplish writig-related goals and overcome writing challenges); (3) writing belefs individuals beles about writing) and (4) writing affect (individuals' emotional experiences and attitudes towards writing).

Firstly, the writing goals scale comprised 11 items, rated on a 5-point scale ranging from 1 (does not describe me at ll to 5 (describes me very well). This scale examined three goal orientations related to learning to writ: master goals, performance goals, and avoidance goals. Masery goals (4 items) focus on the development of knowledge and competence in writing e.g., When I am writing, I am trying to improve how I expres my ideas.). Performance goals (3 items) refer to the desire to appear competent compared tothers ("When I am writing, I am trying to be a better writer than my classmates."). Avoidance goals (4 items) capture effrts to avoid un. favorable judgments from others (e.g., "When I am writing, I am trying to avoid making mistakes in front of my classmates.").

Secondly, the writig confidence self-eficy) scale consisted of 22 items (e.g., " can think of a lot of ideas for my writing."), with responses given on a 5-point scale reflecting varying levels of confidence, from 1 (no chance) to 5 (completely sure).

Thirdly, the writing beliefs scale included 12 items, with responses recorded on a5-point scale ranging from 1 (strongly disagree) to 5 (strongly ge). his sal ases blifs about conten ide clarification and discovery; 6 items; e.,"Writing helps make my ideas clearer.") and conventions (grammar, spelling, and fluency; 6 items; e.g., "Good writers do not make errors in spelling.")

Lastly, the feelings about writing (affect) scale encompassed 5 items (e.g.,"I usually enjoy writing.), also rated on a 5-point scale ranging from 1 (strongly disagree) to 5 (strongly agree).

By completing the pre- and post-workshop questionnaires, participants indicated their writing motivation under two conditions: writing with and without the assistance of self-made RAG chatbots.

To assess the normality of the writing motivation data obtained from the questionnaire the Shapiro-Wilk test was employed. Since the assumption of normal distribution was not met, the Wilcoxon signed-rank test was deemed appropriate for the analysis. Conse quently, the Wilcoxon signed-rank test was conducted for each of the seven writing motivation sub-dimensions (i.e., mastery goal, performance goal, avoidance goal, writing confidence, writing belief in the importance of content, writing belief in the importance of conventions, and writing affect) using SPSS.

# 4. Results

4.1. What purposes do students have when they create their own RAG chatbots to support their writing? (RQ1)

In consideration of their individual challenges in argumentative writing, the students developed their own chatbots for seven distinct purposes (see Table 2). Regarding the first stage of preparation, a significant number of students (23 students; $3 3 \%$ ) identified difficulties in crafting cler thesis statements and compelling subclaims. I response, they created chatbots specifically for idea generation (purpose 1). For instance, student 25 designed a chatbot named "Issue Idea Generator' with the following prompt:

You are an expert in generatig and improving ideas in GRE issue essays. You are going to asst th user in writing subclaims by providing ideas for a specific gument. Pleas reer to the GRE issue essay scoring rubrics (especially the rubric for qualit of ideas") togenerate appropriate, insightful, and innovative ideas.

The chatbot's knowledge base was enriched by integrating the GRE scoring rubrics, specificall focusing on the rubric for \*quality of ideas." This integration aimed to tackle the student's difficulties in generating high-quality ideas. In addition to scoring rubrics, some students uploaded their course materials (PowerPoint slides) as an external knowledge base, which covered effctive techniques for brainstorming and topic analysis, in order to enhance the personalized guidance and support provided by their chatbots.

Table 2 Categorization of purposes for students' self-made chatbots.   

<html><body><table><tr><td>Chatbot creation purposes</td><td>No. of students</td><td>Percentage</td></tr><tr><td>Writing stage 1: Preparation</td><td></td><td></td></tr><tr><td>1. Idea generation</td><td>23</td><td>33%</td></tr><tr><td>2. Idea organization</td><td>13</td><td>19%</td></tr><tr><td>Writing stage 2: Editing</td><td></td><td></td></tr><tr><td>3. Logical progression</td><td>14</td><td>20%</td></tr><tr><td>4. Evidence use</td><td>19</td><td>28%</td></tr><tr><td>Writing stage 3: Proofreading</td><td></td><td></td></tr><tr><td>5. Writing style</td><td>19</td><td>28%</td></tr><tr><td>6. Error correction</td><td>6</td><td>9%</td></tr><tr><td>Writing stage 4: Reflection</td><td></td><td></td></tr><tr><td>7. Essay evaluation</td><td>17</td><td>23%</td></tr></table></body></html>

Additionally, 13 students $( 1 9 \% )$ developed chatbots with a focus on idea organization (purpose 2), such as "Structural Coach Bot" (student 53). These chatbots were designed to aid students in their argumentative writing by providing structural assistance to organize ideas, ensuring a smooth logical flow of subclaims and assisting with the overal structure of their arguments. To align with the expected structure standards for the GRE issue essay, some students uploaded sample essays sourced from the web as their chatbots' external knowledge base.

Regarding the second stage of editing, 14 students $( 2 0 \% )$ developed chatbots with the purpose of enhancing the logical progression of their argumentation (purpose 3). Notable examples included "Logic Coherent Bot" (student 11) and "Logic Check Improve" (student 47). In contrs to chatbots focused on general structural asstance (purpose 2), purpose 3 chatbots targeted the flow and coherence of specific ideas within thessay. This involvedestablishing connections between logical premises and supporting evidence, as well as ensuring the coherent development of supporting evidence. To bolster the efectiveness of their chatbots, students drew upon various sources on logical coherence. These sources included student handouts from previous English language courses and extracurrcular materials. For instance, student 47 uploaded a book titled \*Minto Pyramid Principle: Logic in Writing, Thinking, and Problem Solving" as an external knowledge base to enhance his chatbot'saility to offr tchniques for logical progression. This book, renowned as the McKinsey Firm standard, provides practical guidelines for organizing information in a logical and hierarchical manner.

Furthermore, 19 students $( 2 8 \% )$ built chatbots related to evidence use (purpose 4), such as Evidence Synthesizer Bot" (student 6) and "Evidence Helper' (student 64). These chatbots were designed to assist students to select evidence from credible sources, synthesize the evidence to support subclaims, elaborate on the evidence with adequate detail, and justify how the evidence could support the claim. Sample essays were usually uploaded as the knowledge base to instruct the chatbots to provide GRE-defined high-quality evidence use.

Regarding the third stage of proofreading, 19 students $( 2 8 \% )$ expressed inclination towards enhancing their writing style (purpose 5), focusing particularly on lexical choice and sentence structure. Additionally, 6 participants $( 9 \% )$ suggested their need for error correction (purpose 6), including aspects such as grammar and punctuation. In response to these requirements, chatbots were devel. oped, bearing names such as "Vocabulary Richness" (student 28) and "Grammar Guardian Bot (student 51). The students uploaded relevant knowledge base materials, primarily consisting of files obtained from the web designed to refine word choice and sentence structure in argumentative writing, to enhance the customization of their chatbots. For instance, one of the files elucidated its purpose as follows:

This file can hel you revise your paper for word-lel larit, elinate wordiness and avoid cliches, find the words that best express your ideas, and choose words that suit an academic audience.

Notably, no explicit grammar-related materials were used as external knowledge bases by the students.

Regarding the last stage of reflection, 17 students $( 2 3 \% )$ created chatbots, such as Arg Essay Assessor (student 33) and "GRE Score Improver" (student 60), to support their essay evaluation (purpose 7). GRE scoring rubrics were primarily used as the external knowledge base, which allowed the chatbots to provide feedback and suggestions that were congruent with the expectations and

Table 3 External knowledge base resources uploaded by students.   

<html><body><table><tr><td>Type of resources</td><td>No. of students</td><td> Percentage</td></tr><tr><td>GRE-related materials</td><td></td><td></td></tr><tr><td>Scoring rubrics</td><td>64</td><td>92.75%</td></tr><tr><td>Sample essays</td><td>20</td><td>28.99%</td></tr><tr><td>Course materials</td><td></td><td></td></tr><tr><td>PowerPoint slides</td><td>18</td><td>26.09%</td></tr><tr><td>Course syllabus</td><td>4</td><td>5.80%</td></tr><tr><td>Student handouts</td><td>8</td><td>11.59%</td></tr><tr><td>Extracurricular materials</td><td></td><td></td></tr><tr><td>Writing guides from educational websites</td><td>16</td><td>23.19%</td></tr><tr><td>Books</td><td>3</td><td>4.35%</td></tr></table></body></html>

standards set forth by GRE. As student 33 wrote:

You are an experienced English teacher with years of GRE assessment experience. You are going to use "GRE issue essay scoring rubrics. docx" to ssts ting  ree t the an ubrics. Pe roide oelscref tessa, prvide for score reated to the four dimensions and provide some suggestions to improve the essay. Use positive tones.

This personalized approach facilitated a self-reflective process for student, enabling them to gauge their performance against established evaluation dimensions and make targeted improvements in their writing skills.

As mentioned above, the students integrated various files and resources into their self-made chatbots s an extenal knowledge base to improve their chatbots performance. Here, we provide a summary of these files and resources. As shown in Table 3, the majorit of students primarily utilized GRE-related materials. Specifically, 64 students $( 9 2 . 7 5 \% )$ incorporated the scoring rubrics used for assessing the argumentative writing task, while 20 students $( 2 8 . 9 9 \% )$ employed sample essays sourced from the web. The use of these materials was driven by the specific writing task employed in the study, as students wanted their chatbots to generate responses that met the expectations and requirements of the task. In addition to GRE-related materials, the students also integrated course materials into their chatbots' knowledge base. This included the instructor's PowerPoint slides (18 students; $2 6 . 0 9 \%$ , the course syllabus (4 students; $5 . 8 0 \%$ , and student handouts (8 students; $1 1 . 5 9 \%$ . These materials provided the chatbots with the specific learning context of the students, including the instructor's expectations for writing performance and the students' current writing proficiency. By incorporating this information, the chatbots could offer more tailore asstance. Finally, some students collected extracurricular materials, such as writing guides from educational websites (16 students; $2 3 . 1 9 \%$ and books (3 students; $4 . 3 5 \%$ , which they uploaded to their chatbots' knowledge base, offering additional reference materials.

4.2. How do students engage with their self-made RAG chatbots during the writing process? (RQ2)

As presented in Table 4, the students made 310 requests for assistance $( 7 7 . 8 9 \% )$ , 82 requests for customization $( 2 0 . 6 0 \% )$ , and 6 requests for translation $( 1 . 5 1 \% )$ during their interaction with their chatbots.

The first main category of request fr asistance was divided into threesubcategories. Firstly, within the subcategory of request for information, the students actively sought information pertaining to four critical aspects of argumentative writing: idea generation, idea organization, logical progression, and evidence use. Examples of each aspect are provided below:

$\bullet$ Please give me some ideas for this topic, including counter-argument instead of directly providing the essay you write. (requesting in formation for idea generation; student 21) What kind of argument structure should I choose? (requesting information for idea organization; student 49) Can you give more reasons to support the ide that transparency is necessary? (requesting information for logical progressin; student 35)   
Please give me someexamples regarding witholding inforation from the public and undemining trust in political leders and institions (requesting information for evidence use; student 64)

These specific areas of inquiry directly aligned with their needs to develop personalized chatbots for argumentative writing. Ths subcategory showcased the highest volume of requests, reaching a total of 174 $( 4 3 . 7 2 \% )$

The second and third subcategories, namely request for evaluation and request fr reision, were losely related, with the participants demonstrating greater emphasis on the later. This was evident from the number of requests utilized, with 46 requests $( 1 1 . 5 6 \% )$ attributed to the former and a higher count of 90 requests $( 2 2 . 6 1 \% )$ directed towards the latter. The subcategory of request for evaluation entaled an asssment of essays, with a particular focus on evaluating and critiquing the content, organization, and language aspects, which were often accompanied by assigned scores. The subcategory of request for revision encompassed not only revision suggestions but also tangible revision edits. Here we provided examples that served both requests for evaluation and revision (with "//' used to divide the two requests).

D I have an argument that \*Transparency is the basis of human equality". Would you consider it a good point?/If not, help me revise it (request for evaluating and revising the content of the essay; student 5)

Table 4 Breakdown of student requests.   

<html><body><table><tr><td>Request</td><td>No. of requests</td><td>Percentage</td></tr><tr><td>Request for assistance</td><td>310</td><td>77.89%</td></tr><tr><td> Request for information</td><td>174</td><td>43.72%</td></tr><tr><td>Request for evaluation</td><td>46</td><td>11.56%</td></tr><tr><td> Request for revision</td><td>90</td><td>22.61%</td></tr><tr><td>Request for customization</td><td>82</td><td>20.60%</td></tr><tr><td>Customization of response scope</td><td>54</td><td>13.57%</td></tr><tr><td>Customization of response format</td><td>14</td><td>3.52%</td></tr><tr><td>Customization of response credibility</td><td>8</td><td>2.01%</td></tr><tr><td>Customization of response relevance</td><td>6</td><td>1.51%</td></tr><tr><td>Request for translation</td><td>6</td><td>1.51%</td></tr></table></body></html>

Check the logic/and add some connection phrases to enhance the logical flow among these thre points (request for evaluating and revising the organization of the essay; student 58)   
$\bullet$ Mark and is the gammarerrors I have//and provide me wth the corrct forms. (request for evaluating and revising the language of the essay; student 28)

The second main category of request for customization involved requests made by the students to further customize their chatbots output. It included firstly, customization of response scope, where students specified the expected depth or breadth of the information they requested (54 requests; $1 3 . 5 7 \%$ . Two scope adjustments were observed: scope expansion and scope specification. Scope expansion involved the participants seeking a broader range of information or resources to fulill heir needs. This included requesting further explanations (e.g., \*Elaborate more on the publics right to know and make informed decisions"; student 55) and additional information (e.g., \*Please ofer suggestions on word choice as well; student 36). Scope specification, on the other hand, referred to the participants providing specific requirements for the information they expected. This involved specifying the expected level of detail e. g, \*Please correct grammatical errors and tell me why you make the corrctions"; student 19), particular point to focus on (e.g, "Demonstrate why you add the econd evidence?", student 55), and user identification (e.g., I am a student majoring in data science and you will provide suggestions related to my background"; student 44).

Secondly, the subcategory of customization of response format, encompassed 14 requests $( 3 . 5 2 \% )$ , wherein the students expressed their preferences regarding the desired presentation style or format for the chatbot output. The specific formatting elements mainly included bullet points (eg., \*Give a list of grammar errors in the form of blle points, student 17), old or italicized tex (e.g, "Use bold font to highlight the places you revised", student 43), and word count (e.g., \*Give me an example within 300 words", student 21).

Thirdly, the subcategory of customization of response credibility, comprised 8 requests $( 2 . 0 1 \% )$ , wherein students further refine their prompts for acesing and utilizig information, thereby ensuring the quality of the information they received. Notably, the partici. pants' sense of intellctual rigor and proficiency in evaluating information were observed in their annotated essays. For example, student 14 commented, \*These four underlined sub-arguments are generated by the chatbot but I don't think they are convincing enough."

To enhance the credibility of the received information, the participants deployed three approaches inthe customization process. (1) Reasoning coherence was utilized to ensure that information was presented in a structured and cohesive manner. For example, student 25 requested, I also find that there are some overlaps between subclaims", showcasing her awarenessof the logical inconsistencies that could undermine the credibilit of the response. This request indicated her expectation for the chatbot to deliver a refined and logically coherent version of the argument. (2) Objectivit was employed to present information in an unbiased and balanced maner, without personal biases or factual error. For instance, student 50 critiqued, "ets say not all govenments are democratic, revealing his awarenessof the importance ofacknowledging the diversity of political systems and avoiding sweeping generalizations, ensuring an unbiased and credible presentation of the response. (3) Language opropriatenes was alied to ensure that the conveyed information remained fiting for the targeted academic audience. Some participants displayed a commitment to accuracy, as evidenced by student 22's request to "revise the second paragraph with more precise and scholarly vocabulary'. Furthermore, some participants aimed to maintain acesiilit by rfraining from emploing excesively technicl jagon r unnecessary complexity that could impede comprehension. As student 8 requested, \*The essay includes sophisticated vocabulary. Please revise it once more."

Lastly, the subcategory of customization of response rlevance encompassed the students' requests for information that was directly applicable to their specific needs or context. This involved 6 requests $( 1 . 5 1 \% )$ , where the participants ensured their chatbots referred to the knowledge base provided in generating responses. For instance, student 18's request, \*Please refer to the knowledge base. You should revise the draft acording to the given grading rubrics." highlighted the importance of consulting the specific assessment criteria, in this case, the GRE grading rubrics.

The third main category was request for translation. A total of 6 requests $( 1 . 5 1 \% )$ were dedicated to this category, with a specific focus on seeking translation services, predominantly involving the translation of Chinese into English. An example is as follows:

Table 5 Wilcoxon signed-rank test results.   

<html><body><table><tr><td rowspan="2">Writing motivation dimensions</td><td colspan="2">Median (SD)</td><td rowspan="2">%</td><td rowspan="2">r</td></tr><tr><td>Pre-workshop</td><td>Post-workshop</td></tr><tr><td colspan="5">Writing goals</td></tr><tr><td>Mastery goal</td><td>4.25 (0.52)</td><td>4.25 (0.55)</td><td>1.531</td><td>0.130</td></tr><tr><td> Performance goal</td><td>3.67 (0.76)</td><td>4.00 (0.83)</td><td>4.089a</td><td>0.348</td></tr><tr><td>Avoidance goal</td><td>3.50 (0.73)</td><td>4.00 (0.88)</td><td>2.886a</td><td>0.246</td></tr><tr><td>Writing confidencee</td><td>3.55 (0.51)</td><td>4.05 (0.53)</td><td>6.447a</td><td>0.549</td></tr><tr><td colspan="5">Writing beliefs</td></tr><tr><td>Importance of content</td><td>4.00 (0.50)</td><td>4.17 (0.54)</td><td>2.945a</td><td>0.251</td></tr><tr><td> Importance of conventions</td><td>3.17 (0.66)</td><td>3.83 (0.81)</td><td>4.668a</td><td>0.397</td></tr><tr><td>Writing affect</td><td>3.20 (0.70)</td><td>3.40 (0.70)</td><td>3.996a</td><td>0.340</td></tr></table></body></html>

Note. $p < 0 . 0 5$

[Translate this into English: The exact causes, pathogens, modes of transmission, and symptoms of the disease have not yet been determined.] (student 30)

# 4.3. How does writing with self-made RAG chatbots impact students' writing motivation? (RQ3)

To answer RQ3, we used the Wilcoxon signed-rank test to evaluate the results of the pre- and post-workshop questionnaires. As shown in Table 5, among the seven sub-dimensions of writing motivation, the students had significantly higher levels $( p < 0 . 0 5 )$ of performance goal $\mathbf { \delta } ( z = 4 . 0 8 9$ $p = 0 . 0 0 0$ .A $\begin{array} { r } { r = 0 . 3 4 8 } \end{array}$ , avoidance goal $( z = 2 . 8 8 6$ $p = 0 . 0 0 4$ $r = 0 . 2 4 6 \AA$ , writing confidence $( z = 6 . 4 4 7 ; p$ $= 0 . 0 0 0$ .A $r = 0 . 5 4 9$ , writing beliefs in the importance of content $\textstyle ( z = 2 . 9 4 5$ $p { = } 0 . 0 0 3$ $r = 0 . 2 5 1 $ , writing beliefs in the importance of conventions $\begin{array} { r } { ( z = 4 . 6 6 8 } \end{array}$ . $p = 0 . 0 0 0$ $r = 0 . 3 9 7 $ , and writing affect $\mathbf { \mathscr { z } } = 3 . 9 9 6$ $p = 0 . 0 0 0$ $r = 0 . 3 4 0 \mathrm { \Omega }$ when writing with their self-made chatbots than when writing without thir elf-made chatbots. However, no significant dfferences were found in the sub-dimension of mastery goal $( z = 1 . 5 3 1$ .A $p = 0 . 1 2 6$ $r = 0 . 1 3 0 \mathrm { , }$ between the two conditions.

# 5. Discussion and conclusion

# 5.1. Major findings

This study examined how EFL students created and utilized RAG chatbots to support thir personalized learning of argumentative writing. The reults demonstrated that students built RAG chatbots to address individual challenges at different tages of the writing process including preparation, editing, proofreading, and reflection. Notably, the majorit of students focused on using chatbots to generate ideas and construct arguments, while a smalle number bult chatbots for language error correction. This preference could be attributed to the students relatively high English language proficiency. Due to their proficiency in language usage, the students placed greater emphasis on the content aspect of argumentative writing. To improve the performance of their chatbots, students used various sources of information, such as grading rubrics, course materials, and online resources, to enhance the knowledge bases of their chatbots. These findings align with the study by Su et al. (2023), highlighting the value of LLM-based chatbots in supporting students argumentative writing across various stages. Importantly, the wide range of purposes for which students created chatbots highlights the benefits of empowering students to develop their own chatbots for learning. Since students face different writing challenges, providing them with the opportunity to create personalized chatbots ensures that the tools cater to their specific needs. The participating students captured this sentiment in their written reflections. For example, student 24 stated that, For me, the self-created chatbot was a game-changer in my writing proces. It allowed me to identify and address my weakneses more efectively. The chatbot was customized to provide specific feedback and strategies for improving my reasoning, which was an area I struggled with. With the chatbot's asistance, I noticed a significant improvement in the clarity and persuasiveness of my writing. This customization approach enhances the effectiveness and relevance of chatbots in supporting individual students' writing processes.

In addition to exploring students purposes for creating writing chatbots, this study investigated their engagement with their self made chatbots during the writing proces. It was found that students made various requests to generate output from their chatbots. The most frequently made request category, request for assistance, served as the foundation for addressing the students' challenges throughout the writing proces. This category encompassed different forms of asstance-eeking, including requests for information, evaluation, and revision. When facing challenges in obtaining suitable asstance within the first category, students turned to the second category, request for customization. This involved seeking personalized responses aligned with their specific needs. Customization requests included adjusting the response cope, format, credibility, and relevance. By requesting customization, students were able to refine the information received tioring t to ther indvidal requirements and preferences. Fr instace, by reformatting ther chatbot responses, the students sought to optimize the organization and readability of the information they received, ensuring a clear and succinct delivery of the content. Thisallowed them to engage with their chatbots more effectively and efficiently, enabling a productive interaction. This second category provided an additional layer of refinement and personalization to met the students needs. The third catry, request for tlion, spficlly adesthe ne for tranlatig infomation, allowing tudts toacces and processit in a language in which they were more proficient. Such cross-language assistance enabled students to bridge the lan. guage gap and effectively access and communicate information in their preferred language.

Notably, the students perceived their chatbot-asisted writing as an opportunity for writing skill improvement rather than simply fulilling a writing task. This was evident in their deliberate avoidance of requesting their chatbots to directl complete the assigned writing tass on their behalf. Interestingly, there were instances where students explicitl requested their chatbots to refrain from presenting them with complete essays. For instance, one student stated, You should provide me with diffrent ideas instead of giving me the entire essay' (student 40). This behavior exemplified their intention to enhance their writing skills by actively seeking sug. gestions and guidance from their chatbots. These participants demonstrated a proactive and growth-oriented mindse i their endeavor to utilize self-made chatbots for the purpose of enhancing their competence in argumentative writing.

Furthermore, this study investigated the impact of using self-made chatbots on students' writing motivation, yielding positive outcomes. Specificall, writing with self-made chatbots enhanced students' performance and avoidance goals. This manifested as a heightened desire among students to demonstrate competence compared to their pers, along with increased efforts to avoid unfavorable judgments from others.Surprisingly, no statisticallysignificant difference was observed in students' mastery goal, which pertains to the acquisition of knowledge and proficiency in writing. It could be attributed to the students existing lel of proficiency in writing. Moreover, students reported feeing more confident when writing with thir elf-made chatbots as opposed to writing alone.

This finding contrasts with the study conducted by Zhang et al. (2023b), which indicated a decrease in students writing self-fficacy following their exposureto learning to writ argumentative essays with chatbots. In addition, the uilization of elf-made chatbots also bolstered students' beliefs regarding the significance of both content and conventions in argumentative writing. Lastly, students attitudes toward writing became more favorable subsequent to their experience with self-made chatbots. This finding aligns with previous studies, including those conducted by Zhang e al. (2023a) and Guo et al. (2023), which have also demonstrated the positive effects of incorporating chatbots into EFL students' learning on their learning motivation.

# 5.2. Implications

Our findings contribute to the literature on chatbot-supported language learning and Al in education. Specificall, the study highlights the potential of students creating their own chatbots to support their personalized learning needs. This has significant implications for education as it empowerstudents to take ownership of their learning journey. By designing chatbots that align with their specific learning goals and preferences, students can enhance their engagement and motivation, leading to more effective learning outcomes. Furthermore, this study offers insights into the application of RAG techniques in language classrooms. RAG combines the advantages of rtrieval modes and generative models allowing for the creation of contextually appropriat and tailored responses. Understanding how RAG techniques can be effectively utilized in language instruction can inform the design and implementation of language learning tools and resources.

Our findings also have practical implications. Specifically, the study opens up possibilitie for the development of nnovative pedagogical methods. By integrating students self-made chatbots into language classrooms, educators can provide students with personalized language learning support. The interactive nature of chatbots can offr students opportunities for authentic language practice, instant feedback, and targeted language instruction, leading to improved language proficiency and fluency. Teachers are encouraged to inform their students about the potential role of chatbots in assting with various stages of the writing process.By sharing the findings f this study, particularly the strategies employed by students to generate output from chatbots and implement RAG techniques to build customized chatbots, teachers can guide students on efectively interacting with these tools. This guidance will empower students to make the most of ther chatbot interactions and harness the benefits provided by these personalized language learning aids. Additionall, the tudy highlights he importance of improving students Al literacy, particularly in terms of citical and ethical use of AI (Ng et l., 2021). As students engage in creating and utilizing chatbots, they need to develop an understanding of the limitations and implications of Al technologies. ducators should prioritize teaching students how to critically evaluate and assess the information generated by chatbots, as well as promoting ethical considerations such as privacy, bias, and fairness in AI systems.

# 5.3. Limitations and future research

This study has several limitations that offer opportunties for future research in this area. Firstly, the absence of a control group in this study is limitation. Future studies could employ an experimental design that includes a control group of students who either use teacher-made chatbots or engage in writing practices without the asstance of any chatbots. Such studies would provide robust empirical evidence regarding the ffects of using students self-made chatbots. Moreover, reearchers may consider adopting a withinsubject comparison design for further investigations. This design would involve students using both existing chatbots (e.g., ChatGPT) and their self-created RAG chatbots,allowing for a direct comparison of the two types of chatbots in supporting writing tasks. This approach would provide additional insights into the specific effects of RAG chatbots on students' writing learning.

Secondly, the intervention duration in this study was relatively short, consisting of only a $1 . 5 { \cdot } \mathrm { h }$ workshop where students utilized their self-made chatbots for a single writing task. Future studies could involve students in multiple chatbot-asisted writing tasks over an extended period of time. Longitudinal studies of this nature would yield valuable insights into students' evolving behaviors and perceptions in utilizing self-made chatbots for writing. Additionally, as students' writig skils develop, their learning needs may change, necesstating adjustments to their chatbots. Future studies could explore students chatbot modifications to gain a deeper understanding of their growth and development. Furthermore, it is worth noting that this study primarily focused on the argumentative writing genre. The choice of this specific writing genre may have influenced students creation and use of RAG chatbots due to its distinctive characterisics. To expand our understanding, future studies could explore the use of diffrent writing genre, such as narrative writing, and compare students' use of RAG chatbots across various genres. This comparative approach would provide valuable insights into how students adapt their chatbot usage strategies based on the specific requirements and features of different writing genres.

Thirdly, this study focused on students' self-efficacy in writing. Further investigations could delve into students' actual writing abilities using performance-based asssments, offring a more comprehensive understanding of the effects of employing self-made chatbots. Additionally, exploring the relationship between students' engagement with their slf-made chatbots and their writing motivation would be interesting. We also encourage future studies to examine students' adoption of chatbot assistance and its impact on their writing outcomes. This approach would contribute to a more holistic understanding of the effcts of student-chatbot interactions.

Lastly, it is important to acknowledge that this study had an unbalanced sampling of participants, with 42 males and 27 females. Considering that gender can potentially influence students acceptance and use of information technologies (Cai t al., 2017), it would be valuabl for future studies to explore the potential difference in the effcts of using self-made chatbots for writing between male and female students. By addressing these limitations and conducting future research in these areas, we can further enhance our understanding of the benefits and potential of utilizing students' self-made chatbots for writing instruction.

# Funding details

This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.

# Data availability

Data will be made available on request.

CRediT authorship contribution statement

Kai Guo: Conceptualization, Methodology, Investigation, Formal analysis, Data curation, Writing - riginal draft, Writing - review & editing. Danling Li: Conceptualization, Methodology, Investigation, Formal analysis, Writing - original draft.

# Declaration of generative AI and AI-assisted technologies in the writing process

During the preparation of this work the authors used ChatGPT in order to improve readability and language. After using this tool, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication.

# Declaration of competing interest

None.

# Appendix A. Categorization of students' requests

<html><body><table><tr><td>Request</td><td>Definition</td></tr><tr><td>(1) Request for assistance</td><td>Obtaining assistance to address students&#x27; challenges during the writing process</td></tr><tr><td>a. Request for information</td><td>Seeking information related to argument construction</td></tr><tr><td>b. Request for evaluation</td><td> Inviting an appraisal of the student essay</td></tr><tr><td>c. Request for revision</td><td>Soliciting revision suggestions and revision edits</td></tr><tr><td>(2) Request for customization</td><td>Pursuing tailored responses aligned with students&#x27; requirements</td></tr><tr><td>a. Customization of response scope</td><td>Making adjustments to the scope of chatbot responses</td></tr><tr><td>b. Customization of response format</td><td>Reformatting chatbot responses, such as using bullet points, bold or italicized text</td></tr><tr><td>c. Customization of response credibility</td><td>Enhancing the trustworthiness of chatbot responses</td></tr><tr><td>d. Customization of response relevance</td><td>Ensuring the chatbot refers to the knowledge base provided in generating responses</td></tr><tr><td>(3) Request for translation</td><td>Seeking translation services, usually involving translating Chinese into English</td></tr></table></body></html>

# Appendix B. Writing motivation questionnaire

Section I. Writing goals

- ${ \bf \nabla } ^ { 1 = }$ does not describe me at all; $2 =$ slightly describes me; $3 =$ somewhat describes me; $4 =$ moderately describes me; ${ 5 = }$ describes me very well).

When I am writing, I am trying to:

$\bullet$ improve how I express my ideas. [1 2 3 4 5]   
$\bullet$ keep people from thinking I'm a poor writer. [1 2 3 4 5]   
$\bullet$ hide that I have a hard time writing. [1 2 3 4 5]   
$\bullet$ become a better writer. [1 2 3 4 5]   
$\bullet$ have my classmates believe I can write well. [1 2 3 4 5]   
$\bullet$ avoid making mistakes in front of my classmates. [1 2 3 4 5]   
$\bullet$ persuade others with my writing. [1 2 3 4 5]   
$\bullet$ be a better writer than my classmates. [1 2 3 4 5]   
$\bullet$ hide how nervous I am about writing. [1 2 3 4 5]   
$\bullet$ get my teacher to think I am a good writer. [1 2 3 4 5]   
$\bullet$ better organize my ideas. [1 2 3 4 5]

# Section II. Writing confidence

- $^ { 1 = }$ at $0 \%$ , no chance; $2 =$ at $2 0 { - } 3 0 \%$ , some chance; $3 =$ at $5 0 \%$ , 50/50 chance; $4 =$ at $7 0 { - } 8 0 \%$ , good chance; $5 =$ at $1 0 0 \%$ completely sure).

I can write a paragraph with a clear topic sentence. [1 2 3 4 5] I can write complex sentences without making grammatical errors. [1 2 3 4 5] I can set goals for improving my writing. [1 2 3 4 5] I can think of a lot of ideas for my writing. [1 2 3 4 5] I can write a well-organized essay with an introduction, body, and conclusion. [1 2 3 4 5] I can evaluate whether I am making progress in learning to write. [1 2 3 4 5] I can write a paper using correct grammar. [1 2 3 4 5] I can organize paragraphs with ideas to support the topic sentence. [1 2 3 4 5] I can think of many words to describe my ideas. [1 2 3 4 5] I can plan before I write using an outline or organizer. [1 2 3 4 5] I can use punctuation correctly in all my sentences. [1 2 3 4 5] I can avoid distractions while I write. [1 2 3 4 5] I can end an essay with a strong conclusion. [1 2 3 4 5] I can come up with original ideas for my writing. [1 2 3 4 5] I can use commas and semicolons correctly in my sentences. [1 2 3 4 5] I can plan time to get my writing done by the deadline. [1 2 3 4 5] I can start an essay with an interesting introduction. [1 2 3 4 5] $\bullet$ I can focus on my writing for at least 1 h [1 2 3 4 5] $\bullet$ I can find ideas to write about when I'm given a topic. [1 2 3 4 5] $\bullet$ I can write a paper without spelling mistakes. [1 2 3 4 5] $\bullet$ I can think of the perfect words to express my ideas. [1 2 3 4 5] $\bullet$ I can tell when to use different writing strategies. [1 2 3 4 5]

# Section III. Writing beliefs

- ${ \bf \nabla } ^ { 1 } =$ strongly disagree; $2 =$ disagree; $3 =$ neutral; $4 =$ agree; $5 =$ strongly agree)

Writing helps make my ideas clearer. [1 2 3 4 5] $\bullet$ Revising is mostly about fixing errors in my grammar. [1 2 3 4 5] $\bullet$ Writing helps me think about my topic in a new way. [1 2 3 4 5] $\bullet$ Good writers do not make errors in spelling. [1 2 3 4 5] $\bullet$ The main problem of poor writers is using incorrect grammar. [1 2 3 4 5] $\bullet$ I learn new things from writing. [1 2 3 4 5] $\bullet$ Good writers discover new ideas while writing. [1 2 3 4 5] $\bullet$ Writing quickly is an important part of good writing. [1 2 3 4 5] $\bullet$ Good writers need little revision because they get it right the first time. [1 2 3 4 5] $\bullet$ Good writers have to be able to write long sentences correctly. [1 2 3 4 5] $\bullet$ Writing is one of the best ways to explore new ideas. [1 2 3 4 5] $\bullet$ Revising helps me clarify my ideas. [1 2 3 4 5]

Section IV. Feelings about writing

- ${ \bf \nabla } ^ { 1 } =$ strongly disagree; $2 =$ disagree; $3 =$ neutral; $4 =$ agree; $5 =$ strongly agree).

I usually enjoy writing. [1 2 3 4 5] $\bullet$ I don't like to write. [1 2 3 4 5] $\bullet$ The process of writing is satisfying for me. [1 2 3 4 5] $\bullet$ I think that writing is interesting. [1 2 3 4 5] $\bullet$ I try to avoid writing as much as possible. [1 2 3 4 5]

# References

through bibliometric and content analysis. Sustainability, 15(17), Article 12983. https:/doi.org/10.3390/su151712983 teaching and learning. Journal of AIDs, 7(1), 52-62. https://doi.org/10.61969/jai.1337500   
Bandura, A. (1997). Self-efficacy: The exercise of control. Freeman.   
Barker, D. (2007).  perolied aproch to nalyzing ot d befit in vauly section. System, 35(4) 23-533. hp/doi.org/10.1016/. system.2007.09.001 difterentiated intruction in blended learning. Computers & Education, 120, 197-212. http:/doi.org/10.1016/j.compedu.2018.02.009   
Bruning, R, & Hn, C. (200. Dveloing mtivation to wit. dctional Pscholois, 35(1), 25-37. htps:/oi.rg/10.1207/15326985P35014   
Cai, .,  , D, . (2017. dr a its d  u m-is. e ion 105, 1-13. tp/o.g/10.1016/. mpedu.2016.11.003   
Cai       e   e research and development in information retrieval (pp. 3417-3419). https://doi.org/10.1145/3477495.3532682   
Chen . h . 208. d mbile ish n te n thy nd  my cl. uers Education, 51(2), 624-645. https://doi.org/10.1016/j.compedu.2007.06.011   
hi, 23)c  tive  rctie i d  ditio  of   r. te Learning Environments, 1-17. https://doi.org/10.1080/10494820.2023.2253861   
Dornyei, Z. (2007). Research methods in applied linguisics: Quantative, qualitative and mixed methodologies. Oxford Universty Press.   
Dion .2020g  l ss fo isg  ak g,   , 1) 66, 10125 44705.   
Ellot,  J, c  (197). hcal m f ach ad de achet tivatio. l of iy n , 72, 218-232. https://doi.org/10.1037/0022-3514.72.1.218   
Technology in Higher Education, 20(57), 1-20. https://doi.org/10.1186/s41239-023-00425-2   
Evans, M. (2017). Analysing qualitative data. In E. Wilson (Ed.) Schoo-based research: A guide for educationsudens. Sage publications.   
Fathi, J, , 202)   kl  n t viaiieated interactions. System, 121, Article 103254. https://doi.org/10.1016/j.system.2024.103254   
Fryer r 0       t.    y, 24 (2), 8-22. Retrieved from http://hdl.handle.net/10125/44719.   
G             1.p arXiv:2312.   
e, A 1a  i i  th  ma  o,  nd Education, 28(3), 317-328. https://doi.org/10.1080/1475939x.2019.1610040   
Go, K, i,      2024 dng  s cse iv wi n aciit t ti tion and Information Technologies, 29, 1-20. https://doi.org/10.1007/s10639-023-12230-5   
Guo K g . (2024).  s i or torce  xng hs tl o ut tce eac in witingio an normation Technologies, 29, 8435-8463. doi:10.1007/s10639-023-12146-0.   
Guo      2.c tns ti ngsing 54l106./o.g 10.1016/j.asw.2022.100666 Education, 203, Article 104862. https://doi.org/10.1016/j.compedu.2023.104862   
Han, J, Y , g, J, Kim   ,  Y, h,  (2023.  -    wingo.  prn aiv. 2309.13243.   
Hsu, MHChe,   .. (2023). Pig a id cht sst r  rs s rctic ctiv  s, 31(7) 4297-4308. https:/doi.org/10.1080/10494820.2021.1960864   
Hsu, C. K wang, J  Chan,  K 2013).  peronalid romedtin-basd mobil lng aprch to mproving the reding pefomanceof EFL students. Computers & Education, 63, 327-336. https://doi.org/10.1016/j.compedu.2012.12.004   
Hg,         iio, and outcomes in a flpped classroom. Computer & Education, 194, Article 104684. https://doi.org/10.1016/j.compedu.2022.104684 questions. arXiv preprint arXiv:2311.05232.   
J ,         2in suport seondry sch stdnts tx ision. ing d Intctin, 9, Artil 101847. hp:/i.g/10.1016/.ninsc.2023.101847   
Jen, J.2021).n  f t   eprie atv.e t ng 1-26. https://doi.org/10.1080/09588221.2021.2021241   
Jen, J. 2023).-sst  A) f  a  a. st n 367 38-1364. https://doi.org/10.1080/09588221.2021.1987272   
Je    k t -h  ues & Education, 104898. https://doi.org/10.1016/j.compedu.2023.104898   
Je   teera of large language models. Interactive Learning Environments, 1-19. https://doi.org/10.1080/10494820.2023.2204343   
ndal    3. learning (pp. 15696-15707). PMLR.   
Klkaya, . 2020). ing a chatbot, Ri, to prctice writing though vertions in 2 nglish:  ce stdy. In M ru, &M. Ptron (d.), ew thgpion fr   d  n d h . 1238. al. h/.g.018/978179925913.c01.   
Kim J, hi ,y  .. 2020 ie a  . n P o  ioe on computational linguistics (pp. 2284-2295). https:/doi.org/10.18653/v1/2020.coling-main.207   
Km A , . 2024) w mlm a ts   frg gr iln toa n ystrtie 103256. https://doi.org/10.1016/j.system.2024.103256   
a-Miic, Vin  i,  011- ion   ion t g stle identification. Computers & Education, 56(3), 885-899. https://doi.org/10.1016/j.compedu.2010.11.001   
o, .ti n.i.g 10.1080/09588221.2022.2118783   
won, ., h, D   2023). e apii of ch a  2 w prtie t.   d g, 271), 119 10125/73541.   
, in Neural Information Processing Systems, 33, 9459-9474. & Society, 23(1), 78-92. https://www.jstor.org/stable/26915408.   
Lig, . Ellt,  rste, J.., caffey,  Mar, . & Hma. 2021). witing mivatio  altion std f el-jmn and performance. Assessing Writing, 48, Article 100509. https://doi.org/10.1016/j.asw.2020.100509   
Li, C.    (2   t  d ts t   ues & Education, 189, Article 104576. https://doi.org/10.1016/j.compedu.2022.104576   
Lo C.K (2023).What is the impact of ChtG n ectio?  raid ie of th litre. ction sciees, 13(4), 410. hps/oi.org/10.3390/ educsci13040410   
illicd 2).d   -ag rh i  sh im  g and Instruction, 75, Article 101492. https://doi.org/10.1016/j.learninstruc.2021.101492   
ontnda d , d  - 2023) ct f th i  Ch in ion systematic review. Computers, 12(8), 153. https://doi.org/10.3390/computers12080153   
g D.    . 21) ie , Article 100041. https://doi.org/10.1016/j.caeai.2021.100041   
Ou, A.., tohr, ., & Mamstr,  (2024). Ac concatio with I-werd angge t in hger tion: rom a st-hmanst eretie. System, 121, Article 103225. https://doi.org/10.1016/j.system.2024.103225 learning of EFL. Computer Assisted Language Learning, 35(3), 469-491. htps://doi.org/10.1080/09588221.2019.1705354   
Pic  f  f  rc    n p 451-502). Academic Press.   
Raman e,  23).  n a  i t,  . , 9, 578 /i.g 10.3390/app13095783   
Sh,D   2i  -      1 doi.org/10.1177/00336882231221902   
Shuster,  ff  ,  Kla t . (2021iel i  aion n inng f th r Computational Linguistics: EMNLP, 2021, 3784-3803. https://doi.org/10.18653/V1/2021.FINDINGS-EMNLP.320   
Slaj,     17tt     1-2) 64-90. https://doi.org/10.1080/09588221.2016.1242502   
Sng   23. n l ii  . Frontiers in Psychology, 14, Article 1260843. https://doi.org/10.3389/fpsyg.2023.1260843   
Su,    23.a   tie ting 57 l1075/.106/. asw.2023.100752   
Wight   .   019 l  r th  wing-e, ad ie  mf nts motivation toward writing. Assessing Writing, 39, 64-78. https://doi.org/10.1016/j.asw.2018.12.004   
e,  h  g,   g, . 019).d at i  tiv/lid  tc revie o journal publictions from 2007 to 2017. Computers & Education, 140, Article 103599. htps://doi.org/10.1016/j.compedu.2019.103599   
Yan . 2023)   wn ti ton e, 11 1343-1396. https://doi.org/10.1007/s10639-023-11742-4 motivation. Interactive Learning Environments, 1-18. https://doi.org/10.1080/10494820.2023.2220374   
hng,   03h n  l  iwi   114 https://doi.org/10.1080/17501229.2023.2197417   
Zou ., , u   2023).i dis     h  sh aatie writing. System, 118, Article 103141. https://doi.org/10.1016/j.system.2023.103141