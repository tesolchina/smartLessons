# Linguistic Features Distinguishing Students’ Writing Ability Aligned with CEFR Levels

Hong Ma1, , Jinglei Wang2 , and Lianzhen He1, \*,

1 Institute of Applied Linguistics, Zhejiang University, 866 Yuhangtang Road, Hangzhou, 310058, China   
2 School of International Studies, Zhejiang University, Hangzhou, Zhejiang Province, 310058, China

\* E-mail: hlz@zju.edu.cn

Lianzhen He is currently a full professor at the Department of Linguistics, School of International Studies, Zhejiang University, China. Her primary research interests are language testing and corpus linguistics. Address for correspondence: Institute of Applied Linguistics, Zhejiang University, 866 Yuhangtang Road, Hangzhou, 310058, China. <hlz@ zju.edu.cn>

A substantive body of research has been revolving around the linguistic features that distinguish different levels of students’ writing samples (e.g. Crossley and McNamara 2012; McNamara et al. 2015; Lu 2017). Nevertheless, it is somewhat diffcult to generalize the fndings across various empirical studies, given that different criteria were adopted to measure language learners’ profciency levels (Chen and Baker 2016). Some researchers suggested using the Common European Framework of Reference for Languages (CEFR) (Council of Europe 2001) as the common standard of evaluating and describing students’ profciency levels. Therefore, the current research intends to identify the linguistic features that distinguish students’ writing samples across CEFR levels by adopting a machine-learning method, decision tree, which provides the direct visualization of decisions made in each step of the classifcation procedure. The linguistic features that emerged as predicative of CEFR levels could be employed to (i) inform L2 writing instruction, (ii) track long-term development of writing ability, and (iii) facilitate experts’ judgment in the practice of aligning writing tests/samples with CEFR.

# Introduction

The identifcation of linguistic features that predict students’ writing quality has attracted great attention from researchers in language teaching and testing (Ortega 2003; Casal and Lee 2019). Empirical studies have yielded a series of linguistic features that reliably predict learners’ writing quality (Ortega 2003, 2015; Casal and Lee 2019). These features include but not restricted to lexical complexity (Kyle and Crossley 2016; Lee et al. 2021), syntactic complexity (Lu 2011; Khushik and Huhta 2020; Latif and Gierl 2021; Lee et al. 2021), lexical diversity (Crossley and McNamara 2012; McNamara et al. 2015), word frequency (Crossley and McNamara 2012; Guo et al. 2013), word meaningfulness (Crossley and McNamara 2012), and word familiarity (Crossley and McNamara 2012; Guo et al. 2013).

Nevertheless, it is diffcult to draw a clear conclusion, since students’ profciency levels were measured by different standards (e.g. in-house assessment and standardized tests) (Chen and Baker 2016). In light of this, the current research endeavored to identify linguistic features that distinguish writing qualities of L2 learners, whose profciency was aligned with Common European Framework of Reference for Languages (CEFR) levels, one of the most infuential and widely accepted language frameworks. Specifcally, a machine-learning method, decision tree, was adopted to model the stratifcation of CEFR levels by drawing upon a large-scale learner corpus, EF-Cambridge Open Language Database (EFCAMDAT), and a computational linguistic tool, Coh-Metrix. The linguistic features identifed as distinguishing CEFR levels of students’ writing, could be utilized to (i) inform L2 writing instruction; (ii) track long-term development of students’ writing performance; and (iii) facilitate experts’ judgment in the practice of aligning writing tests with CEFR. The following review focuses on three key components of previous studies: the linguistic features that have been frequently investigated, the measures of L2 learners’ profciency levels, and statistical modeling methods adopted in previous research.

# Linguistic features in L2 writing

The linguistic features that discriminate students’ writing profciency levels have been examined extensively (Khushik and Huhta 2020). The majority of these studies rely on measures of linguistic features obtained through automated writing analyzing tools, such as Coh-Metrix (Graesser et al. 2004), the Writing Analysis Tool (McNamara et al. 2013), the Linguistic Inquiry and Word Count (Pennebaker et al. 2007), the Computerized Language Analysis program (MacWhinney 2000), L2 Syntactic Complexity Analyzer (Lu 2011), and Biber Tagger (Biber et al. 1999). Different from systems targeting the lower-level traits (e.g. the mechanics, grammar, and spelling), these computational tools powered by natural language processing technology are able to evaluate deeper aspects of writing, thus contributing more to the improvement of writing quality (Graham and Perin 2007; McNamara et al. 2015). The previously investigated deeper-level linguistic features mainly include syntactic complexity, lexical sophistication, and cohesion.

Syntactic complexity. For the past two decades, linguistic complexity has drawn great attention from the felds of second language acquisition and L2 writing research (Wolfe-Quintero et al. 1998; Ortega 2003; Khushik and Huhta 2020). Bulté and Housen (2012) emphasized the importance of considering three levels of linguistic complexity in investigating aspects of linguistic complexity: theoretical, observational, and operational. At the operational level, syntactic complexity is considered as a multidimensional construct that generally encompasses ‘global (e.g. mean length of T-unit), clausal (e.g. subordinated or coordinated clauses per T-unit), and phrasal constructs (e.g. mean length of clause, complex nominals per clause)’ (Casal and Lee 2019: 52).

The relationship between syntactic complexity and profciency level has gradually emerged as an important topic in language learning research (Ortega 2003; McNamara et al. 2010; Lu 2011; Guo et al. 2013; Kyle 2016), and has been mainly investigated through longitudinal and cross-sectional designs (Crossley and McNamara 2014). As a typical example of a longitudinal study, Crossley and McNamara (2014) examined the syntactic development of university-aged L2 learners of English across one semester of intensive writing instruction. Their fndings suggested that syntactic features produced by these L2 writers were gradually aligned with academic writing (i.e. more nouns and phrasal complexity). Examining Chinese English as a foreign language (EFL) learners’ writing over a 1-year period, Wang (2022) identifed different developmental paths for syntactic features. To be specifc, the use of prepositional phrases and relative clause in learner writing increases, adverbial adjective signifcantly decreases, and adverb and subordinate frst increased and then declined, suggesting a spiral developmental path for these linguistic features.

Due to the comparative easiness of data collection, more cross-sectional studies have also been conducted to shed light on the connection between syntactic features and language profciency (Ortega 2003; Lu 2011). For instance, by analyzing ESL (English as a Second Language) writing samples from the Written English corpus of Chinese learners (Wen et al. 2005), Lu (2011) found that several measures predicted profciency levels positively (e.g. mean length of clause, mean length of T-unit, coordinate phrases per clause, and coordinate phrase per T-unit), while profciency level was negatively predicted by three measures (i.e. clause per sentence, dependent clauses per clause, and dependent clauses per T-unit). Besides, other fndings indicated that higher-level writing contain more coordinate phrases per clause (Yang et al. 2015), post-noun-modifying prepositional phrases (Taguchi et al. 2013), embedded clauses (Guo et al. 2013), modifers per noun phrase (Guo et al. 2013), and preposition phrases (Aryadoust and Liu 2015).

Lexical sophistication. Lexical sophistication, referring to ‘the learners’ use of sophisticated and advanced words’ (Kim et al. 2018: 121), has been the focus of applied linguistics, psycholinguistics, and computational linguistics (Kim et al. 2018). Applied linguistic research has found that sophisticated words generally demonstrate a low frequency of occurrences (Laufer and Nation 1995). One of the key concerns of extracting word frequency is to distinguish different meanings of the same word form. Very recently, Lu and Hu (2022) proposed three frequency-based lexical sophistication indices to account for meaning variations of polysemous words.

From the psycholinguistic perspective, sophisticated words are less concrete, less imageable, and less familiar (Kim et al. 2018). Words that denote objects, materials, or people generally receive higher concreteness scores, which can be extracted from the Medical Research Council (MRC) Psycholinguistic Database. This database was constructed by requiring native participants to rate the concreteness of specifc words on a 1–7 scale. The imageability (i.e. the easiness of constructing a mental image of a word; Wilson 1988) and familiarity (i.e. the commonness of encountering a word; Wilson 1988) indices included in the MRC database were also obtained by following a similar procedure, where participants rated the imageability of and their familiarity with a given word respectively following a 1–7 interval scale (Paivio et al. 1968; Toglia and Battig 1978).

The relationship between lexical sophistication and students’ essay quality has been investigated in many empirical studies, with different indices of lexical sophistication being operationalized through computational methods (Guo et al. 2013; Kyle and Crossley 2016). For instance, to investigate linguistic features that predict TOEFL iBT (Test of English as a Foreign Language, Internet-based test) writing scores, Guo et al. (2013) suggested that word familiarity and word frequency are negatively associated with students’ performance in integrated writing task, while independent essay scoring was positively predicted by average syllabus per words and noun hypernymy values. Also examining linguistic features relating to integrated writing and independent writing separately, Kyle and Crossley (2016) found that hypernym, imageability, and bigram frequency were predictive of students’ performance in both writing tasks. Although? different lexical sophistication indices were incorporated in empirical studies, the predictive power of lexical sophistication has been underlined repeatedly.

Cohesion. Cohesion refers to ‘the presence or absence of linguistic cues in text that allow the reader to make connections between the ideas in the text’ (Crossley et al. 2016: 2). Cohesion has generally been maintained through local, global, and text-cohesive devices (Crossley et al. 2016). Overlapping words and concepts between sentences and connectives can be categorized as local cohesion cues (Halliday and Hasan 1976), while global and text cohesion cues refer to semantic and lexical overlap occurring between paragraphs (Foltz 2007) and across the whole text, respectively (Crossley et al. 2016).

Longitudinal research on cohesive devices in L1 writing has generally observed a developmental path that moves from the use of local cohesion devices (e.g. referential pronouns and connectives) to lexical overlap (i.e. repetition between words and concepts), and to complex syntactic constructions that implicitly connect ideas (i.e. modifcations and embeddings) (McCutchen and Perfetti 1982; Haswell 2000; Crossley et al. 2016). Consistently, studies relying on computational tools and statistical modeling suggested that local cohesion cues are either unrelated to or negatively correlated with writing quality, while global cohesion cues tend to contribute to writing quality positively (Crossley et al. 2016).

Research on L2 writing, nevertheless, yielded more mixed fndings. For instance, Crossley and McNamara (2012) found that higher-scored essays produced by Hong Kong high school students included less lexical overlap than lower-scored essays. Nevertheless, lexical overlap between paragraphs were found to be positively related to essay quality for both source-based and independent writing tasks in more recent research (Kim and Crossley 2018). Also examining integrated and independent essays separately, Guo et al. (2013) reported that cohesive devices, including semantic similarity, noun overlap, and tense repetition were positively related to essay quality. Other cohesive devices (i.e. aspect repetition, content word overlap, and conditional connectives) actually were negatively related to essay quality.

# Measures of students’ writing ability

In spite of the considerable amount of studies investigating linguistic features predictive of students’ writing ability, it was somewhat diffcult to compare their fndings and make further generalizations, since students’ writing profciency in these empirical studies was operationalized using different criteria (Chen and Baker 2016). Thomas (1994) summarized four different measures of L2 writing profciency in second language acquisition research: impressionistic judgement, institutional status, in-house assessment instrument, and standardized test. Among these measures, in-house assessment instrument and standardized tests have been more widely adopted in empirical studies on linguistic features that discriminate students’ writing profciency levels.

As a typical study that adopted in-house assessment instrument to assess L2 writing profciency, Crossley and McNamara (2014) collected writing samples from 57 L2 writers at Michigan State University and rated their writing quality using a composition grading scale that encompasses fve different analytical features (i.e. content, organization, vocabulary, language use and mechanics). Similarly, students’ essays in Casal and Lee (2019) were extracted from the Corpus of Ohio Learner and Teacher English, a large corpus of ESL students’ writing, and were graded by course instructors as A to F. The essays examined in Aryadoust and Liu (2015) were evaluated using an analytical scoring rubric, a product of Pearson Education Inc., targeting six writing traits—ideas, word choice, organization, voice, conventions, and sentence fuency—on a six-point scale (1–6).

Besides in-house assessment, more studies adopted grades in standardized tests to differentiate their profciency levels. For instance, Guo et al. (2013) collected 240 students’ responses to the integrated writing task and independent writing task in one administration of TOEFL iBT. The rubrics were provided by ETS (Educational Teaching Service) specifying fve levels of writing performance (1–5 points) for each writing task. The rubric for the integrated task focuses on ‘accurate and coherent presentation of the extracted information in the essays in addition to grammatical accuracy’ (Guo et al. 2013: 223), while the independent scoring rubric emphasized lexical and syntactic complexity, coherence, and grammatical accuracy (Guo et al. 2013). The TOEFL rubrics have been adopted in many other similar studies to measure learners’ writing quality, such as Kyle and Crossley (2016) and Kim and Crossley (2018). In McNamara et al. (2015), student essays were scored by using a six-point holistic rating scale developed for the SAT (Scholastic Aptitude Test), which determines writing quality based on general essay properties, such as sophisticated vocabulary and evidence-based reasoning. Crossley and McNamara (2012) investigated essays written by graduating Hong Kong high school students for the Hong Kong Advanced Level Examination. These essays were assigned six grade levels by trained raters from the Hong Kong Examinations and Assessment Authority.

Even though a substantial body of research has examined the relationship between linguistic features and learners’ writing profciency, ‘the lack of a common standard for determining learner profciency still makes it diffcult, if not impossible, to generalize across research results’ (Chen and Baker 2016: 851). The recent practice of adopting CEFR levels as the evaluative criteria for learners’ writing profciency may provide a solution to the issue of non-uniform evaluation criteria (e.g. Hawkins and Filipovic ́ 2012). The CEFR, dividing foreign language competences in six profciency levels ( $\mathrm { A } 1 / \mathrm { A } 2 =$ basic user; $\mathrm { B } \mathrm { 1 / B 2 = }$ independent user; and $\mathrm { C 1 } / \mathrm { C 2 } =$ profcient user), is one of the most infuential language frameworks in language education (Chen and Baker 2016). ‘However, only some studies have operationalized language profciency with reference to the

CEFR’ (Khushik and Huhta 2020: 509). The current research, therefore, intends to identify linguistic features that distinguish students’ writing ability aligned with CEFR levels.

# Statistical modeling methods

To shed light on the interplay between linguistic features in language learners’ writing performance and their writing profciency levels, various methods have been implemented, ranging from in-depth analysis from corpus linguistic perspective (Chen and Baker 2016), to corpus-based dimensional analysis (Friginal and Weigle 2014; Biber et al. 2016; Kim and Nam 2019), and to quantitative analysis realized through statistical modeling (Espada-Gustilo 2011; Crossley and McNamara 2012).

More pertinent to the current project, a line of research investigated linguistic features that predict students’ writing profciency by regressing a batch of quantitative measures of linguistic features against indices of students’ writing quality. Linguistic features that demonstrated statistical signifcance emerged as the key features with greater pedagogical value (Crossley and McNamara 2012; McNamara et al. 2015; Lu 2017). The statistical methods adopted mainly included multiple linear regression (Guo et al. 2013; Kyle and Crossley 2016), discriminant function analysis (Crossley et al. 2011; McNamara et al. 2015), and Random Forest (Latif and Gierl 2021). Compared to multiple linear regression, the machine-learning-based approaches (i.e. discriminant function analysis and Random Forest) are generally able to yield higher accuracy in classifcation tasks, especially when there exists high-order interaction effects and multicollinearity between dependent variables of large dataset (Strobl et al. 2009; Zhai et al. 2021).

Even though multiple linear regression and Random Forest function similarly to identify linguistic features that signifcantly predict students’ writing quality across profciency levels, they are not effcient in disclosing the specifc features that distinguish adjacent levels. To serve this specifc purpose, some studies used Multiple Analysis of Variance (MANOVA) tests instead (Casal and Lee 2019; Khushik and Huhta 2022). For instance, to examine differences between high-, mid-, and low-quality source-based research papers in terms of fve measures of syntactic complexity (i.e. mean length of T-unit, T-unit/sentence, clauses/T-unit, words/clause, complex nominals/clause), Casal and Lee (2019) conducted a one-way MANOVA with post-hoc Tukey test. Similarly, Khushik and Huhta (2022) used MANOVA tests to uncover the differences between learners located at different CEFR levels in count variables and syntactic complexity variables. Nevertheless, the MANOVA test may not be very practical for the current research given the large number of independent variables involved (i.e. 108 linguistic indices extracted from Coh-Metrix). Especially the number of post hoc tests can be overwhelming.

The discriminant function analysis, utilized successfully by McNamara et al. (2015) in computing essays scores and identifying linguistic features that distinguish adjacent levels, is a parametric test that needs to meet some important assumptions: ‘(i) the independent variables are at least interval or ratio scale variables; (ii) the predictor variables are independently and randomly sampled from a population having a multivariate normal distribution, and (iii) the assumption of equal variance/covariance matrices’ (Albayrak 2009: 117). ‘Even though the violation of these assumptions may not reduce its general predictive power’ (Hair et al. 1998: 259), the classifcation error of some groups might be increased (Lachenbruch 1967), thus may affect the distinguishing variables identifed. Therefore, the current study intends to adopt decision tree, a non-parametric classifcation technique that does not assume any distribution of the data (Karel et al. 2004), is not sensitive to strong correlations among explanatory variables (Tabachnick and Fidell 1996), and has been reported with a higher accuracy in classifcation tasks than the discriminant function analysis (Karel et al. 2004; Albayrak 2009).

The aforementioned studies have established a classical agenda of investigating the relationship between linguistic features in language production and profciency. Nevertheless, the majority of these studies adopted different measures of writing profciency, leaving it diffcult to generalize across research fndings. Therefore, the current study intends to reveal linguistic features that signifcantly predict students’ writing quality aligned with CEFR levels, one of the most widely accepted language frameworks. Specifcally, the following research question is answered:

What are the linguistic features that distinguish adjacent levels of students’ writing ability aligned with CEFR levels?

# Methodology

# Materials

EFCAMDAT. L2 writing samples analyzed in the current research were extracted from the EFCAMDAT (Geertzen et al. 2013; Huang et al. 2018), which contains over 83 million words from 1 million assignments written by 174,000 learners worldwide and submitted to Englishtown, the online school of EF Education First. The learners’ nationalities range from Brazilians, Chinese, Mexicans, Russians, and Germans, to Saudi Arabians, Italians, French, and Japanese (Huang et al. 2017). Information on nationality was adopted to approximate learners’ L1 backgrounds. All students’ writing samples were aligned with six CEFR levels (A1, A2, B1, B2, C1, and C2 following an ascending order of English profciency) on the basis of students’ performance in a placement test administered before they start a course at EF or through progression of coursework.

Coh-Metrix. The computational linguistic tool, Coh-Metrix (Graesser et al. 2004), was adopted in the current research to provide ‘multiple measures of language and discourse that are aligned with multilevel theoretical frameworks of comprehension’, including words, syntax, textbase, situation model, and genre and rhetorical structure (Graesser et al. 2011: 223). Quantitative measures of words can be categorized into parts of speech, word frequency, psychological ratings (e.g. age of acquisition for words, meaningfulness, imageability, etc.), and semantic content (e.g. causal and intentional verbs). Measures of syntax, such as frequency of passive voice and syntactic similarity, are realized on the basis of part-of-speech categories. Textbase mainly consists of co-reference (including content word overlap, noun overlap, argument overlap, and stem overlap), lexical diversity (alternative measures of type-token ratio controlling for text length), and conceptual overlap between sentences measured through latent semantic analysis (LSA), a statistical method for capturing word knowledge. The situation model proposed by Zwaan and Radvansky (1998) encompasses fve dimensions (i.e. causation, intentionality (goals), time, space, and protagonists) that are applicable to narrative and informational texts. Coh-Metrix detects whether signaling devices are used to facilitate comprehension when discontinuity of one or more of these dimensions occur. Specifc to genre, Coh-Metrix could provide measures in terms of whether a text is more of a narrative or an informational text (Graesser et al. 2004).

The measures provided by Coh-Metrix have been validated by many empirical studies (e.g. Hempelmann et al. 2005; Crossley et al. 2008; 2011; McNamara et al. 2010; Polio and Yoon 2018), supporting the adoption of Coh-Metrix as a powerful tool for quantitative discourse analysis. For example, McNamara et al. (2010) identifed signifcant differences between high and low-cohesion texts in terms of indices measured by Coh-Metrix (e.g. descriptive and readability statistics, coreference, LSA). Crossley et al. (2008) found that the Coh-Metrix readability demonstrated noticeably higher correlation value with mean cloze scores than the other readability measures (e.g. Flesch reading ease, Flesch-Kincaid grade level, Bormuth formula, Dale-Chall formula, and Miyazaki EFL index). By adopting discriminant function analysis, lexical indices measured by Coh-Metrix were also found to be predicative of L2 learners’ writing profciency categorized as beginning, intermediate, and advanced groups (Crossley et al. 2011). Polio and Yoon (2018) found that measures of Coh-Metrix were suffcient to demonstrate the progression of syntactic complexity from the narrative essays to the argumentative. From a technical perspective, Hempelmann et al. (2005) justifed the integration of Charniak’s Parser into Coh-Metrix by comparing the competitiveness of free parsers, including Apple Pie, Charniak’s Parser, Collins’ (Bikel’s) Parser, and Stanford Parser, in terms of several different dimensions, such as performance, robustness, tagging facility, stability, and length of input they can handle. It can be deduced that all the above fndings provided supportive evidence of the validity of Coh-Metrix.

# Procedure

Sampling. The stratifed random sampling procedure was adopted in this research to extract L2 learners’ writing samples from each Englishtown level of the EFCAMDAT, resulting in a model constructing set with 150 writing samples at each CEFR level (900 essays in total), and a validation set, with 100 writing samples at each CEFR level (600 essays in total). A total of 1,500 essays were extracted from the EFCAMDAT. Table 1 presents the number of writing samples selected from each CEFR and Englishtown level. The specifc composition of the dataset is presented in Table 2.

Analyzing automatically. All writing samples (1,500 essays) were automatically analyzed using CohMetrix, which outputs measures for 106 linguistic features that are organized into eleven broad categories, including descriptive indices, text easability principal component scores, referential cohesion, LSA, lexical diversity, connectives, situation model, syntactic complexity, syntactic pattern density, word information, and readability. These measures were used as the dependent variables in the modeling procedure. Since accuracy issues in students’ writing samples may affect the analysis by Coh-Metrix, we edited grammar (e.g. subject–verb agreement, run-on sentences, and verb form issues) and spelling errors (including homonymies) in students’ writing samples before submitting them to Coh-Metrix.

Constructing the tree. The quantitative measures of linguistic features obtained from Coh-Metrix were modeled against the CEFR levels for the model constructing set (900 essays) with the CEFR levels as the dependent variable and the linguistic indices as the predictor variables. The decision tree, a machine-learning approach to data mining, was adopted since (i) machine-learning approaches have generally yielded higher accuracy in classifcation tasks than traditional regression models (McNamara et al. 2015); and (ii) the decision tree method has demonstrated high interpretability by articulating the most important index (a linguistic index in this case), on the basis of which further partition is performed (Song and Liu 2015).

The decision tree algorithm was performed using R 4.1.3 with the aid of the imported package titled as ‘recursive partitioning and regression trees’. In this modeling procedure, cross-validation was adopted to evaluate and develop the algorithm. In a K-fold cross-validation procedure, the data is randomly divided into K approximately equal sets. Each set (i.e. the testing set) is used to evaluate the model built by drawing upon other K-1 sets (i.e. the training set), and this process iterates for K times (Latif and Gierl 2021). The current project adopted 10-fold cross-validation, since ‘the classifcation performance often does not improve when the number of splits exceeds $1 0 ^ { \prime }$ (Witten et al. 2011: 153). The algorithm was frst built by ftting decision tree algorithm to the training set with each partition realized by selecting the predicting variable that yields the maximum Gini index. The minimal-cost complexity pruning method was then applied to remove the superfuous nodes and branches by adjusting the complexity parameter $\propto$ on the basis of the 10-fold cross-validation (Prodromidis and Stolfo 2001).

As shown in Figure 1, when the complexity parameter $\propto$ equals 0.008, the 10-fold cross-validation error reached to its minimum (0.442), resulting in a decision tree (accuracy $=$ .679) with 15 nodes, ‘the fnal result of a combination of decisions and events’ (Song and Liu 2015: 131), and 10 important predicting linguistic indices.

Table 1: The number of writing samples extracted from each level of Englishtown and CEFR   

<html><body><table><tr><td>Englishtown levels.</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td><td>Total</td></tr><tr><td>Model constructing set (900)</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>150</td><td>900</td></tr><tr><td>Validation set (600)</td><td>33</td><td>33</td><td>34</td><td>33</td><td>33</td><td>34</td><td>33</td><td>33</td><td>34</td><td>33</td><td>33</td><td>34</td><td>33</td><td>33</td><td>34</td><td>100</td><td>600</td></tr><tr><td>CEFR levels</td><td>A1</td><td></td><td></td><td>A2</td><td></td><td></td><td>B1</td><td></td><td></td><td>B2</td><td></td><td></td><td>C1</td><td></td><td></td><td>C2</td><td>1500</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">CEFR levels</td><td colspan="2">Word length</td><td rowspan="2">Nationalities</td><td rowspan="2"></td><td rowspan="2">Topics</td></tr><tr><td></td><td>Min Mean Max.</td></tr><tr><td>A1</td><td>22</td><td>48.9</td><td>112</td><td>br (108), cn (58), mx (23), fr (11), de (11), ru (11), sa (8), us(6), jp (5), it (3), ae (1), gb (1), cm (1), pt (1), co (1), eg (1), tr (1)</td><td>Describing your favorite day (15) Giving suggestions about clothing (15) Describing your family in an email (15) Introducing yourself by email (15) Taking inventory in the office (15) Signing up for a dating website (15) Describing your family&#x27;s eating habits (15) Renovating your home (15) Writing a weather guide for your city (15) Writing your online profile (15) Asking a friend to shop for you (14) Making a dinner party menu (14) Making notes for a visitor (14) Writing a birthday invitation (14)</td></tr><tr><td>A2</td><td>39</td><td></td><td></td><td>82.1 268 br (82), cn(67), ru(15), mx (14), de (13), Writingaresume (16) Summarizinga story(16) it (9), sa (9), us (6), fr (4), kr (4), jp (3), ng (3), tr (2), vn (2), es (2), sv (2), in (2),.</td><td>Replying to a new penpal (12) Choosing a birthday present (12) Telling someone what you&#x27;re doing (10) Updating your online profile (10) Writing an autobiography (15) Complaining about chores (15) Describing people in photos (15) Writing a description of your family (15) sk (1), sy (1), ua (1), gb (1), no (1), eg (1). Giving instructions to a house sitter (15) Writing on the family blog (15) Planning to attend a music festival (15) Rescheduling an appointment (15) Giving cultural tips to a visitor (13) Writing a party invitation (13)</td></tr><tr><td>B1</td><td>47</td><td></td><td></td><td>102.3 224 cn(81),br(49),mx(29),fr (22),de(20), it (12), ru (8), ch (8), sv (7), uy (5), tr (3).</td><td>Writing a holiday postcard (12) Complaining about a meal (12) Creating an office dress code (12) Writing a movie plot (12) Reviewing a song for a website (16) Giving feedback to a restaurant (16) Posting comments on a celebrity forum (15) Writing a job advertisement (15) kz (2), kr (1), sa (1), us (1), om (1), gb (1) Giving instructions to play a game (15) Signing a waiver to go skydiving (15) Writing about a memorable experience (15) Studying online (15) Claiming back your security deposit (15) Making a business proposal (15) Writing a natural remedies pamphlet (14) Planning for the future (12) Issuing a written warning (12) Selling items in an online auction (12)</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">CEFR</td><td colspan="2">Word length</td><td colspan="2">Nationalities</td><td>Topics</td></tr><tr><td>levels</td><td>Min Mean Max</td><td></td><td></td><td></td></tr><tr><td>B2</td><td>70</td><td></td><td>143.3 277</td><td>br (83), cn (44), de (38), fr (37), it (9), ruWriting up survey findings (17) Giving feedback about a colleague (17) (8), sa (8), mx (7),jp (4), es (3), uy (2), kr Doing a survey about discrimination (15) Presenting trends (15) (2), pl (2), co (1), kz (1), kg(1)</td><td>Writing a movie review (15) Improving your study skills (15). Conducting a performance appraisal (15) Attending a leadership course (15) Writing a report on staff satisfaction (15) Writing an apology note (15). Setting rules for social networking (14) Describing a terrifying experience (14) Finding a home for a wealthy client (12) Buying a painting for a friend (12)</td></tr><tr><td>C1</td><td>91</td><td></td><td></td><td>175.7 314 br(78),de(69),fr(16),ru(16),cn(14), es (10), sa (8), tr (7),jp (6), ch (5), it (4), at (4), us (4), mx (3), pr (3), co (1), ae</td><td>Turning down an invitation (10) Helping a coworker deal with a phobia (10) Writing about a disaster relief effort (21) Interpreting a prophecy (21)Writing an ari about NLP techniques (21) Covering a news story (21)Writing about future lifestyles Applying for sponsorship (21) Attending a seminar on stress reduction (21) Reaching your potential (21) Talking a friend out of a risky action (21) Writing advertising copy (21)</td></tr><tr><td>C2</td><td>44</td><td></td><td>177.4 379</td><td>br (59), de (40), it (30), ru (19), cn (19), fr(10), at (9), mx (8), kr (6), es (5), sg (5), pr (4), bo (4), ch (4), pt (4), ae (4), lv</td><td>Choosing a renewable energy source (20) The entertainment industry (20) Following a code of ethics (55) Attending a robotics conference (55) Writing about a symbol of your country (51) Researching a legendary creature (46)</td></tr></table></body></html>

![](img/c15e2417d844355cac391a76bc1d7b34d38d7ef72fb7a1c31be80e28494b28dd.jpg)  
Figure 1: The relationships between number of trees and the error.

Validating the tree. The validation set (600 essays) was imported into the decision tree constructed in the previous step. The agreement between the CEFR levels computed by the decision tree algorithm and the CEFR levels offcially published was considered as the prediction accuracy of the decision tree algorithm (accuracy $=$ .637, adjacent accuracy $=$ .90), further confrming the predicting power of the tree and the predictive linguistic features identifed.

# Results and discussion

Ten linguistic features were found to be predictive of students’ writing quality across CEFR levels on the basis of the decision tree constructed (exact accuracy $=$ .637, adjacent accuracy $=$ .90) (shown in Figure 2), with word count and age of acquisition (AoA) for content words adopted recursively for 4 and 2 times, respectively. These 10 features (the nodes in Figure 2) belong to fve categories: descriptive (word count), word information (AoA for content words, CELEX (Center for Lexical Information) word frequency for content words, frst-person singular, pronoun incidence, and third-person plural pronoun), syntactic pattern density (preposition phrase density), text easability (deep cohesion), and situation model (WordNet verb overlap, intentional verb incidence). Table 3 presents the descriptive statistics of these features. The terminal nodes (T node in Figure 2) indicate fnal decisions on specifc levels with no further partition.

Different from statistical methods adopted in previous research that compute the weights of linguistic indices on the complete model, decision tree calculates the contribution of linguistic indices at each partition, thus disclosing the specifc linguistic features and criterion adopted to distinguish adjacent or near adjacent levels.

As presented in Figure 2, students’ writing samples were divided into two sets at node 1 based on word count: (i) essays at level B1 and lower (word count $< 1 1 0 . 5$ ) and (ii) essays at level B2 and higher (word count $\geq 1 1 0 . 5 )$ . At node 2, the frst set of essays with word count less than 50.0 was recognized as level A1 directly, while longer essays $5 0 . 5 \leq$ word count $< 1 1 0 . 5$ ) were further divided at node 4 into essays with AoA for content words lower than 303.9 and those with AoA for content words at 303.9 and higher. At node 7, essays with frst-person singular index at 152.9 and higher were tagged as level A1. Essays with frst-person singular index lower than 152.9 were further partitioned at node 11, where essays with word count less than 79.5 were categorized as level A2. Essays with word count at 79.5 and higher were classifed as A2 (when the intentional verb index was higher than or equal to 27.92) and B1 (when the intentional verb index was lower than 27.92) (see node 15 in Figure 2).

![](img/25b441b5543f17bc1cc1723a6af657b43710c6a027e8ac14ad0698927ea9c61d.jpg)  
Figure 2: The decision tree of assigning students’ writing to CEFR levels.

Table 3: The descriptive statistics of discriminating linguistic features   

<html><body><table><tr><td>Nodes</td><td>Features</td><td>Pearson r</td><td>A1 M (SD)</td><td>A2 M (SD)</td><td>B1 M (SD)</td><td>B2 M (SD)</td><td>C1 M (SD)</td><td>C2 M (SD)</td></tr><tr><td colspan="9">Text descriptive</td></tr><tr><td>1,2,3, 8, and 11</td><td>Word count</td><td>.84</td><td>42.39 (19.15)</td><td>76.17 (27.31)</td><td>96.42 (28.02)(31.27)</td><td>137.42</td><td>170.02 (28.62)</td><td>170.04 (33.61)</td></tr><tr><td colspan="9">Text easability</td></tr><tr><td>14</td><td>Deep cohesion, z score</td><td>.43</td><td>1.79 (1.49)</td><td>-.41 (1.73)</td><td>.23 (1.58)</td><td>.26 (1.33)</td><td>.73 (1.40)</td><td>.55 (1.22)</td></tr><tr><td colspan="9">Situation model</td></tr><tr><td>6</td><td>WordNet verb overlap</td><td>.15</td><td>.47 (.27)</td><td>.54 (.20)</td><td>.52 (.14)</td><td>.53 (.13)</td><td>.49 (.10)</td><td>.61 (.11)</td></tr><tr><td>15</td><td>Intentional verbs</td><td>-.21</td><td>23.87 (25.13)</td><td>39.00 (24.17)</td><td>22.01 (16.50)</td><td>18.15 (12.61)</td><td>21.79 (14.34)</td><td>17.93 (10.97)</td></tr><tr><td colspan="9">Syntactic pattern density</td></tr><tr><td>12</td><td>Preposition phrase density</td><td>.39</td><td>69.78 (48.69)</td><td>85.64 (41.17)</td><td>90.77 (31.46)</td><td>96.63 (28.16)</td><td>97.68 (23.94)</td><td>122.38 (26.81)</td></tr><tr><td colspan="9">Word information</td></tr><tr><td>4 and 9</td><td>AoA for content words</td><td>.51</td><td>247.15 (68.33)</td><td>285.96 (56.91)</td><td>338.75 (53.12)</td><td>355.84 (47.99)</td><td>335.45 (35.98)</td><td>346.67 (28.45)</td></tr><tr><td>7</td><td>First-person singular pronoun</td><td>-.56</td><td>107.42 (79.81)</td><td>75.91 (53.64)</td><td>57.16 (52.54)</td><td>28.92 (36.29)</td><td>20.62 (30.31)</td><td>8.66 (12.98)</td></tr><tr><td>10</td><td>Pronoun incidence</td><td>-.41</td><td>141.43 (72.49)</td><td>133.80 (49.86)</td><td>115.91 (53.89)</td><td>103.63 (54.06)</td><td>102.82 (44.05)</td><td>62.86 (33.87)</td></tr><tr><td>5</td><td>CELEX word frequency -.30 for content words</td><td></td><td>2.42 (.35)</td><td>2.39 (.20)</td><td>2.39 (.22)</td><td>2.36 (.17)</td><td>2.33 (.20)</td><td>2.17 (.19)</td></tr><tr><td>13</td><td>3p plural</td><td>.22</td><td>1.41 (5.77)</td><td>4.29 (10.09)</td><td>4.15 (10.37)</td><td>11.63 (18.64)</td><td>6.60 (9.44)</td><td>9.91 (14.32)</td></tr></table></body></html>

Note. S stands for variables suppressed by word count.

The second set (word count $\ge 1 1 0 . 5 \AA$ ) was further partitioned at node 3 on the basis of word count. Shorter essays (word count $< 1 4 7 . 5$ ) were tagged as level B2 and level C2 at node 5 on the basis of the CELEX frequency of content word. Longer essays (word count $\ge 1 4 7 . 5$ ) were divided into those with verb overlap lower than .5485 and those with verb overlap higher than or equal to .5485 at node 6. At node 9, essays were partitioned into those with AoA for content words equal to or higher than 362 and those with AoA for content words lower than 362. The essays with AoA for content words equal to or higher than 362 were tagged as level B2 (pre-phrase $< 1 0 1 . 8 \AA$ ) and level C2 (pre-phrase $\ge 1 0 1 . 8 $ ) on the basis of preposition phrase density value at node 12. The essays with AoA for content words lower than 362 were tagged as level C1 (3p plural $< 7 . 4 6$ ) and level C2 (3p plural $\ge 7 . 4 6$ ) on the basis of the third-person plural pronoun value at node 13. At node 10, essays were tagged as level C1 when the pronoun value was higher than or equal to 111.4. Essays with pronoun value lower than 111.4 were classifed as level C1 (deep cohesion $\ge 3 . 3 3 1$ ) and C2 (deep cohesion $< 3 . 3 3 1 $ ) at node 14 on the basis of the deep cohesion value.

Based on the classifcation procedure specifed in Figure 2, the distinguishing linguistic features and classifcation criterion of each CEFR level can be summarized as follows:

As presented in Table 4, the current research not only identifed the linguistic features that distinguish essays aligned with CEFR levels, but also specifed the critical value of these linguistic indices, making it more convenient for writing teachers and raters to utilize the fndings in writing instruction and evaluation. The predictive linguistic features are discussed in the following section by following the categories adopted by the Coh-Metrix offcial document: descriptive, word information, situation model, text easability, and syntactic pattern density.

# Descriptive (word count)

Word count was adopted recursively for partitions at node 1, node 2, node 3, and node 8. Generally, longer essays tend to receive higher gradings, since ‘writing fuency constitutes one of the largest differences distinguishing good and poor essays’ (McNamara et al. 2015: 39). Assuming features that characterize longer and shorter essays differ, McNamara et al. (2015) divided essays into shorter and longer ones on the basis of the threshold set at the average word lengths of the entire corpus and applied hierarchical classifcation to shorter and longer essays separately. The decision tree, nevertheless, computed the threshold and partitioned automatically.

Since word count may wash out the contribution of other linguistic features in this classifcation procedure, we found it equally important to articulate the most competitive feature that is suppressed by word count at each related partition. At each partition, the most competitive but suppressed feature demonstrated a slightly lower improve value (calculated as a weighted sum of the improvement in impurity) than word count but higher than other features. Table 5 presents the linguistic features that are suppressed by word count at nodes 1, 2, 3, and 8: lexical diversity, AoA for content words, CELEX frequency of content words, and second language readability score. The descriptive statistics of these four suppressed features are shown in Table 6.

Lexical diversity, one dimension nested under lexical complexity (Nasseri and Thompson 2021), is defned as ‘the range and variety of vocabulary deployed in a text by either a speaker or a writer’ (McCarthy and Jarvis 2007: 459). VOCD, an arithmetical transformation of the traditional measure of lexical diversity type-token ratio, is less sensitive to the text length (Bonvin and Lambelet 2017), and tend to provide more accurate prediction of students’ writing profciency. Probably due to different lexical diversity measures adopted (Koizumi and In’nami 2012), no consensus has been reached in terms of the strength of the correlation between lexical diversity and L2 writing quality, with some studies reporting strong correlation (e.g. Treffers-Daller 2013; Jarvis 2017), while some others reporting weak correlation (Vidal and Jarvis 2020). Our fnding further confrmed that lexical diversity (VOCD) could be a very strong predictor of writing quality (secondary to word count), which is likely to manifest its signifcance in long-term development. Meanwhile, our results suggested that 44.086 could be an important VOCD value in distinguishing writing lower than B1 (VOCD < 44.086) from those higher than B2 $\mathrm { ( V O C D \ge 4 4 . 0 8 6 ) }$ ), following the general assumption that a higher degree of lexical diversity indicates the use of a wider range of words, thus is associated with greater lexical sophistication (Guo et al. 2013).

Table 4: The distinguishing linguistic features and classifcation criterion of essays aligned with CEFR levels   

<html><body><table><tr><td>Levels</td><td>The distinguishing linguistic features and classification criterion</td></tr><tr><td>A1</td><td>Word count &lt; 50.5</td></tr><tr><td></td><td>AoA &lt; 303.9, 50.5  Word count &lt; 110.5, 1p singular  152.9</td></tr><tr><td>A2</td><td>50.5  word count &lt; 79.5, AoA &lt; 303.9, 1p singular &lt; 152.9</td></tr><tr><td></td><td>79.5  word count &lt; 110.5, AoA &lt; 303.9, 1p singular &lt; 152.9, intentional verb  27.92</td></tr><tr><td></td><td>50.5  word count&lt; 62.5, AoA  303.9</td></tr><tr><td>B1</td><td>62.5  word count &lt; 110.5, AoA  303.9</td></tr><tr><td></td><td>79.5  word count &lt; 110.5, AoA &lt; 303.9, 1p singular &lt; 152.9, intentional verb &lt; 27.92</td></tr><tr><td>B2</td><td>110.5  word count &lt; 147.5, CELEX frequency/content W  2.059</td></tr><tr><td></td><td>147.5  word count, V overlap &lt; 0.5485, AoA  362, prep-phrase &lt; 101.8</td></tr><tr><td>C1</td><td>147.5  word count, V overlap &lt; 0.5485, AoA  362, 3p plural &lt; 7.46</td></tr><tr><td></td><td>147.5  word count, V overlap  0.5485, pron  111.4</td></tr><tr><td></td><td>147.5  word count, V overlap  0.5485, pron &lt; 111.4, deep cohesion  3.331</td></tr><tr><td>C2</td><td>147.5 &gt; word count, CELEX frequency/content W  2.059</td></tr><tr><td></td><td>147.5&lt; word count, V overlap &lt; 0.5485, AoA  362, prep-phrase  01.8</td></tr><tr><td></td><td>147.5  word count, V overlap &lt; 0.5485, AoA  362, 3p plural  7.46</td></tr><tr><td></td><td></td></tr></table></body></html>

At node 2 and node 3, AoA for content words and CELEX frequency of content words (detailed review of these two indices are included in the next section Word Information) are the most competitive linguistic features suppressed by word count. To be more specifc, essays with an AoA for content word index lower than 282.622 are likely to be placed at level A1 (shown in Table 5). Generally, a higher value of AoA for content word, indicating words acquired later in life, is associated with a higher degree of lexical sophistication. It is therefore understandable that essays with AoA for content words lower than 282.622 are placed at level A1. The fnding that CELEX frequency of content words index higher than 2.1615 (shown in Table 5) indicates writing profciency at level B2 or level C2 seems to be a little ambiguous, suggesting that other linguistic features also need to be considered simultaneously in distinguishing the two higher levels (B2 and C2).

Coh-Metrix L2 Readability $( > 3 3 . 7 9 8 5 )$ ) at node 8 is secondary to word count in separating A2 and B1 writing samples. Essays with a readability score of 33.7895 and higher are more likely to be placed at level A2 as compared to B1, consistent with the assumption that a higher score of readability indicates a higher level of easiness of understanding and a lower level of writing profciency (Crossley and McNamara 2012). As can be deduced from the formula listed below, the CohMetrix L2 Reading Index is refective of cognitive and psycholinguistic processes by incorporating deep-level factors, including content word overlap, sentence syntax similarity, and CELEX word frequency.

Table 5: Linguistic features suppressed by word count in the classifcation procedure   

<html><body><table><tr><td rowspan="2">Node No.</td><td colspan="2">Word count</td><td rowspan="2">Suppressed features</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2">Threshold</td><td rowspan="2">Improve</td></tr><tr><td>Threshold</td><td>Improve</td></tr><tr><td>Node 1</td><td>&lt;110.5</td><td>112.318</td><td colspan="2">Lexical diversity (VOCD)</td><td colspan="2">&lt;44.086</td><td>106.4995</td></tr><tr><td>Node 2</td><td>&lt;50.7</td><td>75.79293</td><td colspan="3">AoA for content word</td><td colspan="2">&lt;282.622</td></tr><tr><td>Node 3</td><td>&lt;147.5</td><td>31.50175</td><td colspan="3">CELEX frequency of content words</td><td colspan="2">&gt;2.1615</td></tr><tr><td>Node 8</td><td>&lt;62.5</td><td>6.879688</td><td colspan="2">Coh-Metrix L2 readability (RDL2)</td><td colspan="2">&gt;33.7985</td><td>6.31589</td></tr><tr><td colspan="9">Table 6: The descriptive statistics of linguistic features suppressed by word count</td></tr><tr><td rowspan="2">Nodes Features</td><td rowspan="2"></td><td rowspan="2">Pearson r</td><td rowspan="2">A1 A2 M (SD)</td><td rowspan="2">B1 M (SD)</td><td rowspan="2">B2 M (SD)</td><td rowspan="2">C1 M (SD)</td><td rowspan="2">C2 M (SD)</td><td>M (SD)</td></tr><tr><td>86.81</td></tr><tr><td colspan="9">1 VOCD</td></tr><tr><td rowspan="2">(S) Word information.</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2">(5.74)</td><td rowspan="2">(18.78)</td><td rowspan="2">(33.46)</td><td rowspan="2">(30.36)</td><td rowspan="2">(24.63)</td><td rowspan="2">(28.19)</td></tr><tr><td>338.75 355.84</td></tr><tr><td colspan="9">2 AoA for content words</td></tr><tr><td>(S)</td><td></td><td>.51</td><td>247.15 (68.33)</td><td>285.96 (56.91)</td><td>(53.12)</td><td>(47.99)</td><td>335.45 (35.98)</td><td>346.67 (28.45)</td></tr><tr><td>3 (S)</td><td>CELEX word frequency for content words..</td><td>.30</td><td>2.42 (.35)</td><td>2.39 (.20)</td><td>2.39 (.22)</td><td>2.36 (.17)</td><td>2.33 (.20)</td><td>2.17 (.19)</td></tr><tr><td colspan="9">Readability</td></tr><tr><td rowspan="2">8 (S)</td><td rowspan="2">RDL2</td><td rowspan="2">.53</td><td>32.38</td><td>27.98</td><td>22.36</td><td>20.08</td><td>19.03</td><td>13.65</td></tr><tr><td>(16.09)</td><td>(9.53)</td><td>(8.16)</td><td>(6.25)</td><td>(6.42)</td><td>(5.72)</td></tr></table></body></html>

# (Kiselnikov et al. 2020).

These three linguistic indices demonstrate a positive relationship with the Coh-Metrix L2 Reading Index, and thus correlate negatively with essay quality. The current result seems to support Guo et al’.s (2013) fnding that essays with higher profciency had lower scores in content word overlap, which is contradicting with previous studies (e.g. Connor 1990; Jin 2001). We side with Guo et al. (2013) in arguing that profcient writers may produce fewer cohesive devices due to their belief that their readers are highly knowledgeable and require less cohesive devices to achieve comprehension. A higher syntactic similarity index is associated with less syntactic variation (Crossley and McNamara 2012), and a higher CELEX word frequency indicates the use of more frequent words and a lower degree of lexical sophistication (Guo et al. 2013). Therefore, it is reasonable that Coh-Metrix L2 Reading Index can be adopted as an important linguistic feature that distinguish writing quality.

# Word information

Under the word information category, AoA for content words, CELEX word frequency for content words, and pronoun use (frst-person singular pronoun, third-person plural pronoun, and pronoun incidence) are adopted in the partitions. Essays with low AoA for content words (node 4, node 9, and secondary to word count at node 2) are likely to be categorized with lower levels. AoA for content words is a psycholinguistic variable measuring the earliness/lateness of a word being incorporated into the semantic network (Carroll and White 1973; Gilhooly 1984). Generally, words learned earlier in life tend to be shorter, more frequent, more familiar, concrete, and imageable than words that are acquired later (Cameirão and Vicente 2010). AoA data is measured by (i) using adult estimates of the age when they possibly learned a specifc word or (ii) analyzing the performance of children in naming pictures (Cameirão and Vicente 2010). The MRC norms of the age of acquisition is incorporated into Coh-Metrix. MRC is a machine-usable dictionary, containing 150,837 words, each of which is tagged with 26 linguistic and psycholinguistic attributes (Wilson 1988). According to this norm, words acquired earlier (e.g. milk, smile, and pony; $\mathrm { A o A } = 2 0 2$ ) tend to have lower AoA values than those acquired later in life (e.g. cortex, dogma, and matrix; AoA $=$ 700). AoA effects that ‘words acquired early are processed faster and more accurately than those acquired later’ have been frequently observed (Bonin et al. 2004: 456). Our results suggested that writing samples with AoA for content words lower than 303.9 are very likely to be categorized as A1, A2, or B1 at the highest.

Consistent with previous research that identifed CELEX word frequency for content words as predictive of higher quality essays produced by college students (McNamara et al. 2015), the current study suggested that CELEX word frequency for content words distinguishes two higher levels B2 $( 2 2 . 0 5 9 )$ and C2 $( < 2 . 0 5 9 )$ at node 5. Coh-Metrix provides measures of the word frequency for content words by adopting the CELEX database, which includes word frequencies extracted from the early version of the COBUILD corpus (Baayen et al. 1993). ‘A higher word frequency score indicates that the input text contains more frequent words and is, thus, less lexically sophisticated’ (Guo et al. 2013: 226). By utilizing a corpus of essays from undergraduate students at Mississippi State University, Kim et al. (2018) found that content word frequency explained 8.5 per cent of the variance of L2 writing profciency, providing supportive evidence to the current fnding.

Compared to writings categorized as the C2 level, writings at the C1 level use fewer third-person plural pronouns $( < 7 . 4 6 ;$ ; node 13) but more pronouns (≥111.4; node 10). Since excessive use of pronouns may cause referential cohesion problems, reasonable use of pronouns may demonstrate C2-level writers’ capability of maintaining cohesion. Conversely, Rahman (2013) discovered that third-year college students (the higher-level group) used more personal and demonstrative pronouns but with less repetition in descriptive essays. Therefore, more in-depth investigation is needed to uncover the development of pronoun use by language learners. The frequent use of frst-person singular pronoun is a typical feature of writing samples at level A1. It is reasonable to assume that learners at the very beginning level are only capable of discussing topics highly related to themselves. Hyland (2002) once observed signifcant underuse of I as an individualistic identify in fnal year undergraduates’ project reports, which may be infuenced by students’ collective assumption that frst-person pronoun marks low-level writing samples. Interestingly, Abbuhl (2012) and Bikeliené (2016) found that more profcient college-level writers are more likely to be present in their writing, suggesting a possible learning trajectory starting with excessive use of frst-person pronoun to cautious underuse, and then to confdent presentation.

# Situation model

WordNet verb overlap and intentional verb, nested under situation model, emerged as important features used to classify writing samples into CEFR levels. Compared with verb overlap among sentences computed using LSA, verb overlap estimation using WordNet is more concrete (Wolfe et al. 2019) by counting identical verbs and verbs in the associated synonym set (McNamara et al. 2014). A lack of verb overlap was observed with children (Pine et al. 1998). Texts with more repeated verbs tend to construct a more coherent situation model. Meanwhile, more verb overlaps may suggest narrative texts and reading materials appropriate for younger readers (McNamara et al. 2012). The current decision tree (node 6) also indicates that decisions on levels of students’ writing have to be made by considering other linguistic features simultaneously.

Intentional verbs mainly refer to verbs of thought (e.g. thinks, believes, judges, etc.), desire, and emotion (e.g. wants, desires, fears, etc.) (Carr 1984). As shown in Figure 2, intentional verb incidence (node 15) is an important feature that distinguishes two comparatively lower levels A2 and B1, and more frequent use of intentional verbs is associated with the lower level A2. As opposed to causal verbs, which are associated with ‘events in the material world or psychological word (such as an earthquake erupting, or a person discovering a solution)’, intentional verbs signify actions by animate agents (‘buying groceries, telling a child to behave, or watching a television program’) (Graesser et al. 2011: 227). Therefore, it is apparent that intentional verb use refects topics that are cognitively and linguistically less demanding.

# Text easability

Under the category of text easability, deep cohesion emerged as a key feature that determines level C1 $( 2 3 . 3 3 1 )$ or C2 $( < 3 . 3 3 1 )$ at node 14. This feature refers to the use of causal and intentional connectives, which facilitate the construction of causal and logical relationships within texts. In this light, our fnding that level C1 demonstrated higher indices in using connectives seems to be inconsistent with the expectation that writing placed in a higher level should be more effcient in constructing causal and logical relationships. Our fnding, nevertheless, could be somewhat supported by the developmental path of cause (Mohan et al. 2002), which suggests that more profcient writers tend to use more grammatically metaphoric constructions, such as preposition phrases, verbs, and nominalizations to construct causal relations rather than heavily relying on connectives.

# Syntactic pattern density

As shown in Figure 2, preposition phrase frequency is used to distinguish B2 $( < 1 0 1 . 8 )$ and C2 $( \ge 1 , 0 1 8 )$ at node 12. Generally, lower preposition phrase use is more likely to be placed in the lower level (B2). Frequently being investigated under syntactic complexity, preposition phrases, as characteristic of formal writing, function to increase information density (Parkinson and Musgrave 2014; Biber et al. 2020; Geluso 2022). Biber et al. (2011) suggested that preposition phrase modifying noun is more associated with academic writing than with conversation. Taguchi et al. (2013) found that post-noun-modifying prepositional phrases appeared more frequently in highrated essays. Crossley and McNamara (2014) also identifed a signifcant correlation between preposition phrases and holistic essay scores. The observation that preposition phrases ‘develop later when novice writers become more profcient’ (Geluso 2022: 1) corresponds with our fnding that preposition phrase use distinguishes two comparatively higher levels, B2 and C2.

# Conclusion

The current research successfully identifed 10 linguistic features that are predicative of students’ writing qualities by adopting Coh-Metrix (a computational linguistic tool), EFCAMDAT (a large-scale learner corpus), and decision tree (a machine-learning method). Compared with other statistical methods, decision tree functions more effciently in clarifying the classifcation procedure and identifying linguistic features that discriminate adjacent levels. In terms of pedagogical implications, these predictive linguistic features deserve greater attention in L2 writing pedagogy so as to promote students to the next adjacent level. Since the current research related students’ writing quality to CEFR levels, we argue that the distinguishing linguistic features identifed are also very useful in tracking students’ long-term development of writing ability and facilitating testing alignment practices.

Despite the special attention this research draws to the procedure of classifcation, the predictive linguistic features identifed need to be understood and applied by considering the limitations. For example, grammatical errors, as an important aspect of L2 writing, are not computed by Coh-Metrix and are beyond the scope of the current analysis. Future research could consider building a more comprehensive model that includes grammatical errors. In addition, the texts included in EFCAMDAT are generally shorter than argumentative essays used in other studies, and do not include integrated writing samples. Another issue that may somewhat infuence the applicability of the current algorithm is that profciency levels in the EFCAMDAT might not be strictly defned. On the basis of the recognition that ‘profciency level in computer learner corpora has generally been a fuzzy variable’ (Carlsen 2012: 161), Carlsen (2012) exemplifed a more accurate method of assigning CEFR levels to texts, where experienced raters were carefully trained, and incomplete linking design was implemented to control rater severity and achieve a reliable level assignment. Comparatively, the level assignment of EFCAMDAT (on the basis of students’ performance in a placement test or through the progression of coursework) is less explicit. Therefore, the future construction of learner corpora with clearly defned learner level is indispensable for more accurate extraction of predictive linguistic features.

# Notes on contributor

Hong Ma is currently an associate professor at the Department of Linguistics, School of International Studies, Zhejiang University, China. Her primary research interests are computer-assisted language learning, educational assessment, and educational data mining.

Jinglei Wang is currently a laboratory assistant in the Center of Foreign Language Teaching and Research Experiments, School of International Studies, Zhejiang University, China. His primary research interests are technology-enhanced foreign languages teaching, machine learning, and artifcial intelligence.

Lianzhen He is currently a full professor at the Department of Linguistics, School of International Studies, Zhejiang University, China. Her primary research interests are language testing and corpus linguistics.

# Funding

This research was supported by the National Social Science Fund of China (23BYY153) and the Humanities and Social Sciences Fund of the Ministry of Education in China (21YJC740038).

# References

Abbuhl, R. 2012. ‘Using self-referential pronouns in writing: The effect of explicit instruction on L2 writers at two levels of profciency,’ Language Teaching Research 16: 501–18.   
Albayrak, Y. 2009. ‘Classifcation of domestic and foreign commercial and banks in Turkey based on fnancial effciency: A comparison of decision tree, logistic regression and discriminant analysis models,’ The Journal of Faculty of Economics and Administrative Sciences 14/2: 113–39.   
Aryadoust, V. and S. Liu. 2015. ‘Predicting EFL writing ability from levels of mental representation measured by Coh-Metrix: A structural equation modeling study,’ Assessing Writing 24: 35–58.   
Baayen, R. H., R. Piepenbrock, and H. van Rijn (eds.). 1993. The CELEX Lexical Database CD-ROM. Linguistic Data Consortium.   
Biber, D., et al. 2020. ‘Investigating grammatical complexity in L2 English writing research: Linguistic description versus predictive measurement,’ Journal of English for Academic Purposes 46: 100869–15.   
Biber, D., B. Gray, and K. Poonpon. 2011. ‘Should we use characteristics of conversation to measure grammatical complexity in L2 writing development?,’ TESOL Quarterly 45: 5–35.   
Biber, D., B. Gray, and S. Staples. 2016. ‘Predicting patterns of grammatical complexity across language exam task types and profciency levels,’ Applied Linguistics 37/5: 639–68.   
Biber, J., et al. 1999. Longman Grammar of Spoken and Written English. Longman.   
Bikeliené, L. 2016. ‘Person markers in non-native students’ writing,’ Verbum 7: 34–43.   
Bonin, P., et al. 2004. ‘The infuence of age of acquisition in word reading and other tasks: A never ending story,’ Journal of Memory and Language 50/4: 456–76.   
Bonvin, A. and A. Lambelet. 2017. ‘Algorithmic and subjective measures of lexical diversity in bilingual written corpora: a discussion,’ Cognition, Representation and Language 21/1: 1–16.   
Bulté, B. and A. Housen. 2012. ‘Defning and operationalizing L2 complexity’ in A. Housen, F. Kuiken, and I. Vedder (eds): Dimensions of L2 Performance and Profciency Investigating Complexity, Accuracy and Fluency in SLA. Benjamins, pp. 21–46.   
Cameirão, M. L. and S. G. Vicente. 2010. ‘Age-of-acquisition norms for a set of 1,749 Portuguese words,’ Behavior Research Methods 42: 474–80.   
Carlsen, C. 2012. ‘Profciency level-a fuzzy variable in computer learner corpora,’ Applied Linguistics 33/20: 161–83.   
Carr, D. 1984. ‘The logic of intentional verbs,’ Philosophical Investigation 72: 141–57.   
Carroll, J. B. and M. N. White. 1973. ‘Word frequency and age of acquisition as determiners of picture naming theory,’ Quarterly Journal of Experimental Psychology 25: 85–95.   
Casal, J. E. and J. J. Lee. 2019. ‘Syntactic complexity and writing quality in assessed frst-year L2 writing,’ Journal of Second Language Writing 44: 51–62.   
Chen, Y. and P. Baker. 2016. ‘Investigating critical discourse features across second language development: Lexical bundles in rated learner essay, CEFR B1, B2 and C1,’ Applied Linguistics 376: 849–80.   
Connor, U. 1990. ‘Linguistic/rhetorical measures of international persuasive student writing,’ Research in the Teaching of English 24: 67–87.   
Council of Europe. 2001. Common European Framework of Reference for Languages: Learning, Teaching, Assessment CEFR. Council of Europe/Cambridge, Cambridge University Press.   
Crossley, S. A., J. Greenfeld, and D. S. McNamara. 2008. ‘Assessing Test Readability Using Cognitively Based Indices,’ TESOL Quarterly 42: 475–93.   
Crossley, S. A., T. Salsbury, and D. S. McNamara. 2011. ‘Predicting the profciency of language learners using lexical indices,’ Language Testing 29/2: 243–263.   
Crossley, S. A., K. Kyle, and D. S. McNamara. 2016. ‘The development and use of cohesive devices in L2 writing and their relations to judgments of essay quality,’ Journal of Second Language Writing 32: 1–16.   
Crossley, S. A. and D. S. McNamara. 2014. ‘Does writing development equal writing quality? A computational investigation of syntactic complexity in L2 learners,’ Journal of Second Language Writing 26: 66–79.   
Crossley, S. A. and D. S. McNamara. 2012. ‘Predicting second language writing profciency: The roles of cohesion, and linguistic sophistication,’ Journal of Research in Reading 35/2: 115–35.   
Espada-Gustilo, L. 2011. ‘Linguistic features that impact essay scores: A corpus linguistic analysis of ESL writing in three profciency levels,’ Language, Linguistics, Literature 17/1: 55–64.   
Foltz, P. W. 2007. ‘Discourse coherence and LSA’ in T. K. Landauer, D. S. McNamara, S. Dennis, and W. Kintsch (eds): Handbook of Latent Semantic Analysis. Lawrence Erlbaum, pp. 167–84.   
Friginal, E. and S. Weigle. 2014. ‘Exploring multiple profles of L2 writing using multi-dimensional analysis,’ Journal of Second Language Writing 26: 80–95.   
Geertzen, J., T. Alexopoulou, and A. Korhonen. 2013. Automatic linguistic annotation of large scale L2 databases: The EF-Cambridge Open Language Database EFCAMDAT in Proccedings of the 31st Second Language Research Forum SLRF, Carnegie Mellon, Cascadilla Press.   
Geluso, J. 2022. ‘Grammatical and functional characteristics of preposition-based phrase frames in English argumentative essays by L1 English and Spanish speakers,’ Journal of English for Academic Purposes 55: 101072–13.   
Gilhooly, K. J. 1984. ‘Word age-of-acquisition and residence time in lexical memory as factors in word naming,’ Current Psychological Research and Review 3: 24–31.   
Graesser, A. C., et al. 2004. ‘Coh-Metrix: Analysis of text on cohesion and language,’ Behavior Research Methods, Instruments, and Computers 36: 193–202.   
Graesser, A. C., D. S. McNamara, and J. M. Kulikowich. 2011. ‘Coh-Metrix: Providing multilevel analyses of text characteristics,’ Educational Research 405: 223–34.   
Graham, S. and D. Perin. 2007. ‘A meta-analysis of writing instruction for adolescent students,’ Journal of Educational Psychology 99: 445–76. human qualit in both integrated and independent second language writing samples: A comparison study,’ Assessing Writing 18: 218–38.   
Hair, J. F., et al. 1998. Maltivariate Data Analysis. Prentice Hall.   
Halliday, M. A. K. and R. Hasan. 1976. Cohesion in English. Longman.   
Haswell, R. H. 2000. ‘Documenting improvement in college writing: A longitudinal approach,’ Written Communication 17: 307–52.   
Hawkins, J. and L. Filipovic. 2012. Criterial Features in L2 English: Specifying the Reference Levels of the Common European Framework. CUP.   
Hempelmann, C., et al. 2005. ‘Evaluating State-of-the art treebank-style parser for Coh-Metrix and other learning technology environments’, Proceedings of the 2nd Workshop on Building Educational Applications Using NLP: 69-76.   
Huang, Y., et al. 2018. ‘Dependency parsing of learner English,’ International Journal of Corpus Linguistics 23/1: 28–54.   
Huang, Y., et al. 2017. The EF Cambridge open language database EFCAMDAT: Information for users.   
Hyland, K. 2002. ‘Authority and invisibility: authorial identity in academic writing,’ Journal of Pragmatics 34: 1091–112.   
Jarvis, S. 2017. ‘Grounding lexical diversity in human judgments,’ Language Testing 34: 537–53.   
Jin, W. 2001. A quantitative study of cohesion in Chinese graduate students’ writing: Variations across genres and profciency levels (ERIC Document Reproduction Service No. ED 452 726). Paper presented at the Symposium on Second Language Writing at Purdue University (West Lafayette, Indiana, September 15-16).   
Karel, T., A. Bryant, and D. Hik. 2004. ‘Comparison of discriminant function and classifcation tree analyses for age classifcation of Marmots,’ Oiko 105/3: 575–87.   
Khushik, G. S. and A. Huhta. 2020. ‘Investigating syntactic complexity in EFL learners’ writing across Common European Framework of Reference Levels A1, A2, and B1,’ Applied Linguistics 41/4: 506–32.   
Khushik, G. S. and A. Huhta. 2022. ‘Syntactic complexity in Finnish-background EFL learners’ writing at CEFR levels A1-B2,’ European Journal of Applied Linguistics 10/1: 142–84.   
Kim, J. and H. Nam. 2019. ‘How do textual features of L2 argumentative essays differ across profciency levels? A multidimensional cross-sectional study,’ Reading and Writing 32: 2251–79.   
Kim, M., S. Crossley, and K. Kyle. 2018. ‘Lexical sophistication as a multidimensional phenomenon: Relations to second language lexical profciency, development, and writing quality,’ The Modern Language Journal 102/1: 120–41.   
Kim, M. and S. A. Crossley. 2018. ‘Modeling second language writing quality: A structural equation investigation of lexical, syntactic, and cohesive features in source-based and independent writing,’ Assessing Writing 37: 39–56.   
Kiselnikov, A., D. Vakhitova, and T. Kazymova. 2020. ‘Coh-metrix readability formulas for an academic text analysis,’ IOP Conference Series: Material Science and Engineering 890: 1–6.   
Koizumi, R. and Y. In’nami. 2012564. ‘Effects of text length on lexical diversity measures: Using short texts with less than 200 tokens,’ System 40/4: 554.   
Kyle, K. 2016. ‘Measuring syntactic development in L2 writing: Fine-grained indices of syntactic complexity and usage-based indices of syntactic sophistication’, PhD dissertation, Georgia State University.   
Kyle, K. and S. A. Crossley. 2016. ‘The relationship between lexical sophistication and independent and source-based writing,’ Journal of Second Language Writing 34: 12–24.   
Lachenbruch, P. A. 1967. ‘An almost unbiased method of obtaining confdence intervals for the probability of misclassifcation in discriminant analysis,’ Psychological Bulleti 99: 422–31.   
Latif, S. and M. Gierl. 2021. ‘Automated scoring of junior and senior high essays using Coh-Metrix features: Implications for large-scale language testing,’ Language Testing 381: 62–85.   
Laufer, B. and P. Nation. 1995. ‘Vocabulary size and use: Lexical richness in L2 written production,’ Applied Linguistics 16: 307–22. System 97: 102461–19.   
Lu, X. 2017. ‘Automated measurement of syntactic complexity in corpus-based L2 writing research and implications for writing assessment,’ Language Testing 34: 493–511.   
Lu, X. 2011. ‘A corpus-based evaluation of syntactic complexity measures as indices of college-level ESL writers’ language development,’ TESOL Quarterly 45/1: 36–62.   
Lu, X. and R. Hu. 2022. ‘Sense-aware lexical sophistication indices and their relationship to second language writing quality,’ Behavioral Research 54: 1444–60.   
MacWhinney, B. 2000. The CHILDES Project: Tools for Analyzing Talk 3rd ed, Lawrence Erlbaum Associates.   
McCarthy, P. M. and S. Jarvis. 2007. ‘VOCD: a theoretical and empirical evaluation,’ Language Testing 24: 459–88.   
McCutchen, D. and C. A. Perfetti. 1982. ‘Coherence and connectedness in the development of discourse production,’ Text-Interdisciplinary Journal of Discourse 2/1-3: 113–40.   
McNamara, D., A. S. Crossley, and P. McCarthy. 2010. ‘Linguistic features of writing quality’, Written Communication 27: 57-86.   
McNamara, D. S., et al. 2015. ‘A hierarchical classifcation approach to automated essay scoring,’ Assessing Writing 23: 35–9.   
McNamara, D. S., S. A. Crossley, and R. D. Roscoe. 2013. ‘Natural language processing in an intelligent writing strategy tutoring system,’ Behavior Research Methods 45: 499–515.   
McNamara, D. S., A. C. Graesser, and M. M. Louwerse. 2012. ‘Sources of text diffculty: Across genres and grades’ in J. Sabatini, E. Albro, and T. O’Reilly (eds.), Measuring Up: Advances in How We Assess Reading Ability. Rowman and Littlefeld Education, pp. 89–116.   
McNamara, D. S., et al. 2014. Automated Evaluation of Text and Discourse with Coh-Metrix. Cambridge University Press.   
Mohan, B., et al. 2002. ‘Developmental lexicogrammar of causal explanations in science’, Paper presented at the International Systemic Functional Linguistics Congress ISFC29, Liverpool, UK.   
Nasseri, M. and P. Thompson. 2021. ‘Lexical density and diversity in dissertation abstracts: Revisiting English L1 vs. L2 text differences,’ Assessing Writing 47: 100511–11.   
Ortega, L. 2015. ‘Syntactic complexity in L2 writing: Progress and expansion,’ Journal of Second Language Writing 29: 82–94.   
Ortega, L. 2003. ‘Syntactic complexity measures and their relationship to L2 profciency: A research synthesis of college-level L2 writing,’ Applied Linguistics 24: 492–518.   
Paivio, A., J. C. Yuille, and S. Madigan. 1968. ‘Concreteness, imagery, and meaningfulness values for 925 nouns,’ Journal of Experimental Psychology Monography Supplement 76: 1–25.   
Parkinson, J. and J. Musgrave. 2014. ‘Development of noun phrase complexity in the writing of English for academic purposes students,’ Journal of English for Academic Purposes 14: 48–59.   
Pennebaker, J. W., R. J. Booth, and M. E. Francis. 2007. ‘inguistic inquiry and word count: LIWC 2007’, LLIWC.net.www.liwc.net. 1–10. Retrieved from: http://www.gruberpeplab.com/teaching/ psych231_fall2013/documents/231_Pennebaker2007.pdf   
Pine, J. M., E. V. M. Lieven, and C. F. Rowland. 1998. ‘Comparing different models of the development of the English verb category,’ Linguistics 36: 807–30.   
Polio, C. and H. Yoon. 2018. ‘The reliability and validity of automated tools for examining variation in syntactic complexity across genres,’ International Journal of Applied Linguistics 28/1: 165–88.   
Prodromidis, A. L. and S. J. Stolfo. 2001. ‘Cost complexity-based pruning of ensemble classifers,’ Knowledge and Information Systems 3: 449–69.   
Rahman, Z. A. 2013. ‘The use of cohesive devices in descriptive writing by Omani student-teachers,’ SAGE Open 1–10.   
Song, Y. and Y. Liu. 2015. ‘Decision tree methods: Application for classifcation and prediction,’ Biostatistics in Psychiatry 272: 131–5.   
Strobl, C., J. Malley, and G. Tutz. 2009. ‘An introduction to recursive partitioning: Rationale, application, and characteristics of classifcation and regression trees, bagging, and random forests,’ Psychological Methods 14/4: 323–48.   
Tabachnick, B. G. and L. S. Fidell. 1996. Using Multivariate Statistics. Harper Collins.   
Taguchi, N., B. Crawford, and D. Z. Wetzel. 2013. ‘What linguistic features are indicative of writing quality? A case of argumentative essays in college composition program,’ TESOL Quarterly 472: 420–30.   
Thomas, M. 1994. ‘Assessment of L2 profciency in second language acquisition research,’ Language Learning 44/2: 307–36.   
Toglia, M. P. and Battig, W. F. 1978. Handbook of Semantic Word Norms.: Lawrence Erlbaum.   
Treffers-Daller, J. 2013. ‘Measuring lexical diversity among L2 learners of French: An exploration of the validity of D, MTLD, and HD-D as measures of language ability’ in S. Jarvis and M. Daller (eds): Vocabulary Knowledge: Human Ratings and Automated Measures. Benjamins, pp. 79–103.   
Vidal, K. and S. Jarvis. 2020. ‘Effects of English-medium instruction on Spanish students’ profciency and lexical diversity in English,’ Language Teaching Research 245: 568–87.   
Wang, Z. 2022. ‘Dynamic development of syntactic complexity in second language writing: A longitudinal case study of a young Chinese EFL learner,’ Frontiers in Psychology 13: 974481.   
Wen, Q., L. Wang, and M. Liang. 2005. Spoken and written English corpus of Chinese Learners. Foreign Language Teaching and Research Press.   
Wilson, M. 1988. ‘MRC psycholinguistic database: Machine-usable dictionary, version 200,’ Behavior Research Methods, Instruments, and Computers 20: 6–10.   
Witten, I., E. Frank, and M. Hall. 2011. Data Mining: Practical Machine Learning Tools and Techniques 4th ed. Morgan Kaufmann.   
Wolfe, C. R., M. Dandignac, and V. F. Feyna. 2019. ‘A theoretically motivated method for automatically evaluating texts for gist inferences,’ Behavior Research Methods 51: 2419–37.   
Wolfe-Quintero, K., S. Inagaki, and H. Y. Kim. 1998. Second Language Development in Writing: Measures of Fluency, Accuracy, and Complexity. University of Hawaii Press   
Yang, W., X. Lu, and S. Weigle. 2015. ‘Different topics, different discourse: Relationships among writing topic, measures of syntactic complexity, and judgments of writing quality,’ Journal of Second Language Writing 28: 53–67.   
Zhai, X., J. Krajcik, and J. W. Pellegrino. 2021. ‘On the validity of machine learning-based next generation science assessments: A validity inferential network,’ Journal of Science Education and Technology 30: 298–312.   
Zwaan, R. A. and G. A. Radvansky. 1998. ‘Situation model in language comprehension and memory,’ Psychological Bulletin 123: 162–85.