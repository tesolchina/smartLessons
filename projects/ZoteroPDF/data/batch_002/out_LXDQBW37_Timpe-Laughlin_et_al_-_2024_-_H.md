# Human versus machine: investigating L2 learner output in face-to-face versus fully automated roleplays

Veronika Timpe-Laughlin, Tetyana Sydorenko & Judit Dombi

To cite this article: Veronika Timpe-Laughlin, Tetyana Sydorenko & Judit Dombi (2024) Human versus machine: investigating L2 learner output in face-to-face versus fully automated role-plays, Computer Assisted Language Learning, 37:1-2, 149-178, DOI: 10.1080/09588221.2022.2032184

To link to this article: https://doi.org/10.1080/09588221.2022.2032184

# Human versus machine: investigating L2 learner output in face-to-face versus fully automated roleplays

Veronika Timpe-Laughlina $\textcircled{1}$ , Tetyana Sydorenkob $\textcircled{1}$ and Judit Dombic $\textcircled{1}$

a Educational Testing Service, Princeton, New Jersey, US A; bPortland State University, Portland, Oregon, US A; c University of Pécs, Pécs, Hungary

# ABSTRACT

To examine the utility of spoken dialog systems (SDSs) for learning and low-stakes assessment, we administered the same role-play task in two different modalities to a group of 47 tertiary-level learners of English. Each participant completed the task in an SDS setting with a fully automated agent and engaged in the same task with a human interlocutor in a face-to-face format. Additionally, we gauged students’ perceptions of the two delivery formats. Elicited oral performances were examined for linguistic complexity (syntactic complexity, lexical variety, fluency) and pragmatic functions (number and type of requests). Learner performance data across the two delivery modes were comparable although learners spoke slightly longer in the SDS task and used significantly more turns in the face-to-face setting—a finding that may be due to participants deploying more social rapport building moves, clarification requests, and backchanneling. The attitudinal data indicate that, while many learners liked both delivery formats, there was a slight preference for the face-to-face format, mainly due to the presence of body language. Overall, results show that fully automated SDS tasks may constitute a feasible alternative to face-to-face role-plays. Nevertheless, when possible, learners should be given a choice in task format for both learning and assessment.

# KEYWORDS

Spoken dialogue system; role play; linguistic complexity; pragmatics; perceptions

# Introduction

More than 1 billion students globally are learning English (Beare, 2016) with an underlying motive of being able to communicate with others. Thus, speaking in English is a primary goal for many students who learn English as a second or foreign language. In line with models of communicative language ability, speaking can be broadly subdivided into linguistic and functional aspects of oral language use (Bachman, 1990; Bachman & Palmer, 2010; Celce-Murcia, 2007). Linguistic aspects of speaking ability tend to include fluency and pronunciation, as well as lexical and grammatical resources, while functional aspects are oftentimes subsumed under ‘pragmatic ability’ or the production and comprehension of speech acts and functions. Both linguistic and functional aspects of oral language use are constitutive components of almost any foreign or second language (L2) curriculum (e.g. Nation & Newton, 2008).

However, providing learners with opportunities to practice speaking, and more specifically, linguistic and functional aspects of oral language, can be time and resource consuming. For example, role-plays, which are frequently used in language learning to practice spoken interaction, require interlocutors who may not be readily available in the traditional L2 classroom. Interactions with peer language learners, while useful, can vary relative to how well learners can take on assigned roles and subsequently result in unnatural language use (Sydorenko, 2015). Similarly, the need for human interlocutors as well as the time it takes to administer in-person role-plays limit their practicality as performance assessments (e.g. Ockey & Chukharev-Hudilainen, 2021).

Affordances of computer technology, in particular the use of interactive spoken dialog systems (SDSs) in language learning applications (i.e. when a computer takes on the role of another speaker), may provide a solution insofar as they have the potential to enable learners of English around the globe to converse in English anytime, anywhere, even when human interlocutors (e.g. instructors, peers, target-language partners) are not available. Moreover, they could be deployed as a means of assessment to elicit systematically learners’ oral performances. Automated speech recognition and natural language processing, which are at the heart of SDSs, are now advanced enough so that learners can engage in multi-turn conversations with a computer-automated agent in contexts such as bargaining at a flea market (Wik & Hjalmarsson, 2009), buying a train ticket (Morton et  al., 2012), or disputing a phone bill (Timpe-Laughlin et  al., 2020). Amazon’s Alexa, Google Assistant, and Apple’s Siri are prime examples of SDS systems available to the general public (for an overview of SDS technology, see e.g. Ramanarayanan et  al., 2020).

Although SDS-based technology is potentially beneficial for both language learning and assessment, previous studies have provided mixed results. While some work suggests that computers interact in much the same way as a human interlocutor (e.g. Forsyth et  al., 2019; Reeves & Nass, 1996), other studies have highlighted qualitative differences in the language elicited from L2 learners in interactions with an automated agent versus a human (e.g. Timpe-Laughlin & Dombi, 2020). Thus, in order to obtain a better understanding of the potential and limitations of SDS for language learning and assessment contexts, questions such as the following ones should continue to be explored: Do learners engage in communication with an automated agent in the same way they would converse with a human? Does such interaction approximate real-life language use? To what extent is language output elicited via different modalities comparable? And, equally important, how do learners feel about talking to a machine? To explore some of these questions, this study aims to investigate empirically the potential benefits and limitations of traditional classroom-based role-plays versus fully automated SDS tasks. Thus, we hope to provide insights into whether SDS-based automated agents can serve as a reasonable alternative to human interlocutors in oral role-play tasks.

# Literature review

In teaching and assessing learners’ L2 interaction skills, face-to-face (F2F) role-plays have been used widely as a means of eliciting both students’ oral performance in general (Halleck, 2007; Kormos, 1999; Okada & Greer, 2013) and pragmatics performance in particular (Al-Gahtani & Roever, 2012; Félix-Brasdefer, 2018; Youn, 2020). Role-plays have been found to approximate naturalistic talk-in-interaction insofar as their multi-turn format makes them especially suitable for eliciting and examining the co-constructed nature of interaction (Huth, 2010; Roever & Kasper, 2018). Thus, role-plays are used widely in L2 teaching and learning to provide students with the opportunity to interact in the target language.

In the literature, three types of role-plays can be found: closed, open, and semi-structured or semi-scripted. In closed role-plays (also called ‘oral discourse completion tasks’, ODCTs; e.g. Félix-Brasdefer, 2018; Kasper & Dahl, 1991; Kasper & Rose, 2002), participants are presented with a particular situation. Then, they see or hear a single prompt to respond to (see Figure 1). As such, closed role-plays are constrained to two turns or one adjacency pair, and thus involve only minimal interaction (Huth, 2010). By contrast, in open role-plays, interactants play chosen or assigned roles, which are unscripted, giving participants the freedom to ‘arrive at their communicative goals in their own time and in their own strategic way’ (Huth, 2010, p. 539). Thus, open role-plays allow for communicative language use, including turn-taking, impromptu planning, and negotiation of meaning (Kasper & Dahl, 1991).

The third type, semi-scripted role-plays—used in our study—combine the idea of fixed interlocutor utterances from closed role-plays with

Situation description: Your roommate of two years, who is also a close friend, has left the apartment quite messy after a party that they hosted yesterday. You were at the party for a short time. You want to get your roommate to clean up the apartment. Listen to what they say and respond.

Your roommate says: Hey, what do you think of the party yesterday? You say:

Figure 1. E xample of a Closed Role-Play Task.

You are about to call your boss, Lisa Green. Your goals are to:

1. Get her to agree to have a meeting with you and   
2. Ask her to review your presentation slides before the meeting

Your schedule is free for the rest of the week, so any time proposed by Lisa will work for you.

![](img/3486992adb89d719892e49ff15aaba1b86d72e4d0772645617506253196d1a88.jpg)  
Figure 2. T ask Interface. Copyright $\circledcirc$ 2020 Educational Testing Service. www.ets.org. Note. The task instructions for the face-to-face role-plays were identical, except that the word ‘call’ was replaced with the word ‘meet’ given that it was conducted in person.

the freedom and flexibility of open role-plays (Timpe-Laughlin & Park, 2019; van Batenburg et  al., 2018). Although semi-scripted role-plays require authentic interaction and spontaneous negotiation, they are not open because the interlocutor’s utterances are pre-determined (for an example see Figure 3 below in the Methodology section). Given that in open role-plays interlocutors solve complications or facilitate certain interactive moves in various, often unpredictable, ways (e.g. Okada & Greer, 2013; Ross & O’Connell, 2013), semi-scripted role-plays are frequently used to limit the variability introduced by interlocutors. Thus, they allow for more comparability of the dyadic interaction and the oral output produced by different L2 learners (van Batenburg et  al.,

![](img/c4560909f3a34b473206c9b2b260ff45c57813447437d78cdf7ea56f90509c88.jpg)  
Figure 3. D ialogue Structure of the Semi-scripted Role-play Task (adopted from Timpe-Laughlin & Dombi, 2020, p. 232) $\circledcirc$ 2020 Educational Testing Service. www.ets.org.

2018). This higher level of comparability may be desirable in instructional and assessment contexts, where it is helpful to make direct comparisons between learners or between learners and a performance standard. Incidentally, as we explain below, semi-scripted role-plays are also more suitable for SDSs insofar as the limited variability of user responses results in higher accuracy of automated speech recognition and fewer system failures.

# Role-plays and CALL innovations

The increasing availability of more advanced technologies has brought innovations in computer-assisted language learning (CALL), which also led to computerized versions of semi-scripted role-plays, aimed particularly at providing L2 speaking practice and appropriate feedback (e.g. van Doremalen et  al., 2016; Sydorenko, 2015; Sydorenko et  al., 2020; Timpe-Laughlin et  al., 2020). Sydorenko (2015), for instance, used computer-delivered automatized structured tasks (CASTs) that allowed for multi-turn interactions between pre-determined computer prompts (in a form of video-recordings of a human interlocutor) and a learner’s responses to them. The highly restricted scenarios and the extensive testing of possible dialog branches allowed for a multi-turn design without the use of automatic speech recognition (ASR) technology and natural language processing (NLP) techniques. Sydorenko et  al. (2018) as well as Sydorenko et  al. (2020) used a more advanced version of CASTs by deploying a Simulated Conversations program which via ‘choose your own adventure’ format allowed students to embark on different conversation paths.

Recently, several studies used SDSs that rely on ASR and NLP technologies in order to advance computerized role-play tasks in general and automated agents in particular (Bibauw et  al., 2019, 2022; Chiu et  al., 2007; Johnson & Valente, 2009; Litman et  al., 2018; Ockey & Chukharev-Hudilainen, 2021; Timpe-Laughlin et  al.,

2020; van Doremalen et  al., 2016; Wik & Hjalmarsson, 2009). Van Doremalen et  al. (2016), for example, reported on a case study that investigated an ASR-based dialogue system that offers practice and feedback on pronunciation as well as morphosyntactic aspects of students’ oral language use of Dutch as a second language. By means of expert reviews and user testing, they evaluated the users’ perceptions of the system, finding that students and educators welcomed the pronunciation training, the ease of use, as well as the utility for teaching and learning insofar as the system provided systematic speaking practice and actionable feedback. In particular, they noted that the system was perceived as offering an added value for L2 instruction and oral practice. Along similar lines, Timpe-Laughlin et  al. (2020) investigated the perceptions of $1 6 ~ \mathrm { L } 2$ teachers vis- $\dot { \mathbf { a } }$ -vis four SDS-based speaking tasks that engage users in role-plays such as disputing a phone bill or ordering at a coffee shop. Just like domain experts in Van Doremalen et  al. (2016), the teachers in Timpe-Laughlin et  al.’s study regarded the automated speaking tasks as highly beneficial for practice and even low-stakes assessment. Teachers identified authenticity of the speaking tasks as a key criterion for effective oral practice, thus highlighting the need for these tasks to mimic real-life interaction.

Overall, the current generation of computerized role-plays present the advantage of consistency in that the computer response will always be the same or it will be drawn from a small pool of pre-determined responses (as opposed to more widely varying responses by human interlocutors). Additionally, they are more practical as human interlocutors are not needed. These characteristics make computer-delivered role-plays potentially advantageous for practicing and assessing L2 oral interaction skills. However, a disadvantage, as shown by Sydorenko (2015), is that it may be difficult to elicit interactional features typical in naturalistic discourse, such as clarification requests or backchanneling. Additionally, Timpe-Laughlin and Dombi (2020) found that the quality of requests elicited via multi-turn SDS conversation tasks were different from requests elicited in face-to-face interactions. Timpe-Laughlin and Dombi investigated how $1 0 7 ~ \mathrm { L } 2$ learners engaged in an SDS-based task that required them to make two requests. Focusing on request strategies and modifications, they found that users issued mainly direct requests when interacting with a computer, as well as more internal request modifications than previous studies that featured face-to-face interactions had reported. Such potential differences raise the question of comparability of oral performance elicited via computer versus face-to-face role-play tasks—an issue we aim to explore in our study.

# Studies that compared the output between delivery modes

Given that technologies that provide affordances for oral interaction tasks (e.g. SDSs) are on the rise (Bibauw et  al., 2019; Bibauw et  al., 2022), it is timely to conduct research on how performance data elicited under face-to-face and fully automated conditions compare as it may have implications for L2 teaching and assessment. As mentioned above, computerized tasks have increased practicality, but the question remains whether they elicit comparable language and eventually result in human-like interactions. Although studies of computer-mediated communication have examined the impact of the computer environment on human interactions (see Tang, 2019, and Ziegler, 2016, for reviews), few studies have investigated the quality of interactions produced with a machine-generated interlocutor. However, if learner output is to be used for instruction or other decision-making purposes, then it is important to know if and, if so, how delivery method impacts evidence of learner ability.

Nevertheless, very few studies have explicitly focused on comparing oral response data elicited via the same role-play task administered in both face-to-face and human-machine format (Forsyth et  al., 2019; Sydorenko, 2011). As mentioned above, both linguistic and functional aspects of oral language use are part of communicative competence. As such, Sydorenko (2011) used both linguistic measures of fluency, accuracy, complexity, and a functional measure of pragmatic appropriateness to examine the differences between computer-delivered automatized structured tasks (CASTs) and open-ended role-plays between adult learners of English. Her findings showed that the two groups differed only on pauses: the face-to-face role-play group spent more time pausing than the computer group. However, there were qualitative differences in how the two groups organized their interactions in terms of turn-taking, backchanneling, and creativity of responses. The computer group was direct and to the point in their requests, while the face-toface group used more turns, backchannels, and was overall more creative with the content of their role-plays.

Similarly, Forsyth et  al. (2019) evaluated the quality and features of students’ language elicited by means of a computer-based listen-speak assessment task versus a human interviewer. They investigated the quality and quantity (e.g. syntactic complexity) of the oral output produced by 31 third and fifth grade English language learners in the United States. Overall, they found only slight differences such as a higher number of words used in responses to human interlocutors and concluded that the ‘automated conversational approach elicits the information relevant to language constructs in a manner consistent with human interactions’ (p. 411). However, their study had two limitations.

First, only responses to one question delivered by the human and the computer assessment system were investigated, while the additional interaction across multiple turns with hints and probing for more information was not part of the analysis. Moreover, they utilized TextEvaluator (Sheehan, 2016), an analytics tool designed for longer written text, to compute measures of text complexity (e.g. cohesion, narrativity, syntactic complexity, lexical difficulty), and recommended ‘additional complexity metrics’ (p. 412). We plan to address both aspect in our study—an analysis of multiturn conversation data using an evaluation tool designed specifically for L2 speech—thus providing a more detailed focus on interactional features and a deployment of analyzers designed for oral L2 performance data

Finally, Ockey and Chukharev-Hudilainen (2021) compared human-human versus human-SDS interactions elicited from 40 L2 students via a discussion task rather than a role-play. As opposed to other comparative studies reviewed above, Ockey and Chukharev-Hudilainen utilized the ratings of four human judges. The main findings were that human ratings were similar on fluency, pronunciation, grammar, and vocabulary for the two conditions, but interactional competence received substantially higher ratings in the human-human condition. Additionally, raters believed that the computer provided a more standardized assessment; however, they generally favored the human partner due to the perceived authenticity of such interactions.

# Attitude studies involving role-plays and SDSs

When examining relatively new task formats, researchers frequently ask language learners about their perceptions of such tasks because such perceptions can directly affect learning motivation (Dörnyei, 2005; Kalaja & Barcelos, 2003). Affective variables, such as attitudes and anxiety, have been also shown to affect language achievement (e.g. Skehan, 1989, 1991; Spolsky, 1989). Importantly, studies have shown that L2 speaking tasks result in higher levels of anxiety than other L2 tasks (e.g. listening comprehension) (Hewitt & Stephenson, 2012; Kim, 1998; Philips, 1992). Additional factors inducing anxiety are fear from negative evaluation (Horwitz et  al., 1986), relationship with the interlocutor and setting (e.g. Clément et  al., 1977), and discouraging nonverbal feedback (Wang & Loewen, 2016). In line with these factors that impact anxiety levels, empirical evidence indicates that the language classroom itself can be threatening because of the presence of peers (Horwitz et  al., 1986; Tóth, 2007).

Various computer-mediated environments are thought to potentially reduce foreign language anxiety (FLA) by providing a different modality of interaction that seems to lower inhibition and make learners more comfortable when communicating. Out of the three components of language learning anxiety (Horwitz et  al., 1986)—(1) communication apprehension; (2) testing; and (3) fear of negative evaluation—communication apprehension and fear of negative evaluation may be reduced when talking to an automated agent—an artefact that may be due to computer-mediated communication (CMC) taking place in an ‘anonymous environment’ (Roed, 2003) where potentially challenging paralinguistic and social cues are either missing or can be customized (Warschauer et  al., 1996).

Still, there is little research on whether modality affects FLA, and findings are not unanimous. Existing literature focuses on synchronous or asynchronous oral or written CMC. Studies differ in their findings about significant difference in students’ FLA relative to modality. For example, Arnold (2007) found no significant difference in reduction of communication apprehension between students involved in face-to-face interaction and in CMC. Similarly, Baralt and Gurzynski-Weiss (2011) found that intermediate learners’ anxiety is not differentially affected by modality. However, Coté and Gaffney’s (2018) results show that less advanced learners were significantly less anxious during synchronous CMC than in face-toface interactions. By talking to a fully automated agent instead of a real person, less proficient, anxiety-prone individuals may be less likely to experience anxiety. Anxiety may also be reduced as learners experiment with personalities in multi-player games or simulated environments that carry no real-life consequences (e.g. Sykes et  al., 2008). Overall, although few studies have examined this issue, these studies show that indeed learners can experience less nervousness when talking to automated agents (Sydorenko et  al., 2018). Therefore, additional research is needed in this area.

In terms of how much students like to interact with animated characters or other virtual agents as opposed to human interlocutors, many studies show that students generally react positively to such technologies even when ASR error rates are somewhat high (see Morton et  al., 2012), but the specific design features or individuals’ personalities affect students’ perceptions (e.g. Forsyth et  al., 2019; Morton et  al., 2012; Sydorenko, 2011; Sydorenko et  al., 2018; Taguchi et  al., 2017). For example, some students dislike the computer system when the virtual agents speak too fast or interrupt them (Forsyth et  al., 2019); some students with outgoing personalities prefer to talk to human interlocutors rather than simulated characters (Sydorenko et  al., 2018). Ultimately, individuals can also differ in terms of how engaged they are when interacting with virtual avatars which may be an artifact of familiarity with the given technology or participants’ individual personalities (Ockey et  al., 2017).

# Current study

Previous research has shown continuous advancements in multi-turn SDS-based conversations, in particular role-play tasks, that have been highlighted as beneficial for delivering meaningful L2 speaking practice as well as assessments. Nevertheless, there is an obvious paucity of studies that examined the language elicited by means of a computer versus a human interlocutor in the context of role-play tasks. This is a critical gap in the literature because potential differences may have implications for what is being practiced and/or assessed by a given task in each modality. Building on previous research on role-plays and different types of task delivery modes as well as investigations into the perceptions of users relative to the two delivery modes, this study aimed to answer the following research questions to provide insights into how face-to-face and computer-delivered role-plays compare:

RQ1: How comparable is the language elicited by a role-play task when administered via fully automated SDS versus face-to-face?

RQ2: What are the learners’ perceptions of the SDS versus the face-to-face role-play task?

# Methodology

# The request boss task

In this study, we used a multi-turn conversation task that was developed to elicit approximately two minutes of oral interaction, including the performance of requests. In the SDS version, the task requires participants to make a call to a supervisor character named Lisa Green. Figure 2 shows the task interface, including instructions and the image of Lisa Green that was visible to participants as they engaged with the task. During the conversation, participants need to make two requests: (a) they have to ask for a meeting with the supervisor and (b) ask her if she would be willing to review a deck of presentation slides before the meeting.

The task was administered in two delivery modes: (a) as an interactive speaking task with a fully automated agent and (b) as an in-person, face-to-face role-play with a human interlocutor (for the full dialogue structure see Figure 3.). In the fully automated version, the dialogue structure was implemented in HALEF, an open-source, web-based SDS technology (Ramanarayanan et  al., 2017). The SDS identifies requests in the participants’ responses by means of using regular expressions, matching them against pre-determined semantic tokens in the ASR. For instance, if a participant uses ‘meet’ or ‘meeting’, the system interprets it as a request to schedule a meeting with Lisa Green. We also integrated a branching structure, so the system responds appropriately regardless of where in the dialogue the requests are made. For example, if a participant makes a request for Lisa Green to review the presentation slides (as opposed to a request for a meeting) in turn 2, the dialogue branches to turn 5 with the system responding ‘Sure, no problem. Send them over.’ (see also Timpe-Laughlin & Dombi, 2020). If a participant does not use ‘meet(ing)’, ‘presentation’, or other pre-determined regular expressions, the system offers various clarification requests such as ‘I am sorry I didn’t get that. What can I do for you?’ which allows a participant to reword their initial utterance and try to get the SDS to understand.

In the face-to-face role-play version, a human interlocutor was playing the supervisor. Before conducting the role-plays, the interlocutor was trained to follow the same script and interaction patterns as the automated agent in the SDS version of the task. Utilizing the flowchart presented in Figure 4, the interlocutor was trained by a fellow researcher for approximately three hours prior to administering the role-plays in order to ensure accuracy of response patterns and automaticity. However, as will be seen later in the results and discussion sections, the human interlocutor did sometimes deviate slightly from the script in that they, naturally, provided more backchannels than the automated agent.

# Participants

Initially, 51 English as a second language (ESL) students at a U.S. university participated in this study. However, due to data loss (as described in the Analysis section), our final data set included 47 participants—27 male, 19 female, and one who identified as ‘other’—were on average 23 years old, ranging from 19 to 30 years of age. They had different L1 backgrounds, including Mandarin Chinese ( $\left( n = 2 3 \right)$ , Japanese $( n = 1 2 )$ , Thai $( n = 4 )$ , French $\left( n = 3 \right)$ , and Other $\left( n = 5 \right)$ . Participants had varying degrees of English proficiency. As shown in Figure 5, they came from a range of class levels at the university’s Intensive English Language Program (IELP), ranging from beginner level (Level 2) to advanced level courses (Level 5), while nine of the students had recently finished the IELP program and were taking courses for their degree.

# Procedures

All participants $( N = 4 7 )$ engaged with the Request Boss speaking task delivered in two different formats. Using a counter-balanced design, approximately half of the participants $( n = 2 4 )$ completed the face-to-face delivered task first, while the other half $\left( n = 2 3 \right)$ completed the SDS task first. They completed the SDS task on a computer in the university’s computer lab, while all face-to-face role-plays were conducted in a designated classroom on campus with an interlocutor who was unknown to the study participants. There was about a week between students’ engagement with the SDS task format and the face-to-face version of the task. Both sessions were audio-recorded (through the computer during SDS task and by means of an audio-recording device in the face-to-face sessions). After engaging with the speaking task in each format, students completed a survey, providing their demographic information, their perceptions of the respective task, and their opinion of the administration mode. As a final step, after completing both tasks, participants received a final online survey, in which they were asked to directly compare both delivery formats.

![](img/8bd8694bb4cae4a4a170e6c414164a22fb1558e5adcf85e1a017c869a1518fea.jpg)  
Figure 4. Flowchart of the adaptive branching logic used to train human interlocutor.

![](img/41c93ff7a81f0519c3d7b1dfbddac3a8fb611d1ab25fa323dc4e7a26af746c02.jpg)  
Figure 5. P articipant Distribution Relative to Class Level at the University’s Intensive English Language Program (IE LP). Note. Some participants took classes across two levels, e.g. a level 3 reading class and a level 4 listening class. The IELP levels are roughly equivalent to the following levels of the Common European Framework of Reference (CEFR): Level $\scriptstyle 2 = A 2 / A 2 + ;$ Level $3 = B 1 + ;$ Level $4 = B 2$ ; Level $5 = B 2 + ,$ ; Graduated from $I E L P =$ mid C1 and beyond.

# Analysis

As an initial step, the students’ oral response data were processed in order to prepare the recordings for analysis. First, the audio-recorded data set was cleaned. Four calls to the SDS task were excluded due to call-related issues (e.g. the system stopped responding or the participant hung up after a single turn or called multiple times). Second, all remaining audio-recordings $( 2 \times 4 7 )$ were transcribed verbatim, including pauses, false starts, repairs, and overlaps.

Table 1. D escription of SpeechRater® features.   

<html><body><table><tr><td>Speaking time</td><td>The duration of the entire recording minus any silence at the beginning and end of speech.</td></tr><tr><td>Lexical variety</td><td>Number of tokens as the sum of words and fillers such as &#x27;uh&#x27; and &#x27;uhm.</td></tr><tr><td>Fluency</td><td>Speaking rate in words per second and number of long pauses per word.</td></tr></table></body></html>

Table 2. I ntercoder agreement for pragmatics-related features (percent agreement).   

<html><body><table><tr><td></td><td>SDS</td><td>Face-to-face</td></tr><tr><td>Number of turns containing one or more requests</td><td>92%</td><td>89.4%</td></tr><tr><td>Level of directness of elicited requests.</td><td>92.4%</td><td>86.6%</td></tr></table></body></html>

Then, the oral responses were analyzed with regard to three indices that have been used widely in L2 research: syntactic complexity, lexical variety, and fluency (Larsen-Freeman, 2009; Ortega, 2003). SpeechRater® (Chen et  al., 2018; Zechner $\&$ Evanini, 2020), a spoken-response scoring engine specially adapted for nonnative English speech, was used to investigate participants’ oral output in each delivery mode relative to the number of turns, speaking time, lexical variety, and fluency (see Table 1 for more details). Additionally, the web-based L2 Syntactic Complexity Analyzer (Lu, 2011) was deployed to explore the syntactic complexity of all oral responses across the two delivery formats. Before submitting the data into the L2 Syntactic Complexity Analyzer, care was taken to accurately punctuate utterances.

Third, we examined the requests produced in the two delivery formats. To that end, two researchers individually coded each turn in the transcribed dialogues for (a) the number of elicited requests per dialogue and (b) the respective levels of directness of each request. Following Blum-Kulka et  al. (1989), we coded for three levels of directness: direct (e.g. I want you to…), conventionally indirect (e.g. Could I ask you to…), and non-conventionally indirect (i.e. hints). Following the individual codings, intercoder agreement was calculated for each coding category across the two delivery formats (see Table 2). The discrepancies were resolved in a subsequent consensus coding.

We then compared indices and observations across the two delivery modes. Therefore, we first calculated paired-samples t-tests with Bonferroni corrections for multiple comparisons for each of the linguistic complexity indices (McDonald, 2014). Then, we used the McNemar test ( $2 \times 2$ layout) to account for each subject in our two formats (SDS and face-to-face) and to explore if any of the differences in observed request behavior across the two delivery modes were statistically significant. Finally, with regard to the user perception survey, we analyzed participant responses using both quantitative and qualitative approaches. We used descriptive statistics for the Likert scale responses to determine consensus and/or discrepancies in opinion among participants. Open-ended responses were analyzed for major themes (Ayres, 2008). To identify patterns in the data, frequency counts of the major themes were tallied. Representative responses were extracted as a way of capturing and documenting user opinions.

Table 3. Average number of turns and speaking time per delivery mode $( N { = } 4 7 )$ .   

<html><body><table><tr><td></td><td>Average speaking time in seconds (SD)</td><td>Average number of turns (SD)</td></tr><tr><td>SDS</td><td>38.04 (18.93)</td><td>4.91 (.90)</td></tr><tr><td>F2F</td><td>29.93 (10.88)</td><td>9.74 (2.85)</td></tr></table></body></html>

# Findings

In the following, we will first present the findings from the analyses of the oral performance data before outlining student perceptions about the two delivery modes.

# Oral performance data

As shown in Table 3, the average speaking time was slightly longer during the SDS task when compared to the task in the face-to-face administration. Participants spoke on average eight seconds longer when engaging with the SDS version of the task $\cdot M { = } 3 8 . 0 4$ ; $S D = 1 8 . 9 3 _ { \scriptscriptstyle { \it \cdot } }$ ) than in the face-to-face interactions $\stackrel { \prime } { M } = 2 9 . 9 3$ ; $S D = 1 0 . 8 8 \mathrm { \Omega }$ , a difference that was found to be statistically significant as the result of a paired-samples t-test with Bonferroni correction $( p \ \leq \ . 0 2 5 )$ for multiple significance tests $t ( 4 6 ) { = } 2 . 6 5$ , $\scriptstyle { p = . 0 1 1 }$ . A potential reason for the longer average speaking time found in the SDS version could be related to expectations about having to include more detailed information for the automated system to understand.

By contrast, the average number of turns in face-to-face role-plays $\stackrel { \prime } { M } = 9 . 7 4$ ; $S D = 2 . 8 5$ ) was almost twice as high as the number of turns observed in the SDS version of the task $\mathit { \Omega } ^ { \prime } M = 4 . 9 1$ ; $S D { = } . 9 0 $ ). Again, we found the difference in the number of turns to be statistically significant $t ( 4 6 ) { = } { - } 1 1 . 9 6$ ; $\scriptstyle { p = . 0 0 0 }$ . This observed difference could be an artefact of features such as pause fillers, backchanneling, and single-word or single-phrase turns which are oftentimes found in oral role-plays conducted in a face-to-face setting (Sydorenko, 2015). In SDS tasks, features such as backchanneling and short turns may not be as frequent due to the risk of jeopardizing the conversational flow with the automated agent and the underlying ASR which are oftentimes not as adaptive and seamless as a human interlocutor. In other words, participants may have been more functionally than socially motivated during the SDS tasks, that is, aiming to finish the task successfully and efficiently.

<html><body><table><tr><td rowspan="2"></td><td colspan="4">Syntactic complexity Mean length of clause Mean length of T-unit</td><td colspan="2">Clauses per T-unit</td><td colspan="4">Lexical variety</td><td colspan="4">bFluency</td></tr><tr><td>(MLC)</td><td></td><td>(MLT)</td><td></td><td>(C/T)</td><td></td><td>Number of tokens</td><td></td><td>Number of types</td><td></td><td>Average number of words per second</td><td></td><td>Number of long pauses. per word</td><td></td></tr><tr><td></td><td>M</td><td> SD</td><td>M</td><td> SD</td><td>M</td><td>SD</td><td>M</td><td> SD</td><td>M</td><td> SD</td><td>M</td><td> SD</td><td>M</td><td> SD</td></tr><tr><td>SDs</td><td>6.30</td><td>1.24</td><td>7.84</td><td>2.37</td><td>1.26</td><td>.27</td><td>76.83</td><td>40.10</td><td>41.81</td><td>14.00</td><td>3.19</td><td>.40</td><td>1!</td><td>.06.</td></tr><tr><td>F2F</td><td>6.27</td><td>1.40</td><td>7.17</td><td>1.46</td><td>1.17</td><td>.24</td><td>67.15</td><td>23.19</td><td>41.28</td><td>11.99</td><td>3.30</td><td>.44</td><td>.10</td><td>.07</td></tr></table></body></html>

Additionally, we calculated estimates with regard to syntactic complexity, lexical variety, and fluency and found the numbers for all three measures to be comparable across delivery modes (see Table 4 below). Also, none of the slight differences were found to be statistically significant (after Bonferroni correction, $p { = } . 0 0 7 )$ . For example, with regard to syntactic complexity, the mean length of clause (MLC) in the SDS output was 6.30 $\mathrm { \Delta } S D = 1 . 2 4 )$ while it was 6.27 $\mathrm { \mathit { S D } } { = } 1 . 4 0 ) $ ) for the interactions elicited in face-to-face settings. Similarly, the lexical variety indices between the two delivery modes did not show much discrepancy either, indicating that participants exhibited the same range of vocabulary regardless of the elicitation method. The only lexical index that showed a slight, yet statistically non-significant difference between the delivery modes was the observed number of tokens which was on average slightly higher in the SDS data $\mathit { \Delta } M = 7 6 . 8 3 $ ; $S D = 4 0 . 1 0 \AA$ ) than in the face-to-face data $M = 6 7 . 1 5$ ; $S D = 2 3 . 1 9 \mathrm { \Omega }$ ). However, this observation is in line with the higher average speaking time found in the output produced in the SDS-delivered version of the task and thus another indicator that students potentially perceived the need to provide enough details in order to maintain the flow of the conversation with the automated agent. Finally, with regard to fluency, we calculated for each participant the number of words per second and the number of long pauses per word. Again, on average, there were no considerable differences with regard to fluency between the two delivery modes, suggesting that both delivery formats allowed students to show their abilities in terms of fluency. Hence, with regard to linguistic features associated with language proficiency, the elicited oral output displayed largely comparable indices—potentially good news as far as the applicability of spoken dialogue systems for the purposes of language learning and low-stakes assessment is concerned.

In terms of requests, Table 5 presents the number of elicited requests per dialogue in each delivery mode. Both, the SDS and the face-to-face task elicited at least one request. However, almost $9 0 \%$ of face-to-face conversations included two requests (meeting request and slide review request), as required by the task instructions, while approximately $1 0 \%$ included only one request. By contrast, $8 3 \%$ of the SDS conversations included two request and about $1 7 \%$ included one request. Although this finding suggests that the participants were more successful in making two requests with a human interlocutor than in an SDS environment, an exact McNemar’s test $\left( p = . 5 4 9 \right)$ determined that there was no statistically significant difference in the proportion of participants deploying one or two requests relative to delivery format.

Table 5. N umber of elicited requests per dialogue.   

<html><body><table><tr><td>Number of requests elicited</td><td>SDS (N=47)</td><td>F2F (N=47)</td></tr><tr><td>1</td><td>8 (17%)</td><td>5 (10.6%)</td></tr><tr><td>2</td><td>39 (83%)</td><td>42 (89.4%)</td></tr></table></body></html>

Table 6. D irectness of elicited requests per delivery mode $( N { = } 4 7 )$ .   

<html><body><table><tr><td>Level of directness.</td><td colspan="3">SDS</td><td colspan="3">F2F</td></tr><tr><td></td><td>Meeting (n=41)</td><td>Slide review (n=43)</td><td>TOTAL (n=84)</td><td>Meeting (n=44)a</td><td>Slide review (n=45)</td><td>TOTAL (n=89)</td></tr><tr><td>Direct</td><td>24 (58.5%)</td><td>24 (55.8%)</td><td>48 (57.1%)</td><td>19 (43.2%)</td><td>19 (42.2%)</td><td>38 (42.7%)</td></tr><tr><td>Conventionally indirect 17 (41.5%)</td><td></td><td>19 (44.2%)</td><td>36 (42.9%)</td><td>23 (52.3%)</td><td>23 (51.1%)</td><td>46 (51.7%)</td></tr><tr><td>Non-conventionally indirect (Hint).</td><td></td><td>--</td><td>--</td><td>2 (4.6%)</td><td>3 (6.7%)</td><td>5 (5.6%)</td></tr></table></body></html>

a Note that not all dialogues included both requests. Therefore, n varies. Repeated requests were not counted.

As a final step, we examined the level of directness of the elicited requests across both delivery modes (see Table 6). Overall, participants deployed more direct requests in the SDS version of the task than in the face-to-face version. For example, in the SDS version of the task $5 7 . 1 \%$ were direct requests while $4 2 . 9 \%$ were conventionally indirect requests. By contrast, $4 2 . 7 \%$ of requests found in the face-to-face-delivered task were direct and $5 1 . 7 \%$ were conventionally indirect requests. Moreover, the face-to-face-delivered task also included non-conventionally indirect requests $( 5 . 6 \% )$ while the SDS tasks did not include any. Hence, there was a tendency among participants to use more direct requests in the SDS context as opposed to a stronger use of indirect requests in the face-to-face interactions, an indicator that technology may have had an impact on participants’ request behavior (Timpe-Laughlin & Dombi, 2020; see more in discussion section).

# Student perceptions

Table 7 below displays the frequency counts and descriptive statistics of the responses students provided to the post-task user perception questionnaires. It shows that overall, participants were quite balanced with regard to their perceptions of the two delivery modes. For instance, they did not seem to see any distinct differences in terms of intelligibility of the interlocutors. However, there was variation across task formats with regard to participants’ ratings of enjoyment. For example, 44 $( 9 3 . 6 \% )$ participants highlighted that they enjoyed face-to-face interaction compared to only 32 $( 6 8 \% )$ who reported that they enjoyed the interaction with the computer. Similarly, 45 participants $( 9 7 . 8 \% )$ highlighted that they would like to engage in additional speaking activities after completing the interaction with the human interlocutor, while only 39 $( 8 3 \% )$ indicated their willingness to do additional SDS speaking tasks. Although these numbers still include the majority of participants, they suggest a slight overall preference for in-person delivery.

<html><body><table><tr><td rowspan="3"></td><td colspan="2">I had enough time to. think about what to.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>I enjoyed doing</td><td></td><td>The speaking</td><td></td><td></td><td>The conversation I had with ...</td><td></td><td></td><td></td><td>I would like to do.</td><td></td><td></td><td>I felt nervous.</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>say during the</td><td>speaking activity with understood what</td><td>The ...</td><td></td><td></td><td>The ... spoke</td><td></td><td></td><td>I understood what the ... said.</td><td></td><td></td><td>The ... interrupted me</td><td></td><td>the speaking activity with</td><td></td><td></td><td>activity with the ... were too</td><td></td><td>was similar to</td><td>real-life</td><td></td><td></td><td></td><td></td><td></td><td> more speaking</td><td>doing the speaking activities with the.</td></tr><tr><td>the ...</td><td></td><td></td><td>I said</td><td></td><td></td><td>too fast</td><td></td><td>to me.</td><td></td><td></td><td> too much.</td><td></td><td></td><td>the...</td><td></td><td>difficult.</td><td></td><td></td><td>conversations.</td><td></td><td></td><td></td><td>activity with the...</td><td></td><td></td><td>...</td></tr><tr><td></td><td>computer</td><td> person</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>computer person computer person computer person computer person computer person computer person computer person computer</td><td></td><td></td><td></td><td></td><td></td><td> person</td><td>computer 47</td><td> person</td></tr><tr><td>N</td><td>47</td><td>47.</td><td></td><td>47</td><td>47</td><td>47.</td><td></td><td>47</td><td> 46.</td><td>47</td><td></td><td>47</td><td>47 3.36</td><td></td><td>47</td><td>47.</td><td>47 3.04</td><td>47. 3.14</td><td></td><td>47 2.28</td><td>46. 1.65</td><td></td><td>47. 2.68</td><td>47. 2.57</td><td></td><td>1.94</td><td>46 1.52</td></tr><tr><td>Mean SD</td><td>1.57</td><td>1.43 0.58</td><td></td><td>1.79 0.55</td><td>1.51 0.62</td><td>2.94 0.79</td><td></td><td>3.19 0.82</td><td>1.46 0.59</td><td>1.36 0.53</td><td></td><td>3.04 0.62</td><td>0.82</td><td></td><td>2.09 1.04</td><td>1.53 0.62</td><td>0.88</td><td></td><td>0.86</td><td>0.88</td><td></td><td>0.64</td><td>0.84</td><td></td><td>0.83</td><td>0.82</td><td>0.55</td></tr><tr><td>Aggregate</td><td>0.65</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td> frequencies</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>10</td><td>29</td><td></td><td></td><td>20</td><td></td><td>22</td><td>39</td><td>45</td></tr><tr><td>Agree Disagree.</td><td>45 2</td><td>45 2.</td><td></td><td>44 3</td><td>44 3</td><td></td><td></td><td>6. 41</td><td>44 2.</td><td>46 1</td><td></td><td>4</td><td>6 41</td><td></td><td>32 15</td><td>44 3</td><td>11 36</td><td></td><td>37.</td><td>18</td><td>42</td><td></td><td>27.</td><td>25</td><td></td><td>8</td><td>1</td></tr></table></body></html>

![](img/f7b05f4bbb8928a7bc1e57ad820ac3b8bdcbb56081eb5e1bb49ebf653a96fed9.jpg)  
Figure 6. P articipants’ Preference of Delivery Mode.

The results from the final survey, in which students were asked to compare both delivery formats directly, seem to echo the post-task responses insofar as they show a slight preference for in-person delivery. As shown in Figure 6, $4 6 . 8 \%$ of the students $\left( n = 2 2 \right)$ did not have a preference with regard to how the speaking task was administered, indicating that they liked both modes of administration. By contrast, $4 2 . 6 \%$ of the students $( n = 2 0 )$ preferred the face-to-face version of the task, while only $1 0 . 6 \%$ $\left( n = 5 \right)$ expressed a preference for the SDS-delivered version of the task.

In general, participants’ reasons for preferring one delivery mode over the other seemed primarily related to issues of anxiety (e.g. ‘I am relaxed because I am not speaking to a real person.’ ID20), willingness to communicate (e.g. ‘Give me a chance to speak to a boss figure before going out to the real workforce’ ID19), body language (e.g. ‘I can find their reaction’ ID17), and a perceived ‘real-life feel’ of the interactions (e.g. ‘It’s real.’ ID37).

# Discussion

In the following, we will discuss the findings relative to the two research questions that guided our study.

# How comparable is the language elicited by a role-play task when administered via fully-automated SDS versus face-to-face?

Overall, the language that was elicited in the two formats showed on average very similar features when compared in terms of syntactic complexity, lexical variety, and fluency (see Table 3). Thus, our findings are very much in line with recent studies that reported similarities in the linguistic output produced by learners in response to open-ended role-plays (Sydorenko, 2011) and interview assessment tasks (Forsyth et  al., 2019) that were administered both digitally and in person. For example, similar to Forsyth et  al., we found syntactic complexity measures and lexical variety indices to be largely the same across both types of administrations, suggesting that the language elicited by the Request Boss task in the two delivery modes was ‘reasonably comparable’ (p. 411). Additionally, fluency indices in the output of SDS task and face-to-face role-play were very much the same, indicating that both task delivery formats allowed learners equally well to show their abilities in terms of delivery. Our results are also in line with Ockey and Chukharev-Hudilainen (2021) who found that human judges’ ratings were similar on fluency, pronunciation, grammar, and vocabulary for the human-human and human-computer conditions. A potential reason for eliciting comparable output could be the way technology has advanced in recent years to better approximate real-life interaction.

However, despite being generally comparable with regard to linguistic aspects of oral language use, we also found differences in the outputs, in particular in terms of speaking time and number of turns. As shown in Table 2, learners spoke on average eight seconds longer when engaged in the SDS version of the Request Boss task than when they interacted face-to-face with the human interlocutor. Despite the longer speaking time in the SDS tasks, learners used significantly fewer turns (see Table 2). A potential reason for this observation could be that learners might have been familiar with the functionality of automated agents. All students who participated in this study were from Generation Z (i.e. people born between 1990 and 2010), a generation that tends to be highly familiar with digital technology, including automated agents, because they have used it from a very young age (Seemiller & Grace, 2017). For instance, some participants noted that the automated agent feels like a real person (e.g. ‘I like the computer AI that made me feel like talking with a real person.’—ID49). Thus, they may be used to interacting with automated systems in real-life and are aware of aspects like the limited opportunity to ask for clarification and/or negotiate meaning with an automated agent. Hence, they may have felt the need to include more information in a single turn to keep the floor and ensure a successful and goal-oriented conversation.

<html><body><table><tr><td rowspan="2">Turn</td><td colspan="2">Participant output elicited via the ....</td></tr><tr><td>SDS version</td><td>Face-to-face version</td></tr><tr><td>1</td><td>Hi Lisa, uh... Yeah,good uh uh so,uh I want to talk with it,talk</td><td>Okay, so uh, hi, Lisa.</td></tr><tr><td>2</td><td>with you about something uh, my about my Yeah,how its going today? presentation. Do you have time this week?</td><td></td></tr><tr><td>3</td><td>Hi Lisa,do you have time this week uh.This week I want to have a meet with you about um some situation.</td><td>Yeah, uh sorry.So,I&#x27;m busy now, so have some questions to ask to you so...</td></tr><tr><td>4</td><td>Yeah,it works for me so,how about Friday? So,I can meet with you--</td><td>Yeah.</td></tr><tr><td>5</td><td>Yeah, I&#x27;ll send my presentation about this meeting to you with email. Can you check it for If it&#x27;s possible? me before the meeting?</td><td></td></tr><tr><td>6 7</td><td>Okay, I&#x27;ll send you to email. Thank you so much.</td><td>Yeah, its always you so... Uh yeah, I have my presentation uh need to check, so,uhcan you check my presentation at that time?</td></tr><tr><td>8</td><td></td><td>Yeah.</td></tr><tr><td>9</td><td></td><td>Mm-hmm.Uh...</td></tr><tr><td>10</td><td></td><td>Okay, thank you so much, and I really appreciate it, all the time.</td></tr><tr><td>11</td><td></td><td>Uh,nope.&lt;%&gt;</td></tr><tr><td>12</td><td></td><td>Mm-hmm.</td></tr><tr><td>13</td><td></td><td>Bye.</td></tr></table></body></html>

By contrast, in the dyadic face-to-face interactions the lower average speaking time, but higher number of turns could be due to participants deploying more social rapport building moves, clarification requests, and backchanneling —features that were also found by Sydorenko (2015) in human-administered role-plays. For example, Figure 7 juxtaposes the oral output produced by ID41 in both delivery formats. There are distinct differences with regard to the number of turns, and the length of individual turns, rapport building moves, clarification requests, and backchanneling. First, the SDS-elicited output includes only six turns while the face-to-face role-play resulted in 13 turns. Moreover, the face-to-face version shows opening and closing moves (Turns 1, 2, and 10-13), aspects that are less prominent in the output elicited in the SDS setting. Also, the SDS-elicited speech does not feature any clarification requests, while ID41 utilized a clarification move in Turn 5 of the face-to-face version. Finally, we can observe backchanneling (e.g. Turns $9 \& 1 2$ ) in the face-toface version, but not in the SDS version of the task. Hence, there seems to be a distinction in the elicited speech relative to the two delivery modes at the level of social and functional language use—an observation that we also made with regard to the elicited requests. In other words, the SDS exchanges were more transactional or informational, while the face-to-face interactions were more social. This finding echoes Ockey and Chukharev-Hudilainen (2021) where raters felt that the human-human format is better for assessing interactional competence (i.e. social functions) than the human-computer format. This finding has interesting implications for what students are actually practicing with each format, and why teachers might choose to use one format over the other.

As shown in Table 4, both the human and the automated agent were able to elicit requests from participants, even though the number of elicited requests differed slightly between the two types of delivery modes. The human interlocutor seemed to be slightly more successful in eliciting both requests that participants were required to make as per the task prompt. Since human interlocutors tend to facilitate certain interactive moves in different ways (e.g. Okada & Greer, 2013; Ross & O’Connell, 2013), we assume that in the face-to-face interactions, participants relied more heavily on facial expressions and body language given that 25 out of 45 participants reported that they could rely on body language, not just their listening skills when engaging with the human interlocutor.

Additionally, the quality of the elicited requests differed slightly between the two delivery modes. Overall, there was a stronger tendency among participants to deploy more direct requests and fewer conventionally indirect requests in the SDS than in the human-proctored role-plays (see Table 6). Additionally, in output elicited by the SDS version of the task, we did not find any hints. By contrast, we observed five instances of hints in the performances resulting from the face-toface interactions—a finding that may be due to the perceived differences between interlocutors. Blum-Kulka (1987), for instance, highlighted that hints are difficult to decode, even for human interlocutors given that they strongly rely on context and tend to unfold over several turns in a conversation. Thus, compared to a human, the fully automated agent in the SDS version of the task may have been perceived as even less likely to interpret hints which may have resulted in challenges to advancing the conversation (Timpe-Laughlin & Dombi, 2020).

# What are the learners’ perceptions of the SDS role-play task versus the face-to-face role-play task?

Many students liked both types of activities, though some indicated a preference for face-to-face or SDS tasks. This observation is similar to Sydorenko’s (2011) finding that some students preferred face-toface role-plays whereas others liked computer-delivered tasks better.

Prior studies indicate that participants’ individual characteristics (shy versus outgoing, having an affinity for technology or not, etc.) may have an effect on their preferences towards task formats (e.g. Anyaegbu et  al., 2012; Ockey et  al., 2017; Sydorenko et  al., 2018). In our survey, we found evidence that some participants felt more nervous than others with certain types of delivery modes, and that they varied considerably relative to the perceived importance of conversational features, authenticity, and ability to practice in a safe, self-paced environment.

Overall, the minimal differences between task delivery modes with regard to fluency, syntactic complexity, and lexical variety measures, coupled with participants’ preferences towards one type of task versus another suggest that, when possible, students should be given a choice in how a task is delivered for practice and also for assessment. Although more research is needed before such a recommendation can be applied to high-stakes testing, our results are promising with regard to the feasibility of using SDS tasks for low-stakes assessment.

# Concluding remarks

Taking everything into account, both task delivery formats seemed to allow learners to show what they can do in the target language. The linguistic features in the output resulting from interactions with the automated and the human interlocutors were comparable across the two formats, suggesting that SDS tasks could be used as effective ways of practicing speaking and eliciting oral performance data for measuring general language proficiency and, as such, constitute a reasonable alternative to conducting face-to-face role-plays which tend to be time and resource intensive given that they require a human interlocutor to be present. Moreover, $4 7 \%$ of the students enjoyed both task formats. Nevertheless, the human interlocutor and, in particular body language and facial expressions in the face-to-face interactions, seemed to have introduced a social element that slightly affected the functional language use insofar as the SDS-delivered task elicited more direct requests as opposed to more indirect ones. This observation is supported by the finding that out of $5 3 \%$ of the students who preferred one format over the other, the majority favored face-to-face communication given that they could see the interlocutors’ reactions, emotions, and facial expressions. To mitigate this potential impact, replacing the image of the interlocutor featured in the SDS version of the task with videos that show Lisa Green uttering her turns could possibly help mitigate this preference—an idea that could be pursued in future research and development of SDS tasks.

However, despite the positive findings, this study is not without limitations. First, the study had a relatively small convenience sample; hence, the findings should not be generalized beyond the current context. For example, the study should be repeated with a larger number of participants at each specific proficiency level (beginning, intermediate, advanced). Second, this study only investigated a single task. Additional tasks should be examined across different modes of delivery. Third, we did not video-record interactions. Future research may want to consider including video-recordings in order to allow for a systematic analysis of the impact facial expressions and body language may have on oral interactions in semi-scripted role-plays. Also, while the focus of the current investigation lay on the students’ output, future studies could investigate the linguistic and functional aspects included in the interlocutors’ utterances to examine a potential interlocutor effect. Finally, despite the use of a counterbalanced design, due to the use of the same task, a potential impact of a practice effect or boredom on participants’ performances cannot be ruled out.

Nevertheless, the findings are supportive of SDS-delivered tasks in the context of teaching and assessing L2 oral interaction. Especially for larger classrooms in which speaking time per student tends to be limited, interactive SDS-administered speaking tasks may provide a useful alternative to face-to-face role-plays. Thus, they give all students the opportunity to practice speaking in the target language. Additionally, semi-scripted role-plays delivered via SDS may constitute a good diagnostic tool (Timpe-Laughlin et  al., 2020; van Batenburg et  al., 2018), allowing for a systematic elicitation as well as formative assessment of L2 learners’ oral performance.

# Disclosure statement

No potential conflict of interest was reported by the authors.

# Notes on contributors

Veronika Timpe-Laughlin (Ph.D., TU Dortmund University, Germany) is a research scientist in the Center for Language Education and Assessment Research at Educational Testing Service (ETS) in Princeton, NJ. Her research interests include pragmatics, language assessment, task-based language teaching, and technology in L2 instruction and assessment for both young and adult learners.

Tetyana Sydorenko (Ph.D., Michigan State University) is an Associate Professor of Applied Linguistics at Portland State University. Her research interests include L2 pragmatics, computer-assisted language learning, multimodality, and assessment. Her most recent investigations include the use of computer simulations for the teaching of L2 pragmatics.

Judit Dombi (Ph.D., dr. habil, University of Pécs, Hungary) is an Associate Professor of Linguistics at the University of Pécs, Hungary, where she teaches undergraduate and graduate courses in linguistics. Her fields of interest include theoretical and applied aspects of intercultural communication and pragmatics in intercultural and ELF contexts. Her recent research has focused on the communicative functions of directness and indirectness, communication asymmetries and human-machine interactions.

# ORCID

Veronika Timpe-Laughlin $\textcircled{1}$ http://orcid.org/0000-0001-8757-8881 Tetyana Sydorenko $\textcircled{1}$ http://orcid.org/0000-0002-1173-657X Judit Dombi $\textcircled{1}$ http://orcid.org/0000-0001-9278-9093

# References

Al-Gahtani, S., & Roever, C. (2012). Proficiency and sequential organization of L2 requests. Applied Linguistics, 33(1), 42–65. https://doi.org/10.1093/applin/amr031   
Anyaegbu, R., Ting, W., & Li, Y. (2012). Serious game motivation in an EFL classroom in Chinese primary school. TOJET, 11(1), 154–164.   
Arnold, N. (2007). Reducing foreign language communication apprehension with computer-mediated communication: A preliminary study. System, 35(4), 469–486. https://doi.org/10.1016/j.system.2007.07.002   
Ayres, L. (2008). Thematic coding and analysis. In L. Given (Ed.), The Sage encyclopedia of qualitative research methods (pp. 867–868). Sage.   
Bachman, L. (1990). Fundamental considerations in language testing. Oxford University Press.   
Bachman, L., & Palmer, A. (2010). Language assessment in practice. Oxford University Press.   
Baralt, M., & Gurzynski-Weiss, L. (2011). Comparing learners’ state anxiety during task-based interaction in computermediated and face-to-face communication. Language Teaching Research, 15(2), 201–229. https://doi.org/10.1177/0265532210388717   
Beare, K. (2016). How many people learn English globally? About Education. http:// esl.about.com/od/englishlearningresources/f/f_eslmarket.htm.   
Bibauw, S., François, T., & Desmet, P. (2019). Discussing with a computer to practice a foreign language: From a conceptual framework to a research agenda for dialogue-based CALL. Computer Assisted Language Learning, 32(8), 827–877. https:// doi.org/10.1080/09588221.2018.1535508   
Bibauw, S., François, T., Van den Noortgate, W., & Desmet, P. (2022). Dialogue systems for language learning: a meta-analysis. Language Learning & Technology, 26(1), 1–25.   
Blum-Kulka, S. (1987). Indirectness and politeness in requests: Same or different? Journal of Pragmatics, 11(2), 131–146. https://doi.org/10.1016/0378-2166(87)90192-5   
Blum-Kulka, S., House, J., & Kasper, G. (1989). Cross-cultural pragmatics: Requests and apologies. Ablex.   
Celce-Murcia, M. (2007). Rethinking the role of communicative competence in language teaching. In E. Alcon Sóler & M. P. Safont Jordà (Eds.), Intercultural language use and language learning (pp. 41–57). Springer. https://doi.org/10.1007/978-1-4020-5639-0_3   
Chen, L., Zechner, K., Yoon, S.-Y., Evanini, K., Wang, X., Loukina, A., Tao, J., Davis, L., Lee, C. M., Ma, M., Mundkowsky, R., Lu, C., Leong, C. W., & Gyawali, B. (2018). Automated Scoring of Nonnative Speech Using the SpeechRaterSM v. 5.0 Engine. (Research Report No. RR-18-10). Educational Testing Service. https://doi.org/10.1002/ ets2.12198   
Chiu, T. L., Liou, H. C., & Yeh, Y. (2007). A study of web-based oral activities enhanced by automatic speech recognition for EFL college learning. Computer Assisted Language Learning, 20(3), 209–233. https://doi.org/10.1080/09588220701489374   
Clément, R., Gardner, R. C., & Smythe, P. C. (1977). Motivational variables in second language acquisition: A study of Francophones leaming English. Canadian Journal of Behavioural Science, 9, 123–133. https://doi.org/10.1037/h0081614   
Coté, S., & Gaffney, C. (2018). The effect of synchronous computer-mediated communication on beginner L2 learners’ foreign language anxiety and participation. The Language Learning Journal, 49(1), 105–116. https://doi.org/10.1080/09571736.2018.1484935   
Dörnyei, Z. (2005). The psychology of the language learner: Individual differences in second language acquisition. Erlbaum.   
Félix-Brasdefer, F. (2018). Role plays. In A. H. Jucker, K. P. Schneider, & W. Bublitz (Eds.), Methods in pragmatics (pp. 305–331). Mouton De Gruyter. https://doi. org/10.1515/9783110424928-012   
Forsyth, C. M., Luce, C., Zapata-Rivera, D., Jackson, G. T., Evanini, K., & So, Y. (2019). Evaluating English language learners’ conversations: Man vs. Machine. Computer Assisted Language Learning, 32(4), 398–417. https://doi.org/10.1080/09588221.2018.1517126   
Halleck, G. (2007). Data generation through role-play: assessing oral proficiency. Simulation & Gaming, 38, 91–106. https://doi.org/10.1177/1046878106298268   
Hewitt, E., & Stephenson, J. (2012). Foreign language anxiety and oral exam performance: A replication of Phillips’s MLJ Study. The Modern Language Journal, 96, 170–189. https://doi.org/10.1111/j.1540-4781.2011.01174.x   
Horwitz, E., Horwitz, B., & Cope, J. (1986). Foreign language classroom anxiety. The Modern Language Journal, 70, 125–132. https://doi.org/10.1111/j.1540-4781.1986.tb05256.x   
Huth, T. (2010). Can talk be inconsequential? Social and interactional aspects of elicited second-language interaction. The Modern Language Journal, 94(4), 537–553. https://doi.org/10.1111/j.1540-4781.2010.01092.x   
Johnson, W. L., & Valente, A. (2009). Tactical language and culture training systems: Using AI to teach foreign languages and cultures. AI Magazine, 30(2), 72. https:// doi.org/10.1609/aimag.v30i2.2240   
Kalaja, P., & Barcelos, A. M. F. (2003). Introduction. In P. Kalaja & M. F. Barcelos (Eds.), Beliefs about SLA: New research approaches (pp. 1–4). Kluwer. https://doi. org/10.1007/978-1-4020-4751-0   
Kasper, G., & Dahl, M. (1991). Research methods in interlanguage pragmatics. Studies in Second Language Acquisition, 13, 215–247. https://doi.org/10.1017/S0272263100009955   
Kasper, G., & Rose, K. R. (2002). Pragmatic development in a second language. Blackwell.   
Kim, S. Y. (1998). Affective experience of Korean college students in different instructional contexts: Anxiety and motivation in reading and conversation courses [Unpublished doctoral dissertation]. The University of Texas.   
Kormos, J. (1999). Simulating conversations in oral-proficiency assessment: A conversation analysis of role plays and non-scripted interviews in language exams. Language Testing, 16(2), 163–188. https://doi.org/10.1177/026553229901600203   
Larsen-Freeman, D. (2009). Adjusting expectations: The study of complexity, accuracy, and fluency in second language acquisition. Applied Linguistics, 30(4), 579–589. https://doi.org/10.1093/applin/amp043   
Litman, D., Strik, H., & Lim, G. S. (2018). Speech technologies and the assessment of second language speaking: Approaches, challenges, and opportunities. Language Assessment Quarterly, 15(3), 294–309. https://doi.org/10.1080/15434303.2018.1472265   
Lu, X. (2011). A corpus-based evaluation of syntactic complexity measures as indices of college-level ESL writers’ language development. TESOL Quarterly, 45(1), 36–62. https://doi.org/10.5054/tq.2011.240859   
McDonald, J. H. (2014). Handbook of biological statistics. Sparky House Publishing.   
Morton, H., Gunson, N., & Jack, M. A. (2012). Interactive language learning through speech-enabled virtual scenarios. Advances in Human-Computer Interaction, 2012, 1–14. https://doi.org/10.1155/2012/389523   
Nation, I. S. P., & Newton, J. (2008). Teaching ESL/EFL speaking and listening. Routledge. https://doi.org/10.4324/9780203891704   
Ockey, G. J., & Chukharev-Hudilainen, E. (2021). Human versus computer partner in the paired oral discussion test. Applied Linguistics, 42(5), 924–944. https://doi. org/10.1093/applin/amaa067   
Ockey, G. J., Gu, L., & Keehner, M. (2017). Web-based virtual environments for facilitating assessment of L2 oral communication ability. Language Assessment Quarterly, 14, 346–359. https://doi.org/10.1080/15434303.2017.1400036   
Okada, Y., & Greer, T. (2013). Pursuing a relevant response in oral proficiency interview role plays. In S. J. Ross & G. Kasper (Eds.), Assessing second language pragmatics (pp. 288–310). Palgrave Macmillan. https://doi.org/10.1057/9781137003522_11   
Ortega, L. (2003). Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college-level L2 writing. Applied Linguistics, 24, 492–518. https://doi.org/10.1093/applin/24.4.492   
Philips, E. M. (1992). The effects of language anxiety on students’ oral test performance and attitudes. The Modern Language Journal, 76, 14–26. https://doi. org/10.1111/j.1540-4781.1992.tb02573.x   
Ramanarayanan, V., Evanini, K., & Tsuprun, E. (2020). Beyond monologues: Automated processing of conversational speech. In K. Zechner & Keelan Evanini (Eds.), Automated speaking assessment: Using language technologies to score spontaneous speech (pp. 176–191). Routledge.   
Ramanarayanan, V., Suendermann-Oeft, D., Lange, P., Mundkowsky, R., Ivanov, A. V., Yu, Z., Qian, Y., & Evanini, K. (2017). Assembling the jigsaw: How multiple open standards are synergistically combined in the HALEF multimodal dialog system. In D. A. Dahl (Ed.), Multimodal Interaction with W3C Standards: Towards Natural User Interfaces to Everything (pp. 295–310). Springer. https://doi.org/10.1007/978-3-319-42816-1_13   
Reeves, B., & Nass, C. I. (1996). The media equation: How people treat computers, television, and new media like real people and places. Center for the Study of Language and Information; Cambridge University Press.   
Roed, J. (2003). Language learner behaviour in a virtual environment. Computer Assisted Language Learning, 16(2–3), 155–172. https://doi.org/10.1076/call.16.2.155.15880   
Roever, C., & Kasper, G. (2018). Speaking in turns and sequences: Interactional competence as a target construct in testing speaking. Language Testing, 35(3), 331–355. https://doi.org/10.1177/0265532218758128   
Ross, S. J., & O’Connell, S. P. (2013). The situation with complication as a site for strategic competence. In S. Ross & G. Kasper (Eds.), Assessing second language pragmatics (pp. 311–326). Palgrave Macmillan.   
Seemiller, C., & Grace, M. (2017). Generation Z: Educating and Engaging the Next Generation of Students. About Campus: Enriching the Student Learning Experience, 22(3), 21–26. https://doi.org/10.1002/abc.21293   
Sheehan, K. M. (2016). A review of evidence presented in support of three key claims in the validity argument for the TextEvaluator text analysis tool. (ETS Research Report No. RR-16-12). : Educational Testing Service. https://doi.org/10.1002/ets2.12100   
Skehan, P. (1989). Individual differences in second language learning. Edward Arnold.   
Skehan, P. (1991). Individual differences in second language learning. Studies in Second Language Acquisition, 13, 275–298. https://doi.org/10.1017/S0272263100009979   
Spolsky, B. (1989). Conditions for second language learning. Oxford University Press.   
Sydorenko, T. (2015). The use of computer-delivered structured tasks in pragmatic instruction: An exploratory study. Intercultural Pragmatics, 12, 333–362. https://doi. org/10.1515/ip-2015-0017   
Sydorenko, T., Daurio, P., & Thorne, S. L. (2018). Refining pragmatically-appropriate oral communication via computer-simulated conversations. Computer Assisted Language Learning, 31(1-2), 157–180. https://doi.org/10.1080/09588221.2017.1394326   
Sydorenko, T., Jones, Z. W., Daurio, P., & Thorne, S. L. (2020). Beyond the curriculum: Extended discourse practice through self-access pragmatics simulations. Language Learning & Technology, 24(2), 48–69. http://hdl.handle.net/10125/44725   
Sydorenko, T. (2011). Exploring the potential of rehearsal via automatized structured tasks versus face-to-face pair work to facilitate pragmatic and oral development [Unpublished doctoral dissertation]. Michigan State University.   
Sykes, J., Oskoz, A., & Thorne, S. L. (2008). Web 2.0, synthetic immersive environments, and mobile resources for language education. CALICO Journal, 25(3), 528–546. https://doi.org/10.1558/cj.v25i3.528-546   
Taguchi, N., Li, Q., & Tang, X. (2017). Learning Chinese formulaic expressions in a scenario-based interactive environment. Foreign Language Annals, 50(4), 641–660. https://doi.org/10.1111/flan.12292   
Tang, X. (2019). The effects of task modality on L2 Chinese learners’ pragmatic development: Computer-mediated written chat vs. face-to-face oral chat. System, 80, 48–59. https://doi.org/10.1016/j.system.2018.10.011   
Timpe-Laughlin, V., & Dombi, J. (2020). Exploring L2 learners’ request behavior in a multi-turn conversation with a fully automated agent. Intercultural Pragmatics, 17(2), 221–257. https://doi.org/10.1515/ip-2020-0010   
Timpe-Laughlin, V., & Park, I. (2019). Are you into beer pong?” –Exploring question-answer sequences in an L2 oral performance assessment. Language Assessment Quarterly, 16(1), 21–38. https://doi.org/10.1080/15434303.2019.1609964   
Timpe-Laughlin, V., Sydorenko, T., & Daurio, P. (2020). Using spoken dialogue technology for L2 speaking practice: What do teachers think? Computer Assisted Language Learning, 1–24. https://doi.org/10.1080/09588221.2020.1774904   
Tóth, Z. (2007). Foreign language anxiety: A study of first-year English majors [Unpublished doctoral dissertation]. Eötvös Lóránt University.   
van Batenburg, E. S. L., Oostdam, R. J., van Gelderen, A. J. S., & de Jong, N. H. (2018). Measuring L2 speakers’ interactional ability using interactive speech tasks. Language Testing, 35(1), 75–100. https://doi.org/10.1177/0265532216679452   
van Doremalen, J., Boves, L., Colpaert, J., Cucchiarini, C., & Strik, H. (2016). Evaluating automatic speech recognition-based language learning systems: A case study. Computer Assisted Language Learning, 29(4), 833–851.   
Wang, W., & Loewen, S. (2016). Nonverbal behavior and corrective feedback in nine ESL university-level classrooms. Language Teaching Research, 20(4), 459–478. https:// doi.org/10.1177/1362168815577239   
Warschauer, M., Turbee, L., & Roberts, B. (1996). Computer learning networks and student empowerment. System, 24(1), 1–14. https://doi.org/10.1016/0346-251X(95)00049-P   
Wik, P., & Hjalmarsson, A. (2009). Embodied conversational agents in computer assisted language learning. Speech Communication, 51(10), 1024–1037. https://doi. org/10.1016/j.specom.2009.05.006   
Youn, S. J. (2020). Interactional features of L2 pragmatic interaction in role‐play speaking assessment. TESOL Quarterly, 54(1), 201–233. https://doi.org/10.1002/tesq.542   
Zechner, K., & Evanini, K. (Eds.). (2020). Automated speaking assessment: Using language technologies to score spontaneous speech. Routledge.   
Ziegler, N. (2016). Synchronous computer-mediated communication and interaction. Studies in Second Language Acquisition, 38(3), 553–586. https://doi.org/10.1017/ S027226311500025X