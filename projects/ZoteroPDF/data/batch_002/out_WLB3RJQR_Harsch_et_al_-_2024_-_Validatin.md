# Validating an integrated reading-into-writing scale with trained university students

Claudia Harsch a,\* , Valeriia Koval a , Paraskevi (Voula) Kanistra b Ximena Delgado-Osorio c

a Department of Languages and Literatures, University of Bremen, Universitats-Boulevard ¨ 13, 28359 Bremen, Germany b Trinity College London, London, United Kingdom c Teacher and Teaching Quality Department, DIPF | Leibniz Institute for Research and Information in Education, Frankfurt am Main, German

# A R T I C L E I N F O

# A B S T R A C T

Keywords:   
Rating scale   
Validation   
Integrated reading-into-writing   
Explanatory mixed-methods design   
Scoring validity   
Task-type specific criteria

Integrated tasks are often used in higher education (HE) for diagnostic purposes, with increasing popularity in lingua franca contexts, such as German HE, where English-medium courses are gaining ground. In this context, we report the validation of a new rating scale for assessing reading-into-writing tasks. To examine scoring validity, we employed Weir’s (2005) sociocognitive framework in an explanatory mixed-methods design. We collected 679 integrated performances in four summary and opinion tasks, which were rated by six trained student raters. They are to become writing tutors for first-year students. We utilized a many-facet Rasch model to investigate rater severity, reliability, consistency, and scale functioning. Using thematic analysis, we analyzed think-aloud protocols, retrospective and focus group interviews with the raters. Findings showed that the rating scale overall functions as intended and is perceived by the raters as valid operationalization of the integrated construct. FACETS analyses revealed reasonable reliabilities, yet exposed local issues with certain criteria and band levels. This is corroborated by the challenges reported by the raters, which they mainly attributed to the complexities inherent in such an assessment. Applying Weir’s (2005) framework in a mixed-methods approach facilitated the interpretation of the quantitative findings and yielded insights into potential validity threads.

# 1. Introduction

One of the known factors affecting students’ academic success is academic-linguistic preparedness (Murray, 2016). To evaluate this preparedness, assessment tasks are required which integrate different skills, such as reading and writing in a summary task (Cumming, 2013). In recent years, assessment in academic English has indeed shifted from skills-based towards integrated assessment, along with research on the development and validation of rating scales (or scoring rubrics) for integrated tasks (e.g., Chan et al., 2015; Jia & Zhang, 2023; Li & Wang, 2021; Ono et al., 2019; Uludag & McDonough, 2022; Xie, 2023). The different rubrics operationalize the integrated construct in a variety of ways, i.e., different rubrics use differing criteria. Further variation is introduced by genre-specific criteria, since integrated writing tasks differ in the types of genres they elicit. Here, rubrics also differ in their approach to operationalize genre-related aspects (Xie, 2023; Li & Wang, 2021).

With regard to applying rubrics in the realm of integrated reading-into-writing assessment in a reliable and valid way, there are a number of assessment challenges reported in the literature. For instance, raters find it challenging to differentiate source-text ideas and language from writers’ own ideas and language (Gebril & Plakans, 2014; Knoch & Sitajalabhorn, 2013; Li & Wang, 2021), and it is difficult to assess the appropriate proportions that should focus on the source and on writers’ own ideas development respectively (Wang et al., 2017). Such challenges may potentially threaten scoring validity, which makes them a viable object of validation research. Validation studies, while predominantly quantitative, would benefit from further qualitative insights into such challenges (Lestari & Brunfaut, 2023; Xie, 2023).

The mixed-methods study we report here focuses on the scoring validity of a newly developed rating scale for diagnosing first-year students’ integrated reading-into-writing skills in academic programmes with English as the medium of instruction. Our report takes into account the aforementioned lack of consensus on how to operationalize the integrated construct in (genre-specific) criteria, and our study addresses the need for more qualitative insights into scoring validity by examining the challenges trained novice raters experience when applying the scoring rubrics. We first outline the context of the study, before we review relevant literature on scoring validity issues, on the development and validation of integrated rubrics, and on construct operationalization in integrated rubrics.

# 2. Context of the study

The study we report here is part of a larger research project funded by the German Research Foundation (DFG, details and title to be provided after review), which examines academic-linguistic preparedness among students taking up English-medium instruction (EMI) studies in Germany. A further aim of the project is to model the multidimensional structure of the integrated construct. Integrated skills in English as lingua franca are gaining importance in German higher education (HE), where EMI study programmes are increasing due to internationalization endeavours. While studies exist regarding the linguistic preparedness of international students in Germany (e.g., Motz, 2005), little is known about such competences among so called home students, i.e., students with a German schooling background taking up an EMI programme.

In the German school system, students leave secondary education with the Abitur, an exam comparable to the international baccalaureate that allows access to HE. Besides the compulsory subjects of Maths and German, students can choose two majors in which to pass the Abitur. For majoring in English, the Educational Standards (KMK, 2014) require level B2 (in some parts C1) of the CEFR (Council of Europe, 2001), and the Abitur examines all five skills (including mediation). While there has not yet been a formal alignment study between the Abitur and the CEFR, the Abitur tasks are developed centrally and the Institute for Educational Quality Improvement (www.iqb.hu-berlin.de) monitors the development of the Abitur.

With regard to the writing section in the Abitur for English, there is always one integrated writing task that requires source-based writing, either by requiring a summary of a source text, or by requiring an argumentative opinion text using ideas from a source text. The source texts have a length of up to 1000 words and deal with a topic that relates to students’ experiences. Furthermore, there is an analytic writing task that requires students to analyse certain linguistic or stylistic means used in the source text. A third independent writing task requires students to write an essay or article as response to a short prompt. Overall, students have 180 min for the writing section.

When it comes to language requirements in EMI study programmes in German HE, needs analyses by Ringwald and Harsch (2018) revealed that integrated writing competences are of seminal importance. Most BA programmes in Germany require a proficiency level of B2, and majoring in English in the Abitur is generally accepted as certification of this level.

The DFG-funded project hence examines integrated writing competences at the crossroads between the Abitur (as entrance qualification to HE) and first-years students in EMI programmes (who have successfully passed the Abitur in English). In the project, assessment instruments for evaluating such integrated reading-into-writing skills were developed. These encompass integrated reading-into-writing tasks (summary and opinion) and an analytic task-type-specific rating scale.

The validation of this newly developed rating scale is the focus of the study we report here. We pay specific attention to the scale application, the scoring procedures and the resulting scores. We examine scoring reliabilities, scale functioning and rating procedures, as well as raters’ perceptions of the scale and any challenges that may threaten scoring validity.

The latter aspect is of particular importance, since we plan to employ the integrated assessment to diagnose English writing skills among first-year students, with trained senior students acting as writing tutors. Consequently, we investigate in our validation study to what extent trained senior students can apply the rating scale in comparable and reliable ways so that resulting scores can be interpreted as a valid reflection of first-year student’s integrated writing skills. This way, we take the future assessment purpose into account in the validation approach, as suggested by Knoch et al. (2021). Our findings can inform similar writing support projects that wish to involve student tutors.

# 3. Literature review

# 3.1. Scoring Validity

Scoring validity depends on several factors, such as the valid operationalisation of the construct in the rating scale criteria and levels, the valid interpretation of the scale by the raters, and the rating conditions and procedures (e.g., Knoch & Chapelle, 2018; Weir, 2005). These factors influence the reliability and validity of the resulting scores. However, for the realm of rating scale validation, the traditional “strict separation between reliability and validity is hard to maintain”, as Deygers and Van Gorp (2015, p. 527) argue. In order to account for this interrelatedness of reliability and validity, multi-facet Rasch measurement (MFRM) is nowadays widely accepted for examining scale and rater functioning. The quantitative score analyses are best complemented by qualitative approaches to examining the validity of scale interpretation and rating procedures (cf. Gebril & Plakans, 2014; Lestari & Brunfaut, 2023; Wang et al., 2017), which would provide “valuable evidence of the scoring validity that cannot be obtained by solely relying on quantitative analysis of scores” (Jia & Zhang, 2023, p. 16). Here, questions such as how raters apply the scale, how they form their judgements, and what rating procedures they follow are seminal and under-researched (e.g., Knoch & Chapelle, 2018; Lestari & Brunfaut, 2023; Wang et al., 2017, Xie, 2023).

With regard to a suitable validation framework for examining scoring validity, many of the recent rubric validation studies do not explicitly situate their study in a specific framework (cf., Chan et al., 2015; Lestari & Brunfaut, 2023; Li & Wang, 2021; Ono et al., 2019; Uludag & McDonough, 2022; or Xie, 2023). We would argue that using an established validation framework adds transparency and systematicity. We find Weir’s (2005) socio-cognitive validation framework particularly useful, as it transparently guides the validation process and includes a particular section on writing tests and scoring validity. Weir’s (2005) writing validation framework encompasses all relevant factors: He recommends examining the criteria, the rating scale, the raters and their processes, the rating procedures (encompassing rater training, standardization, rating conditions, rating, moderation, and statistical analyses), and the final grading and awarding.

When developing and validating a new scale, it is important to take challenges into account that raters may encounter, as these may lead to unreliable or inconsistent scores, which in turn may potentially threaten scoring validity. This is particularly relevant when assessing integrated performances, as this entails some inherent complexities (e.g., Gebril & Plakans, 2014; Chan & May, 2023). One such challenge is the difficulty to differentiate source-text language from the language produced by test takers (Cumming, 2013; Gebril & Plakans, 2014). In a similar vein, raters need to differentiate between source-text ideas and writers’ own ideas, particularly in tasks that require such a synthesis of different ideas (Knoch & Sitajalabhorn, 2013; Uludag & McDonough, 2022). Evaluating the appropriacy of the paraphrasing of source-text ideas, i.e., presenting these in the writer’s own language, is yet another challenge for raters (Cumming, 2013; Li & Wang, 2021), as is the evaluation of coverage of main ideas particularly in summary tasks (Li & Wang, 2021). Wang et al. (2017), in their mixed-methods study on factors that raters perceive as challenging, identified three factors as potential sources for inaccurate scores, i.e., the aforementioned textual borrowing and its appropriacy, the proportion of an essay that should focus on the source text, and the proportion of original idea development. To address these challenges, Wang et al. (2017) recommend developing analytic criteria that provide guidance on these factors; including relevant textual borrowing frameworks such as the one by Shi (2004) into the rating scale; selecting difficult-to-rate performances as benchmarks; and conducting regular re-training.

With regard to challenges that may stem from the rating scale itself, first, analytic criteria bear the potential of conceptual interdependency. This might be inevitable, despite trying to conceptualise the criteria as distinctly as possible. Therefore, Lestari and Brunfaut (2023) argue for addressing these “perceived overlaps” (p.35) in rater training. Moreover, the wording of the scale descriptors plays a major role for consistency and severity effects: Descriptors that qualitatively define seminal characteristics of a criterion seem to help raters interpret the descriptors more consistently than descriptors that mainly rely on adjectives or modifiers; the latter may have an effect on greater variability in severity, as Lestari and Brunfaut (2023) report. Furthermore, Xie (2023) found that for certain criteria, raters struggled to differentiate between the levels. They recommended “omitting infrequently used levels or combining adjacent ones” (2023, p. 13) to address this problem.

Concerning the effects certain genres may have on scoring validity, Xie (2023) found that raters were less consistent in their application of the criterion source-text use when rating argumentative essay as compared to rating summaries. However, more research is needed about the effects task types might have on the underlying construct and hence, which genre-specific rating criteria would best contribute towards a valid interpretation of students’ skills (Xie, 2023; Li & Wang, 2021).

We now review literature on the nature of analytic rating criteria, including genre-specific criteria, which serve to validly operationalize the construct of integrated writing.

# 3.2. Construct and assessment criteria of integrated writing

In the academic context, integrated writing is “one of the most typical modes of information use” (Sormunen et al., 2012, p. 2). It is characterised by an interplay between receptive and productive skills. Regarding its construct, Knoch and Sitajalabhorn (2013) depict six constituting aspects: “(1) mining the source texts for ideas, (2) selecting ideas, (3) synthesising ideas from one or more source texts, (4) transforming the language used in the input, (5) organizing ideas and (6) using stylistic conventions such as connecting ideas and acknowledging sources” (p.106). Research suggests that the integrated construct also depends on the task type or genre (e.g., Cumming et al., 2016; Xie, 2023).

Along with research into the nature of the integrated construct, there has been a rise of studies focusing on the development and validation of rating scales (or scoring rubrics) for integrated writing tasks (e.g., Chan et al., 2015; Li & Wang, 2021; Uludag & McDonough, 2022; Xie, 2023). Such integrated rubrics, particularly if they are to be used in educational contexts for diagnostic or formative purposes, differentiate the complex construct into distinct components (Knoch, 2011). This facilitates systematic assessment, guiding raters’ focus on relevant aspects, while simultaneously adding diagnostic value (e.g., Lestari & Brunfaut, 2023 or Uludag & McDonough, 2022).

The number of analytic criteria used in recently published rubrics varies mainly between four (e.g., Chan et al., 2015; Cheong et al., 2019; Li & Wang, 2021; Ono et al., 2019) and five (e.g., Plakans & Gebril, 2015; Shin & Ewert, 2015) criteria, with some genre-specific rubrics using between eight and 14 criteria (e.g., Xie, 2013). The criteria can broadly be categorised along the six areas suggested by Knoch and Sitajalabhorn (2013). Furthermore, all rubrics contain one or more criteria focussing on the writers’ own language production. However, Chan and May (2023), warn against an “over-reliance on linguistic aspects” and recommend “incorporate[ing] relevance of ideas, accuracy of information, and paraphrasing skills in the integrated rating scales" for summary tasks (p. 435). In addition to these areas, rubrics assessing argumentative essays usually add aspects regarding the writer’s stance and argument development (e.g., Shin & Ewert, 2015; Uludag & McDonough, 2022).

With regard to how different rubrics operationalize the different aspects, which elements they combine within a specific criterion, and how these criteria are labelled, we found large and at points confusing heterogeneity. We illustrate this heterogeneity referring to the rubrics from Chan et al. (2015), Cheong et al. (2019), Jia and Zhang (2023), Ono et al. (2019), Plakans and Gebril (2015), Shin and Ewert (2015) and Uludag and McDonough (2022), with a particular focus on their respective operationalizations of the central aspects of source use and synthesis.

To begin with, the criterion source use in Plakans and Gebril’s (2015) rubric contains aspects of relevance and accuracy of selected ideas, along with citation conventions and verbatim use of source language. While a partly comparable combination of correct citation and paraphrasing is found in Cheong et al.’s (2019) rubric, they label this criterion citation and synthesis, including the synthesis of source ideas and own ideas there, as well. Yet the aspects of relevance and accuracy of selected ideas are not explicitly mentioned in their rubric at all.

Source idea selection and accuracy is, however, included in Jia and Zhang’s (2023) rubric, albeit in their content criterion, which also encompasses synthesis and idea development. Ono et al. (2019) use a similar approach, subsuming under content the accurate presentation of key points of the sources and the connection of principal ideas from two sources. A somewhat different definition of the content criterion is found in Uludag and McDonough (2022), who combine stance, synthesis and task fulfillment under content. Task fulfillment as a separate criterion is found in Chan et al. (2015), focusing on the communicative aim, but not including stance and synthesis. Another approach to operationalizing communicative aim is found in Cheong et al.’s (2019) rubric, where communicative purpose is combined with appropriate text type and the focus on self and audience in the criterion contextual awareness.

While aspects of synthesis are often subsumed with other aspects under one criterion, for instance in the aforementioned citation and synthesis criterion or under content, Plakans and Gebril (2015) separate synthesis aspects into three criteria, i.e., authorial voice, development of ideas (including synthesizing source ideas and own ideas) and organisation (encompassing logical development and cohesion). Finally, Shin and Ewert (2015) use an additional criterion text engagement, looking for a balanced engagement with the two sources.

To sum up, the literature shows great variability in operationalizing construct-relevant aspects of integrated writing in different (partly genre-specific) assessment criteria. Given the lack of empirical underpinning for the combination of different aspects in different criteria, we would conclude that it seems most appropriate to disentangle the different aspects of the complex construct into criteria that are as distinct and clearly defined as possible. This conclusion is supported by validations studies that point toward higher discrimination power of more explicitly defined criteria, specifically for criteria operationalizing source-text use (Chan & May, 2023; Lestari & Brunfaut, 2023; Xie, 2023). The ultimate number of criteria in a rating scale, however, will be subject to practicality and efficiency concerns (Li & Wang, 2021).

![](img/eed1c18798156ef0e36ca1aeb79cb1bd6367e75d68fbcef5863b6095d27539a0.jpg)  
Fig. 1. Research design.

# 4. Methods

In the study reported here, we explore the scoring validity of a newly developed rating scale for an integrated writing exam. More specifically, we examine the application of the rating scale for two task types (summary and opinion) by trained student raters, the procedures they followed while rating, the quality of the resulting scores (consistency and reliability), and the scale function. We follow Weir’s (2005) socio-cognitive validation framework, and we employ an explanatory convergent mixed-methods approach (Creswell & Plano Clark, 2018), thus complementing statistical analyses of score qualities and scale functioning with a qualitative angle. We use think-aloud protocols (TAPs) to shed light on how raters apply the scale, and retrospective and focus group interviews to examine raters’ perceptions of the scale and rating conditions. We examine the following research questions (RQs) for our trained novice raters:

1. To what extent do trained student raters vary when applying the rating scale for the two task types?   
2. Can raters differentiate the rating scale criteria and band levels appropriately for the two task types?   
3. In how far do raters follow the trained rating procedures and stages?   
4. What challenges do raters report with regard to interpreting the integrated constructs for summary and opinion tasks operation  
alized in the rating scale criteria and band levels?

# 4.1. Design

Fig. 1 presents the research design, adopting Lestari and Brunfaut’s (2023) illustration.

In this paper, we will first outline the development and nature of the integrated tasks, the development of the rating scale and its criteria, as well as the characteristics of our raters and the rater training, addressing relevant literature for each of these aspects. We treat these aspects as a-priori given, as they form the background to our study on the application of the rating scale, rating procedures, and score quality. Fig. 2 gives an overview of Weir’s (2005) components of scoring validity and shows where we will address them in this paper.

# Weir's (2005) components

# Where and how addressed in our study.

Criteria/rating scale

Development: prior to our study (cf. Authors 2023) Instruments provided in methodology. Rater characteristics and rationale for selection: reported in methodology

<html><body><table><tr><td>Rating procedures:</td></tr><tr><td>Rater training</td></tr><tr><td>Standardization</td></tr><tr><td>Rating conditions / design</td></tr><tr><td>Statistical analyses</td></tr><tr><td>Scale perception and application (not explicitly mentioned in Weir&#x27;s framework).</td></tr></table></body></html>

Design, procedures, outcomes: prior to our study Brief description in methodology.   
Procedures: described in methodology.   
Procedures and design: described in methodology. Focus of study:   
collection and analysis of quantitative rating data. addressed in methodology, findings and discussion Focus of study:   
collection and analysis of qualitative data on how raters used, applied and perceived the rating scale: addressed in methodology, findings and discussion

# 4.2. Test instruments and collection of integrated performances

Two experts (one EAP teacher, one lecturer from the EMI English teacher education programme) developed a suite of four integrated tasks. The source texts should provide enough text to allow for substantial processing and transforming of the language of the source text and its ideas, as emphasized by Knoch and Sitajalabhorn (2013). Hence, the task developers regarded a length of around 1000 words necessary, which also reflects the source texts used in the aforementioned Abitur. Striving for authenticity, the source texts were chosen from introductory study textbooks, two from the larger field of natural sciences and two from the field of social sciences, which the task developers regarded as two broadly accessible subjects. The topics of the texts were chosen by the task developers with a view to accessibility for first-semester students. The source texts were also analysed for their readability characteristics with the help of CohMetrix (Crossley et al., 2011) to control the difficulty of the texts across the tasks. In order to meet the task demands that high school students are familiar with, we strived for readability comparability with source texts used in the Abitur. We also chose two task genres, i.e., summary and opinion, which are requested in the Abitur. The length of the source texts and the tasks´completion time of $6 0 \mathrm { { m i n } }$ was trialled and found suitable.

For each task, we expected performances of 300–350 words. The tasks were piloted among 134 school leavers and freshmen for analysis and improvement. The ensuing task validation will be reported elsewhere. We also developed task-specific expectations, i.e., a list of content points (from the source texts) that we expected to be included in the performances; these lists are to support the raters, as detecting source-text content in integrated performances is one known challenge (Wang et al., 2017) in assessing integrated performances. The task expectations were recommended by four experts (the two task developers and two experienced EAP teachers). The developers then coded source text ideas (T-units) according to their importance for the task (four codes: key idea, supporting idea, specific details or examples, not important). Additionally, 20 student performances per task (from the piloting) were analysed with regard to this coding scheme. The resulting key ideas were incorporated into the task expectations. The expectations are most useful in training, when new raters are familiarised with tasks and expectations; they are also available during rating (if raters wanted to reassure their understanding of task expectations), but the expectations themselves as well as their usage are not accounted for in score assignment.

Using the four integrated reading-into-writing tasks, we collected integrated performances from 116 high-school students in their final year and 298 first-year university students in EMI programs in Germany. Due to the pandemic, all performances were collected online via the CBA Itembuilder platform (Rolke, ¨ 2012). Depending on the test design, students worked on one or two of the tasks, with $6 0 \mathrm { { m i n } }$ allocated for each task. In total, we collected 679 integrated performances.

# 4.3. Rating scale

We developed an analytic rating scale, combining theory-based, descriptor-based, empirical, and intuitive approaches (cf. Chan et al., 2015). With regard to the theory-based first step, we followed the recommendations by Wang et al. (2017), and based our analytic criteria on relevant literature and research, as shown in Fig. 3. To account for the two task types, criterion 1 slightly differs in its wording for the two types, and we added two criteria (3 and 4) for the opinion tasks. Based on the research literature reviewed above, we carefully separated source-idea aspects (criteria 1, 2), transformation and synthesis aspects (criteria 3–7), and language production aspects (criteria 7–9).

In a second step, using a descriptor-based approach, we defined each criterion on five band levels, spanning the CEFR levels B1 + to C1, to capture lower and higher abilities than expected of first-year students in EMI programs in Germany $( \mathbf { B } 2 / \mathbf { B } 2 + )$ . Where possible, we adapted existing descriptors from relevant CEFR scales. Particularly for the first five criteria, we had to also consult other scales, such as the Pearson Education (2015)) Global Scale of English and the CEFR-based rating scale from the Evaluation of the Educational Standards in Germany (Rupp et al., 2008). In addition, resorting again to a theory-based approach, we used the concepts mining, synthesis and structure from Spivey and King (1989), Keck’s (2006) taxonomy of paraphrasing, Shi’s (2004) coding scheme for referencing, and Li’s (2014) concept of rearranging ideas to inform the descriptors.

In a third intuitive step, the draft of the rating scale was presented to the aforementioned experts for feedback. They rated three performances per task and provided critical feedback on the comprehensibility of the descriptors and the feasibility and applicability of the scale. They also tried to fill the gaps in the scale for band level 4 $( \mathbf { B } 2 + )$ , as there were no existing descriptors for this level and the theoretical taxonomies also did not provide descriptors for five band levels. Despite all endeavours, we could not develop suitable descriptors for band level 4, criteria 2 to 6, which would allow a qualitative differentiation of characteristics rather than a differentiation by quantifiers such as “more”. After careful consideration with the experts, we decided to keep the five levels across all criteria consistent. For space reasons, we present the rating scale separately for the two task types in Appendix A. More details on the scale development and the construct operationalized in the scale are reported in Harsch et al. (2024).

# 4.4. Participants and rater training

In validation approaches, the future assessment purpose and context also need to be taken into account, as Knoch et al. (2021) recommend. As mentioned above, we aim to involve trained senior students in the diagnosis in the future. Hence, it is important to conduct our validation study with trained students rather than with experts. Findings reported on differences between novice and expert raters are inconclusive, with some studies suggesting that experts and novices seem to focus on different performance aspects (e. g., Barkaoui, 2010a), with analytic approaches having a positive effect on the consistency of novice raters (Barkaoui, 2010b). Other studies imply that with targeted training, novice and expert raters can achieve similar rating quality (Attali, 2015; Lim, 2011).

# Criterion (abbreviation)

![](img/1b64f39d2ee9b87f0e094f87e35ad79f398c721f040baa17b167789e3fbfabbc.jpg)  
Fig. 3. Analytic criteria and their grounding.

Although novice raters may achieve high reliabilities, some studies (Deygers & Van Gorp, 2015; Harsch & Martin, 2012), indicate that they nevertheless may not agree in scale interpretation. While we cannot examine differences between novice and expert raters, our study can shed further light on whether trained novice raters can validly apply the rating scale and achieve satisfactory scoring reliabilities.

We recruited our raters among senior students from our English Language and teaching programs at University A (details to be applied after review). Students enter these programs with a C1 proficiency in English and study to become English language teachers or (applied) linguists. We wanted to offer the training experience to as many senior students as were interested, and to train more raters than we actually needed, as we are aware that despite training, raters may still exhibit idiosyncratic rating behaviour, as, for instance, Eckes (2012) found. Hence, we offered the training as a seminar (over the course of one semester) open to all interested English program students. To achieve the best possible results, we selected the six best fitting raters at the end of the training, as required by the rating design of the study (see below). This was communicated to the training participants upfront. While such a selection could be regarded as bias for best, we see successful training by its nature as striving towards favourable outcomes in the sense of consistent and reliable scores. The reliabilities achieved at the end of the training are provided in Appendix B. Unfortunately, we did not have the resources to explore in more depth why some raters were less consistent than others at the end of the training.

The training took place over 14 weeks, covering $^ { 5 0 \mathrm { h } }$ of synchronous training sessions and asynchronous practice rounds (all online due to the pandemic). As recommended, for example, by Wang et al. (2017), we familiarized our raters with all instruments, the rating approach, and the rating stages based on Lumley’s (2005) stage model. As this adapted model serves as the basis for validating whether the raters follow the trained procedures, we present it in Appendix C. Furthermore, we familiarized raters with a copy-detection application (henceforth app) that was developed for our project by University B to address the aforementioned challenge of detecting textual borrowing. Raters upload source texts and performances into the web-based app, which automatically color-codes lifted passages. Raters were trained with 80 performances (20 per task) and rated 40 performances (10 per task) at the end as reliability control.

Out of the six1 selected raters, all but one were enrolled in graduate studies in the second half of the respective programs, and one rater was in his/her final BA semester. Four raters had prior experience in teaching or tutoring, and none had prior experience in rating. Three raters had German as their first language, two Russian and one Turkish.

# 4.5. Rating design, conditions and collection of rating data

Prior to rating the performances, the six raters received seven hours of online re-standardization over three days via Zoom. In the synchronous phase, we reminded raters of the rating procedures and re-acquainted them with the instruments and app usage. Raters then rated the benchmark texts independently, and we discussed the feedback on their ratings.

Due to time and financial restraints, we could not afford double-rating of all 679 performances. Moreover, in the future diagnosis, tutors are to work with single ratings. Hence, we employed a connected design (e.g., Eckes, 2015), in which all raters rated the same 72 performances (18 per task) twice, once at the beginning of the rating period and a second time towards the end. The second time, these performances were reordered and presented in different batches, interspersed with other performances to avoid memory effects. The remaining 607 performances were single-rated. This connected design allowed linking all raters, tasks and performances to explore intra- and inter-rater reliability, consistency and agreement. All raters assessed a roughly equal number of performances and worked on all four tasks. Table 1 shows the rating design.

Because of the pandemic, all raters worked at home. The performances were distributed in weekly batches via a secure server, and raters were made aware of data handling requirements. All rating-relevant materials (tasks with source texts, task expectations, rating scale, and handbook including benchmarks) were provided online via a secure server. Raters could print the documents if they wished so. They worked with the app to support them with the detection of source-text borrowings, and entered their ratings in a secure online platform. They worked on average six hours per week on 25 performances, based on the assumption that, on average, they would manage to rate four performances per hour.

# 4.6. Collection of qualitative data

In order to examine rating processes and rater cognition, think-aloud protocols (TAPs) are considered a feasible approach (Barkaoui, 2011; Lumley, 2005), despite showing some limitations, such as veridicality or reactivity. In order to address these limitations, we included familiarization and training at the beginning of the TAP sessions with the five available raters. Furthermore, we employed retrospective interviews immediately after the TAPs (e.g., Charters, 2003; Myford, 2012), as often found in studies on rating processes (e.g., Gebril & Plakans, 2014; Lumley, 2005). We asked raters about their experience, perception of the rating scale, task effects, and any challenges they encountered.

The five TAP sessions and the prior training were conducted online via Zoom (cf. guidelines in Appendix D). For the training, we followed Cumming et al. (2002) and chose a behaviour2 that had nothing to do with the actual rating, in order to not bias “raters´assessment behaviors” (p. 70). As suggested by Collins (2014), we asked the raters to verbalise their thoughts aloud while counting windows in different rooms of their home. The training lasted about three to five minutes. After the training phase, raters were asked to share their screen and work with the app and all materials on this screen. Raters then rated four performances, two from a summary task and two from an opinion task. In order to observe rater behaviour under different conditions, we selected one medium-to-strong performance per task that showed rather little copying; and a second performance per task that was overall medium-to-weak and showed rather more copying. We ensured to not include performances at the extreme ends of the rating scale, and we selected performances close to the average length. The TAP sessions and retrospective interviews were video-recorded and lasted between 1.5 and $2 . 5 \mathrm { h }$ , resulting in almost $^ \mathrm { 1 0 \ h }$ of recording.

To further triangulate the findings from TAPs and retrospective interviews, we employed focus-group interviews (FGIs), as they allow participants to discuss and explain their opinions and perspectives with their peers, thus shedding light on commonalities as well as controversial aspects within the group (e.g., Frey & Fontana, 1991). We conducted the FGIs with our five raters two weeks after the TAP sessions (cf. guidelines in Appendix E). As we had to conduct the FGIs online, we held two sessions, FGI1 with R3, R4, and R6 lasting $1 . 5 \mathrm { h }$ and FGI2 with R2 and R5 lasting one hour. The two Zoom sessions were video-recorded. All qualitative data were collected by the second Author.

# 4.7. Data analysis

# 4.7.1. Quantitative data

Scoring validity was explored via many-facet Rasch measurement (MFRM) using the FACETS software (version No. 3.86; Linacre, 2023). We used four facets: 1) students, 2) raters, 3) tasks, and 4) rating criteria. As our rating scale criteria differed for the two task types, we treated task type as a fifth dummy facet and used the hybrid model, which allows each task type to have its own rating scale. This way, we can analyse both rating scales together in one combined analysis to avoid having data-dependency issues. Such issues arise when analysing rating scales in different local contexts since the measures constructed from different tasks and groups of respondents have their “own origin (zero-point) and unit size” (Linacre, 2023, p. 166). The model we used to answer both RQ 1 and RQ 2 was:

$\begin{array} { r } { \ln \left[ \frac { P _ { n i j k } } { P _ { n i j k - 1 } } \right] = \theta _ { n } - \beta _ { i } - \delta _ { l } - \alpha _ { j } - \tau _ { i k } \mathrm { E q u a t i o n ~ 1 ~ ( a d a p t e d ~ f r o m ~ E c k e s , ~ 2 0 1 5 , p . ~ 1 4 8 ) , } } \end{array}$ where:   
$\theta _ { n }$ test-taker proficiency.   
$\beta _ { i }$ criterion difficulty.   
$\delta _ { l }$ task difficulty.   
$\alpha _ { j }$ rater severity.

Table 1 Rating design.   

<html><body><table><tr><td>Task</td><td>R1</td><td>R2</td><td>R3</td><td>R4</td><td>R5</td><td>R6</td><td>All raters</td><td>Total</td></tr><tr><td>T01Neth</td><td>24</td><td>24</td><td>23</td><td>23</td><td>23</td><td>23</td><td>18</td><td>158</td></tr><tr><td>T02Emot</td><td>29</td><td>29</td><td>30</td><td>30</td><td>29</td><td>29</td><td>18</td><td>194</td></tr><tr><td>T03Sea</td><td>25</td><td>25</td><td>25</td><td>25</td><td>26</td><td>25</td><td>18</td><td>169</td></tr><tr><td>T04Bili</td><td>23</td><td>24</td><td>23</td><td>23</td><td>23</td><td>24</td><td>18</td><td>158</td></tr><tr><td>Total</td><td>101</td><td>102</td><td>101</td><td>101</td><td>101</td><td>101</td><td>72</td><td>679 *</td></tr></table></body></html>

\* We had 674 valid attempts, as five invalid performances (source texts copied verbatim) were excluded after rating.

$\tau _ { i l k }$ variable structure of the rating scale for criteria and tasks.

More specifically, to answer RQ1, we evaluated intra-and inter-rater reliability, consistency and agreement by examining the fit statistics, exact observed $\%$ and expected $\%$ agreement, the separation indices (separation ratio (G) strata (H), and reliability (R), the single-rater rest of rater (SR/ROR) point-measure correlation coefficient indices, (Myford & Wolfe, 2004a; b) and Rasch kappa (Linacre, 2023). To answer RQ2, we investigated central tendency and halo effects, as well as the functioning and application of the rating scale following Linacre’s (2004) guidelines. All quantitative analyses were conducted by the third Author.

# 4.7.2. Qualitative data, coding approaches and coding schemes

The five TAPs were first transcribed following the conventions by Kukartz and Radiker¨ (2019). We then deductively coded the transcripts employing coding scheme 1 (see Appendix F) based on Lumley’s (2005) model of rating stages, which we had adapted and used as basis for rater training. With this coding approach, we aimed to answer RQ3, i.e., to what extent raters followed the trained stages and procedures, and RQ4, i.e., what challenges raters exhibited with regard to interpreting the integrated constructs for summary and opinion tasks operationalized in the rating scale criteria and band levels. The first two authors coded the TAPs for R3 and R6 together, and the second author then coded R2, R4, and R5. We used a color-coding system for the six main codes, and the commentary function in Microsoft Word to address the sub-codes. For analysing the TAPs, we used a case study approach, writing case summaries for each rater and task, condensing these summaries for each rater, and finally summarizing across the five cases, highlighting commonalities and differences. The summaries were discussed by both coders until agreement was reached.

All interviews were transcribed following the same conventions. The transcripts were coded in MAXQDA 2022 (VERBI Software, 2021) using coding scheme 2 (see Appendix G). The coding scheme was deducted from the RQs, with some aspects being added inductively during the first round of coding (conducted by the first two authors together, following Cohen et al., 2011). During the second round of coding, the focus group interviews were double-coded, as were the retrospective interviews for R3 and R6, while the retrospective interviews for R2, R4, and R5 were single-coded by the second author. Both coders together then analysed the passages coded under the six codes for tendencies, trends, and commonalities, employing a thematic analysis approach (Braun & Clarke, 2006). The interviews served to triangulate the TAP findings and shed additional light on RQ3 and RQ4.

![](img/49e2cc9a1e155324ecf0976bff29f52b6001eeaa76a23713e8ce383708a2892e.jpg)  
Fig. 4. Wright map.

# 5. Results for the quantitative strand

We first present the overall results of the MFRM analysis in the form of the Wright map (Fig. 4) before we report results for RQ1 and RQ2, respectively.

The Wright map shows that rater severity differed for the two task types (summary and opinion) and that raters applied the criteria differently, depending on the task type. We will now report the results for each RQ in turn.

RQ1. To what extent do trained student raters vary when applying the rating scale for the two task types?

We first present the summary statistics for the rater measurement report in Table 2.

# 5.1. Severity

Overall, the raters’ severity spread was smaller than half a logit in either task type (0.38 in summary; 0.23 in opinion). The raters were slightly more severe in the summary tasks (mean measure $= 0 . 0 5$ ) than in the opinion tasks $( - 0 . 0 5 )$ , with a high degree of precision $S E = 0 . 0 5$ in summary; $S E = 0 . 0 4 $ in opinion). Overall, this severity difference was statistically significant as its significance value $p$ was smaller than.05 (mean difference $= 0 . 1 0$ , Welch t-test $= 1 . 6 1$ , $d . f . = 8$ , $p = 0 . 1 5$ , $d = 1 . 1 3$ ). More specifically, R1 was the most severe rater in the summary tasks (severity measure $= 0 . 2 5$ ), and R2 was the most lenient $( - 0 . 1 3 )$ . Rater instability (Linacre, 2023) was observed for some of the raters since the rater hierarchy changed when assessing the opinion tasks, with R6 being the most severe (0.07), and R2 again the most lenient (− 0.16). R1, R3 and R4 were more severe when rating the summary tasks. R5 and R6 were comparably severe in both task types.

The differing severity levels the raters exercised in the two task types were corroborated by the separation ratio $( G )$ and strata $( H )$ indices. The raters were slightly more homogenous in the opinion tasks $( G = 1 . 5 9 ; H = 2 . 4 5 )$ than in the summary tasks $G = 2 . 4 1$ ; $H =$ 3.55). The raters could be separated into roughly three groups based on the severity levels they exhibited in either task type. This grouping could not be attributed to chance as the separation ratio $( G )$ values were three times larger than the RMSE (0.05), and the chisquare was significant ${ \cal ( \chi 2 ) } = 4 1 . 0 $ , d.f. 5, $p = . 0 0$ , $\mathbb { p } < . 0 1$ in summary; $\chi 2 = 2 1 . 0 , d . f . 5 , p = . 0 0 , p < . 0 1$ in opinion). Additionally, the high separation reliability $R$ (.85 in summary;.72 in opinion) corroborated that for at least the most severe and most lenient raters, the scores were not comparable within the task types.

# 5.2. Intra-rater reliability

Intra-rater reliability in the Rasch measurement framework is explored by the Infit Mean-square (Infit Mnsq) and the Infit z standardized (Infit Zstd) statistics. Fit values of 1.00 are considered ideal in the Rasch model; acceptable fit statistics for non-high-stakes contexts range from 0.50 to 1.50, according to Linacre (2023). As presented in Table 2, the overall Infit Mnsq was very close to the ideal 1.00 for both task types, indicating that overall, intra-rater reliability was in line with the Rasch model expectations (Myford & Wolfe, 2004a). It was only R4 who approached the maximum criterion for misfit in both tasks (Summary: Infit $M n s q = 1 . 3 7$ , $Z s t d = 6 . 8$ ; Opinion: Infit Mnsq $= 1 . 4 3$ , ${ Z s t d = 8 . 4 }$ ), suggesting that there may be some randomness in R4′s ratings. All other raters were within the acceptable range.

# 5.2.1. Inter-rater reliabilities and consistencies

Inter-rater reliability was explored through exact observed $\%$ agreement and agreement $\%$ expected. As shown in Table 2, the exact observed $\%$ agreement values were slightly higher than the agreement $\%$ expected, as anticipated for trained raters rating as independent experts (Linacre, 2023). Raters were associated with moderate exact observed $\%$ agreement ranging from $4 3 . 0 \%$ (R4) to $5 1 . 8 \%$ (R5) for the summary tasks and from $4 2 . 3 \%$ (R4) to $5 0 . 4 \%$ (R5) for the opinion tasks. The overall agreement $\%$ expected did not exceed the $3 8 . 4 \%$ agreement percentage for either task type. Additionally, Rasch kappa was within the $- 0 . 2 \ : \mathrm { t o } \ : + 0 . 2$ acceptable range (Linacre, 2023) both at the group level (0.15 for both task types) and the individual rater level, ranging from 0.07 (R4) to 0.22 (R5) in the summary tasks and from 0.07 (R4) to 0.20 (R5) in the opinion tasks. These values indicate agreement levels as expected of trained raters.

When examining the degree to which a particular rater was in line with (or deviated from) the other raters (as indicated by the single rater / rest of raters correlation SR/ROR), correlations below 0.30 are considered low, while values above 0.70 are considered high for scales including several categories (Myford & Wolfe, 2004a). R6 was associated with the lowest SR/ROR irrespective of task type (0.43 in summary; 0.60 in opinion) together with R5 $( S R / R O R = 0 . 6 0$ in opinion), while R2 and R3 exhibited the highest values in both task types $( S R / R O R \ge 0 . 7 0$ in summary; $\ge 0 . 6 6$ in opinion). Hence, R2 and R3 can be considered to be most consistent with the other raters, while R5 and R6 may have rank ordered the performances slightly differently from the rest of the raters. This may partly be explained by the fact that R5 and R6 are consistently severe across both task types, while the other raters showed instability tendencies in their severity across the two task types.

5.3. RQ2. Can raters differentiate the rating scale criteria and band levels appropriately for the two task types?

In this section, we focus on the rating scale, and the functioning of its criteria and band levels. We first report central tendencies and halo effects for the raters as a group by examining the criterion measurement reports. The summary statistics for these are presented in Table 3. Since the two tasks have different constructs, we report the findings separately for each type.

Table 2 Measurement Report (arranged by measure) .   

<html><body><table><tr><td>Total</td><td>Obsvd</td><td>Fair(M)</td><td>Severity</td><td>Model</td><td>Infit</td><td>ZStd</td><td>Outfit</td><td>ZStd</td><td colspan="2">Correlation</td><td>Exact Agree.</td><td colspan="2"></td><td>Rater ID</td></tr><tr><td>Count</td><td>Average</td><td>Average</td><td>Measure</td><td>S.E.</td><td>MnSq</td><td></td><td>MnSq</td><td></td><td>PtMea</td><td>PtExp</td><td>Obs %</td><td>Exp % 38.3</td><td>Kappa 0.18</td><td></td></tr><tr><td>819</td><td>2.53</td><td>2.44</td><td>0.25</td><td>0.05</td><td>1.07</td><td>1.30</td><td>1.06</td><td>1.10</td><td>0.63</td><td>0.65</td><td>49.4</td><td></td><td></td><td>R01 R03</td></tr><tr><td>826</td><td>2.57</td><td>2.52</td><td>0.11</td><td>0.05</td><td>1.09</td><td>1.80</td><td>1.09</td><td>1.70</td><td>0.70</td><td>0.65</td><td>46.1</td><td>38.4</td><td>0.13</td><td></td></tr><tr><td>833</td><td>2.57</td><td>2.54</td><td>0.07</td><td>0.05</td><td>0.86</td><td>3.10</td><td>0.87</td><td>2.80</td><td>0.57</td><td>0.64</td><td>46.8</td><td>38.4</td><td>0.14</td><td>R06</td></tr><tr><td>826</td><td>2.66</td><td>2.55</td><td>0.05</td><td>0.05</td><td>1.37</td><td>6.80</td><td>1.38</td><td>7.00</td><td>0.65</td><td>0.66</td><td>43.0</td><td>38.4</td><td>0.07</td><td>R04</td></tr><tr><td>826</td><td>2.68</td><td>2.61</td><td>0.06</td><td>0.05</td><td>0.77</td><td>5.20</td><td>0.76</td><td>5.40</td><td>0.67</td><td>0.64</td><td>51.8</td><td>38.3</td><td>0.22</td><td>R05</td></tr><tr><td>840</td><td>2.77</td><td>2.65</td><td>0.13</td><td>0.05</td><td>0.94</td><td>-1.30</td><td>0.91</td><td>1.90</td><td>0.71</td><td>0.64</td><td>50.5</td><td>38.2</td><td>0.20</td><td>R02</td></tr><tr><td>828.3 6.6</td><td>2.63</td><td>2.55</td><td>0.05</td><td>0.05</td><td>1.01</td><td>0.10</td><td>1.01</td><td>0.00</td><td>0.66</td><td></td><td></td><td></td><td></td><td>Mean</td></tr><tr><td>RMSE.05 Adj (True) S.D.11 Separation 2.41 Strata 3.55 Reliability (not inter-rater).85</td><td>0.08</td><td>0.07</td><td>0.12</td><td>0.00</td><td>0.20</td><td>3.90</td><td>0.20</td><td>4.00</td><td>0.05</td><td></td><td></td><td></td><td></td><td>S.D.</td></tr><tr><td colspan="14">Fixed (all same) chi-squared: 41.0 d.f.: 5 significance (probability):.00 Inter-Rater agreement opportunities: 15120 Exact agreements: 7245 = 47.9% Expected: 5805.4 = 38.4%, Rasch Kappa: 0.15</td></tr><tr><td colspan="14"></td></tr><tr><td>Opinion Tasks</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Total Count</td><td>Obsvd</td><td>Fair(M)</td><td>Severity</td><td>Model</td><td>Infit</td><td>ZStd</td><td>Outfit</td><td>ZStd</td><td>Correlation</td><td></td><td></td><td>Exact Agree.</td><td>Rasch</td><td>Rater ID</td></tr><tr><td></td><td>Average</td><td> Average</td><td> Measure</td><td>S.E.</td><td>MnSq</td><td></td><td>MnSq</td><td></td><td>PtMea 0.60</td><td>PtExp</td><td>Obs %</td><td>Exp %</td><td>Kappa 0.15</td><td>R06</td></tr><tr><td>1134</td><td>2.61</td><td>2.48</td><td>0.07</td><td>0.04</td><td>0.87</td><td>2.90 1.20</td><td>0.90 0.91</td><td>2.20 1.90</td><td>0.72</td><td>0.63 0.65</td><td>47.2 46.0</td><td>38.2 38.2</td><td>0.13</td><td>R03</td></tr><tr><td>1143</td><td>2.59</td><td>2.53 2.55</td><td>- 0.01</td><td>0.04</td><td>0.94</td><td>8.40</td><td>1.53</td><td>9.00</td><td>0.65</td><td>0.66</td><td>42.3</td><td>38.1</td><td>0.07</td><td>R04</td></tr><tr><td>1134 1125</td><td>2.74</td><td></td><td>0.03 0.07</td><td>0.04</td><td>1.43 0.95</td><td>1.10</td><td>0.92</td><td>1.80</td><td>0.64</td><td>0.64</td><td>49.5</td><td>38.1</td><td>0.18</td><td>R01</td></tr><tr><td></td><td>2.77</td><td>2.57</td><td></td><td>0.04</td><td></td><td>1.80</td><td>0.90</td><td>2.10</td><td>0.60</td><td>0.63</td><td></td><td></td><td>0.20</td><td>R05</td></tr><tr><td>1134</td><td>2.72</td><td>2.58</td><td>0.08</td><td>0.04</td><td>0.92</td><td></td><td></td><td></td><td></td><td></td><td>50.4</td><td>38.1</td><td></td><td>R02</td></tr><tr><td>1134</td><td>2.77</td><td>2.64</td><td>0.16</td><td>0.04</td><td>1.14</td><td>2.90</td><td>1.10</td><td>2.10</td><td>0.66</td><td>0.65</td><td>49.3</td><td>37.9</td><td>0.18</td><td></td></tr><tr><td>1134.0 5.2</td><td>2.70 0.07</td><td>2.56 0.05</td><td>0.05 0.07</td><td>0.04 0.00</td><td>1.04 0.19</td><td>0.70 3.90</td><td>1.04 0.23</td><td>0.50 4.10</td><td>0.65 0.04</td><td></td><td></td><td></td><td></td><td>Mean S.D.</td></tr><tr><td colspan="9">RMSE.04 Adj (True) S.D.06 Separation 1.59 Strata 2.45 Reliability (not inter-rater).72</td><td colspan="6"></td></tr><tr><td colspan="14">Model, Fixed (all same) chi-squared: 21.0 d.f.: 5 significance (probability):.00 Inter-Rater agreement opportunities: 19440 Exact agreements: 922 = 47.4% Expected: 7407.6 = 38.1%, Rasch Kappa: 0.15</td></tr></table></body></html>

# 5.3.1. Differentiating the criteria / halo effects

Halo effects occur when raters cannot appropriately distinguish the different criteria of a rating scale and hence award similar scores across several criteria (Myford & Wolfe, 2004b). When raters as a group exhibit halo effects, the difficulty measures of the criteria of a rating scale would not vary significantly. In our study, the statistically significant chi-square $( \chi 2 = 2 3 5 2 . 2 , d . f . 6 , p = . 0 0 _ { \mathrm { : } }$ , $\mathbb { p } < . 0 1$ in the summary tasks; $\chi 2 = 1 5 2 7 . 2$ , d.f. $8 , p = . 0 0 , p < . 0 1$ in the opinion tasks) suggests that the difficulty measures of at least two of the criteria from each scale were significantly different. Furthermore, the separation ratio $( G )$ revealed that the spread of the difficulty measures of the criteria for both task types were more than 18 $G = 1 7 . 5 9$ , summary) respectively 13 $G = 1 2 . 4 1$ , opinion) times larger than their RMSE (0.05), suggesting that the raters could systematically differentiate between the criteria. Moreover, the high number of strata $( H )$ (23.79 in summary; 16.88 in opinion) and the high reliability (1.00 in summary;.99 in opinion) corroborate the assumption that there was no halo effect at the group-level (Eckes, 2015; Myford & Wolfe, 2004b).

# 5.3.2. Differentiating the band levels / central tendency effects

Central tendencies refer to the clustering of scores in the mid-range of the band levels of a rating scale, caused by effects other than test-taker characteristics. Such unwanted central tendencies can be detected by examining the infit statistics. As shown in Table 3 above, the infit mean for both rating scales was very close to the ideal 1.00, suggesting that raters as a group did not exhibit any unwanted central tendency effects when using the two scales. Looking at the individual criteria, in the summary tasks, all criteria are within the acceptable range; for the opinion tasks, it was only criterion processing that showed a slight misfit. This supports the assumption that overall, there were no central tendency effects in the ratings.

# 5.3.3. Rating scale functioning

When analysing how the band levels within each criterion function (Linacre, 2023), we focused on the following aspects: score distribution, the ascendance of average ability measures, outfit values, and the step difficulty advance, i.e., the distance (in logits) between two adjacent band levels. For space reasons, we provide the complete output files from FACETS in the Supplementary material. Here, we summarize the results, thereby paying special attention to conspicuous criteria presented in Table 4.

Score distribution: We found for the summary texts that the majority of the awarded scores clustered between band levels 2 and 3 for all criteria but processing. For mining and precision, band level 3 was most commonly assigned. For structure, cohesion and vocabulary, band level 2 was most commonly awarded. Band levels 1 and 5 were minimally assigned across all criteria. For the opinion tasks, we found clusters between band levels 2 and 3 for cohesion, vocabulary, grammar and structure. Scores for mining and precision clustered at band levels 3 and 4. As we did not find indications for unwanted central tendencies, these scores may be best explained by the abilities of our test-taker sample.

Ascendance of average measure: The average measures should increase with higher band levels. In both task types, for all criteria apart from processing, the average measures ascended monotonically as expected, indicating that higher scores were awarded to higher ability students. Only processing showed unexpected ability measures for both task types, as presented in Table 4.

Table 3 Criterion Measurement report (arranged by difficulty measure, from difficult to easy).   

<html><body><table><tr><td colspan="10"> Summary tasks</td></tr><tr><td>Total Score</td><td>Total Count</td><td>Obsvd Average</td><td>Fair (M) Average</td><td>Difficulty Measure</td><td>Model S.E.</td><td>Infit MnSq</td><td>ZStd</td><td>Outfit MnSq</td><td>ZStd</td><td>Rating Criteria</td></tr><tr><td>1587</td><td>710</td><td>2.24</td><td>2.14</td><td>1.10</td><td></td><td>1.01</td><td>0.10</td><td>1.01</td><td>0.10</td><td>Structure</td></tr><tr><td>1747</td><td>710</td><td>2.46</td><td>2.41</td><td>0.70</td><td>0.05 0.05</td><td>0.81</td><td>3.80</td><td>0.82</td><td>3.70</td><td>Vocabulary</td></tr><tr><td>1930</td><td>710</td><td>2.72</td><td>2.70</td><td>0.60</td><td>0.05</td><td>0.80</td><td>4.20</td><td>0.80</td><td>4.00</td><td>Grammar</td></tr><tr><td>1747</td><td>710</td><td>2.46</td><td>2.39</td><td>0.57</td><td>0.05</td><td>0.88</td><td>2.30</td><td>0.87</td><td>2.50</td><td>Cohesion</td></tr><tr><td>1878</td><td>710</td><td>2.65</td><td>2.65</td><td>0.55</td><td>0.06</td><td>0.89</td><td>-1.90</td><td>0.90</td><td>1.70</td><td>Mining</td></tr><tr><td>1927</td><td>710</td><td>2.71</td><td>2.62</td><td>1.09</td><td>0.04</td><td>1.32</td><td>5.90</td><td>1.46</td><td>8.10</td><td>Processing</td></tr><tr><td>2261</td><td>710</td><td>3.18</td><td>3.13</td><td>-1.33</td><td>0.05</td><td>1.20</td><td>3.50</td><td>1.21</td><td>3.70</td><td>Precision</td></tr><tr><td>1868.1</td><td>710.0</td><td>2.63</td><td>2.58</td><td>0.00</td><td>0.05</td><td>0.99</td><td>0.40</td><td>1.01</td><td>0.00</td><td></td></tr><tr><td>196.4</td><td>0.00</td><td>0.28</td><td>0.29</td><td>0.90</td><td>0.00</td><td>0.19</td><td>3.60</td><td>0.22</td><td>4.10</td><td>Mean S.D.</td></tr><tr><td colspan="9"> RMSE 0.05 Adj (True) S.D. 0.90 Separation 17.59 Strata 23.79 Reliability 1.00</td><td></td></tr><tr><td></td><td>Model, Fixed (all same) chi-squared: 2352.2 d.f.: 6 significance (probability):.00 Opinion tasks</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Total Score</td><td>Total</td><td>Obsvd</td><td>Fair(M)</td><td>Difficulty</td><td>Model</td><td>Infit</td><td></td><td>Outfit</td><td></td><td>Rating Criteria</td></tr><tr><td></td><td>Count</td><td>Average</td><td>Average</td><td> Measure</td><td>S.E.</td><td>MnSq</td><td>ZStd</td><td>MnSq</td><td>ZStd</td><td></td></tr><tr><td>1827</td><td>756</td><td>2.42</td><td>2.31</td><td>0.76</td><td>0.05</td><td>0.77</td><td>4.70</td><td>0.77</td><td>4.70</td><td>Vocabulary</td></tr><tr><td>1795</td><td>756</td><td>2.37</td><td>2.25</td><td>0.71</td><td> 0.05</td><td>0.90</td><td>2.10</td><td>0.90</td><td>1.90</td><td>Structure</td></tr><tr><td>1984</td><td>756</td><td>2.62</td><td>2.54</td><td>0.29</td><td>0.05</td><td>0.90</td><td>1.80</td><td>0.90</td><td>1.80</td><td>Cohesion</td></tr><tr><td>2067</td><td>756</td><td>2.73</td><td>2.67</td><td>0.26</td><td>0.05</td><td>0.80</td><td>-3.80</td><td>0.80</td><td>3.80</td><td> Grammar</td></tr><tr><td>1236</td><td>756</td><td>1.63</td><td>1.37</td><td>0.12</td><td>0.04</td><td>1.27</td><td>3.90</td><td>1.59</td><td>6.60</td><td>Attribution</td></tr><tr><td>2032</td><td>756</td><td>2.69</td><td>2.54</td><td>0.09</td><td>0.04</td><td>0.93</td><td>1.20</td><td>0.93</td><td>1.20</td><td>Synthesis</td></tr><tr><td>2342</td><td>756</td><td>3.10</td><td>3.07</td><td>0.52</td><td>0.04</td><td>0.93</td><td>1.20</td><td>0.94</td><td>1.00</td><td>Opinion</td></tr><tr><td>2466</td><td>756</td><td>3.26</td><td>3.21</td><td>0.76</td><td>0.04</td><td>1.04</td><td>0.60</td><td>1.03</td><td>0.50</td><td>Precision</td></tr><tr><td>2612</td><td>756</td><td>3.46</td><td>3.40</td><td>0.96</td><td>0.04</td><td>1.51</td><td>8.40</td><td>1.53</td><td>8.80</td><td> Processing</td></tr><tr><td>2040.1</td><td>756.0</td><td>2.70</td><td>2.60</td><td>0.00</td><td>0.05</td><td>1.01</td><td>0.20</td><td>1.04</td><td>0.20</td><td> Mean</td></tr><tr><td>388.1</td><td>0.00</td><td>0.51</td><td>0.57</td><td>0.58</td><td>0.00</td><td>0.23</td><td>3.90</td><td>0.29</td><td>4.40</td><td>S.D.</td></tr><tr><td colspan="9">RMSE 0.05 Adj (True) S.D. 0.58 Separation 12.41 Strata 16.88 Reliability.99 Fixed (all same) chi-squared: 1527.2 d.f.: 8 significance (probability):.00</td></tr></table></body></html>

Table 4 Scale functioning for criterion processing.   

<html><body><table><tr><td colspan="7">Summary tasks</td></tr><tr><td colspan="3">DATA</td><td colspan="3">QUALITY CONTROL</td><td colspan="2">RASCH-ANDRICH Thresholds</td></tr><tr><td>Band level</td><td>Counts Used</td><td>%</td><td>Average Measures</td><td>Expected Measures</td><td>OUTFIT MnSq</td><td>Measure</td><td>S.E.</td></tr><tr><td>0</td><td>1</td><td>0%</td><td>2.08</td><td>0.44</td><td>1.10</td><td></td><td></td></tr><tr><td>1</td><td>112</td><td>16%</td><td>0.11</td><td>0.08</td><td>1.00</td><td>4.90</td><td>1.00</td></tr><tr><td>2</td><td>184</td><td>26%</td><td>0.74</td><td>0.58</td><td>1.50</td><td>0.17</td><td>0.12</td></tr><tr><td>3</td><td>252</td><td>35%</td><td>1.16</td><td>1.06</td><td>1.00</td><td>0.51</td><td>0.09</td></tr><tr><td>4 5</td><td>114</td><td>16%</td><td>1.24 1.12 *</td><td>1.46</td><td>1.70</td><td>2.06</td><td>0.10</td></tr><tr><td> Opinion tasks</td><td>47</td><td>7%</td><td></td><td>1.75</td><td>2.30</td><td>2.50</td><td>0.16</td></tr><tr><td colspan="6">DATA</td><td colspan="2"></td></tr><tr><td colspan="6"></td><td colspan="2">RASCH-ANDRICH</td></tr><tr><td rowspan="2"> Band level</td><td></td><td rowspan="2"></td><td></td><td>Expected</td><td rowspan="2"></td><td colspan="2">Thresholds</td></tr><tr><td>Counts</td><td>Average</td><td>OUTFIT</td><td>Measure</td><td>S.E.</td></tr><tr><td rowspan="2"></td><td>Used</td><td>%</td><td> Measures</td><td> Measures</td><td colspan="2">MnSq</td><td rowspan="2"></td></tr><tr><td>18</td><td>2%</td><td>1.09</td><td>0.97</td><td>1.10</td><td></td></tr><tr><td>0 1</td><td>17</td><td>2%</td><td>0.06</td><td>0.19</td><td>1.00</td><td>0.47</td><td>0.28</td></tr><tr><td>2</td><td>98</td><td>13%</td><td>0.68</td><td>0.28</td><td>1.90</td><td>1.70</td><td>0.20</td></tr><tr><td>3</td><td>238</td><td>31%</td><td>0.91</td><td>0.68</td><td>1.50</td><td>0.41</td><td>0.11</td></tr><tr><td>4</td><td>240</td><td>32%</td><td>0.90 *</td><td>1.04</td><td>1.50</td><td>0.85</td><td>0.08</td></tr><tr><td>5</td><td>145</td><td>19%</td><td>0.96</td><td>1.39</td><td>1.50</td><td>1.72</td><td>0.10</td></tr></table></body></html>

For processing in the summary tasks, the mean ability measures for band level 4 were higher (1.24) than for band level 5 (1.12), indicating that performances rated at band level 5 did not manifest higher student abilities than those rated at band level 4. Similarly, for the opinion tasks, mean measures for band level 3 were higher (0.91) than for band level 4 (0.90).

Outfit mean square values: Outfit values are expected to be lower than 2.00. Outfit values greater than 2.00 indicate that the band level has been used in unwarranted ways. Apart from one exception, the band levels of all criteria in both task types were associated with outfit values $\geq \pm 2 . 0$ , suggesting that the raters applied the band levels appropriately. It was only for processing in the summary tasks that the outfit for band level 5 was 2.30, indicating that the raters used this band level in some unexpected ways (e.g., to award higher than expected scores to lower ability students).

Distance between adjacent band levels: The distance between adjacent band levels (Rasch-Andrich thresholds) was within Linacre’s (2004) acceptable range (larger than one logit and smaller than five logits) for the majority of the band levels, indicating that raters applied these band levels as expected. The band levels with less than one logit distance are summarized in Table 5.

For the bands and respective criteria listed in the table, raters appeared to have problems differentiating between the listed adjacent levels. This could indicate that the rating scale descriptors may not meaningfully differentiate between these two band levels. It is noteworthy that for criteria processing, structure, attribution and synthesis, the rating scale does not provide explicit descriptors band level 4, as mentioned above, which could partially explain these findings.

We will now turn to the qualitative perspective to further explore the issues identified in the quantitative analyses.

# 6. Results for the qualitative strand

# 6.1. RQ3. In how far do raters follow the trained procedures and stages?

On the whole, we could observe most of the stages of the model we used for rater training (see Appendix C) in the TAPs. The first stage of preparation (opening and setting up the app to detect source-text copying, opening the task and script) was exhibited by all raters, with R4 and R5 sometimes struggling to set the app up in time, and R6 always starting the app too late. All raters reminded themselves of the task, and all but R5 started with reading the performance first. Next, all raters moved to the rating stage. With regard to the order in which raters addressed the criteria, R2, R3, and R5 followed the trained order (i.e., the order the criteria appear in the rating scale) for all four performances during the TAP, while R4 and R6 exhibited some jumpiness. They seemed to be guided by the perceived “easiness” of the criteria attribution and structure, which they rated first, mainly looking for surface features here. These two raters also always addressed the criterion processing last as they did not set up the app in time, and they never used it for the linguistic criteria, which was also observed with R2. Interestingly, the quantitative data revealed the lowest inter- and intra-rater reliability indices for R4, and low SR/ROR values for R6, while R2 and R3 had the highest inter-rater reliability indices, and R5 was within the acceptable range.

Most raters did not monitor their ratings or re-evaluate their ratings in the end. Notably, no rater used the benchmarks or the rating handbook during the TAP ratings, while all raters used the source texts and the task expectations (particularly for mining), with R2 and R5 working extensively with the source texts. Interestingly, R2 shows high inter-rater reliabilities in both task types, while R5 exhibits high reliability only in the summary tasks.

Table 5 Threshold distances $< 1$ logit.   

<html><body><table><tr><td colspan="2">summary tasks</td><td colspan="2">opinion tasks</td></tr><tr><td>criterion</td><td>adjacent band levels</td><td>criterion</td><td>adjacent band levels</td></tr><tr><td>processing</td><td>4/5</td><td>processing</td><td>4/5</td></tr><tr><td>structure</td><td>3/4</td><td>structure</td><td>4/5</td></tr><tr><td>cohesion</td><td>4/5</td><td> cohesion</td><td>4/5</td></tr><tr><td></td><td></td><td>mining</td><td>1/2</td></tr><tr><td></td><td></td><td>attribution</td><td>2/3, 3/4, 4/5</td></tr><tr><td></td><td></td><td>synthesis</td><td>4/5</td></tr><tr><td></td><td></td><td> grammar</td><td>4/5</td></tr></table></body></html>

When further analysing how raters applied the criteria and descriptors, we found that all raters seemed to have a clear understanding of what features to focus on in the performances and they seemed to be able to differentiate between the criteria. Yet raters varied in their use of the rating scale and descriptors. R3 and R5 methodically read the complete descriptors of the level in question along with the adjacent levels and compared the descriptors constantly against performance features. They also tended to consider multiple aspects in the descriptors. The quantitative results yielded the highest inter-rater reliabilities for these two raters. R2 and R4, however, often gave a score without referring to a descriptor, and sometimes came to judgements very quickly without verbalizing their reasoning, or by comparisons with previous performances. It is noteworthy that R2 seemed overall rather insecure when evaluating performance features, while R4 seemed to analyse the content criteria in more depth, but often relied on his/her analysis from the first reading, without necessarily re-reading or re-analysing the performances. Interestingly, as mentioned above, R4 showed the lowest intra-and inter-reliability indices, while ${ \tt R 2 ^ { \prime } s }$ indices were amongst the highest.

We found no points in the TAP data that would indicate that raters struggled to differentiate between the levels, except for some insecurities by R4, who, for example stated, “others may argue for another level”. R3 often started with level 3, which could hint toward a central tendency, yet s/he usually checked the adjacent levels, while R5 was hesitant to award level 4 in cases where there were no descriptors (see rating scale development above).

We found hints towards some task-type effects, as the opinion task seemed to have posed more challenges for raters than the summary task. R4, for instance, struggled with the amount of source-text ideas present in one of the performances, and R4 and R6 resorted to the aforementioned “easiest criteria first” (attribution and structure) approach only for the opinion task.

Overall, raters seemed at points challenged by the complexity of the rating task: they had to handle the app and the materials, switch between different windows on the computer, and keep in mind a long source text and task expectations, while analysing dense performances. All raters at different times during the TAPs showed signs of struggling with these complex steps and tasks, albeit to a different degree, and more so towards the end of the TAP session. Being well prepared by setting up the app and opening all documents seemed to help. In the TAPs, it was difficult to judge whether printed materials could mitigate this challenge.

6.2. RQ4. What challenges do raters report with regard to interpreting the integrated constructs for summary and opinion tasks operationalized in the rating scale criteria and band levels?

We now report our findings from the analyses of the focus group and reflective interviews (henceforth FGI and RI respectively), based on coding scheme 2 (Appendix G), i.e., challenges and factors that may have affected rating quality.

# 6.2.1. Challenges regarding criteria

All raters expressed a sound understanding of the assessment criteria and how to differentiate between them. They seemed to categorize the criteria into two broad categories: source-related criteria (mining, precision, processing, synthesis) and language production. For example, R4 stated: “…for me, it was always divided into two parts: basically, the understanding of the text, or if they actually thoroughly understood the ideas, so relating to the first criteria, and then the language” (FGI1). With regard to the perceived importance of certain criteria, R3 and R4 found mining and precision most important as these indicated source-text comprehension as basis for answering the task. Yet, the other raters disagreed, finding all criteria equally important. All raters stated to weigh all criteria equally.

Regarding challenges that certain criteria posed, all raters found rating mining and precision most challenging, as, for example, R5 stated: “The most challenging criterion was precision. But it’s not because of the rating scale, it is just challenging in general to find whether the idea is precise, and to what extent” (FGI2). R2 and R6 noticed they spent more time on mining and precision, which might be an indication of the challenge that these criteria posed. An additional challenge with regard to rating precision was mentioned by R3 and R4 for cases where most ideas were copied from the source: “Sometimes I was wondering if that counts as correctness and preciseness if they just copy the original, obviously, it’s going to be very precise” (R4, FGI1). All raters reported difficulties distinguishing ‘source text ideas’ from ‘own ideas’, particularly for synthesis (opinion task).

In a similar vein, regarding the language criteria, all raters found it challenging to differentiate lifted ‘source-text language’ from ‘own language’, particularly for performances showing heavy lifting. Nevertheless, all raters were aware that they had to focus on students’ own language here. Yet, as we found in the TAP analyses, R2, R4, and R6 never actually used the app to differentiate between own words and lifted passages when rating the language criteria. On the other hand, we found that interview results were corroborating some of the

TAP findings. For instance, R4 (RI) and R6 (FGI1) were aware that they were focusing on surface features such as paragraphs when rating structure rather than analysing the deeper thematic development, a behaviour that we also observed in the TAPs.

Another aspect worth mentioning is raters’ awareness of the effects of writers’ linguistic repertoire on certain criteria such as processing, as for example R4 mentioned: “If someone has a broad lexical repertoire or a broad repertoire of linguistic structures, that will affect paraphrasing, … and the other way round” (RI). Moreover, R2, R3 and R4 showed awareness for the effects of heavy lifting. While performances with heavy lifting were rated low in the linguistic criteria, R2 for instance, noted that “in terms of the other [i.e., source-related] criteria, … they were quite correct” (FGI2). This awareness supports the aforementioned perception that raters could differentiate between the criteria.

# 6.2.2. Challenges regarding band levels

All raters commented on the challenge to award the appropriate band level if a performance showed features which were matching descriptors from adjacent band levels (e.g., instances of academic register but also colloquialisms). R4 found at points that one “could argue for either level” (RI), and R2 found it sometimes challenging to attribute a band level that was not defined by a descriptor (FGI). Again, we found challenges originating in performances with heavy lifting from source texts: all raters but R6 mentioned the difficulty of matching such performances with descriptors across all criteria.

# 6.2.3. Challenges arising from the task types

The two task types were perceived differently, with all raters finding the summary tasks easier to rate than the opinion tasks. With the latter, raters found it harder to differentiate ‘source-text ideas’ from ‘own ideas’ due to the borrowing, modification and rephrasing of ideas. R2, R3, and R4 (FGIs) wondered how much language needed to be modified before an idea or example became the student’s own idea. R2 stated: “So it is always difficult for me to see that a) an opinion develops, b) something …that is taken [from] the source text …, or is it something just more general that the person came up with themselves?” (FGI2). With regard to rater severity in the two task types, R6 was consistently severe in both task types, despite expressing a preference for summary tasks, while R1, R3, and R4 were more severe in the summary tasks, and R2 and R5 were consistently lenient in both tasks.

# 6.2.4. Perception of materials and procedures

All raters (FGIs) mentioned the helpfulness of the approach and the rating scale, with R3 and R4 mentioning that it reduced subjectivity while raising one’s own awareness of good academic writing. R4 and R6 reported they used the scale less frequently with growing experience. It is noteworthy that all raters regarded the assessment criteria as valid operationalization of the academic reading-into-writing construct, as illustrated by R2: “I just really can relate to this scale. Because I’m writing my thesis right now. I’m basically, I’m expected to do all these things when I’m writing it” (FGI2). R2 also mentioned that the band levels helped differentiate different levels of ability.

All raters agreed that the reported challenges were not caused by the rating scale but by the nature of assessing the integrated constructs. As aforementioned, all raters found the app helpful and easy to use; they reported employing it for detecting liftings and source-text use, as well as for the linguistic criteria (despite R2, R4, and R6 not actually using it during the TAP ratings for rating the linguistic criteria). R3 found it led to fairer judgment, while R2 and R5 mentioned its efficiency and time-saving effects. Overall, procedures and workload were never problematized, with R3 and R6 (RIs) mentioning that the workload was fine.

# 7. Discussion

Before we discuss our results, we have to acknowledge the limitations of our study. We only had a small data set from one context, only four tasks from two task types, and only five raters in the qualitative part, which is why our findings have to be cautiously interpreted. As is the case with such small data sets, results may also have been influenced by the selection of the performances for the TAPs, yet we tried to account for this by our selection rationale. We also acknowledge that the extensive training that we provided may not be feasible in all contexts; whether similar results could be achieved with less training, particularly if experienced teachers were employed, is an interesting question that could be pursued in the future. Unfortunately, we did not have the resources to compare our trained student raters with experts. Furthermore, we have to concede that we cannot strictly separate training conditions from scale functioning and rater reliability, yet this interrelatedness can never fully be disentangled in such contexts.

We structure the discussion along the main components of Weir’s (2005) scoring validity frame that we employed, along with our added qualitative perspective (cf. Fig. 2): We first discuss the findings from the statistical analyses, encompassing reliabilities and rating scale functioning, in light of the qualitative findings and the literature reviewed above. This is followed by a discussion of the findings on rating procedures and the application and perception of the rating scale, with a particular focus on the challenges that raters faced. Finally, we discuss implications for training novice student raters.

Beginning with statistical analyses, we found moderate inter- and intra-rater reliabilities in an acceptable range, with R4 displaying misfit regarding the intra-rater reliabilities and showing the lowest inter-rater indices, and R5 and R6 rank-ordering performances differently from the rest of the raters. This could be partly explained by the qualitative analyses, which revealed that R4 and R6 did not use the copy-detection app for rating the linguistic criteria, made less use of the rating scale with growing experience, and exhibited jumpiness in addressing the criteria. The different rank-ordering by R6 may also be explained by his/her consistent severity across both task types, while the different rank-ordering by R5 may be attributed to this rater following the trained procedures. This behaviour may also show in R5′s high intra-rater reliabilities.

Looking at R2 and R3, these two raters exhibited the highest inter-rater reliabilities, with R3 showing desired rating behaviour such as always referring to the descriptors. R2, despite high-reliability indices, did show some conspicuous behaviour, such as not using the app for the linguistic criteria and often not referring to the descriptors. Hence, we have to concede that indices of rater quality may not always be successful in detecting all phenomena associated with unwanted or desired rater cognition, which underscores the contribution of qualitative approaches to scale validation endeavours, as also stressed by Knoch and Chapelle (2018).

The FACETS analyses of scale functioning did not indicate halo or central tendency effects. They revealed that raters could differentiate the criteria, but at points faced challenges with reliably differentiating band levels for certain criteria, depending on the task type. Raters tended to be more severe in the summaries, while they perceived this type as easier to rate. The Rasch analysis revealed issues particularly with attribution (opinion tasks) and processing (both task types, but more pronounced for the opinion tasks). This in part reverberates Xie’s (2023) findings of lower consistencies with certain source-related aspects in the argumentative essays. While these issues matched raters’ reported challenges with this task type, their perceptions of the most challenging criteria did not always align with the quantitative results, a finding also reported by Deygers and Van Gorp (2015) or Lestari and Brunfaut (2023). While the raters considered all source-related criteria very challenging, only processing and attribution showed issues in the quantitative data, whereas mining (apart from one band level for the opinion tasks) and precision were applied equally well in both task types. This finding differs from Lestari and Brunfaut (2023), who report the reading-for-writing criterion to be challenging.

Interestingly, the writing-related criteria vocabulary and grammar were not flagged as noteworthy (apart from one band level for the opinion tasks) despite three raters not using the copy-detection app to distinguish source-text language from students’ own language. Either these raters were apt at detecting these differences, or it may be that quantitative indices cannot always indicate validity issues, as reported by Deygers and Van Gorp (2015) or Harsch and Martin (2012), which again emphasizes the importance of using qualitative windows on scoring validity.

Another fact worth mentioning is the clustering of scores: in the summary tasks, the scores clustered around band levels 2 and 3, while in the opinion tasks, they clustered between band levels 3 and 4 for the source-ideas-related criteria, and band levels 2 and 3 for the writing-related criteria. Yet, the FACETS analyses revealed no unwanted central tendencies. Hence, the clustering may be partly explained by the rather homogeneous sample of students, who are not performing at the higher levels of our rating scale. This may have contributed to the raters’ difficulties in differentiating between scores 4 and 5, which may have been exacerbated by some missing descriptors for level 4. We agree with Xie (2023) that it would have been preferable to reduce the levels for those criteria and operate with four levels only, rather than leaving blanks in the rating scale.

Despite the rather high number of criteria in our study compared to the four to five criteria used in most integrated rubrics (cf. Chan et al., 2015; Cheong et al., 2019; Li & Wang, 2021; Ono et al., 2019; Plakans & Gebril, 2015; Shin & Ewert, 2015), the FACETS analyses revealed that the raters in our study nevertheless were able to differentiate the criteria reliably and consistently, which is also supported by the raters’ reports. Whereas existing rubrics exhibit rather heterogeneous approaches to combining several aspects in one criterion (as critiqued in the literature review above), we tried to compartmentalize separate aspects in separate criteria. Our raters perceived the criteria as a valid operationalisation of the integrated writing construct, encompassing all aspects relevant for the academic integrated writing context. One possible explanation for our finding may lie in our criteria effectively disentangling the complex integrated construct into distinct, valid and separable criteria for two integrated task types, as recommended also by Chan and May (2022) and Xie (2023). Another possible explanation may lie in the training focusing on treating these aspects separately. Yet, as conceded above, we cannot strictly disentangle training and scale effects.

Regarding rating procedures, our raters mainly followed the trained stages, yet they showed variability when it came to micromanaging the complexities of the rating procedures. Interestingly, all raters showed strengths in some behaviours and weaknesses in others. While all raters exhibited a valid understanding of the criteria and how they should be applied, three of our raters in the TAPs did not follow the trained procedures, such as setting up the app and preparing the materials in advance, or following the order of the criteria. Furthermore, they did not show appropriate application behaviours, as they did not always focus on the features in question nor did they always make use of the rating scale descriptors to form their judgments. The statistical analyses, however, only flagged one of them (R4) as conspicuous. These findings yet again underscore the necessity of qualitative perspectives.

We found several challenges with applying and interpreting the rating scale. Raters particularly struggled with textual borrowing and idea development, and assessing language in heavily borrowed texts, mirroring the findings by Cumming (2013), Gebril and Plakans (2014), or Wang et al. (2017). Despite our efforts to address these researchers’ recommendations in our scale development and training procedures, we still found raters struggling with these known aspects. We assume, along with our raters, that the challenges do originate not so much in raters, rating approach, or scale, but rather in the inherent complexities of integrated academic writing, and the aforementioned cognitive challenges arising from assessing this construct. Our raters dealt differently with these complexities: some approached the rating systematically as trained (e.g., R3 or R5, who interestingly also exhibited higher reliability indices in our study), while others seemed overwhelmed, showing some erratic behaviour or simplifying their task by e.g., not always using the rating scale, which at least for R4 may partly explain their lower reliability indices.

The heterogeneous approaches we found in dealing with the inherent challenges of integrated writing, despite three months and $^ \mathrm { 5 0 \ h }$ of training, may not be surprising, given the complexity of the construct and insights on the stability of rater types despite training (e.g., Eckes, 2012), and given our novice student raters. Nevertheless, we found indications in our qualitative data on how to further improve training and rating procedures: it may make sense to control the use of materials, stages and sequences of the criteria; some behaviours (such as first reading, use of benchmarks) may need to be more explicitly trained and monitored. While rating, monitoring rater behaviour and reliabilities, along with providing continuous feedback, seems beneficial to keep raters calibrated, as also suggested by Wang et al. (2017). Facilitators could enable ongoing discussions amongst raters to address challenges as they arise.

Given our results, the question is warranted whether novice student raters are trainable for the job of fostering first-semester student’s integrated academic writing skills. While we would support Barkaoui’s (2010b) conclusion that analytic approaches have positive effects on rater consistency, we are aware of the moderate reliabilities of our raters and of rater instabilities across the two task types. It may be a more feasible approach to train a larger group of students by offering rater training in the form of a regular seminar that is integrated into pre-service teacher education, with the outlook of becoming a paid tutor if certain quality criteria are fulfilled. The variability in rating approaches and the in parts low reliability indices, despite substantial training, may, however, point towards the need for stricter selection criteria at the end of rater training.

# 8. Conclusions

Overall, we conclude that the rating scale criteria and band levels were understood and applied by our raters largely as intended, albeit with some variability in their scoring approaches, which led to reliability indices in the mid-range of acceptability. Some criteria did not fulfil Linacre’s (2004) quality criteria, mostly in the opinion task. It emerged that the complexity inherent in assessing integrated performances was the most challenging factor for our raters, who perceived the rating scale as a valid operationalization of the integrated construct for both task types and a helpful support for assessment.

For the future, it may be worthwhile exploring the potential of pairing student tutors and freshmen to come to a collaborative assessment. Given the complexity of the integrated construct and its assessment, we would also assume that similar challenges with assessing academic writing exist for faculty or language teachers; hence, we are currently conducting a follow-up study to compare our novice raters with experts. We also want to examine with expert raters whether we could reduce or combine our criteria, while bearing in mind Deygers and Van Gorp’s (2015) conclusion that some aspects of complex criteria may be better scored separately. We agree with Xie (2023) that more research is needed with regard to scale structure and the operationalization particularly of source use criteria in relation to other criteria. We are currently running dimensionality and factor analyses to support such decisions.

Despite these open questions, our study yielded useful insights into the scoring validity of a newly developed integrated readinginto-writing scale targeting two task types, as well as the challenges trained novice raters faced when using this scale. Our study has implications for future scale validation endeavours. We found Weir’s (2005) framework helpful for guiding our validation study, structuring the data collection and analysis, as well as reporting our findings. Adding a qualitative perspective to this frame enriched the interpretation of the quantitative findings and shed light on possible reasons for lower reliabilities and heterogeneity, while pointing towards potential remedies and future research agendas.

# Funding

This work was supported by the German Research Foundation [ grant number GZ: HA 5050/11–2 | HA 5943/2–2]. The open access publication of this manuscript was supported by the Staats- und Universit¨atsbibliothek Bremen. Voula Kanistra did not receive any funding for conducting the multi-faceted Rasch analyses and writing the results.

# Ethical Statement

Following the ethical standards of the German Research Foundation, all participants were informed of the research, the data collection process, the handling and future usage of their anonymized data, and the voluntary nature of their participation. All gave their written consent and were remunerated for their time.

# CRediT authorship contribution statement

Valeriia Koval: Writing – review & editing, Writing – original draft, Methodology, Investigation, Formal analysis, Data curation. Claudia Harsch: Writing – review & editing, Writing – original draft, Visualization, Validation, Supervision, Resources, Project administration, Methodology, Investigation, Funding acquisition, Formal analysis, Data curation, Conceptualization. Ximena Delgado-Osorio: Writing – review & editing, Investigation, Formal analysis, Data curation. Paraskevi Voula Kanistra: Writing – review & editing, Writing – original draft, Methodology, Formal analysis, Data curation.

# Declaration of Competing Interest

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

# Data availability

Our data is unsuitable to post due to the data security requirements of the institutions involved.

# Acknowledgments

We would like to acknowledge the second co-investigator Prof. Dr. Johannes Hartig, who was responsible for funding acquisition, investigation, project administration, resources and supervision. We would like to thank our student assistants Annika Marike Lamers and Larissa Junker, who helped with data collection, data cleaning and transcription. We express our thank to Kathrin Eberharter,

Benjamin Kremmel and Anika Müller-Karabil, as well as three anonymous reviewers, who gave invaluable feedback on earlier drafts of the manuscript. All remaining errors are ours.

Supplementary material

FACET Output for the rating criteria.

Authors’ own citations

Harsch, C., Koval, V., Delgado-Osorio, X. & Hartig, J. (accepted). Usability of CEFR Companion Volume scales for the development of an analytic rating scale for academic integrated writing assessment. CEFR Journal - Research and Practice.

Appendix A. Rating scales

Table A.1 Rating scale for summary tasks.   

<html><body><table><tr><td></td><td>1. Mining</td><td>2. Precision</td><td>3. Processing Substantial</td><td>6. Structure Macrostructure</td><td>7. Cohesion Shows</td><td>8. Vocabulary Broad lexical</td><td>9. Grammar Broad</td></tr><tr><td>5 | C1 and above</td><td>All relevant main ideas selected, presented in necessary depth. No irrelevant details or own ideas.</td><td>All ST ideas are presented correctly and precisely.</td><td>revision: Expresses all ST ideas in own words (only key words are used). Reformulates syntax of ST.</td><td>clear/ appropriate for task. (Re-)arranges ST elements (and if apl. own ideas) into logical order (not necessarily that of ST). Appropriate paragraphs that are logical in themselves.</td><td>consistent and continuous controlled use of a broad repertoire of cohesive devices (e.g., referencing, semantic fields, connectors, including elaborate ones from the academic field) on sentence and paragraph levels. (This contributes to</td><td>repertoire allowing gaps to be readily overcome with circumlocutions. Good command of common idiomatic expressions and academic register (no colloquialisms) Occasional minor slips but no significant vocabulary or spelling errors</td><td>repertoire of linguistic structures and complex sentence patterns. Consistently maintains a high degree of grammatical accuracy (including complex structures); errors are rare and difficult to spot.</td></tr><tr><td>4 | B2 +</td><td>All relevant and useful ideas selected but not all presented in necessary depth, or there may be some irrelevant details.</td><td>More than 3, but not yet enough for 5</td><td>More than 3, but not yet enough for 5</td><td>More than 3, but not yet enough for 5</td><td>the coherence of the text). Uses a variety of elaborate and common cohesive connectors, semantic fields, pronouns) efficiently to mark clearly the relationships between ideas. Errors in the field of cohesive devices are rare. devices (e.g.,</td><td>Good and varied range of topic specific vocabulary and collocations and good command. On the whole adopts appropriate formal register</td><td>Punctuation is consistent and helpful. Good and varied grammatical range and control. Occasional &#x27;slips&#x27; or non- systematic errors and minor flaws in sentence structure may occur, but they are rare. Very few mistakes in punctuation.</td></tr><tr><td>3|B2</td><td>Majority of the relevant content selected, but not necessarily all in required depth, and/or some irrelevant</td><td>Majority of the ideas are presented correctly. There may be some misinterpretations or imprecisions.</td><td>Moderate revision: Paraphrases majority of ST ideas (There may be occasional use of ST strings of</td><td>Macrostructure on the whole clearly developed, although there may be some &#x27;jumpiness&#x27;. Attempts to</td><td>Uses a sufficient range of common and some elaborate cohesive devices to link</td><td>Good range of topic-specific vocabulary and collocations. Attempts to vary formulation to avoid frequent repetition, though</td><td>Good range of also infrequent structures and some complex sentence patterns. (continued on next page)</td></tr></table></body></html>

Table A.1 (continued )   

<html><body><table><tr><td colspan="3">1. Mining 2. Precision 3. Processing</td><td colspan="3">6. Structure 7. Cohesion</td><td colspan="2">influence. 8. Vocabulary 9. Grammar</td></tr><tr><td>2 | B1 +</td><td>Some (about half or less) of the desired information selected, not necessarily in required depth.</td><td>Some of the ideas are presented correctly and/or Ideas presented with some</td><td>Minimal revision: Attempts to paraphrase but not always successful (e.g., Strings of words slightly modified by</td><td>Identifiable attempt at macrostructure, but not fully successful (e.g., intro &amp; conclusion but no appropriate middle part).</td><td>Uses a simple range of common cohesive devices (e.g., pointing words, repetitions, common</td><td>Sufficient range of vocabulary. Some repetitive use of vocabulary. Register is neutral, not yet academic. May make</td><td>Good range of frequent structures. There may be attempts at more complex structures, but these are not</td></tr><tr><td rowspan="2">1 | B1 and below</td><td>irrelevant details. Only the most significant points/ a minority of the relevant main ideas selected. Includes many irrelevant details or own ideas.</td><td>Majority of selected ideas misinterpreted.</td><td>adding/ deleting words or using synonyms for content words). Reformulates some syntactical structures. Near copy: Major instances of lifting from ST (usually without</td><td>paragraphs that may not always be logical. Orders a series of shorter discrete elements into a linear sequence of points.</td><td>occasional errors. Links discrete elements using a limited number of cohesive devices. Shows reasonable common cohesive</td><td>words. Sufficient range of vocabulary, with some circumlocutions, repetitions. May show some instances of inappropriate vocabulary use. Major errors may</td><td>Generally good control though mother tongue influence may be noticeable. Errors may occur, but it is clear what he/ she is trying to express. Uses a repertoire of frequently used &quot;routines&quot;</td></tr><tr><td></td><td></td><td>referencing).</td><td>Thematic development follows ST, although not appropriate for the task. Or: may lack a logical order appropriate for the task. Paragraphs usually not appropriate (if used at all).</td><td>control of devices but may overuse certain devices or show a mechanical use. The use of more elaborate cohesive devices may sometimes impede communication.</td><td>occur when expressing more complex thoughts. Register not necessarily appropriate, e.g., may overuse colloquial expressions. Spelling is accurate enough to be followed most of the time.</td><td>and patterns associated with more predictable situations reasonably accurately. May attempt complex patterns but generally unsuccessfully. Punctuation is accurate enough to be followed most Of the time.</td></tr></table></body></html>

Note: The features that are described in italics are acceptable for a specific level, but they are not expected.

<html><body><table><tr><td colspan="11">Rating scale for opinion tasks.</td></tr><tr><td></td><td>1. Mining All selected</td><td>2. Precision All ST ideas</td><td>3. Processing Substantial</td><td>4. Attribution All ideas taken from source text</td><td>5. Synthesis Takes a clear stance with a well-</td><td>6. Structure Macrostructure clear/ appropriate</td><td>7. Cohesion Shows consistent and</td><td>8. Vocabulary Broad lexical</td><td>9. Grammar Broad repertoire of linguistic</td></tr><tr><td>5 | C1 and above</td><td>ideas are relevant, presented in necessary depth. No irrelevant ST ideas Appropriate amount of main ideas and supporting details selected, in good balance</td><td>are presented correctly and precisely.</td><td>revision: Expresses all ST ideas in own words (only key words are used). Reformulates syntax of ST.</td><td>are appropriately attributed. Clear distinction between own and ST ideas.</td><td>informed opinion, meaningfully relating ST ideas (and own ideas) to task at hand. Bases argumentation on relevant ST and/or relevant own ideas throughout the text.</td><td>for task. (Re-)arranges ST elements (and if apl. own ideas) into logical order (not necessarily that of ST). Appropriate paragraphs that are logical in themselves.</td><td>continuous controlled use of a broad repertoire of cohesive referencing, semantic fields, connectors, including elaborate ones from the academic field) on sentence and paragraph levels. (This contributes to the coherence of the text). devices (e.g.,</td><td>repertoire allowing gaps to be readily overcome with circumlocutions. Good command of common idiomatic expressions and academic register (no colloquialisms) Occasional minor slips but no significant vocabulary or spelling errors</td><td>structures and complex sentence patterns. Consistently maintains a high degree of grammatical accuracy (including complex structures); errors are rare and difficult to spot. Punctuation is consistent and</td></tr><tr><td>4 | B2 +</td><td>All selected ideas are relevant but not all in necessary depth, or there may be some irrelevant details</td><td>More than 3, but not yet enough for 5</td><td>More than 3, but not yet enough for 5</td><td>More than 3, but not yet enough for 5</td><td>More than 3, but not yet enough for 5</td><td>More than 3, but not yet enough for 5</td><td>Uses a variety of elaborate and common cohesive devices (e.g. connectors, semantic fields, pronouns) efficiently to mark clearly the relationships between ideas. Errors in the field of cohesive devices are rare.</td><td>Good and varied range of topic specific vocabulary and collocations and good command. On the whole adopts appropriate formal register</td><td>helpful. Good and varied grammatical range and control. Occasional slips&#x27; or non-systematic errors and minor flaws in sentence structure may occur, but they are rare. Very few mistakes</td></tr><tr><td>3|B2</td><td>1. Mining Majority of the selected content is relevant, but not necessarily all in required depth,</td><td>2. Precision Majority of the ideas are presented correctly.</td><td>3. Processing Moderate revision: Paraphrases</td><td>4. Attribution ST attribution becomes apparent, but some ST ideas</td><td>5. Synthesis Takes a (more or less informed) stance and on the whole manages to</td><td>6. Structure Macrostructure on the whole clearly developed,</td><td>7. Cohesion Uses a sufficient range of common and some</td><td>8. Vocabulary Good range of topic- specific vocabulary and collocations.</td><td>in punctuation. 9. Grammar Good range of also infrequent structures and some complex</td></tr><tr><td></td><td>and/or some irrelevant details (differentiation between relevant and irrelevant ideas not yet fully consistent)</td><td>There may be some misinterpre- tations or imprecisions.</td><td>majority of ST ideas (There may be occasional use of ST strings of words that are only slightly modified by</td><td>may not be appropriately attributed. Overall manages to adding/ deleting distinguish</td><td>relate ST (and own) ideas meaningfully to task. Argumentation may not fully be</td><td>although there may be some jumpiness&#x27;. Attempts to rearrange ST elements (and if apl. own ideas) into a logical order,</td><td>elaborate cohesive devices to link his/her utterances into clear, coherent discourse. Errors in the field of</td><td>Attempts to vary formulation to avoid frequent repetition, though not always successful. Accuracy is generally high,</td><td>sentence patterns. Shows a relatively high degree of grammatical</td></tr></table></body></html>

<html><body><table><tr><td colspan="10">Table A.2 (continued)</td><td rowspan="2">. Harsch et al.</td></tr><tr><td rowspan="2"></td><td rowspan="2">1. Mining not yet given (may use too few main ideas, too many irrelevant details or examples)</td><td rowspan="2">2. Precision</td><td rowspan="2">3. Processing syntactical structures.</td><td rowspan="2">4. Attribution</td><td rowspan="2">5. Synthesis missing (or illogically</td><td rowspan="2">6. Structure 7. Cohesion development may compensate for</td><td rowspan="2">8. Vocabulary reasonably accurate but may show signs of mother tongue influence.</td><td rowspan="2">9. Grammar reasonably</td><td></td></tr><tr><td>impeding errors. Punctuation is accurate but may show signs of mother tongue</td></tr><tr><td>2 | B1 +</td><td>Some of the selected content is relevant, not in required depth. Includes many irrelevant ideas. (may overuse examples.)</td><td>Some of the ideas are presented correctly and/or Ideas presented with some imprecision.</td><td>Minimal revision: Attempts to paraphrase but not always successful (e.g., Strings of words slightly modified by adding/ deleting words or using synonyms for content words). Reformulates some syntactical</td><td>More than 1, but not yet enough for 3. Attempts to attribute some ST ideas to the ST, but not necessarily successful.</td><td>May take a stance but opinion is not well-informed. Only partially manages to relate ST (and own) ideas to task (e.g. does not provide reasoning to support stance) May only partially base argumentation on ST ideas. Own ideas, if present, may not all</td><td>developed) paragraphs. Identifiable attempt at macrostructure, but not fully Successful (e.g. intro &amp; conclusion but no appropriate middle part). Attempt at paragraphs that may not always be logical.</td><td>Uses a simple range of common cohesive devices (e.g., pointing words, repetitions, common connectors). Makes occasional errors.</td><td>Sufficient range of vocabulary. Some repetitive use Of vocabulary. Register is neutral, not yet academic. May make mistakes in spelling of less familiar words.</td><td>influence. Good range of frequent structures. There may be attempts at more complex structures, but these are not necessarily correct. Generally good control though mother tongue influence may be noticeable.</td><td>Errors may occur,</td></tr><tr><td>1 | B1 and below</td><td>1. Mining Majority of selected content is not relevant, and not in necessary depth Selects too many details or examples.</td><td>2. Precision Majority of selected ideas misinterpreted.</td><td>3. Processing Near copy: Major instances of lifting from ST (usually without referencing).</td><td>4. Attribution Ideas from source text are generally not attributed to ST. Generally difficult for reader to distinguish between ST and own ideas. not adding</td><td>5. Synthesis No clear stance. Barely relies on ST for argumentation. Offers own (mis-) interpretation of the topic. Or Presents ST ideas out of context (thus distorting original meaning). Or Merely summarizes ST,</td><td>6. Structure Orders a series of shorter discrete elements into a linear sequence of points. Thematic development follows ST, although not appropriate for the task. Or: may lack a logical order appropriate for the task. Paragraphs usually</td><td>7. Cohesion Links discrete elements using a limited number of cohesive devices. Shows reasonable control of common cohesive devices but may overuse certain devices or show a mechanical use. The use of more elaborate cohesive devices may sometimes impede</td><td>8. Vocabulary Sufficient range of vocabulary, with some circumlocutions, repetitions. May show some instances of inappropriate vocabulary use. Major errors may occur when expressing more complex thoughts.</td><td>express. 9. Grammar Uses a repertoire of frequently used &quot;routines&quot; and patterns associated with more predictable situations reasonably accurately. May attempt complex patterns but generally unsuccessfully.</td><td>he/she is trying to Assesing Writing 62 (202)10089 Punctuation is</td></tr></table></body></html>

# Appendix B. Reliabilities at the end of rater training

The selection of the raters was based on bivariate correlation analysis and intraclass correlation coefficients (de Raadtet al., 2021). First, we conducted bivariate correlation analyses between each rater’s ratings and the average ratings from all raters (see Table B.1). The six raters with the strongest correlation indices $( r _ { s } < 0 . 4 0 )$ in all criteria were pre-selected for the main study (R1 to R6).

Table B.1 Spearman correlation indices $\left( r _ { s } \right)$ between mean of ratings from all raters and individual ratings.   

<html><body><table><tr><td colspan="9"> Rating scale criteria</td></tr><tr><td>Rater ID</td><td>1</td><td>2</td><td>3</td><td>4 5</td><td>6</td><td></td><td>7</td><td>8</td><td>9</td><td>Main study</td></tr><tr><td>R1</td><td>0.61 * **</td><td>0.41 * *</td><td>0.85 * **</td><td>0.79 * **</td><td>0.46 *</td><td>0.72 * **</td><td>0.63 * **</td><td>0.68 * **</td><td>0.71 * **</td><td>Yes</td></tr><tr><td>R2</td><td>0.76 * **</td><td>0.76 * **</td><td>0.91 * **</td><td>0.92 * **</td><td>0.76 * **</td><td>0.71 * **</td><td>0.70 * **</td><td>0.70 * **</td><td>0.73 * **</td><td>Yes</td></tr><tr><td>R3</td><td>0.72 * **</td><td>0.69 * **</td><td>0.65 * **</td><td>0.85 * **</td><td>0.69 * **</td><td>0.78 * **</td><td>0.64 * **</td><td>0.60 * **</td><td>0.61 * **</td><td>Yes</td></tr><tr><td>R4</td><td>0.84 * **</td><td>0.71 * **</td><td>0.86 * **</td><td>0.96 * **</td><td>0.45 *</td><td>0.70 * **</td><td>0.65 * **</td><td>0.79 * **</td><td>0.81 * **</td><td>Yes</td></tr><tr><td>R5</td><td>0.77 * **</td><td>0.71 * **</td><td>0.90 * **</td><td>0.88 * **</td><td>0.61 * **</td><td>0.73 * **</td><td>0.57 * **</td><td>0.72 * **</td><td>0.71 * **</td><td>Yes</td></tr><tr><td>R6</td><td>0.64 * **</td><td>0.61 * **</td><td>0.71 * **</td><td>0.91 * **</td><td>0.53 *</td><td>0.69 * **</td><td>0.46 * **</td><td>0.55 * **</td><td>0.43 * **</td><td>Yes</td></tr><tr><td>R7</td><td>0.51 * **</td><td>0.49 * **</td><td>0.62 * **</td><td>0.86 * **</td><td>0.54 * *</td><td>0.67 * **</td><td>0.30</td><td>0.63 * **</td><td>0.74 * *</td><td>No</td></tr><tr><td>R8</td><td>0.71 * **</td><td>0.70 * **</td><td>0.47 * **</td><td>0.07</td><td>0.29</td><td>0.46 * **</td><td>0.41 * *</td><td>0.22</td><td>A</td><td>No</td></tr><tr><td>R9</td><td>0.49 * **</td><td>0.66 * **</td><td>0.88 * **</td><td>0.83 * **</td><td>0.75 * **</td><td>0.74 * **</td><td>0.63 * **</td><td>0.68 * **</td><td>0.35 *</td><td>No</td></tr><tr><td>E10</td><td>0.66 * **</td><td>0.45 * **</td><td>0.66 * **</td><td>0.83 * **</td><td>0.30</td><td>0.54 * **</td><td>0.62 * **</td><td>0.52 * **</td><td>0.48 * **</td><td>No</td></tr></table></body></html>

Aundefined correlation due to a standard deviation equal to 0 To confirm our selection, we calculated intraclass correlation coefficients (ICCs; Koo & Li, 2016), employing the two-way random effects, absolute agreement model both for single measures $( I C C _ { 2 , 1 } )$ and multiple measures $\mathrm { ( I C C _ { 2 , k } ) }$ . ICCs can range from poor $_ { ( < 0 . 5 ) }$ to excellent $_ { ( < 0 . 9 ) }$ (Shrout & Fleiss, 1979). Table B.2 presents the individual ICCs for the six selected raters in comparison to the ICCs for all raters (both individual and aggregated indices). $^ { * } p \leq 0 . 0 5 , ^ { * * } p \leq 0 . 0 1 , ^ { * * * } p \leq 0 . 0 0 1$

Table B.2 Intraclass correlation coefficients (ICCs).   

<html><body><table><tr><td rowspan="2">Criterion</td><td>Selected raters</td><td colspan="2"> All raters</td></tr><tr><td>ICC2,1 [95% CI]</td><td>ICC2,1 [95% CI]</td><td>ICC2,k [95% CI]</td></tr><tr><td>1</td><td>0.52 [0.39; 0.66]</td><td>0.43 [0.31; 0.57]</td><td>0.88 [0.82; 0.93]</td></tr><tr><td>2</td><td>0.50 [0.37; 0.65]</td><td>0.39 [0.28; 0.53]</td><td>0.87 [0.79; 0.92]</td></tr><tr><td>3</td><td>0.54 [0.41; 0.68]</td><td>0.45 [0.34; 0.59]</td><td>0.89 [0.83; 0.94]</td></tr><tr><td>4</td><td>0.73 [0.59; 0.83]</td><td>0.57 [0.45; 0.70]</td><td>0.93 [0.89; 0.96]</td></tr><tr><td>5</td><td>0.33 [0.20; 0.49]</td><td>0.23 [0.14; 0.36]</td><td>0.75 [0.62; 0.85]</td></tr><tr><td>6</td><td>0.53 [0.40; 0.67]</td><td>0.43 [0.31; 0.57]</td><td>0.88 [0.82; 0.93]</td></tr><tr><td>7</td><td>0.28 [0.16; 0.44]</td><td>0.22 [0.13; 0.35]</td><td>0.74 [0.61; 0.85]</td></tr><tr><td>8</td><td>0.41 [0.28; 0.56]</td><td>0.31 [0.21; 0.45]</td><td>0.82 [0.72; 0.89]</td></tr><tr><td>9</td><td>0.39 [0.26; 0.54]</td><td>0.29 [0.19; 0.43]</td><td>0.80 [0.70; 0.88]</td></tr></table></body></html>

Note: $\mathrm { I C C } _ { 2 , 1 }$ : Reliability for single rater ratings, $\mathrm { I C C } _ { 2 , 1 }$ : Reliability for mean of ratings from all raters.

Reliabilities for single measures are mainly in the poor region $( < 0 . 5 )$ , while aggregated measures range from acceptable to excellent $( < 0 . 9 )$ . The group of selected raters show stronger individual ICCs than the group of all raters, corroborating our selection.

# References Appendix B

de Raadt, A., Warrens, M. J., Bosker, R. J., & Kiers, H. A. (2021). A comparison of reliability coefficients for ordinal rating scales. Journal of Classification, 38(3), 519–543. https://doi.org/10.1007/s00357–021-09386–5. Koo, T.K., & Li, M.Y. (2016). A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research. Journal of Chiropractic Medicine, 15, 155–163. https://doi.org/10.1016/j.jcm.2016.02.012. Shrout P.E. & Fleiss J.L. (1979). Intraclass correlations: uses in assessing rater reliability. Psychological Bulletin, 86(2), 420–8. https://doi.org/10.1037//0033–2909.86.2.420.

# Appendix C. Model of rating stages used in training, based on Lumley (2005)

Table C.1 Rating stages.   

<html><body><table><tr><td></td><td>Stage</td><td>Rater&#x27;s focus</td><td>Observable behaviours</td></tr><tr><td>1.</td><td>Pre-rating</td><td>Student ID Task ID Materials, copy-detection app</td><td>Identify script Identify task Set up app and open all relevant</td></tr><tr><td>2.</td><td>First reading</td><td>Task and demands Script</td><td>Read task and task expectations Read script</td></tr><tr><td>3.</td><td>Rate the script against the descriptors of each criterion in turn</td><td>Scale and script: focus only on the very criterion being rated</td><td>Reread task and scripte Refer to scale descriptors Comment on salient features Articulate and justify rating.</td></tr><tr><td>4.</td><td>Consider ratings given</td><td>Scale and script: justification and consistency of ratings</td><td>Compare with benchmark texts Confirm or revise existing ratings</td></tr></table></body></html>

Check the script ID and the task ID. Set up your app, open all relevant documents.

Read the task, the expectations and the script.

Assess the criteria in turn and continuously refer to the scale descriptors and script. For each criterion, read the script anew, focusing on the relevant features described for that criterion. Comment on salient features. Articulate and justify the reasons for allotting a rating.

Use the benchmark texts to come to a consistent result. Consider your ratings: Are they justified and consistent?

# Appendix D. TAP Guidelines

\* \*Introduction to thinking aloud\* \*.

Before we start, I would like you to make sure that your phone is turned off and you are in a quiet environment without interruptions and with a good internet connection.

As mentioned in the consent form you signed, I will record the audio and your screen during this session.

As you may already read on the document I sent you, today we are doing something called thinking aloud. It is a method that is used o get insights about people’s thoughts and cognitive processes while they are doing a specific task.

Today, I am going to observe your rating. Your main task is to talk aloud constantly from the minute you start, I would like you to express everything you happen to think of, no matter how irrelevant it may seem.

I am not primarily interested in your final solution or scores, but in your thinking behaviour, in all your attempts, in whatever comes to your mind, no matter whether it is a good or a less good idea.

It is important that you keep talking all the time, expressing your thoughts all the time. If you spend time reading the materials, then you should do that aloud as well, so that I can understand what you are doing at that time.

I will say nothing more than give you periodic feedback such as $\cdot _ { \mathrm { m h m } } ,$ ’, although I will prompt you to keep talking if you fall silent for more than 10 or $_ { 1 5 s }$

It may be a bit awkward at first, but you will get used to it. We´ve found that it helps to have some practice at doing this. So let me give you an example.

\* \* Demonstration and practice\* \* (from Collins, 2014).

Let’s say I am asked a question about “how many windows are there in your house?”.

If I was thinking aloud, I would say… well, there´s one window in the kitchen, and then in the living room, there are two windows. Well, I say windows, they are panes of glass – they don’t open. Not sure if I should include those. I think I will. And there is a small one in the bathroom… and so on.

Now let me ask you the same question. Think about how many windows there are in your house. As you count up the windows, tell me what you are seeing and thinking about.

\* \*Give the participant time to do that\* \*.

$^ { \ast } \ast _ { \mathrm { I n } }$ case further explanation is needed\* \*.

Good.

Do you have any questions regarding the thinking aloud procedure before we start with the task?

\* \*Answer participant questions\* \*.

\* \* Rating task\* \*.

Okay, now we will start the main task. I will now start to record the meeting.

\* \*Start recording\* \*.   
\* \*Send the scripts which are to be rated in the TAP session \* \*.   
\* \*Let the participant save the received scripts. \* \*.   
Now please share your screen using the Zoom option: “Bildshirm freigeben” or “Share screen”.   
In case there are no questions, we will start the thinking aloud task now.   
\* \*If participant becomes quiet for a long time or does something without explanation\* \*.

1. keep talking or what are you thinking about?   
2. when necessary, ask the participant to explain what s/he means by something, and/or ask her/him to explain her/his solution   
processes   
3. Please say out loud what you are thinking…. \* \*(cf. Collins, 2014)\* \*   
4. What’s going through your mind? \* \*(cf. Collins, 2014)\* \*

After having finished the rating.   
\* \*Ask the participant to send the word scripts with assigned scores. \* \*.   
Thank you for your time and participation.   
\* \*Stop recording\* \*.   
\* \*Close the zoom meeting\* \*.   
References Appendix D.   
Collins, D. (2014). Cognitive interviewing practice. Thousand Oaks, CA: Sage.

# Appendix E. Guidelines for FGIs

As not all questions and sections in the FGIs are relevant for the study reported here, the following guide contains only those questions that bear a direct relevance for this paper.

# Welcome and recommendations.

Good afternoon/morning and welcome to our session.

You’ve probably noticed the recording. We will record the interview because we don’t want to miss any of your comments. This is your last activity as a rater in the MASK Project and you´ve come a long way and gained a lot of expertise in this area. Thanks for taking the time to join us and talk about your experience. I will lead the discussion today and my assistant will take notes and support us in case of technical difficulties. We will be on a first name basis tonight, but we won’t use any names in our reports, just your rater IDs. You may be assured of complete confidentiality.

You do not have to turn on your camera; however, it makes communication online easier for all of us. Make sure you have your phone muted and you are not distracted by it.

Please participate actively in the discussion; we are very interested in your opinion to every question. Discuss your ideas with other participant more than with me. There are no wrong answers but rather differing points of view. Please feel free to share your point of view even if it differs from what others have said. Keep in mind that we’re just as interested in negative comments as positive comments, and at times the negative comments are the most helpful. Unlike rater training and re-training sessions that we had before, you do not have to agree with each other this time and come to common understanding of the issue. Nonetheless, please respect other participants and try not to interrupt them. You can also use raise hand option if it facilitates the discussion. Any questions?

Introductory question.

• Now you have gained extensive experience in rating, how would you describe your rating experience in general? Did you notice a change over time? What was the most challenging? Liftings and the copy-detection app   
• What was your main strategy to detect and validate liftings?   
• How did liftings affect your ratings of the different criteria?   
• How would you evaluate the app and its assistance during rating?   
• When did you use the app? (in the beginning, middle or end of the rating process)?   
• For which criteria did you use it?   
• How helpful were the separate features of the app?   
• Do you have suggestions for improvements?   
• How did you perceive liftings in summary and opinion tasks? Have you noticed differences or peculiarities? Rating scale application, rater effects and possible improvements   
• How manageable was it for you to rate with the rating scale? Is there a way we can further improve it?   
• Were there criteria that were challenging for you? If so, which ones and why?   
• Did you put more weight on certain criteria?   
• Thinking back of the training and your experience, do you wish you would have received more guidance to prepare you for the ratings?   
• How well do you think do the tasks and the rating scale assess academic writing ability/preparedness for academic studies?

• Can you think of any aspects that may have affected your rating? • What is your estimated level of proficiency in English? • Do you have other aspects that you would like to address?

# Appendix F. Coding scheme 1 employed for TAPs

1. Preparation

a. as trained (open all documents and copy-detection app before starting) b. lack of preparation

2. First reading (along with raters’ commenting on script features)

a. analysis of script features, closeness to source text b. evaluation of script features, closeness to source text c. mentioning missing aspects

3. Addressing a specific criterion

a. checking descriptors (within the criterion, correct – incorrect ones)   
b. (re) reading / analysing / interpreting / evaluating script features against descriptors (appropriately or inappropriately)   
c. Coming to judgement (with / without justification)

4. Sequence of criteria / Focus a. focus on criteria independently from other criteria as trained b. addressing criteria in the order that they are presented in rating scale c. instances where raters are off focus or jump

5. Re-evaluating decisions, self-monitoring (with – without justifications)

a. re-checking descriptors   
b. re-reading script   
c. confirming or changing rating (with – without justification)

6. Use of rating materials (comment on level of appropri a. copy-detection app (used for which criteria?) b. rating scale, benchmarks, task expectations

# Appendix G. Coding scheme 2 employed for interviews

1. Challenges regarding the interpretation and differentiation of the CRITERIA   
2. Challenges regarding the interpretation and differentiation of the LEVELS   
3. Challenges arising from task types on rating   
4. Perceptions of rating scale, additional materials, copy-detection app, procedures, workload).

# Appendix H. Supporting information

Supplementary data associated with this article can be found in the online version at doi:10.1016/j.asw.2024.100894.

# References

Attali, Y. (2015). A comparison of newly-trained and experienced raters on a standardized writing assessment. Language Testing, 33(1), 99–115. https://doi.org/ 10.1177/0265532215582283   
Barkaoui, K. (2010a). Do ESL essay raters’ evaluation criteria change with experience? A mixed-methods, cross-sectional study. TESOL Quarterly, 44(1), 31–57. https://doi.org/10.5054/tq.2010.214047   
Barkaoui, K. (2010b). Variability in ESL essay rating processes: The role of the rating scale and rater experience. Language Assessment Quarterly, 7(1), 54–74. https:// doi.org/10.1080/15434300903464418   
Barkaoui, K. (2011). Think-aloud protocols in research on essay rating: An empirical study of their veridicality and reactivity. Language Testing, 28(1), 51–75. https:// doi.org/10.1177/0265532210376379   
Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. Qualitative Research in Psychology, 3(2), 77–101. https://doi.org/10.1191/1478088706qp063oa   
Chan, S., Inoue, C., & Taylor, L. (2015). Developing rubrics to assess the reading-into-writing skills: A case study. Assessing Writing, 26, 20–37. https://doi.org/ 10.1016/j.asw.2015.07.004   
Chan, S., & May, L. (2023). Towards more valid scoring criteria for integrated reading-writing and listening-writing summary tasks. Language Testing, 40(2), 410–439. https://doi.org/10.1177/02655322221135025   
Charters, E. (2003). The use of think-aloud methods in qualitative research. An introduction to think-aloud methods. Brock Education, 12(2), 68–82. https://doi.org/ 10.26522/brocked.v12i2.38   
Cheong, C. M., Zhu, X.-H., Li, G. Y., & Wen, H. (2019). Effects of intertextual processing on L2 integrated writing. Journal of Second Language Writing, 44, 63–75.   
Collins, D. (2014). Cognitive interviewing practice. Thousand Oaks, CA: Sage.   
Cohen, L., Manion, L., & Morrison, K. (2011). Research Methods in Education (7th ed.)..,). Routledge. https://doi.org/10.4324/9780203720967   
Council of Europe (2001). Common European Framework of Reference for Languages: Learning, teaching, assessment. Language Policy Unit, Strasbourg. 〈www.coe.int/ lang-CEFR〉.   
Creswell, J. W., & Plano Clark, V. L. (2018). Designing and conducting mixed methods research (3rd ed..,). Los Angeles: Sage.   
Cumming, A. (2013). Assessing Integrated Writing Tasks for Academic Purposes: Promises and Perils. Language Assessment Quarterly, 10(1), 1–8. https://doi.org/ 10.1080/15434303.2011.622016   
Cumming, A., Kantor, R., & Powers, D. (2002). Decision making while rating ESL/EFL writing tasks: A descriptive framework. Modern Language Journal, 86(1), 67–96.   
Cumming, A., Lai, C., & Cho, H. (2016). Students’ writing from sources for academic purposes: A synthesis of recent research. Journal of English for Academic Purposes, 23, 47–58. https://doi.org/10.1016/j.jeap.2016.06.002   
Deygers, B., & Van Gorp, K. (2015). Determining the scoring validity of a co-constructed CEFR-based rating scale. Language Testing, 32(4), 521–541. https://doi.org/ 10.1177/0265532215575626   
Eckes, T. (2012). Operational rater types in writing assessment: Linking rater cognition to rater behavior. Language Assessment Quarterly, 9(3), 270–292. https://doi. rg/10.1080/15434303.2011.649381   
Eckes, T. (2015). Introduction to Many-facet Rasch measurement: Analysing and evaluating rater-mediated assessments (2nd Revised and updated edition. Frankfurt: Peter Lang.   
Frey, J. H., & Fontana, A. (1991). The group interview in social research. The Social Science Journal, 28(2), 175–187. https://doi.org/10.1016/0362-3319(91)90003-M   
Gebril, A., & Plakans, L. (2014). Assembling validity evidence for assessing academic writing: Rater reactions to integrated tasks. Assessing Writing, 21, 56–73. https:// doi.org/10.1016/j.asw.2014.03.002   
Harsch, C., Koval, V., Delgado-Osorio, X. & Hartig, J. (2024). Usability of CEFR Companion Volume scales for the development of an analytic rating scale for academic integrated writing assessment. CEFR Journal - Research and Practice. Vol. 6, 155-177, https://doi.org/10.37546/JALTSIG.CEFR6-9.   
Harsch, C., & Martin, G. (2012). Adapting CEF-descriptors for rating purposes: Validation by a combined rater training and scale revision approach. Assessing Writing, 17(4), 228–250. https://doi.org/10.1016/j.asw.2012.06.003   
Jia, W., & Zhang, P. (2023). Rater cognitive processes in integrated writing tasks: From the perspective of problem-solving (Article) Lang Test Asia, 13, 50. https://doi. org/10.1186/s40468-023-00265-x.   
KMK, Ed. (2014). Bildungsstandards für die fortgeführte Fremdsprache (Englisch/Franz¨osisch) für die Allgemeine Hochschulreife (Beschluss der Kultusministerkonferenz vom 18.10.2012). Koln, ¨ Carl Link.   
Keck, C. (2006). The use of paraphrase in summary writing: A comparison of L1 and L2 writers. Journal of Second Language Writing, 15(4), 261–278. https://doi.org/ 10.1016/j.jslw.2006.09.006   
Knoch, U. (2011). Rating scales for diagnostic assessment of writing: What should they look like and where should the criteria come from? Assessing Writing, 16(2), 81–96. https://doi.org/10.1016/j.asw.2011.02.003.   
Knoch, U., & Chapelle, C. A. (2018). Validation of rating processes within an argument-based framework. Language Testing, 35(4), 477–499. https://doi.org/10.1177/ 0265532217710049   
Knoch, U., Deygers, B., & Khamboonruang, A. (2021). Revisiting rating scale development for rater-mediated language performance assessments: Modelling construct and contextual choices made by scale developers. Language Testing, 38(4), 602–626. https://doi.org/10.1177/0265532221994052   
Knoch, U., & Sitajalabhorn, W. (2013). A closer look at integrated writing tasks: Towards a more focused definition for assessment purposes. Assessing Writing, 18(4), 300–308. https://doi.org/10.1016/j.asw.2013.09.003   
Kukartz, U., & R¨adiker, S. (2019). Analyzing Qualitative Data with MAXQDA. Springer International Publishing. https://doi.org/10.1007/978-3-030-15671-8   
Lestari, S. B., & Brunfaut, T. (2023). Operationalizing the reading-into-writing construct in analytic rating scales: Effects of different approaches on rating. Language Testing, 40(3). https://doi.org/10.1177/02655322231155561   
Li, J. (2014). The role of reading and writing in summarization as an integrated task. Language Testing in Asia, 4(1), 3. https://doi.org/10.1186/2229-0443-4-3   
Li, J., & Wang, Q. (2021). Development and validation of a rating scale for summarization as an integrated task (Article) Asian-Pacific Journal of Second and Foreign Language Education, 6, 11. https://doi.org/10.1186/s40862-021-00113-6.   
Lim, G. S. (2011). The development and maintenance of rating quality in performance writing assessment: A longitudinal study of new and experienced raters. Language Testing, 28(4), 543–560. https://doi.org/10.1177/0265532211406422   
Linacre, J. M. (2004). Optimizing rating scale category effectiveness. In In. J. Everett V. Smith, & R. M. Smith (Eds.), Introduction to Rasch measurement. Theory, models, and applications (pp. 258–278). Maple Grove, Minnesota: JAM Press.   
Linacre, J. M. (2023). A user’s guide to FACETS Rasch-Model computer programs. Program Manual, 3, 86.   
Lumley, T. (2005). Assessing Second Language Writing: The Rater’s Perspective. Peter Lang.   
Motz, M. (2005). Englisch oder Deutsch in Internationalen Studiengangen? ¨ [English or German in international degree programs? (Ed.). Peter Lang,.   
Murray, N. (2016). Standards of English in higher education: issues, challenges and strategies. Cambridge University Press. https://doi.org/10.1017/CBO9781139507189   
Myford, C. M. (2012). Rater cognition research: Some possible directions for the future. Educational Measurement: Issues and Practice, 31(3), 48–49. https://doi.org/ 10.1111/j.1745-3992.2012.00243.x   
Myford, C. M., & Wolfe, E. W. (2004a). Detecting and measuring rater effects using Many-Facet Rasch Measurement: Part I. In. In E. V. Jr., & R. M. Smith (Eds.), Introduction to Rasch measurement: Theory, models, and applications (pp. 460–517). Maple Grove: JAM Press.   
Myford, C. M., & Wolfe, E. W. (2004b). Detecting and measuring rater effects using many-facet Rasch measurement: Part II. In. In E. V. Jr., & R. M. Smith (Eds.), Introduction to Rasch Measurement: Theory, models, and applications (pp. 518–575). Maple Grove: JAM Press.   
Ono, M., Yamanishi, H., & Hijikata, Y. (2019). Holistic and analytic assessments of the TOEFL iBT® integrated writing task. JLTA (Japan Language Testing Association) Journal, 22, 65–88.   
Pearson Education (2015). Global Scale of English Learning Objectives for Academic English. Pearson Education Limited. 〈https://www.pearson.com/content/dam/onedot-com/one-dot-com/english/TeacherResources/GSE/GSE-learning-objectives-brochure.pdf〉.   
Plakans, L., & Gebril, A. (2015). Assessment myths: Applying second language research to classroom teaching. University of Michigan Press,.   
Ringwald, C., & Harsch, C. (2018). Und dann kommt das große Erwachen an der Uni“ – Eine explorative Bedarfsanalyse. Fremdsprachen und Hochschule, 93, 9–18.   
Rolke, ¨ H. (2012). The ItemBuilder: A graphical authoring system for complex item development. In In. T. Bastiaens, & G. Marks (Eds.), Proceedings of E-Learn: World Conference on E-Learning in Corporate, Government, Healthcare, and Higher Education (pp. 344–353). Association for the Advancement of Computing in Education (AACE). https://www.fachportal-paedagogik.de/literatur/vollanzeige.html?FId=1004067.   
Rupp, A.A., Vock, M., Harsch, C., & Koller, ¨ O. (2008). Developing standards-based assessment tasks for English as a first foreign language. Waxmann.   
Shi, L. (2004). Textual Borrowing in Second-Language Writing. Written Communication, 21(2), 171–200. https://doi.org/10.1177/0741088303262846   
Shin, S.-Y., & Ewert, D. (2015). What accounts for integrated reading-to-write task scores? Language Testing, 32(2), 259–281. https://doi.org/10.1177/ 026553221456025   
Sormunen, E., Heinstrom, J., Romu, L. & Turunen, R. (2012). A Method for the Analysis of Information Use in Source-Based Writing. Information Research: An International Electronic Journal, 17(4), Article 535 Retrieved April 16, 2024 from https://www.learntechlib.org/p/54646.   
Spivey, N. N., & King, J. R. (1989). Readers as writers composing from sources. Reading Research Quarterly, 24(1), 7–26. https://www.jstor.org/stable/748008.   
Uludag, P., & McDonough, K. (2022). Validating a rubric for assessing integrated writing in an EAP context (Article) Assessing Writing, 52, Article 100609. https://doi. org/10.1016/j.asw.2022.100609.   
Wang, J., Engelhard, G., Raczynski, K., Song, T., & Wolfe, E. (2017). Evaluating rater accuracy and perception for integrated writing assessments using a mixedmethods approach. Assessing Writing, 33, 36–47. https://doi.org/10.1016/j.asw.2017.03.003   
Weir, C. J. (2005). Language testing and validation: an evidence-based approach. Palgrave Macmillan,. https://doi.org/10.1057/9780230514577   
Xie, Q. (2023). Assessing source use: Summary vs. reading-to-write argumentative essay (Article) Assessing Writing, 57, Article 100755. https://doi.org/10.1016/j. asw.2023.100755.   
V.E.R.B.I. Software 2021, MAXQDA 2022, computer program, VERBI Software, Berlin.

Claudia Harsch is a professor at the University of Bremen, specialising in language learning, teaching and assessment. She has worked in Germany and in the UK, and is active in teacher training worldwide. Her research focuses on language assessment, language and migration, assessment literacy, and the implementation of the CEFR.

Valeriia Koval is a PhD candidate and a research associate at the University of Bremen since 2020. She holds a Master‘s Degree in applied linguistics from the University of Bonn, Germany. Her research interest is assessment of academic integrated writing and rater cognition.

Paraskevi (Voula) Kanistra is an Associate Director/ Senior Researcher at Trinity College London. She recently completed her PhD in Language Testing at Bremen University. Voula has held several language assessment positions ranging from test developer, test analyst to standard setting practitioner. Her research interests include standard setting, test-fairness, and validity.

Ximena Delgado-Osorio is a Ph.D. candidate in the Department of Teacher and Teaching Quality at the DIPF | Leibniz Institute for Research and Information in Education. She studied Psychology at the University of the Andes (Colombia) and received her M.Sc. in Psychology: Learning Sciences at the University of Munich.