# Getting a load of linguistic reasoning: How L1 student teachers process rules of thumb and linguistic manipulations in discussions about grammar

1,\*, JIMMY VAN RIJT, 2 ARINA BANGA, AND   
3 MARTIJN GOUDBEEK

1 Tilburg Center of the Learning Sciences, Tilburg University, Warandelaan 2, 5037 AB   
Tilburg, the Netherlands   
2 Department of Teacher Education, Tilburg University of Applied Sciences, Professor   
Goossenslaan 1, 5022 DM Tilburg, the Netherlands   
3 Department of Communication and Cognition, Tilburg University, Warandelaan 2,   
5037 AB Tilburg, the Netherlands   
$^ { * } \mathrm { E }$ -mail: j.h.m.vanrijt@tilburguniversity.edu

An important skill for L1 language teachers when teaching grammar is the ability to produce and quickly evaluate arguments that underpin a grammatical analysis. Previous research has revealed that the strongest arguments in favour of a particular grammatical analysis are based on linguistic manipulations (LM) rather than on rules of thumb (RoT). This makes it critical for teachers to be able to handle arguments based on LM. If LM are considered too difficult to process compared to RoT, however, (student) teachers may avoid grammatical argumentation based on LM altogether, and they might struggle to evaluate their pupils’ LM-based grammatical argumentation. The current study has therefore examined whether LM impose a higher cognitive load on Dutch student teachers than RoT, using grammatical discussion tasks in which participants $( N = 2 9 8$ ) evaluated arguments based on RoT and on LM. Multilevel analyses indicate that LM are indeed more difficult to process than RoT, as measured by response times, correct classifications, and perceived difficulty ratings. This effect is partly influenced by student teachers’ need for cognition and their willingness to engage in grammar.

# INTRODUCTION

The place of grammar in the L1 curriculum has been fiercely debated over the last decades (Locke 2010; Fontich and Camps 2014; Myhill 2018). Myhill (2021) shows that most of the research on L1 grammar teaching has contributed to the question of whether grammar should be part of the curriculum at all, rather than investigating how grammar can best be taught and learned. In recent years, however, research about L1 grammar teaching seems to have shifted perspective. Rather than attempting to contribute to the polarized debate of whether grammar should be taught at all, recent studies appear to focus more on how grammar might be taught meaningfully (e.g. Myhill et al. 2012; Fontich 2016; Watson and Newman 2017; Boivin et al. 2018; Rättyä et al. 2019; Kabel et al. 2021; Van Rijt et al. 2022). Such studies, which are dedicated to the didactic and pedagogical conditions under which grammar can best be learned and taught are thus primarily concerned with students’ grammatical understanding. According to Macken-Horarik et al. (2011: 11) students exhibit grammatical understanding if they possess ‘grammatically informed knowledge about language’.

Teachers tend to struggle greatly with the question of how to foster their students’ grammatical understanding, as they are often insecure or even anxious about their own grammatical knowledge (Cajkler and Hislam 2002; Watson 2012; Giovanelli 2015). Indeed, there are several studies to suggest that (student) teachers’ grammatical knowledge is mostly underdeveloped (e.g. Alderson and Hudson 2013; Sangster et al. 2013; Macken-Horarik et al. 2018; Nygård and Brøseth 2021). An additional difficulty for (student) teachers is that understanding is not the same as knowledge. While knowledge is certainly a critical part of understanding, as is asserted by Macken-Horarik et al. (2011), understanding also requires linking different pieces of knowledge, inferring conclusions from actual and counterfactual cases, and being able to give explanations for a certain phenomenon (see De Regt 2009; Baumberger 2019). In other words: grammatical understanding requires not only knowledge, but also higher-order thinking or reasoning skills in the domain of linguistics. Such reasoning skills have been identified as linguistic reasoning skills (Coppen 2011; Dielemans and Coppen 2021; Leenders et al. 2021). It has been argued that in L1 grammar education, paying attention to linguistic reasoning can be a valuable addition to (student) teachers’ grammar teaching repertoires, as the analysis of sentences and phrases can be characterized as a ‘messy problem’ (see Coppen 2010; Wijnands et al. 2021). This means that even sentences that seem simple on the outset may present a learner with analysing difficulties, among other things because such sentences can linguistically be analysed in different ways. A typical example is the Dutch sentence Zij liep een rondje (‘She walked one round’), in which the phrase een rondje (‘one round’) can be thought of as a direct object (a form of complementation) or as an adverbial (a form of modification). There is evidence to support both analyses. For instance, the sentence can be passivized, which is a strong argument in favour of the direct object interpretation (Er werd een rondje gelopen, ‘One round was being walked’). On the other hand, ‘one round’ seems to denote a distance, which is something more closely associated with modifiers, and the valency pattern of the verb lopen (‘to walk’) typically does not allow for the occurrence of an object alongside the subject. This raises the question which of these potential analyses is best (i.e. given the arguments presented, is een rondje (‘one round’) an object or an adverbial?). Most real-life sentences pose parsing difficulties in this way, which cannot be solved with the means offered to the learner by traditional grammar practices. Such traditional practices (which can still be observed throughout the world, cf. Van Gelderen 2010; Fontich and García-Folgado 2018; Macken-Horarik et al. 2018; Van Rijt et al. 2019a; Kabel et al. 2021) have been criticized for using rules of thumb (RoT) or mnemonic devices as their primary tools for analysing sentences (see Berry 2015). This is problematic. One reason for this is that RoT are generally devoid of any deeper insights (Myhill 2000); another is that RoT suggest to the learner that sentences can only be analysed in one way, and that if conflicting evidence emerges, there is still only one correct answer. In reality, things are not so black and white (Coppen 2009). Which is the best analysis, depends on the context at hand and on the argumentation provided to underpin each of the analyses (Dielemans and Coppen 2021; Wijnands et al. 2021). In other words: to adequately deal with sentences such as the above, students require the ability to reason linguistically.

Previous empirical research suggests that teaching students how to reason linguistically can be beneficial for their level of grammatical understanding (Van Rijt et al. 2019b, 2020, 2022). For secondary school students to be able to exhibit such reasoning skills, their (student) teachers will need the ability to reason about grammar themselves. In the present study, we therefore examine student teachers’ linguistic reasoning abilities. More specifically, we are interested in how student teachers process different types of arguments in a grammatical discussion, in which two analyses of a particular sentence can be defended. Evaluating different grammatical arguments within a narrow space of time constitutes a task teachers are likely to encounter regularly when teaching grammar. This is especially true if teachers intend to develop their students’ grammatical reasoning skills. At the same time, student teachers who are enrolled in Dutch teacher education seem to have great difficulty in performing tasks related to linguistic reasoning. When asked to think of grammatical arguments in odd one out tasks (e.g. which of the following three verbs is the odd one out?), student teachers did not perform better than the 14-year-old pre-university students they are allowed to teach after graduating, with just over half of their produced arguments being valid (Van Rijt et al. 2021). Obtaining a better understanding of the process of linguistic reasoning for student teachers is thus a necessary step to provide teacher education programs with the tools they need to train student teachers in linguistic reasoning.

# LINGUISTIC REASONING

Dielemans and Coppen (2021) constructed a model of linguistic reasoning, based on a widely used model of historical reasoning in history education (cf. Van Drie and Van Boxtel 2008). The authors observe that linguistic reasoning typically centres around form and meaning, as well as context and variation. In their model, Dielemans and Coppen (2021) identify six main components of linguistic reasoning, which are based on the ways in which expert (theoretical)

linguists reason about ill-structured linguistic problems. Three of these components relate to the process of reasoning (asking linguistic questions, using linguistic argumentation, and linguistic contextualization); the other three deal with source use (using linguistic sources, using linguistic concepts, using linguistic repertoire). A few of these components are of particular importance to the current study, namely using linguistic repertoire (i.e. methods that can be used to investigate linguistic form and meaning), using linguistic concepts (i.e. explicit linguistic terminology and the (meta)concepts these terms reflect—cf. Van Rijt and Coppen 2017), and—most notably—using linguistic argumentation (i.e. thinking of arguments and counterarguments to support a grammatical analysis). As previously mentioned, different types of linguistic arguments can be used to strengthen a grammatical analysis.

Van Rijt et al. (2019b) found that university students of Dutch language and literature deploy three types of arguments when tackling a grammatical problem (e.g. the one round-problem mentioned earlier). Two of these argument types are relevant for the current study. First, students used rules of thumb (RoT), which include mnemonic devices and audit questions (e.g. who or what $^ +$ verb $^ +$ subject $=$ direct object). Such arguments did not predict the university students’ grammatical reasoning quality. Secondly, students used linguistic manipulations (LM), in which case they manipulated the construction they were dealing with somehow, in a way that is compatible with what a linguist would do (e.g. topicalizing a phrase, replacing a phrase with another one, constructing a parallel example, omitting a phrase, or switching the sentence from active to passive— see Honda and $\mathrm { O ^ { \prime } }$ Neil 2007). Contrary to RoT, using LMs did predict students’ reasoning quality. These findings suggest that teaching students how to use LMs is valuable for their ability to reason linguistically, which in turn seems to positively affect their overall grammatical understanding. There are, however, reasons to assume that LMs are more difficult to learn and harder to process. Put differently: they may impose a higher cognitive load on the learner (Sweller et al. 1998). The primary reason for this is that manipulations tend to involve more steps than RoT. For instance, returning to the example above, one could find a direct object in two ways: via RoT, or via linguistic manipulation such as passivization. The former might involve asking an audit question, such as ‘What did she walk?’, and it yields an immediate answer (‘one round’), so the thinking process can stop there. The latter, on the other hand, involves transforming the sentence from active to passive, evaluating the result, and inferring what that result means considering the problem at hand. In addition, performing a linguistic manipulation may require more conceptual understanding than using RoT, which may also affect cognitive load.

# COGNITIVE LOAD THEORY

Cognitive load theory, according to Paas et al. (2003: 63), ‘is concerned with the development of instructional methods that efficiently use people’s limited cognitive processing capacity to stimulate their ability to apply acquired knowledge and skills to new situations (i.e., transfer)’. Central to the theory is the idea that certain tasks or instructions can be too cognitively demanding for the learner because of working memory limitations, thus causing a cognitive overload, which severely hampers learning (Paas et al. 2003; De Jong 2010). DeLeeuw and Mayer (2008) distinguish three types of cognitive load during a learning task: extraneous load, in which ‘the learner engages in cognitive processing that does not support the learning objective’ (e.g. when poor lay-out causes distractions in the learning); intrinsic load, which is the load associated with comprehending the task or material; and finally, germane load, which refers to deep cognitive processing, in which information within a task is related to prior knowledge and other relevant knowledge (e.g. relevant cues from within a task). Cognitive load theory is particularly useful for examining ‘highly demanding, complex, time critical tasks (…) for which learners must use all available resources to make the right decisions in a very short time’ (De Jong 2010: 123). Evaluating different grammatical argument types in a real-time classroom situation constitutes such a task.

For measuring cognitive load, various instruments have been used (De Jong 2010). One of these is Paas’ (1992) frequently used Mental Effort Rating Scale (MERS)—a one-item scale in which participants are asked to evaluate how difficult they feel a certain task was, ranging from 1 (very, very little effort) to 9 (very, very much effort). Such a simple difficulty rating immediately after learning is believed to be most useful in determining the level of germane load (DeLeeuw and Mayer 2008), and it deals with learners’ perceived amount of mental effort.

Another way to measure cognitive load is through (secondary) response times, in which participants are asked to complete a certain task as quickly, but also as adequately as possible, with longer response times reflecting greater levels of cognitive load (DeLeeuw and Mayer 2008). Given that correlations between various measures of cognitive load are often low (DeLeeuw and Mayer 2008), it is recommended to use more than one measure of cognitive load. In addition, a more accurate view of cognitive load is obtained if it is measured more often (Paas et al. 2003). This study therefore adopts different ways of measuring cognitive load (see methods section).

# FACTORS AFFECTING COGNITIVE LOAD

A previous study (Shehab and Nussbaum 2015) investigating cognitive load in argument-counterargument integration, has shown that participants’ Need for Cognition (NFC, see Cacioppo et al. 1984), which concerns their willingness and enjoyment of cognitively demanding tasks, influences their cognitive load. We have therefore also considered the NFC a relevant variable in the current study. Similarly, students’ linguistic reasoning ability may be affected by their own affinity with grammar and grammar teaching (their ‘grammar willingness’, GW)—another variable that should be considered.

# RESEARCH QUESTIONS

To the best of our knowledge, the processing of different types of grammatical arguments (RoT vs. LMs) has never been empirically investigated, especially within the context of teacher education. Our specific research questions are as follows:

(1) To what extent are arguments in a grammatical discussion based on LMs more cognitively demanding for student teachers than arguments based on RoT?   
(2) To what extent do master students outperform bachelor students in processing grammatical arguments, and do senior bachelor students outperform junior bachelor students?   
(3) What is the influence of students’ NFC and their affinity with grammar (GW) in processing grammatical arguments?

We hypothesize that arguments based on LMs will be more difficult for student teachers to process than those based on RoT, and that senior student teachers will outperform junior student teachers on these tasks (i.e. masters will outperform bachelors and senior bachelors will outperform junior bachelors). We also expect NFC and GW to impact grammatical argument processing, in that higher NFC scores correlate with longer processing times for harder arguments (LM), and with a better performance. We expect higher GW to also correlate positively with better performances.

# METHODS

# Participants

298 student teachers participated in this study (68 male, 229 female, and 1 other). The participants were recruited at eight universities of applied sciences in the Netherlands that offer teacher training (of nine universities of applied sciences with teacher education tracks in total). 43 of them were part-time master students (MEd students), 99 were part-time bachelor students, and 156 were full-time bachelor students (BEd students). Full-time bachelor students were further divided into junior and senior student teachers because all teacher training institutes offer courses on grammar in either the first or second year of the BEd full-time program. This will allow us to distinguish between student teachers who definitively have taken courses on grammar (the seniors) and those who may not have (the juniors) as the time when grammar is addressed within those first two years differs per institute. All student teachers take courses on traditional grammar in their first two years, usually followed up by courses on general linguistics from the second year onwards. The timing, and to a lesser extent the content of these courses differs per institute. In addition, student teachers take ‘didactic’ or pedagogical courses on grammar teaching and language awareness (covering a variety of linguistic subjects), and they have internships in secondary schools in each year, alongside their theoretical courses. They therefore also have some hands-on classroom experience, which typically increases as they progress. The master students (MEd) do not take additional courses on traditional grammar, although they are taught more advanced linguistics, intended to broaden and deepen their existing knowledge on linguistics in general and sentence structure specifically.

See Table 1 for some participant characteristics. The participants were studying to become a teacher of Dutch language and literature. As can be inferred from Table 1, there are some notable age differences between the student– teacher groups. This is because bachelor full-time students are the only group that typically enrols in teacher education immediately after their completion of secondary school. Students usually enrol in the part-time bachelor program and the master program at a later stage of their life.

Students signed a digital consent form in which they stated that their data could be used for scientific research anonymously. In addition, the universities for applied sciences they studied at, approved of the investigation. Data were stored in accordance with university guidelines. All relevant data used in this study can be found on the Open Science Framework repository at: https:// tinyurl.com/yntx4fsy.

# Experimental tasks

We designed an experiment intended to measure student teachers’ grammatical reasoning skills under time pressure, thereby mimicking, to an extent, real-life classroom situations in which teachers need to evaluate grammatical arguments on the spot. The participants took part in the experiment during class. Although they were encouraged to take part in the experiment by their teacher (who also gave some practical instructions), participation was not obligatory to satisfy a course requirement. Critically, students were not informed in advance about the aim of the study. To collect data on different variables we used a Qualtrics setup on the internet.

Table 1: Characteristics of the participants per study track.   

<html><body><table><tr><td>Study track (&#x27;Education&#x27;)</td><td>N of par- ticipants</td><td>Mean age (SD)</td><td>Gender (num- ber of m/f/o)</td></tr><tr><td>Bachelor full-</td><td>156</td><td>20.56 (2.28)</td><td>28, 128, 0</td></tr><tr><td>time (BF)</td><td></td><td></td><td>17, 87, 0</td></tr><tr><td>Junior BF Senior BF</td><td>104 51</td><td>20.06 (2.22) 21.51 (2.04)</td><td>10, 41, 0</td></tr><tr><td>Bachelor part-</td><td>99</td><td>36.72 (10.64)</td><td>30, 68, 1</td></tr><tr><td>time Master</td><td>43</td><td>34.95 (11.44)</td><td>10, 33, 0</td></tr></table></body></html>

Note One participant could not be classified as either Junior BF or senior BF because of a technical error.

The main task for the subjects was to analyse two case studies containing a grammatical problem, for example, the case of the Dutch sentence Afgelopen winter hebben we voor het eerst in tijden weer de 400 m geschaatst (‘We skated the 400 m for the first time in ages last winter’). In this sentence, the phrase de 400 m can be analysed both as an adverbial or as a direct object. We provided arguments in favour of both analyses and some arguments that were not true or irrelevant for the case given, 10 arguments in total. The participants were asked to categorize these arguments into three categories: in favour of analysis X (in the case mentioned: adverbial phrase), in favour of analysis Y (in the case mentioned: direct object), and not true or irrelevant. They did this by dragging an argument to the category it belonged to according to them with a computer mouse or mouse pad of a laptop. See Figure 1 for the participants’ task screen. Because students were confronted with conflicting arguments on the same sentence in this task, the task deliberately challenged a binary understanding of grammar. This means there was not one clear answer to be favoured within the task: both analysis X and analysis Y could theoretically be defended in each case. The participants completed two sets of arguments per case: one set containing arguments based on RoT and one set containing arguments based on LMs. Thus, in total four sets of arguments were categorized, two sets per grammatical problem. The order of these sets was assigned randomly to an individual participant resulting in four different orders. We measured the time that was needed per condition per case per participant to complete the categorization of the arguments (‘Response time’).

Before conducting our core analyses, we checked the dataset for extreme outliers in terms of response times. This was done per subgroup (bachelor fulltime, bachelor part-time, or master). If a participant’s response time was ${ > } 3$ SD above the mean of the subgroup, they were excluded from the analysis, as we took this as a sign that participants had not acted in accordance with the task they were given (i.e. categorize the arguments as adequately and as quickly as possible).

![](img/35b5e8d59284632235de19350b6d81d9faaf6706073b7d1238309f7dd9292b05.jpg)  
Figure 1: Participants’ task screen.

Prior to the categorization task, participants completed the NFC test and a GW test, measuring their affinity with grammar and grammar teaching. They also completed a sample task unrelated to grammar to familiarize them with the categorization task. For this sample task, participants categorized arguments in favour of teaching in a physical classroom and in favour of distance teaching via a video call. After each set of arguments, participants rated the effort it took to categorize the arguments within that set using the MERS. See Figure 2 for a visual representation of the order of the different parts of the experimental setup. We explain these variables in the next section.

# Materials

We collected data for several variables. First, participants completed the NFC test. This test was a Dutch translation by the authors of the validated NFC test by Cacioppo et al. (1984) and expresses participants’ willingness and enjoyment of cognitively demanding tasks. The translation itself was carefully checked by a colleague of the English department. In addition, an environmental scientist and a master in Dutch literature tested the questionnaire by completing it to rule out any incomprehensible or odd statements. For all participants in our experiment, the NFC’s Cronbach’s alpha was 0.85 (0.88 for master students, 0.83 for parttime bachelor students, and 0.83 for full-time bachelor students), indicating that our translation of the NFC test is sufficiently reliable. See Supplementary Appendix A for the NFC test.

![](img/d0a217f18ea07c7dbe89ecac9e50b21ba8099eec01530ec502e1c69e641a989a.jpg)  
Figure 2: Experimental setup. Case A and case B represent different grammatical discussions. The dotted lines indicate that upon completing either case A or case $B _ { \iota }$ , students are automatically starting the other case. When the tasks in both cases were completed, the experiment came to an end.

The NFC was followed by a GW test that consisted of four statements created by the authors, to which students had to respond on a five-point Likert scale, ranging from (1) ‘Highly disagree’ to (5) ‘Highly agree’, for example, I think grammar is interesting. See Supplementary Appendix B for the GW test. The GW’s Cronbach’s alpha was 0.71 (0.74 for master students, 0.70 for part-time bachelor students, and 0.70 for full-time bachelor students), indicating that the GW test is sufficiently reliable. The GW test, in its focus on beliefs about grammar (teaching), thus measures a more narrow construct than the NFC test.

For each case study, the set of arguments for RoT was matched with the set of arguments for LM for number of words, number of grammatical terms (i.e. terms denoting parts of speech and phrases from traditional grammar), and syntactic structure of the argument, because these characteristics may influence the required mental effort during the task. In other words: in both the LM conditions and in the RoT conditions, participants processed the exact same number of words, the same number of different sentence types, and the same number of grammatical terms.

We distinguished three types of syntactic structures in presenting the arguments: simple sentences (e.g. De 400 m is een woordgroep met een zelfstandig naamwoord als kern, ‘The 400 meters is a phrase with a noun as its head’), compound sentences containing other conjunctions than if (e.g. De 400 m schaatsen is een handeling die al ten einde is, dus de persoonsvorm moet eigenlijk hadden zijn, ‘Skating the 400 meters is an act that has already came to an end, so the verb must actually be had’), and composed sentences only containing the conjunction if (‘als’) (e.g. Als je deze zin in de lijdende vorm zet, krijg je zoiets als ‘De 400 meter wordt door ons geschaatst’, If you put this sentence in the passive voice you get something like ‘The 400 meters will be skated by us’). These characteristics are listed in Table 2. See Appendix C for all grammatical arguments.

We also recorded the number of correct categorizations for both conditions (RoT vs. LM) in both cases, with 10 being the maximum number of correct answers per condition per case. Thus, we collected 4 scores per participants: 2 scores for condition RoT and 2 scores for condition LM.

In addition, response times were measured as one of the variables reflecting cognitive load (DeLeeuw and Mayer 2008). Whilst categorizing the arguments into the three categories participants were asked to do this as correctly and quickly as possible. The Qualtrics setup invisibly measured the time it took to categorize a set of 10 arguments per participant for both conditions (RoT vs. LM) for the two cases. As a third variable reflecting cognitive load, participants’ ratings on Paas’ (1992) MERS were collected. Using the MERS, participants were asked to evaluate on this one-item scale how difficult it was for them to categorize each set of 10 arguments, on a scale ranging from 1 (very, very little effort) to 9 (very, very much effort). Thus, for each participant 4 MERS ratings were collected: 2 ratings for the RoT condition and 2 ratings for the LM condition.

Table 2: Characteristics of the arguments per grammatical discussion (case). For each case, there was a RoT condition and a LMs condition.   

<html><body><table><tr><td>Characteristic</td><td>Case A</td><td>Case B</td></tr><tr><td>Number of words</td><td>181</td><td>153</td></tr><tr><td>Number of grammatical terms</td><td>5</td><td>8</td></tr><tr><td>Singular sentence</td><td>2</td><td>5</td></tr><tr><td>Compound sentence: other than conjunction if</td><td>4</td><td>3</td></tr><tr><td>Compound sentence: conjunction if only</td><td>4</td><td>2</td></tr></table></body></html>

# Analysis

We investigated the role of argument type (LMs vs. RoT) and seniority (bachelor vs. master) in affecting the three relevant dependent variables (correct classifications, response times, and the MERS scores). Because of the inherent hierarchical nature of the study where observations are nested under tasks and participants are nested under institutes, the data were analysed in a linear mixed effects model using the GAMLj package (Gallucci 2019) available in jamovi (The Jamovi Project 2022) with a graphical user interface for R (R Core Team 2021). The data visualizations were done using the r package ggplot2 (Wickham 2016) and the YaRrr package (Phillips 2017). We entered argument type and education as fixed factors and manually added the interaction. Participant, institute, and task were entered as random factors. As recommended by Barr et al. (2013), we started out with a full model with random slopes and intercepts for both random factors. Since the inclusion of random slopes resulted in a (nearly) singular fit, we report analyses with random intercepts only. However, when the inclusion of random effects results in different effects for the fixed factors, this is reported during the analysis. Finally, to assess the effect of age, each analysis was run with age as a covariate.

For all three dependent variables (Correct classifications, response time, and MERS), the syntax for the model is as follows $\mathrm { D V } =$ dependent variable):

$\mathrm { D V } \sim \mathrm { ~ 1 ~ + ~ }$ Education $^ +$ Argument type $^ +$ Age $^ +$ Education $:$ Argument $^ +$ (1 Institute) $^ +$ (1 StudentID) $^ +$ (1 Task)

# RESULTS

# Descriptive statistics

Table 3 shows that bachelor full-time, bachelor part-time, and master students differ on their GW and NFC scores. The table also displays differences in mean response times.

Per subgroup we checked for outliers, excluding students with a mean response time that was ${ > } 3$ SD above the mean. This resulted in the exclusion

Table 3: Participants’ GW, NFC scores, and mean response times for the different teacher training tracks.   

<html><body><table><tr><td>Track</td><td>Mean GW (SD)</td><td>Mean NFC (SD)</td><td>Mean response time for all tasks</td></tr><tr><td>B full-time</td><td>3.62 (0.67)</td><td>3.32 (0.52)</td><td>105.76 (39.35)</td></tr><tr><td>B part-time</td><td>3.86 (0.58)</td><td>3.58 (0.49)</td><td>120.33 (36.83)</td></tr><tr><td>Master</td><td>3.92 (0.61)</td><td>3.82 (0.54)</td><td>132.60 (60.60)</td></tr></table></body></html>

of 5 students, 2 in the bachelor full-time track, 1 in the part-time track, and 2 in the master track. The following analyses are thus based on 293 observations.

# The cognitive load of RoT versus LM and the effect of seniority

For convenience, we will answer our first two research questions together, meaning that our analyses will target both RoT and LM processing as well as the effect of teacher education track (seniority). We will discuss (i) response times, (ii) correct classifications, and (iii) MERS.

# Response times

Figure 3A and 3B shows the data, the mean difference, and the confidence intervals of the two main effects for argument type (RoT and LM) and education for response times.

The mixed effect model $( \mathrm { B I C } = 1 2 1 1 8$ , marginal $R ^ { 2 } = 0 . 0 6$ ) for response times (in seconds), showed that the mean response time for tasks involving LM was higher $M = 1 1 7$ , $\mathrm { S D } = 5 1$ ) than the score for tasks with RoT ( $M = 1 0 6$ , $\mathrm { S D } = 4 5$ ); $\beta = 1 4 . 3$ , $\mathrm { S E } = 2 . 9 3 $ , $t \left( 8 7 6 . 0 0 \right) = 4 . 8 8$ , $p < 0 . 0 0 1$ , indicating that tasks with LM took more effort. However, bachelor $M = 1 1 0$ , $\mathrm { S D } = 4 8$ ) and master students (M $= 1 2 2$ , $\mathrm { S D } = 4 7$ ) were equally fast in providing an answer to the tasks $\mathrm { \mathit { \Omega } } \mathrm { \beta } = 4 . 8 6$ , $\mathrm { S E } = 6 . 4 1 , t \ : ( 1 1 9 . 8 7 ) = 0 . 7 5 8 ,$ , $p = 0 . 4 5$ ). Table 4 presents the statistics of the remaining fixed effects and interactions, showing the absence of an interaction between education and argument type and the well know effect of age (older student have higher response times).

The results show that engaging in LM is indeed more cognitively demanding, but that there are no significant differences in response times between bachelor and master students. The latter is likely due to the large amount of variation in response time in the bachelor group, because the raw difference $( D = 1 2 ~ s )$ ) between bachelor and master students is slightly larger than that between tasks involving LMs and tasks involving RoT $\mathbf { \Phi } ^ { \prime } D = 1 1 \mathrm { ~ s ~ }$ ).

# Correct classifications

Figure 4A and 4B show the data, the mean difference and the confidence intervals of the two main effects for argument type (RoT and LM) and education for correct classifications.

![](img/e3f3a8c48ea42de59516a8b1542b9d2691761270a6ec580d68db364953588ce0.jpg)  
Figure 3: Response times (in seconds), means and confidence intervals for the two argument types (A) and two education levels (B).

Table 4: Parameter estimates for the fixed effects of education, argument type, and age predicting response times.   

<html><body><table><tr><td></td><td colspan="7">95 per cent CI</td></tr><tr><td>Effect</td><td></td><td>SE</td><td>Lower</td><td>Upper</td><td>df</td><td>t</td><td>p</td></tr><tr><td>Educationa</td><td>4.86</td><td>6.41</td><td>-7.71</td><td>17.43</td><td>119.87</td><td>0.76</td><td>0.450</td></tr><tr><td>Argument type</td><td>14.30</td><td>2.93</td><td>8.56</td><td>20.04</td><td>876.00</td><td>4.88</td><td>&lt;0.001</td></tr><tr><td>Age</td><td>0.83</td><td>0.21</td><td>0.43</td><td>1.23</td><td>51.23</td><td>4.05</td><td>&lt;0.001</td></tr><tr><td>Education * Argument type</td><td>7.84</td><td>5.86</td><td>-3.64</td><td>19.33</td><td>876.00</td><td>1.34</td><td>0.181</td></tr></table></body></html>

a The significance of the main effect of education is contingent on the absence of the random slope of argument type over task (e.g. argument type | task) in the model. Model with this random slope had an almost singular fit and a non-significant effect of argument type $\mathrm { ( p > 0 . 4 4 }$ )

The mixed effect model $( \mathrm { B I C } = 1 2 1 1 8$ , marginal $R ^ { 2 } = 0 . 0 6$ ) for correct classifications (in seconds), showed that the mean number of correct classifications for tasks involving LM was lower ( $M = 5 . 0 7$ , $\mathrm { { S D = 1 } } . 7 7 \mathrm { { } }$ ) than the score for tasks with RoT $M = 6 . 3 2$ , $\mathrm { S D } = 1 . 9 3$ ); $\ S = - 1 . 2 1 , \ S \mathrm { E } = 0 . 1 3 , \ t \ ( 8 7 6 . 0 0 ) = - 9 . 0 4 ,$ $p <$ 0.001. This indicates that tasks with LM were harder. In line with this, bachelor students $M = 5 . 5 8$ , $\mathrm { S D } = 1 . 9 4  $ ) had less correct classifications than master students $\mathit { M } = 6 . 4 0$ , $\mathrm { S D } = 1 . 9 0 $ ); $\beta = 0 . 5 1$ $0 . 5 1 , \mathrm { { } S E } = 0 . 2 3 , t \left( 2 5 5 . 5 2 \right) = 2 . 2 0 ,$ $p = 0 . 0 3$ ). Table 5 presents the statistics of the remaining fixed effects and interactions, showing the absence of a main effect of age and the interaction between education and argument type.

# MERS

Figure 5A and 5B show the data, the mean difference, and the confidence intervals of the MERS score for the two main effects (argument type and education).

As can be expected, the MERS scores follow a pattern opposite to that of the number of correct classifications (this model had a $\mathrm { B I C } = 3 7 8 6$ and a marginal $R ^ { 2 }$ $= 0 . 1 3 $ ). Master students ( $\mathrm { \Delta } M = 5 . 2 6$ , $\mathrm { S D } = 1 . 3 8 $ ) report less effort than bachelor students $( M = 6 . 0 0$ , $\mathrm { S D } = 1 . 5 9$ ); $\beta = - 0 . 5 8$ , $\mathrm { S E } = 0 . 2 5 $ , $t \left( 9 7 . 4 8 \right) = - 2 . 3 7$ , $p = 0 . 0 2$ . In addition, tasks involving LM ( $M = 6 . 0 7$ , $\mathrm { S D } = 1 . 5 5$ ) were considered more demanding than tasks involving RoT $M = 5 . 7 2$ , $\mathrm { S D } = 1 . 5 9$ ). No other significant effects were present (Table 6).

![](img/cbfbfaad433c887d94756facca97bc9678b97348b11ed1267d64328d63cd7ef4.jpg)  
Figure 4: Number of correct classifications, means, and confidence intervals for the two argument types (A) and two education levels $( B )$ .

Table 5: Parameter estimates for the fixed effects of education, argument type, and age predicting the number of correct classifications.   

<html><body><table><tr><td>Effect</td><td></td><td>SE</td><td colspan="2">95 per cent CI</td><td>df</td><td>t</td><td>p</td></tr><tr><td></td><td></td><td></td><td>Lower</td><td>Upper</td><td></td><td></td><td></td></tr><tr><td>Education</td><td>0.51</td><td>0.23</td><td>0.05</td><td>0.96</td><td>255.52</td><td>2.20</td><td>0.03</td></tr><tr><td>Argument typea</td><td>-1.21</td><td>0.13</td><td>1.47</td><td>0.95</td><td>876.00</td><td>-9.04</td><td>&lt;0.001</td></tr><tr><td>Age</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.02</td><td>258.19</td><td>0.84</td><td>0.400</td></tr><tr><td>Education * Argument type</td><td>0.10</td><td>0.42</td><td>0.42</td><td>0.63</td><td>876.00</td><td>0.39</td><td>0.699</td></tr></table></body></html>

a The significance of the main effect of argument type is contingent on the absence of the random slope of argument type over task (e.g. argument type | task) in the model. Model with this random slope had an almost singular fit and a non-significant effect of argument type $( p > 0 . 4 0 )$ )

![](img/88e3d0550d3b3fe4d1df09761e3e69f350e7ef5a30a75b0ccbead271d0b3ecaf.jpg)  
Figure 5: The MERS scores, means, and confidence intervals for the two argument types (A) and two education levels $( B )$ .

Table 6: Parameter estimates for the fixed effects of education, argument type, and age predicting the MERS scores.   

<html><body><table><tr><td colspan="8">95 per cent CI</td></tr><tr><td>Effect</td><td></td><td>SE</td><td>Lower</td><td>Upper</td><td>df</td><td>t</td><td>p</td></tr><tr><td>Education</td><td>-0.58</td><td>0.25</td><td>-1.06</td><td>0.10</td><td>97.48</td><td>-2.37</td><td>0.020</td></tr><tr><td>Argument typea</td><td>0.37</td><td>0.08</td><td>0.22</td><td>0.52</td><td>875.02</td><td>4.90</td><td>&lt;0.001</td></tr><tr><td>Age</td><td>-0.01</td><td>0.01</td><td>0.03</td><td>0.00</td><td>62.19</td><td>-1.55</td><td>0.126</td></tr><tr><td>Education * Argument type</td><td>0.06</td><td>0.15</td><td>-0.23</td><td>0.36</td><td>875.02</td><td>0.43</td><td>0.670</td></tr></table></body></html>

a The significance of the main effect of argument type is contingent on the absence of the random slope of argument type over task (e.g. argument type | task) in the model. Model with this random slope had an almost singular fit and a non-significant effect of argument type $( p > 0 . 4 0 )$ .

# A different perspective on seniority

In the previous analysis, the variable education was operationalized as the difference between bachelor and master students. To assess the generality of the effect of education, a second analysis was conducted with only bachelor students who followed a full-time program (see Methods section). As before, we ran a linear mixed effects analysis with argument type and age as fixed factors and task, institute and participant as random effects. All random intercepts were included in the model, but since random effects led to singular fits (and did not substantially alter the results), these were not included in the final model.

# Response times

For response times $( \mathrm { B I C } = 6 3 8 0$ , marginal $R ^ { 2 } = 0 . 0 3$ ), the pattern of results is very similar to that of the first analysis: a significant effect of age $\beta = - 2 . 7 0$ , SE $= 1 . 3 5 , t \left( 1 4 9 . 6 3 \right) = - 1 . 2 0$ , $p = 0 . 0 4 7$ with more senior participants $( M = 1 0 7 . 3$ , $\mathrm { S D } = 4 6 . 3 $ ) taking more time to respond than junior participants $( M = 1 0 2 . 2$ , $\mathrm { S D } = 4 9 . 7 )$ and of argument type $( \beta = 1 1 . 1 6 , \mathrm { S E } = 3 . 1 2 , t ( 4 5 6 . 0 0 ) = 3 . 5 7 , p <$ 0.001 indicating that participants spend less time on tasks involving RoT $M =$ 106.0, $\mathrm { S D } = 4 5 . 1$ ) compared to task involving LM $M = 1 1 7 . 5$ , $\mathrm { S D } = 5 0 . 7$ ). As in the initial analysis, there was no main effect of seniority and no interaction (see Table 7).

# Correct classifications

In contrast to the previous analyses, there was no significant effect of argument type on the number of correct classifications $( \beta = - 1 . 2 2 , \mathrm { S E } = 0 . 9 1 , t ( 0 . 9 9 2 ) =$ $- 1 . 3 5$ , $p = 0 . 4 0 7$ , $\mathrm { B I C } = 2 4 6 3$ , marginal $R ^ { 2 } = 0 . 1 0$ ), indicating that for full-time bachelor students, argument type does not affect their performance. There was an effect of seniority $( \beta = 0 . 5 6 , \mathrm { S E } = 0 . 2 2 , t ( 1 4 9 . 6 6 5 ) = 2 . 5 1 ,$ , $p = - 0 . 0 1 3$ ): senior students $M = 5 . 7 5$ , $\mathrm { S D } = 1 . 8 2 $ ) perform better than junior students $( M = 5 . 1 4 $ , $\mathrm { S D } = 2 . 0 1 $ ), an effect that was independent of argument type as per the non-significant interaction (see Table 8).

Table 7: Parameter estimates for the fixed effects of seniority, argument type, and age predicting response times.   

<html><body><table><tr><td>Effect</td><td colspan="7">95 per cent CI</td></tr><tr><td></td><td></td><td>SE</td><td>Lower</td><td>Upper</td><td>df</td><td>t</td><td>p</td></tr><tr><td>Seniority</td><td>9.07</td><td>6.51</td><td>-3.70</td><td>21.84</td><td>144.20</td><td>1.39</td><td>0.166</td></tr><tr><td>Argument type</td><td>11.16</td><td>3.12</td><td>5.04</td><td>17.29</td><td>456.00</td><td>3.57</td><td>&lt;0.001</td></tr><tr><td>Age</td><td>2.70</td><td>1.35</td><td>-5.35</td><td>-0.053</td><td>149.63</td><td>-1.20</td><td>0.047</td></tr><tr><td>Argument type * Seniority</td><td>0.37</td><td>6.25</td><td>-11.88</td><td>12.62</td><td>456.00</td><td>0.06</td><td>0.95</td></tr></table></body></html>

Table 8: Parameter estimates for the fixed effects of seniority, argument type, and age predicting the number of correct classifications.   
SE 95 per cent CI   

<html><body><table><tr><td>Effect</td><td></td><td></td><td>Lower</td><td>Upper</td><td>df</td><td>t</td><td>p</td></tr><tr><td>Seniority</td><td>0.56</td><td>0.22</td><td>0.12</td><td>0.99</td><td>149.665</td><td>2.51</td><td>0.013</td></tr><tr><td>Argument type</td><td>-1.22</td><td>0.91</td><td>3.00</td><td>0.55</td><td>0.992</td><td>-1.35</td><td>0.407</td></tr><tr><td>Age</td><td>-0.04</td><td>0.05</td><td>0.13</td><td>0.13</td><td>148.84</td><td>0.52</td><td>0.605</td></tr><tr><td>Argument type * Seniority -0.14</td><td></td><td>0.26</td><td>0.65</td><td>0.36</td><td>447.239</td><td>0.55</td><td>0.58</td></tr></table></body></html>

# MERS

There was also no difference between the two seniority groups for the MERS; $\beta =$ −0.16, $\mathrm { S E } = 0 . 2 6 , t \left( 1 4 1 . 0 0 \right) = - 0 . 6 1$ , $p = 0 . 5 4 5$ $\mathrm { B I C } = 2 0 6 3$ , marginal $R ^ { 2 } = 0 . 0 1$ ). Juniors and seniors thus reported the same level of difficulty. There was an effect of argument type: both groups considered tasks with LMs $\begin{array} { r } { M = 6 . 2 8 } \end{array}$ , $\mathrm { S D } = 1 . 6 0 $ ) to be slightly more difficult than tasks involving RoT $M = 6 . 2 0$ , $\mathrm { S D } = 1 . 6 9 $ ). Cf. Table 9.

# Need for cognition (NFC) and grammar willingness (GW) influence

The final research question was whether students’ NFC and GW predict response times, number of correct classifications and reported mental effort. Crucially, we were interested in whether these relationships were affected by argument type. In other words, is the relationship between NFC or GW and the three dependent variables different for tasks involving RoT or LM? Table 10 shows the correlation matrix for the variables involved and Figure 6 depicts these relationships, separated for argument type.

As before, this relationship was investigated in a linear mixed effects model with the following syntax:

DV $\sim 1 ~ +$ Argument type $^ +$ NFC + GW $^ +$ Argument type : NFC + GW $:$ Argument type $^ +$ NFC : GW $^ +$ Argument type : NFC : GW $^ +$ (1 Institute) + (1  StudentID) $^ +$ (1  Task)

As can be seen from this syntax, we included all possible two- and three-way fixed effect interactions in the model and entered random intercepts for all cluster variables. Like in the previous models, adding random sloped led to singular fits, but did not substantially alter the interpretation of the results, which is why random intercept only models are reported.

For correct classifications (see the panels on the top and bottom left of Figure 6 and Table 11), the model $( \mathrm { B I C } = 2 5 2 1$ , Marginal $R ^ { 2 } = . 1 3$ ) showed the wellknown effect of argument type with more correct classifications with tasks involving RoT, and an effect of GW, with more correct classifications in students with a higher GW. A similar but non-significant trend is visible for the relationship between NFC and the number of correct classifications. Crucially, both GW and NFC interact significantly with argument type. The advantage of tasks involving RoT gets smaller with increasing NFC. For GW, the pattern is opposite (as is the sign of the estimate): the advantage of tasks involving RoT gets larger with increasing GW.

Table 9: Parameter estimates for the fixed effects of education, argument type, and age predicting the MERS score.   
95 per cent CI   

<html><body><table><tr><td>Effect</td><td></td><td>SE</td><td>Lower</td><td>Upper</td><td>df</td><td>t</td><td>p</td></tr><tr><td>Seniority</td><td>-0.16</td><td>0.26</td><td>-0.67</td><td>0.35</td><td>141.00</td><td>-0.61</td><td>0.545</td></tr><tr><td>Argument type</td><td>0.23</td><td>0.08</td><td>0.07</td><td>0.39</td><td>5.85</td><td>2.78</td><td>0.033</td></tr><tr><td>Age</td><td>0.03</td><td>0.05</td><td>0.08</td><td>0.13</td><td>148.84</td><td>0.52</td><td>0.605</td></tr><tr><td>Argument type * Seniority</td><td>0.23</td><td>0.16</td><td>0.53</td><td>0.08</td><td>456.00</td><td>-1.14</td><td>0.153</td></tr></table></body></html>

Table 10: The correlations among NFC, GW, and the three dependent variables. The lower diagonal of the table shows that Pearson correlations, the upper diagonal show the significance of the correlations (significant correlations are in italic script).   

<html><body><table><tr><td></td><td>NFC</td><td>GW</td><td>MERS</td><td>Correct</td><td>RT</td></tr><tr><td>NFC</td><td></td><td>&lt;0.001</td><td>&lt;0.001</td><td>=0.02</td><td>&lt;0.001</td></tr><tr><td>GW</td><td>0.134</td><td></td><td>&lt;0.001</td><td>&lt;0.001</td><td>=0.019</td></tr><tr><td>MERS</td><td>0.200</td><td>-0.268</td><td></td><td>&lt;0.001</td><td>=0.086</td></tr><tr><td>Correct classifications</td><td>0.094</td><td>0.137</td><td>-0.229</td><td></td><td>=0.028</td></tr><tr><td>Response times</td><td>0.138</td><td>0.094</td><td>0.069</td><td>0.492</td><td></td></tr></table></body></html>

![](img/f07bf68ea478b873d8d22ce9fc5ae7c716402304fbfc65735b88de66337b391f.jpg)  
Figure 6: Scatterplots of the relationships among NFC, GW, and the number of correct classifications, response times, and the MERS score. Fit lines show the linear fit for each argument type separately.

Table 11: Parameter estimates for the fixed effects of education, argument type, and age predicting the number of correct classifications.   
95 per cent CI   

<html><body><table><tr><td>Effect</td><td></td><td>SE</td><td>Lower</td><td>Upper</td><td>df</td><td>t</td><td>p</td></tr><tr><td>NFC</td><td>0.34</td><td>0.19</td><td>-0.02</td><td>0.71</td><td>146.28</td><td>1.84</td><td>0.068</td></tr><tr><td>GW</td><td>0.41</td><td>0.15</td><td>0.12</td><td>0.69</td><td>145.55</td><td>2.79</td><td>0.006</td></tr><tr><td>Argument type</td><td>-1.19</td><td>0.13</td><td>-1.44</td><td>0.94</td><td>457.00</td><td>9.30</td><td>&lt;0.001</td></tr><tr><td>NFC*GW</td><td>0.01</td><td>0.25</td><td>-0.49</td><td>0.48</td><td>146.55</td><td>0.03</td><td>0.980</td></tr><tr><td>Argument type*NFC</td><td>0.56</td><td>0.25</td><td>0.08</td><td>1.04</td><td>457.00</td><td>2.29</td><td>0.023</td></tr><tr><td>Argument type*GW</td><td>-0.44</td><td>0.19</td><td>-0.82</td><td>-0.06</td><td>457.00</td><td>-2.28</td><td>0.023</td></tr><tr><td>Argument type*NFC*GW</td><td>0.10</td><td>0.33</td><td>-0.74</td><td>0.54</td><td>457.00</td><td>0.32</td><td>0.752</td></tr></table></body></html>

The panels on the top and bottom in the middle of Figure 6 depict the relationship between NFC and GW with response times separated for argument type. Table 12 depicts the parameters estimates for all predictors for response times. The model $\mathrm { ( B I C = 6 4 4 7 }$ , Marginal $R ^ { 2 } = 0 . 0 4$ ) showed that NFC and GW are both significant predictors of response times; participants with higher scores on NFC or GW used more time to answer their task. In addition, the well-known effect of argument type was present, indicating that arguments involving LM took participants more time. This effect was independent of their NFC or GW, since none of the interactions were significant.

Table 12: Parameter estimates for the fixed effects of education, argument type, and age predicting response times.   

<html><body><table><tr><td colspan="8">95 per cent CI</td></tr><tr><td>Effect</td><td></td><td>SE</td><td>Lower</td><td>Upper</td><td>df</td><td>t</td><td>p</td></tr><tr><td>NFC</td><td>12.35</td><td>5.60</td><td>1.37</td><td>23.33</td><td>148.95</td><td>2.21</td><td>0.029</td></tr><tr><td>GW</td><td>5.51</td><td>4.38</td><td>3.08</td><td>14.10</td><td>149.57</td><td>1.26</td><td>0.0211</td></tr><tr><td>Argument type</td><td>11.08</td><td>3.06</td><td>5.09</td><td>17.08</td><td>457.00</td><td>3.62</td><td>&lt;0.001</td></tr><tr><td>NFC*GW</td><td>5.71</td><td>7.44</td><td>-8.87</td><td>20.29</td><td>149.90</td><td>0.77</td><td>0.444</td></tr><tr><td>Argument type*NFC</td><td>7.37</td><td>5.88</td><td>-4.15</td><td>18.90</td><td>457.00</td><td>1.25</td><td>0.211</td></tr><tr><td>Argument type*GW</td><td>-1.73</td><td>4.62</td><td>-10.78</td><td>7.32</td><td>457.00</td><td>0.37</td><td>0.708</td></tr><tr><td>Argument type*NFC*GW</td><td>11.94</td><td>7.83</td><td>-27.29</td><td>3.42</td><td>457.00</td><td>-1.52</td><td>0.128</td></tr></table></body></html>

Table 13: Parameter estimates for the fixed effects of education, argument type, and age predicting the MERS score   

<html><body><table><tr><td colspan="8">95 per cent CI</td></tr><tr><td>Effect</td><td></td><td>SE</td><td>Lower</td><td>Upper</td><td>df</td><td>t</td><td>p</td></tr><tr><td>NFC</td><td>0.53</td><td>0.21</td><td>-0.95</td><td>0.12</td><td>149.83</td><td>-2.53</td><td>0.012</td></tr><tr><td>GW</td><td>-0.61</td><td>-0.16</td><td>-0.93</td><td>-0.29</td><td>148.35</td><td>-3.73</td><td>&lt;0.001</td></tr><tr><td>Argument type</td><td>0.28</td><td>0.08</td><td>0.13</td><td>0.43</td><td>457.00</td><td>3.70</td><td>&lt;0.001</td></tr><tr><td>NFC*GW</td><td>-0.17</td><td>0.28</td><td>-0.72</td><td>0.38</td><td>149.39</td><td>-0.61</td><td>0.546</td></tr><tr><td>Argument type*NFC</td><td>0.04</td><td>0.15</td><td>0.24</td><td>0.33</td><td>457.00</td><td>0.31</td><td>0.754</td></tr><tr><td>Argument type*GW</td><td>0.14</td><td>0.11</td><td>0.08</td><td>0.37</td><td>457.00</td><td>1.26</td><td>0.207</td></tr><tr><td>Argument type*NFC*GW</td><td>0.04</td><td>0.19</td><td>0.34</td><td>0.42</td><td>457.00</td><td>0.19</td><td>0.849</td></tr></table></body></html>

Finally, the top and bottom panel on the right of Figure 6 depict the relationship between NFC and GW with the MERS score separated for argument type. Table 13 shows the parameter estimates for predicting the MERS score. As with response times, the model $( \mathrm { B I C } = 2 0 5 3$ , Marginal $R ^ { 2 } = 0 . 1 1$ ) showed that NFC and GW were significant predictors of mental effort. However, in contrast to the effects for response time, here participants with a higher NFC or a higher level of GW indicated less effort (e.g. they had a lower MERS score). As before, tasks with LM were considered harder than tasks involving RoT.

NFC and GW predict all dependent variables; they are positively related to correct classifications and response times. This shows that participants with a higher NFC and GW think longer about a task and—perhaps consequently— perform better. NFC and GW are negatively related to mental effort: participants with a higher NFC and those who like to engage with grammatical material were less inclined to consider the task mentally taxing. Crucially, however, only for the number of correct classifications do NFC and GW interact with argument type. For response times and the MERS, the relationship with NFC and GW is independent of argument type. For the number of correct classifications however, there is an interaction: tasks involving LM have a steeper relationship with NFC (i.e. the number of correct classifications increases more) compared to tasks involving RoT. This is different for GW, where the correct number of classifications increases more for tasks involving RoT.

# DISCUSSION

# Interpretation of main findings

The results of this study paint a picture that is by and large consistent with our hypotheses. Tasks involving LMs indeed seem to be more mentally demanding than tasks involving RoT, as the effect was found for all dependent variables: LM tasks take longer, result in less correct classifications, and are considered more difficult than RoT tasks. In terms of seniority (bachelor vs. master, and junior bachelor vs. senior bachelor full-time) we conclude that more senior student teachers typically outperform more junior ones, but that this is not dependent on argument type (i.e. more senior students are not better at processing or evaluating LM than RoT). This indicates that teacher education does seem to have an impact on students’ overall grammatical argument processing, although it is perhaps a little surprising that this improvement does not extend to argument type. It might be expected that as students progress through teacher education, their ability to engage in more cognitively demanding tasks (i.e. LM) increases. In addition, students’ overall scores are not very high (i.e. students hardly achieve (near) perfect scores), especially where LM are concerned. This seems to indicate that teacher trainer institutes are not sufficiently equipping students to tackle more difficult grammatical argumentation, and that therefore, students might struggle if they encounter situations in their daily practice in which they need to process and evaluate grammatical arguments. This especially holds for LMs. The fact that RoT are more easily processed by student teachers, could mean that when given the choice between using RoT or LMs, they will opt for RoT (which are easier), rather than choosing to use LMs (which are more indicative of grammatical understanding, cf. Van Rijt et al. 2019b). This makes it more likely that grammar education remains traditional, and less likely that grammar education will grow towards a more linguistically accepted and insightful approach. Teacher education has an important role to play if it wants to prepare students to engage in more meaningful grammar teaching practices. Based on our results, teacher education institutes might consider spending more time on developing students’ linguistic reasoning skills (Fontich 2014; Dielemans and Coppen 2021; Van Rijt et al. 2021). Teacher education could do so by exposing students to increasingly complex grammatical problems and use good practices to illustrate what linguistic reasoning looks like, using Dielemans and Coppen’s (2021) framework for linguistic reasoning. This way, student teachers are systematically introduced to specific components from the linguistic reasoning framework, such as learning how to use linguistic repertoire (LM and RoT), using linguistic argumentation, and using concepts and metaconcepts from theoretical linguistics (Van Rijt and Coppen 2017). When introducing this linguistic reasoning model, student teachers should be mentored in learning how to cope with ill-structured linguistic issues that challenge a binary understanding of grammar, as it is clear that ill-structured linguistic problems are a great challenge for (student) teachers (Elsner 2021; Wijnands et al. 2022a, b). Combining Dielemans and Coppen’s (2021) linguistic reasoning framework with Wijnands et al.’s (2021) pedagogical template for stimulating reflective thinking on ill-structured problems is therefore recommendable. This can deal with the co-dependency between epistemic cognition and grammatical reasoning skills.

In addition, student teachers should be encouraged to think about how they can bring linguistic reasoning into the classroom, which means teacher education institutes will also need to pay specific attention to the pedagogical implications of linguistic reasoning. In doing so, courses dealing with pedagogical content knowledge about grammar (‘grammar didactics’) should be aligned with student teachers’ courses on disciplinary content knowledge (‘grammar’) as to avoid incongruences in the teacher education program (cf. Graus and Coppen 2018). As for our final research question, we conclude that both NFC and GW influence the processing of grammatical arguments. The higher students’ scores on these variables, the longer they took to complete the tasks, and the less hard they felt the tasks were. Interestingly, higher NFC scores correlate with better LM performance, while higher GW scores correlate with better RoT performance. This indicates that the GW test might predominantly measure students’ willingness to engage in traditional school grammar practices (rather than their willingness to engage in school grammar practices that emphasize linguistic reasoning), as traditional school grammar is associated with RoT. Future research might consider ways of improving on the phrasing of items in the GW, so it may become clearer what the construct measures (e.g. one of the statements is ‘I think grammar is interesting’, which is a question in which the notion of ‘grammar’ may be interpreted by some as traditional school grammar, whereas it might be equally interpreted by others as more modern forms of grammar). We leave this matter open for future research.

# LIMITATIONS

The current study also has a few limitations that might influence the interpretation of the results.

First, while we used three different measures to establish cognitive load (as is recommended in the literature), it might be argued that only two of the measures we employed actually reflect cognitive load, namely the response times and the MERS. The number of correct classifications might reveal something about cognitive load, but it could also be seen as a consequence of cognitive load, rather than indicating cognitive load itself. Nevertheless, even if the number of correct classifications is not an accurate measure of cognitive load, it does reveal something important about the processing of RoT arguments versus LM arguments. In addition, all the measures we used find the same main effect of argument type, granting some validity to our choice to also include correct classifications.

Secondly, while the student teachers’ scores within this experiment might be cause for some concern, they cannot be taken as fully representative for their’ actual ability to assess grammatical arguments, as student teachers were deliberately put under time pressure. If the time pressure had been removed, their scores would have likely been higher, as there is a speed-accuracy trade-off (Katsimpokis et al. 2020). At the same time, student teachers are likely to come across classroom situations in which such a trade-off occurs, so the results give an impression of student teachers’ performances under pressure. It is also worth mentioning that while (student) teachers will need a certain ability to perform under time pressure when reasoning grammatically, it would be a misconception to think that (student) teachers are not allowed time to unfold a grammatical reasoning process, even in real-time classroom situations. Sometimes, especially when teaching grammatical reasoning, it may be worthwhile to deliberately slow down thinking in the classroom, and to embrace the uncertainty that is associated with slowing down thinking. Yet, for the purposes of measuring cognitive load, some form of time pressure was necessary from a methodological point of view, otherwise comparing the cognitive load of RoT versus LM would have been much harder.

A related point is that different linguistic reasoning tasks, for instance tasks dealing with punctuation, semantics, or pragmatics, could have shown slightly different results, as tasks related to parsing (which were used in the current study) may privilege arguments based on RoT, as parsing is often highly associated with those (cf. Coppen 2009; Berry 2015).

Thirdly, our task was designed to mimic classroom settings where student teachers would have to evaluate grammatical arguments under time pressure. We attempted to do so by asking student teachers to categorize the arguments as rapidly as possible. While both real classroom situations and the current experiment share an element of time pressure, there are nevertheless some important differences between this experiment and actual classroom situations. By all accounts, actual classroom situations are much more demanding, as student teachers are not only required to deal with the grammatical subject content, but also with classroom management—a particularly challenging aspect for (student) teachers (cf. Pillen et al. 2013). In real classrooms, student teachers are therefore in danger of cognitive overload much faster than in a controlled experiment. One could therefore argue that the current experiment has captured time-pressured performances under fairly favourable conditions compared to an actual classroom setting. This could indicate that student teachers’ grammatical reasoning performances could be poorer in ecologically valid conditions, in which case our results overestimate student teachers’ time-pressured abilities. Future studies might therefore examine (student) teachers’ grammatical reasoning skills in actual classroom contexts.

Fourthly, while we obtained a large student sample in which most Dutch teacher trainer institutes are represented, some institutions participated with more students than others. This makes it harder to establish a clear effect of students’ institutions. In a similar vein, the group of master students is relatively limited compared to the other groups, which may have impacted some of the results. Likewise, as (grammar) teacher education is organized differently in different educational jurisdictions, our results cannot be fully generalized to other educational contexts (cf. Boivin et al. 2018). This merits further research.

In spite of these limitations, the current study has provided relevant insights into student teachers’ processing of grammatical arguments.

# SUPPLEMENTARY DATA

Supplementary material is available at Applied Linguistics online.

# REFERENCES

Alderson, J. and R. Hudson. 2013. ‘The metalinguistic knowledge of undergraduate students of English language or linguistics,’ Language Awareness 22/4: 320–37. doi:10.1080/096584 16.2012.722644.   
Barr, D. J., et al. 2013. ‘Random effects structure for confirmatory hypothesis testing: Keep it maximal’. Journal of Memory and Language 68/3: 255–78. doi:10.1016/j.jml.2012.11.001.   
Baumberger, C. 2019. ‘Explicating objectual understanding: Taking degrees seriously,’ Journal for General Philosophy of Science 50: 367–88. doi:10.1007/s10838-019-09474-6.   
Berry, R. 2015. ‘Grammar myths,’ Language Awareness 24/1: 15–37. doi:10.1080/0965841 6.2013.873803.   
Boivin, M. -C., et al. 2018. ‘Working on grammar at school in L1 education: Empirical research across linguistic regions,’ L1-Educational Studies in Language and Literature 18/3: 1–6. doi:10.17239/L1ESLL-2018.18.04.01.   
Cacioppo, J., R. Petty, and C. Kao. 1984. ‘The efficient assessment of need for cognition,’ Journal of Personality Assessment 48/3: 306–7. doi:10.1207/s15327752jpa4803_13.   
Cajkler, W. and J. Hislam. 2002. ‘Trainee teachers’ grammatical knowledge: The tension between public expectation and individual competence,’ Language Awareness 11/3: 161– 77. doi: 10.1080/09658410208667054.   
Coppen, P.-A. 2009. Leren tasten in het duister. [Learning to grope in the dark]. Inaugural address Radboud University Nijmegen.   
Coppen, P.-A. 2010. ‘Grammatica, waar gaat dat eigenlijk over?’ [Grammar, what’s that really about?] in S. Vanhooren and A. Mottart (eds.): Vierentwintingste Conferentie het Schoolvak Nederlands. Academia Press, pp. 174–82.   
Coppen, P. -A. 2011. ‘Emancipatie van het grammaticaonderwijs,’ [The emancipation of grammar education’],’ Tijdschrift Taal 2/3: 30–3.   
DeLeeuw, K. and R. Mayer. 2008. ‘A comparison of three measures of cognitive load: Evidence for separable measures of intrinsic, extraneous, and germane load,’ Journal of Educational Psychology 100/1: 223–34. doi: 10.1037/0022-0663.100.1.223.   
De Jong, T. 2010. ‘Cognitive load theory, educational research and instructional design: Some food for thought,’ Instructional Science 38: 105– 34. doi:10.1007/s11251-009-9110-0.   
De Regt, H. 2009. ‘The epistemic value of understanding,’ Philosophy of Science 76/5: 585–97. doi: 10.1086/605795.   
Dielemans, R. and P. -A. Coppen. 2021. ‘Defining linguistic reasoning Transposing and grounding a model for historical reasoning to the linguistic domain,’ Dutch Journal of Applied Linguistics 9/1/2: 182–206. doi:10.1075/ dujal.19038.die.   
Elsner, D. 2021. ‘Knowledge about grammar and the role of epistemological beliefs,’ Pedagogical Linguistics 2/2: 107–28. doi:10.1075/pl.21003. els.   
Fontich, X. 2014. ‘Grammar and language reflection at school: Checking out the whats and the hows of grammar instruction,’ in T. Ribas, X. Fontich and O. Guasch (eds.): Grammar at School: Research on Metalinguistic Activity in Language Education. Peter Lang, pp. 255–84.   
Fontich, X. 2016. ‘Grammar instruction and writing: Metalinguistic activity as a teaching and research focus,’ Language and Linguistics Compass 10/5: 238–54. doi: 10.1111/lnc3.12184.   
Fontich, X. and A. Camps. 2014. ‘Towards a rationale for research into grammar teaching at schools,’ Research Papers in Education 29/5: 598– 625. doi:10.1080/02671522.2013.813579.   
Fontich, X. and M. J. García-Folgado. 2018. ‘Grammar instruction in the Hispanic area: The case of Spain with attention to empirical studies on metalinguistic activity,’ L1-Educational Studies in Language and Literature 18/3: 1–39. doi: 10.17239/L1ESLL-2018.18.04.02.   
Gallucci, M. 2019. ‘GAMLj: General analyses for linear models. [jamovi module],’ available at https://gamlj.github.io/. Accessed 15 July 2022.   
Giovanelli, M. 2015. ‘Becoming an English language teacher: Linguistic knowledge, anxieties and the shifting sense of identity,’ Language and Education 29/5: 416–29. doi:10.1080/0950 0782.2015.1031677.   
Graus, J. and P. -A. Coppen. 2018. ‘Influencing student teacher grammar cognitions: The case of the incongruous curriculum,’ The Modern Language Journal 102/4: 693–712.   
Honda, M. and W. O’Neil. 2007. Thinking Linguistically. A Scientific Approach to Language. Wiley-Blackwell.   
Kabel, K., M. Vedsgaard Christensen, and L. Storgaard Brok. 2021. ‘A focused ethnographic study on grammar teaching practices across language subjects in schools,’ Language, Culture and Curriculum 35/1: 51–66. doi:10.108 0/07908318.2021.1918144.   
Katsimpokis, D., G. E. Hawkins, and L. van Maanen. 2020. ‘Not all speed-accuracy tradeoff manipulations have the same psychological effect,’ Computional Brain & Behavior 3: 252–68. doi:10.1007/s42113-020-00074-y.   
Leenders, G., R. De Graaff, and M. Van Koppen. 2021. ‘Hoe meet je bewuste taalvaardigheid? Grammaticaal redeneren in de vakken Nederlands, Engels en Duits,’ [How do you measure conscious language proficiency? Grammatical reasoning in the subjects Dutch, English and German],’ Pedagogische Studiën 98/1: 67–93.   
Locke, T. (ed.). 2010. Beyond the Grammar Wars. A Resource for Teachers and Students on Developing Language Knowledge in the English/Literacy Classroom. Routledge.   
Macken-Horarik, M., K. Love and S. Horarik. 2018. ‘Rethinking grammar in language arts: Insights from an Australian survey of teachers’ subject knowledge,’ Research in the Teaching of English 52/3: 288–316.   
Macken-Horarik, M., K. Love, and L. Unsworth. 2011. ‘A grammatics “good enough” for school English in the 21st century: Four challenges in realising the potential,’ Australian Journal of Language and Literacy 34/1: 9–23.   
Myhill, D. 2000. ‘Misconceptions and difficulties in the acquisition of metalinguistic knowledge,’ Language and Education 14/3: 151–163. doi:10.1080/09500780008666787   
Myhill, D. 2018. ‘Grammar as a meaning-making resource for improving writing,’ L1 Educational Studies in Language and Literature 18/3: 1–21. doi:10.17239/L1ESLL-2018.18.04.04.   
Myhill, D. 2021. ‘Grammar re-imagined: Foregrounding understanding of language choice in writing,’ English in Education 55/3: 265–78. doi:10.1080/04250494.2021.1885975.   
Myhill, D., et al. 2012. ‘Re-thinking grammar: The impact of embedded grammar teaching on students’ writing and students’ metalinguistic understanding,’ Research Papers in Education 27/2: 151–63. doi:10.1080/02671522.2011.637640.   
Nygård, M. and H. Brøseth. 2021. ‘Norwegian teacher students’ conceptions of grammar,’ Pedagogical Linguistics 2/2: 129–52. doi:10.1075/pl.21005.nyg.   
Paas, F. 1992. . ‘Training strategies for attaining transfer of problem-solving skills in statistics: A cognitive load approach,’ Journal of Educational Psychology 84/4: 429–34. doi:10.1037/0022-0663.84.4.429.   
Paas, F., et al. 2003. ‘Cognitive load measurement as a means to advance cognitive load theory,’ Educational Psychologist 38/1: 63–72. doi:10.1207/S15326985EP3801_8.   
Phillips, N. 2017. ‘YaRrr! The Pirate’s Guide to R,’ APS Observer 30.   
Pillen, M., D. Beijaard, and P. den Brok. 2013. ‘Tensions in beginning teachers’ professional development, accompanying feelings and coping strategies,’ European Journal of Teacher Education 36/3: 240–60. doi: 10.1080/02619768.2012.696192.   
R Core Team. 2021. ‘R: A Language and environment for statistical computing. (Version 4.1) [Computer software]’, available at https:// cran.r-project.org. (R packages retrieved from MRAN snapshot 2022-01-01).   
Rättyä, K., E. Awramiuk and X. Fontich. 2019. ‘What is grammar in L1 education today?,’ L1-Educational Studies in Language and Literature 19/2: 1–8. doi:10.17239/ L1ESLL-2019.19.02.01.   
Sangster, P., C. Anderson, and P. O’Hara. 2013. ‘Perceived and actual levels of knowledge about language amongst primary and secondary student teachers: Do they know what they think they know?,’ Language Awareness 22/4: 293–319. doi: 10.1080/09658416.2012.722643.   
Shehab, H. and M. Nussbaum. 2015. ‘Cognitive load of critical thinking strategies,’ Learning and Instruction 35: 51–61. doi: 10.1016/j. learninstruc.2014.09.004.   
Sweller, J., J. Van Merriënboer, and F. Paas. 1998. ‘Cognitive architecture and instructional design,’ Educational Psychology Review 10: 251– 96. doi:10.1023/A:1022193728205.   
The Jamovi Project. 2022. ‘Jamovi. (Version 2.3) [Computer Software]’, available at https:// www.jamovi.org.   
Van Drie, J. and C. Van Boxtel. 2008. ‘Historical reasoning: Towards a framework for analyzing students’ reasoning about the past,’ Educational Psychology Review 20/2: 87–110. doi:10.1007/ s10648-007-9056-1.   
Van Gelderen, A. 2010. ‘Does explicit teaching of grammar help students to become better writers? Insights from empirical research’ in Locke (ed.): Beyond the Grammar Wars. A Resource for Teachers and Students on Developing Language Knowledge in the English/Literacy Classroom. Routledge.   
Van Rijt, J. and P.-A. Coppen. 2017. ‘Bridging the gap between linguistic theory and L1 grammar education – experts’ views on essential linguistic concepts,’ Language Awareness 26/4: 360–80.   
Van Rijt, J., A. Wijnands, and P.-A. Coppen. 2019a. ‘Dutch teachers’ beliefs on linguistic concepts and reflective judgement in grammar teaching,’ L1-Educational Studies in Language and Literature’ 19: 1–28.   
Van Rijt, J., A. Wijnands, and P.-A. Coppen. 2019b. ‘When students tackle grammatical problems: Exploring linguistic reasoning with linguistic metaconcepts in L1 grammar education,’ Linguistics and Education 52: 78–88.   
Van Rijt, J., A. Wijnands, and P.-A. Coppen. 2020. ‘How secondary school students may benefit from linguistic metaconcepts to reason about L1 grammatical problems,’ Language and Education 34/3: 231–48.   
Van Rijt, J., H. Hulshof, and P.-A. Coppen. 2021. ‘X is the odd one out, because the other two are more about the farmland’ – Dutch L1 student teachers’ struggles to reason about grammar in odd one out tasks,’ Studies in Educational Evaluation 70: 1–13.   
Van Rijt, J., et al. 2022. ‘Linguistic metaconcepts can improve grammatical understanding in L1 education evidence from a Dutch quasi-experimental study,’ PLoS ONE 17/2: 1–25.   
Watson, A. 2012. ‘Navigating ‘the pit of doom’: Affective responses to teaching ‘grammar,’ English in Education 46/1: 22–37. doi:10.1111/j.1754-8845.2011.01113.x.   
Watson, A. and R. Newman. 2017. ‘Talking grammatically: L1 adolescent metalinguistic reflection on writing,’ Language Awareness 26/4: 381–98. doi:10.1080/09658416.2017.1 410554.   
Wickham, H. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag.   
Wijnands, A., J. Van Rijt, and P.-A. Coppen. 2021. ‘Learning to think about language step by step: a pedagogical template for the development of cognitive and reflective thinking skills in L1 grammar education,’ Language Awareness 30/4: 317–35.   
Wijnands, A., J. Van Rijt, and P.-A. Coppen. 2022a. ‘Measuring epistemic beliefs about grammar,’ L1-Educational Studies in Language and Literature 22/1: 1–29.   
Wijnands, A., et al. 2022b. ‘Balancing between uncertainty and control: Teaching reflective thinking about language in the classroom,’ Linguistics and Education 71: 1–19.