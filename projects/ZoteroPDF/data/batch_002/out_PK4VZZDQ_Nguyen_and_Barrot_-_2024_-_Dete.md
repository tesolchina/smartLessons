# Detecting and assessing AI-generated and human-produced texts: The case of second language writing teachers

Loc Nguyen a,1 , Jessie S. Barrot a,b,\*,2

a School of Foreign Languages, University of Economics Ho Chi Minh City, Vietnam b College of Education, Arts and Sciences, National University, Manila, Philippines

# A R T I C L E I N F O

# A B S T R A C T

Keywords:   
ChatGPT   
Computer-assisted language learning   
Generative artificial intelligence   
Second language writing   
Writing assessment

Artificial intelligence (AI) technologies have recently attracted the attention of second language (L2) writing scholars and practitioners. While they recognize the tool’s viability, they also raised the potential adverse effects of these tools on accurately reflecting students’ actual level of writing performance. It is, therefore, crucial for teachers to discern AI-generated essays from humanproduced work for more accurate assessment. However, limited information is available about how they assess and distinguish between essays produced by AI and human authors. Thus, this study analyzed the scores and comments teachers gave and looked into their strategies for identifying the source of the essays. Findings showed that essays by a native English-speaking (NS) lecturer and ChatGPT were rated highly. Meanwhile, essays by an NS college student, non-native English-speaking (NNS) college student, and NNS lecturer scored lower, which made them distinguishable from an AI-generated text. The study also revealed that teachers could not consistently identify the AI-generated text, particularly those written by an NS professional. These findings were attributed to teachers’ past engagement with AI writing tools, familiarity with common L2 learner errors, and exposure to native and non-native English writing. From these results, implications for L2 writing instruction and future research are discussed.

# 1. Introduction

Artificial intelligence (AI) has introduced personalized instruction, immersive learning experiences, and state-of-the-art language learning tools, which provide individualized feedback, adapt instructional content, and enhance learners’ engagement (Hockly, 2023; Huang et al., 2023). One AI tool that has attracted the attention of scholars and practitioners is ChatGPT (Chat Generative Pre-trained Transformer), an advanced language model developed by OpenAI. Recent students have found that ChatGPT-enhanced instruction reduced anxiety, facilitated learning, and promoted learning and motivation (Lo, 2023; Ng et al., 2024). Its ability to respond contextually and coherently to prompts also makes it suitable as a language input source and platform for language learning activities, such as chat-based conversations.

ChatGPT’s use extends to second language (L2) writing as it offers various writing affordances. Aside from its ability to produce coherent and grammatically correct full-length texts, this generative AI tool can perform various writing assistance tasks, such as formulating essay topics, creating outlines, providing content feedback, editing texts, and assigning automated scores (Barrot, 2023; Imran & Almusharraf, 2023; Song & Song, 2023). As in the case of other AI tools, ChatGPT exhibits certain limitations, including unintelligible responses, inaccurate bibliographic information and citations, and a lack of emotional depth and voice. Some scholars and practitioners have also raised its potential adverse effect on academic integrity, writing instruction, and writing assessment (Cotton et al., 2024; Yan et al., 2024). For instance, ChatGPT-assisted writing may not accurately reflect students’ actual level of writing performance (Currie, 2023). This problem could mislead the teachers on the pedagogical strategies they need to use in their respective writing classrooms. Teachers’ lack of skills in distinguishing between students’ own writing and ChatGPT-produced texts can also give an unfair advantage to students who have access to the tool (Cotton et al., 2024). It is, therefore, crucial for teachers to discern AI-generated essays from student-produced work because it speaks to the authenticity of a student’s writing abilities. Generative AI tools can produce sophisticated texts that may mask a student’s actual writing proficiency, hindering accurate assessment and feedback.

Thus, the current study sheds light on how L2 writing teachers assess and distinguish ChatGPT-generated essays from humanproduced ones. With this study, we hope to gain nuanced insights into how they can better practice writing assessment, distinguish ChatGPT-generated texts from those written by human writers, and take appropriate actions to counter any form of academic dishonesty. Our findings would also be useful in developing pedagogical strategies or interventions based on L2 writing teachers’ challenges in assessing potentially AI-generated content. Finally, this study could help schools in designing teacher training programs and recalibrating their policies to support the use of generative AI tools in writing classrooms.

# 2. Literature review

# 2.1. ChatGPT as a writing assistant tool

ChatGPT, developed by OpenAI, stands at the forefront of conversational AI technology, encapsulating complex algorithms to understand and respond to queries with human-like accuracy (OpenAI, 2024). It is underpinned by a Large Language Model (LLM), which is continuously refined to enhance its ability to produce varied and contextually relevant responses (Schade, 2023). As it evolves, ChatGPT is regularly updated to interpret user prompts better, employing a dynamic algorithmic structure that fosters diverse linguistic patterns (Ray, 2023). Despite its sophistication, efforts are made to mitigate biases within its responses. OpenAI has implemented specific strategies, such as fine-tuning the model with diverse training data, applying reinforcement learning with human feedback, and continually refining prompt guidelines to reduce potentially harmful or biased outputs (Hua et al., 2024; Ray, 2023). These measures aim to promote fairness, inclusivity, and sensitivity in ChatGPT’s interactions by addressing biases that may arise from the data it was trained on. These iterative enhancements are vital as generative AI becomes more embedded in educational, profes sional, and casual settings, where the ability to converse naturally and informatively is paramount. ChatGPT serves as a transformative tool in language teaching and learning as it offers learners personalized and accessible resources for practicing various language skills, particularly beneficial for those without traditional learning opportunities (Barrot, 2024; Huang et al., 2022; Bin-Hady et al., 2023). Agustini (2023) underscores its role in fostering autonomous and reflective learning. Shaikh et al. (2023) observe that simulated conversations with ChatGPT enhance learners’ language abilities, although it necessitates careful integration into teaching practices to maintain authenticity (Su et al., 2023).

ChatGPT’s affordances extend to L2 writing as it encompasses the full spectrum of the writing process (Barrot, 2023). During topic generation, this generative AI tool uses its extensive database to identify and suggest topics that align with students’ interests. This process involves analyzing patterns and themes from a wide-ranging collection of information, which provides personalized and relevant options for students to explore in their writing tasks (Ray, 2023; Su et al., 2023). ChatGPT also offers a variety of ideas and subtopics related to the main topic of interest during brainstorming. After brainstorming, it assists in structuring and organizing ideas into a coherent framework. This process ensures that the subsequent draft will have a logical flow (Imran & Almusharraf, 2023; Bin-Hady et al., 2023). Subsequently, the tool can act as a collaborative partner by offering suggestions during drafting. It is capable of offering feedback that can lead to substantive revisions and refinement of the text. Once the draft is complete, students can use ChatGPT’s language editing features to fine-tune their work’s grammar, syntax, and style. Despite these features, ChatGPT should be used ethically and responsibly by ensuring it supports the writing process rather than a means to bypass original effort (Huallpa, 2023; Lund et al., 2023).

# 2.2. Teachers’ ability to detect AI-generated texts

While there have been a plethora of studies that examined the use of ChatGPT in L2 writing contexts (Imran & Almusharraf, 2023), L2 writing teachers’ ability to distinguish AI-generated texts and assessment practices in this context remains an underexplored ter ritory. One of those who attempted to shed light on this area of inquiry is Herbold et al. (2023), who compared human-written and ChatGPT-generated essays. One of their objectives was to determine how teachers scored these two types of essays. The teachers were not provided with information on whether the essays were human-written or AI-generated, indicating that their assessments were based solely on the quality of the text. Results revealed that AI-generated argumentative essays were found to be of significantly higher quality than essays written by human users as assessed by teachers. As a quantitative study, their work did not examine teachers’ comments on the scored texts and their ability to detect AI-generated essays.

To complement Herbold et al.’s (2023) work, Alexander et al. (2023) qualitatively focused on the challenges faced by six experienced English as a Second Language (ESL) lecturers in identifying ChaTGPT-generated texts. They found that ESL lecturers failed to recognize the characteristics specific to AI-generated content fully. However, this study revealed that those teachers who had previous exposure to ChatGPT could recognize generative AI features in written texts. Teachers also tended to focus on errors as an indicator of L2 writing output and high levels of technical and grammatical accuracy as indicators of AI-generated. However, due to the small sample, the authors disclaimed that these findings should be treated with caution.

In a parallel study, Waltzer et al. (2023) focused on a broader demographic, examining both teachers’ and students’ abilities to differentiate between AI-generated essays and those written by high school students. Sixty-nine high school teachers were asked to read a pair of essays (one student-produced and one ChatGPT-generated) and assess which was generated by ChatGPT. Teachers’ accuracy was reported to be at 70 percent, which is slightly higher than students’ accuracy at 63 percent. Unlike Alexander et al.’s (2023) findings, Waltzer et al. (2023) did not find prior experience with ChatGPT as a factor in predicting accuracy. The study also highlighted the challenge of distinguishing the two types of texts when the quality of writing is higher. However, their work did not delve into the teachers’ assessment of these essays.

Fleckenstein et al. (2024) extended this inquiry by investigating novice $\left( N = 8 9 \right.$ ) and experienced teachers’ $\begin{array} { r } { \left( N = 2 0 0 \right. } \end{array}$ ) ability to identify AI-generated texts using randomized-controlled experiments. Despite teachers’ confidence in their ability to identify the source of the texts, the two experimental studies revealed that both groups could not identify texts generated by ChatGPT among student-produced texts. The ability of teachers to correctly identify the source of the texts was not perfect, with both AI-generated and student-generated texts being incorrectly identified to some extent. In terms of assessing the two types of essays, high-quality AI-generated texts received even higher scores compared to high-quality student-written texts, while low-quality student-written texts tended to receive higher scores than low-quality AI-generated texts. This could be due to the wider range of text quality in AI-generated texts. Their study, however, did not specifically examine teachers’ comments on the two types of essays. Without examining their comments, we miss out on understanding their specific observations, interpretations, and feedback on the essays. This information could have provided valuable insights into how teachers engage with and evaluate AI-generated texts.

Collectively, these three studies highlight the challenges that AI-generated texts pose for teachers in accurately assessing essays and the need to address these challenges in educational contexts. These studies also emphasized the need to understand further the complexities of teachers’ ability to distinguish AI-generated from human-produced essays. They also emphasize the need for institutional policies that safeguard academic integrity and additional training to effectively assess AI-generated texts and to ensure fair and accurate grading of students’ writing.

# 2.3. Research questions

As reviewed, the above studies have provided insights into teachers’ ability to distinguish AI-generated texts. However, it is not clear which text features distinguish AI-generated texts from human-produced texts and the strategies teachers employed in identifying AI-generated texts. To address this gap, our investigation was guided by the following specific research questions: (1) How do L2 writing teachers assess ChatGPT-generated texts and human-produced texts? (2) How do they distinguish between these two texts?

# 2.4. Theoretical framework

In this study, we adopt sense-making theory as the theoretical framework to investigate teachers’ capacity to identify ChatGPTgenerated texts. Sense-making theory refers to the cognitive process wherein individuals actively and continuously derive meaning from the information they encounter, framing it and using it to guide their actions and behaviors, thereby attributing significance to their experiences (Evans, 2007; Weick, 1996). This theory encompasses both cognitive processes and implementation behavior (Spillane et al., 2002; Weick, 1996), highlighting the active and adaptive nature of human cognition as individuals construct their realities by making sense of their beliefs, values, experience, and knowledge (Evans, 2007; Spillane & Anderson, 2014). Weick (1996) further emphasizes that sense-making is intertwined with individuals’ identities and actions, socially constructed, and retrospective, allowing individuals to reflect on past events and construct narratives. It also involves synthesizing various information to create plausible explanations. Within the context of this study, sense-making theory suggests that teachers make decisions and act based on their understanding of ChatGPT and students, teaching experience, and environment both within and beyond the school. The theory’s emphasis on perception, cognitive frames, social context, and retrospective sense-making can offer valuable insights into the in tricacies of L2 writing teachers’ decision-making processes.

# 3. Methodology

This qualitative study used a case study approach that enabled an in-depth investigation of L2 writing teachers’ experience in assessing and detecting AI-generated essays. We consider the case study design appropriate because we are exploring a relatively new area of research. It also allows us to gather preliminary insights for further investigation and obtain an emic perspective of L2 writing teachers’ assessment practices within their real-life contexts (Rashid et al., 2019).

# 3.1. Context and participants

Out of 10 teachers invited to participate in this study, six L2 writing instructors from various public universities in Vietnam accepted the invitation. They are non-native speakers of English and were selected using purposive sampling based on set criteria, which included a minimum of two years teaching EFL writing, experience in language assessment training, possession of at least a

master’s degree, and familiarity with ChatGPT. Their ages ranged from 29 to 54, with teaching experience spanning from 2 to 20 years. Of the eight teachers, three have earned master’s degrees, and three have doctorate degrees. Except for one teacher, identified as T2, who has used the tool only once, all have engaged with ChatGPT for at least three months.

# 3.2. Instrument

This study used semi-structured interviews with teachers and document analysis of assessed essays. The semi-structured interview comprises three parts. The first part is the background information section, which asks about the teacher’s name, affiliation, sex, age, current position, years of teaching EFL writing, English courses handled, and educational level. The second part is the preliminary questions section, which asks about the teachers’ familiarity and exposure to ChatGPT, technological tools used in the writing class, and language assessment training. The last part is the main questions section, which asks the following questions: (1) Can you explain the comments you provided in each of the essays? (2) Which of the five essays is likely the ChatGPT-generated text? (3) What specific features or patterns in the essay indicate that it is likely ChatGPT-generated or human-produced? (4) What strategies do you use when attempting to identify ChatGPT-generated texts? This interview guide was driven by the research questions and evaluated by two experts based on completeness, alignment with the research questions, clarity of questions and instructions, language accuracy, and length. Thereafter, we piloted the revised instrument to two teachers who were not part of the actual study to further enhance the instrument.

# 3.3. Data-gathering procedure

All data were gathered following these three phases: generation and assessment of essays, interview with the teachers, and ex amination of assessed essays. As part of the first phase, five essays of 300–400 words were provided to each teacher. These essays were produced by the following: native English-speaking (NS) college student (Essay 1), Vietnamese English teacher (Essay 2), NS lecturer (Essay 3), Vietnamese college student (Essay 4), and ChatGPT 3.5 (Essay 5). Each of them was asked to write an essay on the following topics: (1) Should teenagers be prohibited to use social media? (2) Should school uniforms be mandatory among college students? The sample writing prompt is as follows: In 300–400 words, state your stand on the following topic:. Provide at least three arguments to support your position. You will have 90 minutes to complete this essay without the aid of any reference materials. In MS Word, computerize your essay using the following format: Font 12 Calibri, double-spaced, indented paragraphs.

Once all the needed essays were available, they were forwarded to each of the six teachers for their evaluation using the IELTS Writing Band Descriptors: Task 2 rubric. This rubric comprises four criteria, namely Task Response, which judges the relevance and development of ideas; Coherence and Cohesion, which looks at the organization and flow of the essay; Lexical Resource, assessing vo cabulary use and precision; and Grammatical Range and Accuracy, focusing on sentence structure and correctness. Each of these criteria has band descriptors ranging from 0 to 9, with 9 being the highest. Similar to Herbold et al. (2023), these teachers were not provided with information on whether the essays were human-written or AI-generated. Teachers were asked to examine each essay very carefully and provide feedback on its strong and weak points. They were given 60–90 minutes to complete the assessment of each essay without the aid of any automated writing evaluation tools or similar technologies. Note that informed consent was obtained from the participants prior to data gathering.

The interviews were conducted face-to-face, with efforts made to create a comfortable and engaging environment to encourage openness. While this setting helped facilitate candid responses, additional measures were taken to directly address social desirability bias. Participants were assured of confidentiality and anonymity, and they were reminded that there were no right or wrong answers to reduce any pressure to respond in socially desirable ways. This approach aimed to minimize bias by fostering an atmosphere where participants felt free to share their genuine perspectives (Bergen & Labont´e, 2020). The interview came in two stages. During the first stage, the teachers were asked to explain all their comments in each of the essays. They were allowed to freely share their thoughts and the reasons for each of the comments. The interviewer made follow-up questions only when necessary. Thereafter, they were asked to identify which among the five essays was likely to be ChatGPT-generated. During the second stage, teachers were asked about the specific features or patterns that guided them and strategies they used in distinguishing ChatGPT-generated texts from human-produced essays. Each interview lasted for about 60 minutes. All interviews were recorded to help the researchers accurately capture teachers’ responses for transcription. As a way of triangulation, we examined the comments in the assessed essays to see if they aligned with the teachers’ responses during the interview on the specific features and patterns in the texts that they identified as indicators of machine-generated and human-produced content.

# 3.4. Data analysis

The data from interviews and document analysis were analyzed using cross-case analysis, which involves examining a series of cases to determine their areas of similarities and differences (Creswell & Creswell, 2018). Specifically, this approach seeks to examine, interpret, code, and compare data across multiple cases through cyclic iteration (Miles et al., 2019). This means that we began to process and analyze the transcript of the first participant before we proceeded to each of the succeeding participants. Using a thematic analysis driven by the two research questions as the main themes, we constructed the subthemes under each main theme. Then, we analyzed the transcript of the second participant and integrated the identified subthemes into the subthemes of the preceding data. To do this, we combined related subthemes, while the unique ones were allowed to emerge as a new subtheme. This progressive interplay between coding and analysis was repeated until all interview transcripts were processed. Two intercoders analyzed the data separately to ensure the reliability and rigor of the analysis. Prior to data analysis, they underwent a calibration session which involved reviewing the research questions, the instruments, and the data analysis procedure.

# 4. Results

Our study attempted to illustrate how L2 writing teachers assess and differentiate essays generated by ChatGPT from those written by humans. Specifically, we examined the scores and comments provided by teachers to these two types of texts. We also probed into their ability, strategies, and text features that help them in distinguishing between these essays. (Table 1).

4.1. How L2 writing teachers assess ChatGPT-generated and human-produced texts

Table 2 shows noteworthy patterns that emerge from L2 writing teachers’ evaluation of ChatGPT- and human-produced essays. Overall, the highest scores were given to the essays written by the NS lecturer (essay 3), with scores ranging from 7.0 to 9.0 and ChatGPT 3.5 (essay 5), with scores ranging from 8.0 to 9.0. This data suggests that ChatGPT’s writing quality mimics, quite convincingly, the writing level expected of a NS academic. Meanwhile, the essay produced by an NS college student (essay 1) was evaluated as on par with Vietnamese English teacher’s work, receiving moderate scores. However, these two essays did not have as high score as essays 3 and 5. This scoring indicates that NS style of writing does not necessarily equate to the highest scores, which may imply that other aspects of writing (e.g., argument construction and the ability to articulate ideas) may have contributed to their higher scores. The essay written by a Vietnamese college student (essay 4), which received the lowest scores overall, exhibits a stark contrast to essays 1 and 2, underscoring a perceived deficit in writing quality of less experienced non-native writers. This data further accentuates the clear distinction between the writing quality of an L2 writing student and ChatGPT.

To further understand how the teachers assessed the essays, we examined the comments received by each essay. The feedback from various teachers on five different essays reveals insights into the perceived strengths and weaknesses of each piece of writing from multiple perspectives. The comments on essays 3 and 5 stand out in their consistent recognition of high quality and sophistication compared to the other essays. For both of these essays, the teachers’ feedback centers on the strong, clear positions presented throughout, with ideas being relevant, extended, and well-supported. For instance, T1 commented that essay 3 “presents a welldeveloped response to the question with relevant, extended and supported ideas.” The same observation applies to essay 5 which received a comment that it “maintains a clear position supporting the prohibition of teenagers from using social media” (T1). In terms of cohesion and coherence, teachers complimented the two essays’ logical sequencing of ideas and skillful paragraphing. T2 com mented that “ideas are well-supported and logically organized” in essay 3, while T1 noted that “the ideas are presented coherently, with smooth transitions between paragraphs” in essay 5. All teachers also characterize the vocabulary use of essays 3 and 5 as being both wide-ranging and sophisticated. In terms of grammatical range and accuracy, teachers noted very few grammatical lapses in essay 3 (T1, T2) and none in essay 5. This high-level writing quality led some teachers to raise the possibility of generative AI assistance in the two essays, a mention not made for the other essays.

In contrast, the feedback on essays 1, 2, and 4 highlights various deficiencies and areas for improvement. The comments for essay 1 note some issues with repetition and focus. T1 commented on the "repetitive explanation" and that the essay sometimes went "off track." However, they acknowledge that it "addresses all parts of the task," (T1) even if some parts may be more fully covered than others. The essay arranges information coherently and shows clear progression, but it suffers from some mechanical cohesion and referencing issues. The vocabulary range is described as adequate, with a mix of simple and complex sentences. Teacher 2 adds that the essay is "generally well-written and coherent" but is marred by "some irrelevant information" and is "arguably overlong as an IELTS essay." The control of grammar is fairly good, despite some typical ESL errors. Essay 2, while meeting the basic requirements of the task, demonstrates weaknesses in clearly stating a thesis and addressing the task (T1) as well as maintaining focus (T4). The coherence and cohesion of essay 2 are noted to be less than ideal. For instance, some teachers described it as lacking a clear overall progression (T1) and overusing cohesive devices (T6). The use of vocabulary, while adequate, lacks the range necessary for high-level academic writing. Essay 2 received comments on vocabulary, such as “uses a limited range of vocabulary” (T1) and “adequate with some word choice errors” (T2). While there are attempts at complex grammatical structures, T1 noted that “these tend to be less accurate than simple sentences.”

Essay 4, while adequate in vocabulary and demonstrating grammatical complexity, notably deviates from the assigned topic, undermining its task response. Teachers consistently identified the essay’s tendency to stray into discussions. According to T1, “the essay is off-topic, deviating from the question.” T2 added that “some information is irrelevant to the prompts and there is no conclusion drawn.” Cohesion is another area of concern; the essay is marked by a lack of clear progression and insufficient use of cohesive devices, which, coupled with a blend of formal and informal tones, leads to a disjointed narrative (T1, T4, T6). Despite the sophisticated use of vocabulary, the actual lexical choices were not inconsistent and did not fully align with the content. T1 said that “there is a mix of formal and informal tones”, while T3 commented that “this seems like a script for an oral debate.” In terms of grammar, the essay exhibits several errors in forms and mechanics, as described by T4 and T6.

Table 1 Teachers’ assigned scores to ChatGPT- and human-produced essays.   

<html><body><table><tr><td>Essays</td><td>T1</td><td>T2</td><td>T3</td><td>T4</td><td>T5</td><td>T6</td></tr><tr><td>Essay 1</td><td>6.0</td><td>6.5</td><td>7.0</td><td>8.0</td><td>7.5</td><td>7.5</td></tr><tr><td>Essay 2</td><td>7.0</td><td>6.5</td><td>7.5</td><td>7.5</td><td>6.5</td><td>7.0</td></tr><tr><td>Essay 3</td><td>8.5</td><td>9.0</td><td>8.0</td><td>9.0</td><td>7.0</td><td>9.0</td></tr><tr><td>Essay 4</td><td>5.5</td><td>5.5</td><td>7.5</td><td>5.0</td><td>6.5</td><td>6.5</td></tr><tr><td>Essay 5</td><td>8.5</td><td>9.0</td><td>8.5</td><td>8.0</td><td>8.5</td><td>9.0</td></tr></table></body></html>

Note: Native English-speaking college student (Essay 1), Vietnamese English teacher (Essay 2), native English-speaking lecturer (Essay 3), Vietnamese college student (Essay 4), and ChatGPT 3.5 (Essay 5)

Table 2 Teachers’ Judgment if AI-generated or Not.   

<html><body><table><tr><td>Essays</td><td>T1</td><td>T2</td><td>T3</td><td>T4</td><td>T5</td><td>T6</td></tr><tr><td>Essay 1</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td></tr><tr><td>Essay 2</td><td> Human</td><td>Human</td><td>Human</td><td> Human</td><td>Human</td><td> Human</td></tr><tr><td>Essay 3</td><td>AI</td><td>Human</td><td>Human</td><td>AI</td><td>Human</td><td>AI</td></tr><tr><td>Essay 4</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td></tr><tr><td>Essay 5</td><td>Human</td><td>AI</td><td>AI</td><td>AI</td><td>AI</td><td>AI</td></tr></table></body></html>

Note: Native English-speaking college student (Essay 1), Vietnamese English teacher (Essay 2), native English-speaking lecturer (Essay 3), Vietnamese college student (Essay 4), and ChatGPT 3.5 (Essay 5)

# 4.2. How L2 writing teachers differentiate between ChatGPT-generated and human-produced texts

Table 2 presents an intriguing look at the ability of teachers to discern between human-generated and AI-generated essays. Essays 1, 2, and 4 were unanimously and accurately identified as human-written by all six teacher evaluators. This uniformity suggests that these essays likely presented qualities or characteristics that teachers commonly associate with human writing. Specifically, teachers noted features such as a mix of simple and complex sentence structures, some cohesion issues, and occasional grammatical errors typical of ESL learners. Additionally, these essays displayed an overall progression and logical organization, but sometimes lacked clarity in certain arguments or included off-topic elements, especially in Essay 4, which drifted from the prompt. Vocabulary use varied, with some essays containing repetitive phrases or informal tones, and a few included unique phrases that seemed more characteristic of a human touch than AI-generated language. Conversely, essay 5, created by ChatGPT, was predominantly identified as AI-generated by teachers T2 through T6, with varying degrees of certainty. This could indicate that despite the sophisticated linguistic capabilities of ChatGPT, it may still produce discernible patterns or lack certain nuances that human writing typically exhibits, which these teachers were able to detect. Essay 3, penned by an NS lecturer, produced a divided response, where it was considered AI-generated by three teachers (T1, T4, T6) and human-generated by the other three (T2, T3, T5). T1 was particularly certain about the generative AI authorship, rating their certainty at the highest level, similar to T4 and T5. This split judgment suggests that the advanced writing of the NS lecturer resembles the structured and formal nature typically associated with AI-generated text. However, it is also possible that the generative AI’s writing, designed to replicate sophisticated human styles through extensive training on human feedback, mirrors the complexity and coherence of professional-level writing. This alignment between the lecturer’s and generative AI’s writing styles highlights the challenge of differentiating advanced human writing from AI-generated text when both exhibit high levels of linguistic sophistication.

When asked about the specific features that made them consider certain essays AI-generated, teachers’ responses offer a nuanced understanding of the distinguishing features between ChatGPT-generated texts and those penned by humans. They identified several lexical, syntactic, semantic, discourse, and stylistic features that suggest a difference in how generative AI and humans approach essay writing. In terms of lexical features, T2 remarked that ChatGPT’s writing “seems to have more formal, academic words with precise meanings, and higher lexical density like complex noun phrases.” T1 added that “there is a consistent use of advanced vocabulary, occasionally using overly intricate words or phrases to convey meaning” in essays they considered as AI-generated. Human writers, on the other hand, tend to employ a mix of formal and colloquial language peppered with familiar phrases indicative of traditional essay writing.

From a syntactic and grammatical perspective, T2 and T3 reported that ChatGPT-generated essays are characterized by complex sentence structures and a wide range of grammar, typically error-free and with a sophistication that mirrors NS. T1 echoes this observation, saying that “ChatGPT-generated text seldom includes simple sentences, showcasing a preference for more complex structures” and does not commit “grammar mistakes, even the most minor ones.” Human essays, conversely, contained the occasional grammatical slip-ups typical of both L2 learners and NS, suggesting a more authentic and less polished use of language. Meanwhile, T5 noticed that “Vietnamese students tend to use familiar structures such as $\mathrm { \Phi _ { i t + } }$ be $^ +$ adj’, ‘ $\mathrm { m a k e } + s \mathrm { b } / s \mathrm { t h } +$ adj’”.

Semantically and content-wise, ChatGPT’s essays effectively convey meaning but sometimes miss the mark on appropriacy or clarity (T2), and may lack the depth found in human writing (T3, T5). T1 added that “ChatGPT-generated content may lack the depth found in human-authored pieces, often presenting arguments in a more generalized manner.” T3 further added that the AI-generated text “lack of ability to analyse and evaluate ideas or give specific examples” and “often discusses both sides of an issue or sometimes one side of an issue and leave it to the user to make evaluative comments.” Human writing, as noted, often reflects a cultural thinking style, particularly when non-native English speakers (NNS) are authoring the text, integrating their native cognitive patterns into English expressions (T5). Despite these notable differences, T2 felt it was “hard to tell which one is AI-generated or human-produced” because both types of texts “convey meaning in a clear manner.”

Discourse features reveal that ChatGPT tends to construct essays with consistent paragraph structures and a clear single idea per paragraph, adhering to conventional writing patterns (T1). On one hand, T2 noted that human-produced texts display greater diversity in discourse elements, with a more varied use of markers and connectors and a tendency to weave multiple ideas within a single paragraph. T5 added that “each paragraph of human-produced text may contain two or more ideas while ChatGPT-generated texts has clear structure – one main idea/one paragraph.”

Stylistically, teachers noted that ChatGPT’s content often exhibited a neutral tone, precision, and formal structure, aligning with features commonly associated with academic writing. According to T1, AI-generated text does not exhibit a distinctive writer’s voice and “tends to lack sentiment elements.” T2 also observed that there is very little evidence of emotional tone in the AI-generated text. Meanwhile, T2 felt that the essay is written by human through its use of “figurative language, literary devices, and rhetorical tech niques.” While this neutrality may distinguish AI-generated texts from more expressive forms of human writing that include sentiment and stylistic flourishes, it can also resemble academic writing produced by humans, where such neutrality, precision, and formal structure are standard. This similarity adds to the challenge of distinguishing AI-generated content from high-quality human-authored academic texts.

Aside from the text features, T1, T4, T5, and T6 shared that another strategy that helped them distinguish AI-generated texts from human-produced texts was their exposure to ChatGPT itself. T1 and T5 added that their previous engagement with AI tools, such as Grammarly, also contributed to their ability to spot AI-generated texts. Meanwhile, other teachers pointed their ability to spot AIgenerated text to their experience in teaching and assessing EFL students’ writing (T2, T4, T5, T6) and their knowledge of stu dents’ writing proficiency (T3, T4, T5). For instance, T2 said that he was “quite familiar with the types of mistakes and errors learners usually make…. Once that is absent, it poses an immediate question to me of whether the essay is written by a learner with native-like proficiency or by ChatGPT-generated.”

# 5. Discussion

Our study probed into six L2 writing teachers’ assessment and detection of AI-generated and human-produced essays as well as their strategies in distinguishing these two types of texts. Data revealed that they often awarded higher scores to essays by an NS lecturer and those generated by ChatGPT. This information provided empirical support for claims that the default writing proficiency level of ChatGPT is equal to that of an advanced professional writer (Barrot, 2023). The implications are profound because advanced generative AI cannot only imitate the complexity of native English prose but can do so to a degree that is indistinguishable from high-caliber human writing in the eyes of teachers. Essays by less experienced NNS writers received lower scores. This highlights a perceived deficit in their writing quality compared to the two high-scored texts. Teachers were able to identify most of the human-produced texts (essays 1, 2, and 4) with consistency, but there was less consensus when distinguishing between advanced human-authored texts and those produced by generative AI, with prior exposure to AI influencing their ability to detect AI-generated writing. Comments on essays revealed teachers’ reliance on specific text features, exposure to AI tools, experience in teaching and assessing essays, and knowledge of students to guide their assessments. This approach emphasizes the complexity and multidimensionality of evaluating AI-generated content in L2 writing contexts.

Anchored on sense-making theory, the results of our study suggest that teachers construct their understanding and judgments of student texts through a complex interplay of previous knowledge, contextual cues, and the content of the essays themselves (Evans, 2007; Spillane & Anderson, 2014; Weick, 1996). The teachers’ evaluations point to an interpretative process where the high scores awarded to the essays by the NS lecturer and ChatGPT indicate a constructed equivalence between technical proficiency and high writing quality. This can be seen as an outcome of teachers making sense of the texts in light of their expectations and experiences with native English writing, where a strong command of language and argumentation is highly valued. Furthermore, the study results reveal how teachers apply their cognitive frameworks to distinguish between human and AI-generated texts. The consistent identification of essays 1, 2, and 4 as human-produced demonstrates teachers’ reliance on certain writing characteristics associated with human authors, such as error patterns and stylistic idiosyncrasies, to make sense of the essays’ origins. Conversely, the divided opinion on essay 3 and the predominant identification of essay 5 as AI-generated reflect the complexity of this sense-making process, where certain qualities can blur the lines between human and machine output. The specific features highlighted by teachers, such as lexical choice and sentence structure, further demonstrate the retrospective aspect of sense-making, as teachers reflect on the patterns they have come to associate with generative AI versus human writing. This retrospective reflection shapes their expectations and guides their assessment behavior. For instance, the presence of complex noun phrases and a high degree of grammatical accuracy led some teachers to attribute essays to ChatGPT, as these features fit within their constructed understanding of AI-generated writing. Moreover, the teachers’ sense-making is evidently a socially constructed process. This is evident in the fact that teachers with prior exposure to ChatGPT or similar AI tools, like Grammarly, demonstrated a different evaluative approach to those without such exposure.

With reference to previous studies, our study resonates with the findings of Herbold et al. (2023), who noted that teachers scored AI-generated essays as higher quality based solely on textual assessment. The challenge of identifying AI-generated content, as highlighted in Alexander et al. (2023), is exemplified in our study’s findings where essays produced by ChatGPT received some of the highest scores. This correlation suggests that prior exposure to generative AI can play a role in the recognition of such content, supporting the idea that experience with AI-generated texts can enhance detection capabilities. However, this notion is contrasted by Waltzer et al. (2023), whose broader demographic study did not find a significant link between prior exposure to ChatGPT and the accuracy of identifying AI-generated texts. Their results, alongside ours, illuminate the difficulties teachers face when the quality of writing is high, underpinning the need for refined assessment skills in discerning the origins of sophisticated texts. Furthermore, the work of Fleckenstein et al. (2024) aligns with our study by showing the challenges in AI text detection, even among experienced teachers. Both studies reveal that without explicit markers or an in-depth examination of comments and assessments, distinguishing between generative AI and human writing remains a nuanced task. The discrepancies in scoring, where AI-generated essays sometimes receive higher scores than human-produced ones, underscore the need for more research and training for teachers to ensure fair and accurate assessment practices in an L2 writing landscape increasingly influenced by generative AI.

Our study enriches the current understanding of the capacity of L2 writing teachers to assess and differentiate between AIgenerated and human-produced texts. While previous studies have laid the groundwork for recognizing the infiltration of genera tive AI in L2 writing contexts, our research provides additional layers of insight by examining not only the scores but also the qual itative feedback and the strategies teachers employed during writing assessment. By doing so, we shed light on why teachers might score AI-generated texts higher and the factors that contribute to these assessments. Our study also adds nuance to how generative AI can mimic high-quality writing by suggesting that the detection of AI-generated texts by teachers may not solely be a matter of recognizing the absence of errors or the presence of complex structures but also involves appreciating the depth of argument and articulation of ideas, voice and emotional depth, areas where generative AI might still lag behind human writers. One insightful aspect of our findings is the indication that there is not a clear-cut method for distinguishing between human and generative AI writing, especially as AI writing tools become more advanced. The detection of AI-generated text by some teachers, particularly for essay 5, may point to the presence of certain tell-tale linguistic or structural signatures that generative AI tends to produce. Alternatively, it may reflect a bias or an expectation of what AI-written text should look like.

# 6. Conclusion

This research explores how L2 writing instructors distinguish between essays produced by ChatGPT and human authors. It involved an analysis of both the scores and comments given by teachers and looked into their methods for identifying the source of the essays. The study found that essays by an NS lecturer and ChatGPT were rated highly, suggesting ChatGPT can replicate advanced English writing. In contrast, essays by less experienced L2 writers and NNS scored lower, which makes them distinguishable from an AIgenerated text. The study also revealed that teachers could not consistently identify the AI-generated text, particularly those writ ten by proficient writers. Their past engagement with AI writing tools, their familiarity with common L2 learner errors, and their exposure to native and non-native English writing seemed to have influenced their ability to discern AI-generated content from humanproduced texts. This points to the nuanced challenges in distinguishing between human and generative AI writing. Overall, the research underscores the complexities in evaluating AI-generated texts in L2 writing contexts.

Our findings have several implications. Practically speaking, our findings highlight the need to train teachers in recognizing and understanding the characteristics of AI-produced texts, which could involve adding specific modules on digital literacy to teacher education programs. Teachers might also need to be trained to look for new indicators of generative AI authorship or to consider a wider range of qualitative factors beyond mere linguistic and argumentative competency. The results of our study could stimulate debate and policy-making about the ethical implications of generative AI in education, particularly concerning fairness, transparency, and the development of student writing skills in an increasingly digital and AI-integrated landscape. Since the line between AI assistance and academic dishonesty could become blurred, educational institutions may also need to establish clear guidelines about using generative AI tools in coursework. From a theoretical standpoint, these findings challenge existing frameworks of writing assessment by introducing a non-human writer into the mix. They call for an expansion of composition theory to include the capabilities and limitations of generative AI. The traditional process of writing as solely a human endeavor is being redefined, urging scholars to reconsider concepts of authorship and originality in light of generative AI’s growing role. Methodologically, the study highlights the importance of including qualitative data, such as teacher comments, in research on writing assessment. Quantitative scoring alone might not capture the full picture; in-depth analysis of teacher feedback provides richer insight into the evaluative process. This may encourage future research to adopt mixed-methods approaches for a comprehensive evaluation of writing, taking into account both the end product and the cognitive processes behind the scoring.

While our study provided important insights into L2 writing teachers’ assessment practices within a generative AI environment, certain limitations should be acknowledged. Notably, we used GPT-3.5 to generate AI texts in this study. While GPT-3.5 was a capable model, AI language tools have since advanced, and newer models exhibit enhanced linguistic abilities and content generation. Thus, findings related to teacher accuracy in distinguishing between AI-generated and human-generated texts may not fully translate to interactions with more recent generative AI models that display increasingly sophisticated writing styles and nuanced responses. Nevertheless, our methodology and approach offer a valuable foundation for future research involving more advanced AI technologies. We recommend that future studies adopt a broader scope by including a more comprehensive set of texts written by diverse students and incorporating multiple cutting-edge AI tools. This expanded approach would deepen insights into how teachers assess writing across different AI technologies and student demographics, addressing the rapidly evolving nature of AI and the challenges it presents in education. As generative AI evolves rapidly, research must focus on developing robust methods and theoretical frameworks that can be adapted to examine the efficacy of varied and more advanced models, given that educational research will inevitably lag behind the pace of AI development. Additionally, limitations related to sample size and the specific demographic of Vietnamese L2 writing teachers may constrain the generalizability of our findings. A broader range of educational contexts would yield more widely applicable insights. The exclusive focus on teachers also leaves out the perspective of students, whose experiences with AI-generated texts are equally significant. Future research should aim for more extensive and diverse educational settings to enhance the generalizability of the findings. The methodology could be expanded beyond case studies to incorporate experimental or correlational designs that measure the reliability of teachers’ assessments. Furthermore, longitudinal studies could examine how continued expo sure to AI writing tools affects teachers’ assessment capabilities over time. It is also essential to investigate the psychological and cognitive processes involved in assessing AI-generated texts, which might reveal inherent biases or patterns in human judgment. Finally, the study underscores the need for interdisciplinary research, combining insights from education, linguistics, cognitive sci ence, and computer science, to develop more sophisticated methods for detecting and understanding AI’s role in educational contexts.

# CRediT authorship contribution statement

Jessie Barrot: Writing – original draft, Methodology, Formal analysis, Conceptualization. Loc Nguyen: Writing – review & editing, Resources, Methodology, Investigation, Funding acquisition, Formal analysis, Data curation, Conceptualization.

# Acknowledgements

This research is funded by the University of Economics Ho Chi Minh City, Vietnam.

# Data Availability

Data will be made available on request.

# References

Agustini, N. P. O. (2023). Examining the role of ChatGPT as a learning tool in promoting students’ English language learning autonomy relevant to Kurikulum Merdeka Belajar. Edukasia: Jurnal Pendidikan Dan Pembelajaran, 4(2), 921–934.   
Alexander, K., Savvidou, C., & Alexander, C. (2023). Who wrote this essay? Detecting AI-generated writing in second language education in higher education. Teaching English with Technology, 23(2), 25–43.   
Barrot, J. S. (2023). Using ChatGPT for second language writing: Pitfalls and potentials. Assessing Writing, 57, 100745. https://doi.org/10.1016/j.asw.2023.100745   
Barrot, J. S. (2024). ChatGPT as a language learning tool: An emerging technology report. Technology, Knowledge and Learning, 29(2), 1151–1156. https://doi.org/ 10.1007/s10758-023-09711-4   
Bergen, N., & Labont´e, R. (2020). “Everything is perfect, and we have no problems”: Detecting and limiting social desirability bias in qualitative research. Qualitative Health Research, 30(5), 783–792.   
Bin-Hady, W. R. A., Al-Kadi, A., Hazaea, A., & Ali, J. K. M. (2023). Exploring the dimensions of ChatGPT in English language learning: A global perspective. Library Hi Tech. https://doi.org/10.1108/LHT-05-2023-0200   
Cotton, D. R., Cotton, P. A., & Shipway, J. R. (2024). Chatting and cheating: Ensuring academic integrity in the era of ChatGPT. Innovations in Education and Teaching International, 61(2), 228–239. https://doi.org/10.1080/14703297.2023.2190148   
Creswell, J. W., & Creswell, J. D. (2018). Research design: Qualitative, quantitative, and mixed methods approaches (fifth edition). Sage.   
Currie, G. (2023). Academic integrity and artificial intelligence: Is ChatGPT hype, hero or heresy? Seminars in Nuclear Medicine, 53(5), 719–730.   
Evans, A. E. (2007). School leaders and their sensemaking about race and demographic change. Educational Administration Quarterly, 43(2), 159–188.   
Fleckenstein, J., Meyer, J., Jansen, T., Keller, S. D., Koller, ¨ O., & Moller, ¨ J. (2024). Do teachers spot AI? Evaluating the detectability of AI-generated texts among student essays. Computers and Education: Artificial Intelligence, 6, Article 100209.   
Herbold, S., Hautli-Janisz, A., Heuer, U., Kikteva, Z., & Trautsch, A. (2023). A large-scale comparison of human-written versus ChatGPT-generated essays. Scientific Reports, 13(1), 18617.   
Hockly, N. (2023). Artificial intelligence in English language teaching: The good, the bad and the ugly. RELC Journal. https://doi.org/10.1177/003368822311685   
Hua, S., Jin, S., & Jiang, S. (2024). The limitations and ethical considerations of ChatGPT. Data Intelligence, 6(1), 201–239. https://doi.org/10.1162/dint_a_00243   
Huallpa, J. J. (2023). Exploring the ethical considerations of using Chat GPT in university education. Periodicals of Engineering and Natural Sciences, 11(4), 105–115.   
Huang, W., Hew, K., & Fryer, L. (2022). Chatbots for language learning—Are they really useful? A systematic review of chatbot-supported language learning. Journal of Computer Assisted Learning, 38(1), 237–257. https://doi.org/10.1111/jcal.12610   
Huang, X., Zou, D., Cheng, G., Chen, X., & Xie, H. (2023). Trends, research issues and applications of artificial intelligence in language education. Educational Technology Society, 26(1), 112–131.   
Imran, M., & Almusharraf, N. (2023). Analyzing the role of ChatGPT as a writing assistant at higher education level: A systematic review of the literature. Contemporary Educational Technology, 15(4), ep464.   
Lo, C. K. (2023). What is the impact of ChatGPT on education? A rapid review of the literature. Education Sciences, 13(4), 410. https://doi.org/10.3390/ educsci13040410   
Lund, B. D., Wang, T., Mannuru, N. R., Nie, B., Shimray, S., & Wang, Z. (2023). ChatGPT and a new academic reality: Artificial Intelligence-written research papers and the ethics of the large language models in scholarly publishing. Journal of the Association for Information Science and Technology, 74(5), 570–581.   
Miles, M. B., Huberman, A. M., & Saldana, J. (2019). Qualitative data analysis (fourth ed.). Sage Publications.   
Ng, D. T. K., Tan, C. W., & Leung, J. K. L. (2024). Empowering student self-regulated learning and science education through ChatGPT: A pioneering pilot study. British Journal of Educational Technology. https://doi.org/10.1111/bjet.13454   
OpenAI. (2024). ChatGPT: Optimizing language models for dialogue. 〈https://openai.com/blog/chatgpt/〉.   
Rashid, Y., Rashid, A., Warraich, M. A., Sabir, S. S., & Waseem, A. (2019). Case study method: A step-by-step guide for business researchers. International Journal of Qualitative Methods, 18, 1–13.   
Ray, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key challenges, bias. Ethics, limitations and Future scope Internet of Things and CyberPhysical Systems, 3, 121–154.   
Schade, M. (2023). How ChatGPT and our language models are developed. 〈https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-language-models-aredeveloped〉.   
Shaikh, S., Yayilgan, S. Y., Klimova, B., & Pikhart, M. (2023). Assessing the usability of chatgpt for formal English language learning. European Journal of Investigation in Health, Psychology and Education, 13(9), 1937–1960. https://doi.org/10.3390/ejihpe13090140   
Song, C., & Song, Y. (2023). Enhancing academic writing skills and motivation: Assessing the efficacy of ChatGPT in AI-assisted language learning for EFL students. Frontiers in Psychology, 14, 1260843.   
Spillane, J. P., & Anderson, L. (2014). The architecture of anticipation and novices’ emerging understandings of the principal position: Occupational sense making at the intersection of individual, organization, and institution. Teachers College Record, 116(7), 1–42.   
Spillane, J. P., Reiser, B. J., & Reimer, T. (2002). Policy implementation and cognition: Reframing and refocusing implementation research. Review of Educational Research, 72(3), 387–431.   
Su, Y., Lin, Y., & Lai, C. (2023). Collaborating with ChatGPT in argumentative writing classrooms. Assessing Writing, 57, Article 100752. https://doi.org/10.1016/j. asw.2023.100752   
Waltzer, T., Cox, R. L., & Heyman, G. D. (2023). Testing the ability of teachers and students to differentiate between essays generated by ChatGPT and high school students. Human Behavior and Emerging Technologies, 2023, 1923981.   
Weick, K. E. (1996). Sensemaking in organizations. Sage.   
Yan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonado, R., Chen, G., … & Gaˇsevi´c, D. (2024). Practical and ethical challenges of large language models in education: A systematic scoping review. British Journal of Educational Technology, 55(1), 90–112.   
Loc Nguyen is an academic researcher and lecturer from University of Economics, Ho Chi Minh City, Vietnam. His research interest includes teaching pronunciation,   
corrective feedback, and L2 writing.