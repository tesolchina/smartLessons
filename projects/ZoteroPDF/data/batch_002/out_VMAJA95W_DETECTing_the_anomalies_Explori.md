# DETECTing the anomalies: Exploring implications of qualitative research in identifying AI-generated text for AI-assisted composition instruction

Ali Gariba,\*, Tina A. Coffelt'

a Program in Writing and Communication, Rice University, USA b Department of English, Iowa State University, USA

# ARTICLEINFO

# ABSTRACT

Keywords:   
ChatGPT   
Post-plagiarism era   
Qualitative research   
The DETECT approach   
Composition instruction

This study inspires new pedagogical practices with evolving technological innovations. One example of such innovation is the emergence of artificial intelligence (AI) in education. The potential impact of generative AI, such as ChatGPT, on composition education has caused concerns among educators due to its human-like writing capabilities. However, there is no escape from ChatGPT-generated text, which is influenced by prompt engineering. Such engineering can lead to underlying issues in content, prompting a pedagogical opportunity for understanding human-written and AI-generated texts. Since, to this date, there is no single reliable source for identifying AI-generated text, this study introduces a pedagogical approach, DETECT, with two major goals: (1) explore the nuances that differentiate human expression from the algorithmic patterns and tendencies of generative AI writing and (2) inspire ways to integrate generative AI in composition instruction in a post-plagiarism era. Using exploratory practice research, this article examines DETECT in composition instruction of 32 students during Fall 2023 and Spring 2024. The findings showed that using DETEcT improved students' confidence in analyzing humanwritten and AI-generated texts, which enhanced their recognition and appreciation of their own writing voice. The study concludes with pedagogical implications for the possibilities of generative AI in writing instruction.

# Introduction

The recent artifial intelligence (Al) breakthrough by OpenAl in November 2022 has left many in awe. Being perplexed by the Chat Generative Pre-trained Transformer's (ChatGPT) generation of AI text, many feared the emergence of this AI would be the end of education, human composition, liteacy, and creativity (Kumar, 2023; Pavlik, 2023; Tate et al., 2023). It is t0 early to know if ChatGPT-generated text may have benefits or will be detrimental, but is current form can lack human capabilities (Mitrovic t al., 2023) due to prompt engineering.

Such shortcomings in ChatGPT's responses led to the creation of AI detectors, where text complexity, perplexity, and burstiness act as identifiers of AI-generated text. However, these tools currently lack sophistication and can be misleading. The current study promotes a more positive relationship between the composition instructors and students whil navigating the AI detection scare-not as a "detector' but as one that helps students engage with all kinds f texts. While we acknowledge the complicated history of plagiarism detectors in composition pedagogy, we also realize that such detection has always had challenges in establishing a fixed definition of plagiarism (Price, 2002). With the advent of generative AI, these chllenges are further complicated as generative AI tools can easily produce text that may blur the lines between original and plagiarized content unles composition instructors shift from detecting students' AI use to fostering a pedagogy that embraces AI's role in the composition classroom (Hubbard, 2023).

To address this need, the current paper (1) describes recent AI detectors, (2) analyzes Al detectors limitations in a post-plagiarism era, (3) overviews the relevance of qualitative research methods, natural language processing, and the Turing Test in introducing a pedagogical practice that promotes using Al-generated text detection techniques for pedagogical practices rather than policing the use of generative AI to integrate this approach in a composition class and (4) provides implications for the teaching of composition, drawing on data collected from 32 first-year composition students in Fall 2023 and Spring 2024.

# State-of-the-Art AI-generated text detectors

In response to the emergence of ChatGPT, developers created Al-text detectors to identify machine-generated text. Generally, AItext detectors, also known as text cassifiers, use natural anguage processing (NL techniques and deep learning algorithms to analyze the content f texs and asign it to predefined categories (Bayer et al., 2023; Kim, 2014). Such techniques involve preprocessing, fine tuning, word embedding and flering, training, and deployment (Hastie et al., 2009; Wolf et al., 2020) to perform text analysis by deconstructing text into smaler chunks that determine predictive AI-generated text (Gluska, 2023). In other words, an AI machine dissects a written piece of text and applie algorithms, which analyze language patterns to determine if a text was AI-generated.

Three AI-text detectors are popular, namely GPTZero, The Clssifier, and a new feature in the existing plagiarism checker, Turnitin. Edward Tian designed and released GPTZero, an AI-text identifier on January 2, 2023. This tol attempts to mitigate false claims and academic dishonesty about authorship of texts gnerated with ChatGPT. Tians classfier proceses text or tex fle of 250 characters or greater and displays likel or les likel to indicate I-generated r uman-written. Geros reults also provide a brif interpretation of text with scores fr perplexity and burstiness. The perplexity score measures the randomnes of a text and the burstiness score measures the variation in perplexity. In the same domain, OpenAI relased The Clasifer, an AI-text detector, on January 31, 2023.The Classifier examnes text int with a minmmof 000 chacter and prid score: vey ulikel, uikely, ulr f itis ossbly, or likely Al-generated to distinguish between ChatGPT-written and human-written text. Turnitin, a familiar plagiarism detector among educators, released another Al-detecion tool on April 4, 2023. Turnitin's Al-detector analyzes student submissions by examining structure, style, and language paterns to differentiat between human-writen and AI-generated text. Educators receive a report indicating the probability of generative AI involvement in the text.

GPTZero, Classifier, and Turntin tools provide hope' for educators, and their results are not full eliable for Al-text identification (Ali et al., 2023; OpenAI, 2023). Tian openly acknowledges the inaccuracy of his GPTZero tool. Additionall, OpenAl (February 2023) admits that its text Classifier tool cannot be trusted to reliably detect all I-written text. For instance, the Classfier struggled with nuanced stinctions in tone flow, and stlisti ements, of unabl differntiate betwe man and -written texs that closely mimic natural writing styles. In fact, OpenAI discontinued their AI text classifier tol in July 2023 (OpenAlI, 2023). Similarly, only seven weeks ater the release of Turnitin's AI-detector, their CP0 Chechitell (2023) published a report that highlighted two major issues with this tool.

Chechitelli's report states that in cases where less than $2 0 \%$ of AI writing was detected in a document, there was a higher incidence of false positives. This inconsistency can lead to misinterpretation and may unfairl penalize student. The report adds that educators found it challenging to interpret the generative AI writing merics provided by Turnitin. This ifficult can result in misunderstandings and potential misjudgments in evaluating student work. For instance, Vanderbilt University (2023), among others, disabled Turnitin due to the $1 \ \%$ false positive rate with AI-detection. Coley (2023), a Vanderbilt representative, explained that \*Vanderbilt submitted 75,000 papers to Turnitin in 2022. f this Al detection tool was available then, around 750student papers could have bee incorrectly labeled as having some of it written by Ar (para.2).It is therefore evident that even a small false positive rate can have significant consequences on students by wrongfully categorizing their work as AI-generated.

Al detectors, overall, were found to exhibit bias against non-native English writers (Liang et al., 2023). Liang et al.' study revealed that detectors consistently misclassified non-native English writing samples as Al-generated, while native writing samples were accuratel identified. This finding suggests that AI detectors may unintentionall penalize writers with constrained linguistic expressions, revisiting the composition field's years-long complicated relationship with plagiarism-detection software.

Plagiarism software and composition instruction: The post-plagiarism era

In the field of composition instruction, the post-plagiarism era marks a pedagogical shift. Educators move towards acritical view of the conventional use of plagiarism software has become a necessity (edington et al., 2024; Cummings e al., 2024), highlighting the limitations of plagiarism detection tools and the need for a more nuanced approach to teaching writing. Composition education is centered on improving writers' critical thinking and expression with clarity and originality. The historical dependence on plagiarism-detection software has oten been at odds with these goals (Dehnart, 199; Howard, 2007; Vie, 2013), especially in the ea of generative AI. An era where Vie's (2013) wish that \*we might someday realize a utopian setting in which plagiarism detection services are demed entirely unnecessary (p. 5) has become  realit. Plagiarism-detction tools can be useful for identfying verbatim copying but les o at discerning the subtle skill of arumentation, synthesis and author voice development, but with generative AI's capabilities, such detection tools are useless (Anson, 2022). In this domain, Wang et al. (2024) examined the response of 100 top US universities to generative AI and found that none considered AI detection tools reliable, nor did they encourage their faculty to use these tools for detecting plagiarism or maintaining academic integrity.

With the emergence of generative Al in education, there is a growing recognition that these toos must evolve from policing plagiarism to facilitating the development of writing skill that ris easy duplication lkhatat et al., 2023; Weber-Wulff t al., 2023; Sadasivan et al., 2024). This evolution necestates a focus on AI-assted intruction in composition education, namely developing critical digitl litracies (Li, 2024; Anderson, 2023). While plagiarism software programs have played avital role in the past, they are no longer suitable toos for upholding academic integrity in the current time. Instead, the shift is moving towards using these tools to practically teach students how to engage with AI-generated text ethically and creatively (Vetter et al., 2024), as is the focus of the current study.

Through this lens, plagiarism-detection software and generative Al can serve as a starting point for instruction that solidifies the human element in writing (Anson, 2022; Pig, 2024). While definitivedifferentiation between human-written and AI-generated text is difficult to maintain (lkhatat e l., 2023; Gao et al., 2022), the process tself can inspire an understanding of witing, its human/AI traces (emotional nuance, capriciousness, and critcal thinking), and its structural components (linguistic functions and markers, rhetorical move, and organizational patterns), leading to a pedagogical opportunit for developing analytical skill and appreciating the complexity of students' own human language expression. For instance, to compare the capabilities of AI detectors and human reviewers in distinguishing between AIgenerated and human-written research abstracts, Gao e al. (2022) found that both humans and AI-output detectors exhibited a relatively similar ability in identifying Al-generated abstracts, although they were not entirely good discriminators. In another similar study, Uchendu et al. (2021, p. 8) found that \*humans detect machine-generated texts at chance level. However, ths study did not provide the participants with any systematic guidelines for AI-detection, which could be beneficial for humans to improve their text discrimination (Berber Sardinha, 2024). The AI detection toos in these studies were created to identify writing that was produced by an AI text generator, but what role could prompt engineering play in the detection process?

# The role of prompt engineering in AI-Text detection

Prompt engineering requires careful attention, communication skils, and expertise (Liu et a., 2023). Poorly constructed prompts lead to inaccurate, undesired, or rlatively similar outputs. These deficiencies can be attributed to a number f factors, including (1) "ChatGPT does not perform well with various prompt templates" (Cao et al., 2023, p. 10) and (2) a significant portion of users may have limited experience or expertise in prompt engineering (White et l., 2023). In this domain, Cotton et l. (2023, p. 9) added that multiple students all using similar prompts for their coursework would generate very similar material, and this is very easy for a human to detect." Furthermore, novice prompt engineers may struggle with the nuances of prompt engineering, such as providing clear instructions, providing appropriate context, or efectively guiding ChatGPT towards the desired output. This lack of expertise can result in text that lacks coherence and relevance and ultimately fails to address the intended writing objectives. Therefore, when students use generic prompts, the output shows a lack of originality and individualit, repetition, and minimal variation in their academic writing (e.g, Krigel e al., 2023). These qualities make it easier for instructors to detect the use of AI text generators.

While prompt enginering can be impactful (Krause, 2023), generative AI models, like ChatGPT, may stil suffer from chutzpah, hallucination, bias, or absence of emotions. Such limitations can result in a mechanical and stif stance in ChatGPT's writing style, subjecting it to deeper interpretation. These inherent limitations of ChatGPT responses and their dependency on prompt crafing contribute to a more formulaic, robotic, and rote writing style. Recognizing such limitations is crucial in teaching students to analyze texts (Cummings et al., 2024), leading students to distinguish good writing from bad. So, instead of focusing on the detection of Al-generated texts s a policing effrt developing students' abilities to discern and appreciate the unique qualitie of human writing, author voice, writing originalit, and personal expression can foster new dcational paradigm (Gari et l., 2024) spcificall in the composition classroom, which is an ideal setting for the current study's initiative. A practical startig point for enhancing students awareness of the differences between Al-generated and human-written text is through analytical approaches, such as qualitative research. The following section briefl overviews the relevance of qualitative research to Al-text detection, which inspires the framing of the proposed pedagogical approach in this study.

# The relevance of qualitative research to AI-text detection

Qualitative research can pave the way for deeper understanding of human traces, or lack of them, in the written form of communication (Thabet & Zghal, 2013). In qualitative analysis, researchers work on the identification of repetitive patterns in data through thematic analysis (Braun & Clarke, 2021). Thematic analysis delves deeper into the interpretation of themes and depicts the human voice, opinion, and emotional nuance, as well as stance (bias, sexism, or racism) in tex, allof which the current AI detectors have failed to identify. Unlike Al detectors, thematic analysis emphasizes the importance of analyst refeivity, meaning that analysts reflect on their own assumptions, biases, and values when conducting the analysis (Cutclife & McKenna, 199). This approach helps ensure that the themes that emerge are not just a reflection of an analyst's preconceptions of texts. Therefore, the development of a framework to guide humans' interpretation of a text that is potentially Al-generated would be beneficial to instructors of writing.

This paper proposes a new approach to the analysis of AI-generated tex stemming from a qualitative stance. The proposed research steps suggest ways to analyze the characterisics fa text for composition instruction. Several methodological implications are outlined below to optimize the use of thematic analysis within qualitative research for human-written and Al-generated text interpretation.

he DETECT approach: Learning from human-written and AI-generated texts

This section introduces an analytical method to develop a pedagogical approach that engages students in closely analyzing human written and Al-generated texts. Developing systematic text analysis steps aligns with Berber Sardinha (2024) finding that while \*the AI-generated output might appear convincingly human, subjecting the texts to a rigorous inguistic analysis employing robust measue..reveas significant disarities (p. 9). Since the proposed techniques in this study can be subjective, itis essential tonote that such steps are not meant to police the use of generative Al in writing but rather to foster an environment where students can understand the structure of common AI-generated texts and appreciate the unique power of their own human writing capabilities.

These steps are inspired by qualitative research's thematic analysis, the Alan Turing Test's Imitation Game (1950), and AI de tectors' natural language processing (NLP) techniques. Following such a systematic interpretation of a text, analysts engage in determining potential Al-generated text, employing pre-existing text knowledge, tracing unusual pattrns/language expression, examining emotions and opinions in text, checking trustworthiness and potential bias/halucination of information, and finally tallying drawn conclusions by examining analytical evidence (DETECT), see as illustrated in Fig. 1.

These DETECT steps are tailored for a practical text analysis approach, making such practices doable for instructors to foster the teaching of composition. As in the case with both thematic analysis (Braun & Clarke, 2006; Creswell & Poth, 2018), Alan Turing's (1950) dialogue initiation in the Turing Test, and AI detectors' training, the first step typically begins with text scanning, familiarization, and topic modeling (seestep 1). Applied to the identification of potentially Al-generated text, exploration of the text genre, purpose, audience, and context (see step 2) occurs using pre-existing knowledge of text types, which relates to the concept of indis tinguishability in Turing's game. Step 3 includes the selection of significant statements,coding, and categorizing of these statements. Such steps are represented in pinpointing unusual patterns and labeling emotions and opinions, making these components an integral part of the content analys and emotional nuance mining se steps 3 and 4). Both steps 3 and 4 lead to deeper thinking about the underlying unctions of text; that i, what seems to be unusual about the functionality of the messages being conveyed through the text and whether the text channels any emotions and opinions, which aligns with Turing's examination of machine thought. Inspired by the AI detectors inaccurate supply of information and Turing's blievability in machine behavior, the next step examines the accuracy of information (see step 5) to determine its trustworthines, bias, and hallucination. Rather than thematizing the emerging categories, this paper proposes achieving the end goal, which is drawing conclusions on the AI- or human-status of a given text (se step 6) based on the findings in steps 1, 2, 3, 4, and 5. See Table 1 below for a more detailed description of each step.

These six steps set the stage to systematize the interpretation of texts in composition instruction by equipping students and in structors with the necessary digital literacy kils to develop an understanding of rhetorical choices in Al-generated or human-written texts, which was a missing element in Gao et al.'s (2022) study. To examine the impact of AI-assted writing and generative AI on business communication, Cardon et al. (2023) proposed that the concept of AI literacy is a necessary skil for students and professionals. Cardon et al. idntfied four sets of capabilitie for Al literacy: application, authentict, accountability, and agency. While Cardon et al. emphasized application literacy, their pproach appears to lack the "understanding of AI-generated content" component. This raises a question: How could students efectively identify and address ssues in AI-generated text to add the human element in authenticity if they lacked a deep understanding of AI-generated text?

![](img/d837e5b12090933c53daab8c275827c6e36bc9ad1a55ea07fafcb2f711011393.jpg)  
Fig. 1. The DETECT steps.

<html><body><table><tr><td colspan="2">Detailed description of the DETECT steps.d</td></tr><tr><td>Steps 1. Topic Modeling: Determine a potential Al-text by conducting topic</td><td>Description Inspired by NLP text processing and Alan Turing&#x27;s dialogue initiation by the</td></tr><tr><td> modeling.</td><td>interrogator, topic modeling begins by first identifying a text that is potentially AI- generated. Readers, instructors, or students (analysts) may become suspicious when a text either exhibits some artificial characteristics or lacks the essence of human expression. These anomalies can emerge to the human eye. Reading through a text, keywords or certain expressions can grab a reader&#x27;s attention as indicative of AI- generated text. For example, a robotic presentation of arguments, systemic tone, lack of capriciousness, bluntness, or even exaggerated expressions can be some of the many possible qualities that suggest artificial authorship. This step can occur naturally while</td></tr><tr><td>genre, purpose, audience, and context of the text for deeper analysis.</td><td>identifying the genre, purpose, audience, and context of the target piece of writing. Such information determines the type of pre-existing knowledge and text-type familiarity that needs to be used to analyze a text with distinguishable/ indistinguishable characteristics, to understand its general and specific purpose, to consider its expected audience, and to determine the context. In this way, an analyst has an expectation of what a human-written text should contain, which is an opportunity to solidify students understanding of human-written text. The goal of textual analysis is to uncover underlying insights, themes, patterns, and understanding about the text that may not be immediately apparent to the casual reader. Without</td></tr><tr><td>3. Content Analysis: Trace and pinpoint unusual patterns by looking for language signs of artificiality.</td><td>To trace machine thought as Turing did, an analyst conducts content analysis for unusual patterns within the text and then pinpoints those unusual patterns to assist in gathering adequate evidence that can be indicative of AI-generated text. This step can include looking for inconsistencies, repetitive structures, or unnatural language in the writing style. These inconsistencies may include abrupt shifts in tone, voice, or vocabulary, as well as a lack of coherence. Additionally, it is crucial to pay attention to the use of repetitive structures, which may be a common feature of AI-generated text. Unnatural language can include the use of uncommon words, complex sentence structures, lack of varied punctuation, or over-use of contractions, which may not be typical of human-written text. Such classification can identify writing styles that are typical of humans or machines. For example, humans may notice that AI-generated</td></tr><tr><td>4. Emotion &amp; Opinion Mining: Examine and label presence/absence of emotions and opinions in the text..</td><td>step 1. Drawing on affective computing, identify the presence/lack of emotions and opinions, which are typically associated with human expression. To conduct emotion and opinion mining, the analyst may use sentiment analysis techniques to analyze the text&#x27;s tone and subjective values in the text. In NLP, sentiment analysis algorithms can detect positive, negative, or neutral emotions expressed in the text, as well as opinions or attitudes towards specific entities, topics, or events. The presence of extreme or inconsistent emotions or opinions may be indicative of AI-generated text, while a more nuanced and varied emotional and opinionated expression may suggest human</td></tr><tr><td>5. Accuracy Analysis: Check the truthfulness, potential bias, and hallucination in the text information and sources..</td><td>on intuition of what constitutes artificial or human composition. Reflecting Turing&#x27;s concern with believability in machine behavior, analysts check information accuracy and potential bias and hallucination to evaluate the trustworthiness and credibility of text. Analysts examine the quality and truthfulness of the information in text to ensure that it is reliable and verifiable. Accuracy analysis supplements the identification of AI-generated text since human-written text contains verifiable information. However, spotting inaccurate or fake information/sources in text can be a strong indication of AI-generated text. Fact-checking and verifying the</td></tr><tr><td>6. Draw Conclusions: Tally the drawn conclusions by putting all the pieces of analytical evidence together, leading to informed conclusions.</td><td>information in the text with other reliable sources is a necessary step since generative AI tools currently suffer from information hallucination and inaccuracy. Think about all the above analyses and appraise the quality of the text to determine whether it is human-written or AI-generated. The analysis could include closely examining the categories of the previously pinpointed unusual patterns, labeled emotions and opinions, comparing the characteristics of those patterns to determine the likelihood of being AI-generated or human-written, looking for cues that lead to a human essence in text, analyzing how the target text is situated within a specific context, using prior knowledge of the text content to assess its trustworthiness, and</td></tr></table></body></html>

The current study

The body of literature n generative AI use in the composition clssroom is growing (Sharples, 2022; Liang et al., 2023). However, to date, no study has introduced an AI-related teaching approach, such as DETEc, that focuses n solidifying students understanding of their own writing, which is a key step forward in teaching methodologies amidst the rise of generative AI tools, like ChatGPT, in academic settings (Liang et al., 2023).

This study ills the gap by offring an examination of how the DETEcT approach can be integrated into writig instruction to enhance students' abilit to understand AI-generated content and cultivate their writing skill. Further, this research moves beyond identification of AI-generated text to foster a deeper comprehension of the nuances and complexities of AI-generated writing. This understanding of Al-generated text is essential in modern composition instruction as it aids students in strengthening their own authentic witing bilities (Ng et al., 2021). The current rearch is timely giventhe incesing visbility of gerative l in ducational contexts and the necessity to develop strategies that promote authentic student authorship and expression (Warschauer et al., 2023) This study, therefore, asks the following question: What are student' perceptions of the DETEC approach in the context of the composition classroom?

# Research methodology

# Research design

This study employs a qualitative, exploratory practice (EP) research approach (Allwright, 2003; Kostoulas & Lammerer, 2015; Hanks, 2017) to understand how students perceive and reflect upon using the DETECT approach within their learning environment. The researchers secured IRB approval to conduct this study. Towards the end of both semesters, thefirt author, who was also the class instructor in both semesters, sought students' consent to use their reflections on DETECT as part of a research study afer assignment submission and asessment to ensure that students provided authentic responses and to avoid social bias.All students involved agreed to participate. Pseudonyms are used to maintain student confidentiality.

Research setting and participants

In this EP study, atargeted sampling method was applied within the context of a topic-based First-Year Writing Intensive Seminar (FWIS) entitled Innovations in Educational Technology at Rice University in Texas, USA. Each seminar included a group of sixen undergraduate students during Fall 2023 and Spring 2024, majoring in diverse fields of study ranging from sTEM to social sciences to humanities. These students $( n = 3 2 )$ were selected based on their enrollment in the seminar and their expressed interest in exploring the intersection of education and technology through a writing-intensive curriculum.

The course introduced and trained students on a basic level of proficiency with generative AI and other digital tools. Throughout the semester, students engaged in various writing assignments and projects that focused on critically analyzing and articulating the nuances of educational technology generally and AI advancements particularly.

Materials and procedures

The data for this study was collected through two distinct ssignments integrated into the FWI course. The first author introduced the DETECT approach as part of an assignment (See Appendix I in the supplementary marterials, where the students read this approach, iscssed it in class and wrote reflections on their percetions before classdiscussions and in their assinment submission.

The first source of data came from the Comparative Portfolio asignment, which engaged students in a reflective exploration of the differences between human-written and I-generated texts using a three-step proces: (1) writing a personal human-written narrative essay about a technology experience, (2) prompting ChatGPT to generate a personal narrative essay on the same topic, and (3) composing a comparative essay to analyze and contrast these two pieces. Students used the DETECT approach to understand AIgenerated content and its implications for writing practices (See Appendix I in the supplementary material). The task emphasized an evaluation of AI's capabilities in academic writing, which covered aspects such as coherence, creativit, and authorship. This assinment aso required students to identify their peers human-written and AI-generated essays using the DETECT approach (See Appendix II in the supplementary materials), which prompted the students to conduct a detailed text analysis.

The second source of data came from a reflective assgnment. In this task, students were encouraged to share their personal experiences and thoughts about using the DETECT approach in thir comparative essays and peer reviews (See Appendix II in the supplementary materials).

Both assignments were naturally embedded into the course curriculum, ensuring that the data collection was unobtrusive and relevant to the students learning. The students had a regular learning experience while providing authentic, contextual data for this study.

# Data analysis

From an exploratory practice research perspective, using thematic analysis provided an appropriate methodological choice (Braun & Clarke, 2006) beause it allows for the idenification and analysi of emerging themes and pattens in the data, facilitating a deeper understanding of students' experiences and perceptions (Bryman, 2016). By employing thematic analysis, the study can systematically interpret the rich, detailed information gathered from the students essays and reflections, aligning with the exploratory and qual. tative nature of the research (Creswell & Creswell, 2017).

To examine students' perceptions and reflections, the data analysis followed a number of recursive steps guided by Creswell and Poth (2018). These stes included data familiarization, wherein allstudent esays and reflections were read thoroughly to gain a deep understanding of the content. This was followed by generating codes, where key concepts and ideas related to the DETECT approach were identified and taged. Next, categories were formed based on relevant codes. Subsequently, emerging themes were identified from the categories. The researchers achieved saturation in their thematic analysis by repetitively applying these steps until no additional themes emerged (Glaser & Strauss, 1967).

# Results and discussion

The research question asked about students' perceptions of the DETECT approach in a writing classroom. Two major themes emerged from the students' reflctions: (1) AI versus human writing and (2) DETECT's educational potential. Representative excerpts from he students assignments are includd ovefy the theme and catries. llof the stdent rflected n differentiating between generative AI and human writing through the lens of the DETECT approach. This theme included two subthemes: practicalit of the DETECT approach and detecting the anomalies.

All 32 students found the DETECT approach practical in their text analysis. James, for instance, appreciated the methodology, noting that  fel i was very easy to apply the DETEc method..And it helped me analyze the text from the perspective of different audiences, which allowed for a critical evaluation of \*the cliche writing and basic structure of the AI essay." He added:

For most AI-generated texts, the \*Textual Analysis" and \*Content Analysis" steps would likely be enough to illuminate an AIgenerated text quite quickly. Nevertheles, the addition of the other steps is crucial for identfying more human-like AI works. I believe that the most significant portion of the DETECT method would be the detection of \*unusual patterns" and the presence of "abrupt shift in tone, voice, or vocabulary, as these are easy characteristics to view with the untrained eye. Additionall, the "Accuracy Analysis" step within the paper was extremely helpful and is especially important for individuals writing about a topic they are not familiar with. As such, this step helps inform the readers of AI text that they must check the claims made by AI generative technology.

The practicality of the DETECT aproach lies in its systematic traceability of identifiable elements in writing, which makes this process sraghtforward. In the same vein, Noor stated, I also liked using the DETCT method because it was good to have asystematic approach to analyzing AI, or suspected AI content. This finding both aligns with Berber Sardinha (2024) emphasis on using a sytematic analysis approach and supports the claim that systematic guidelines can better asst i understanding Al-generated text, which was a missing element in other research studies (Gao et al., 2022; Uchendu et al., 2021).

The identifiable elements include a wide range of linguistic anomalies, which lead to the second subtheme: detecting the anomalies For example, Wen stressed that she found Al-generated content problematic due to its "excessve descriptive language and metaphorical overuse." Rita shared that \*AI never alludes to any niche information that its audience would know, which focuses on more general topics and avoids assuming prior expertise in a particular subject. Surprisingly, the majority of students $\left( n = 2 7 \right)$ discussed the identification of "unusual patterns" and "abrupt shifs in tone, voice, or vocabulary' as particularly noticeable findings in their analyses. Generative Al often struggles with maintaining consistent tone and voice throughout a text,especially with basic prompting techniques. However, these students' analytil skill in detction reflected great interest in the crft of writing as wllas confidence in maintaining their own author voice, which relates to Anson's (2022) and Pigg's (2024) emphasis on using generative AI to solidify students composition learning. Similarl, with reference to ChatGPT-generated text, Kevin added that, \*nobody tals like that." Kevin noted that,

Humans writ so differently that even with all the data on writers that ChatGPT has, its essay is just too much of the perfect cliche so much so that it is actually not and can easily be spotted as one that was not written by a human.

Kevin further explained that human writing exhibits "natural flow in its content. Although coherent, the ideas are mixed up altogether, which is much like the random and unpredictable nature of a human mind." Like, Kevin, Adam stressed that action vers come across as excessive and flowery to experienced readers. For example inthe narrative, AI uses the words leap, thrust and 'teetering'"

The notion of capriciousness in writing exemplifies one aspect of the diverse range of human abilites.In the same domain, Cory described ChatGPT-generated text as, choppy and formulaic. The language doesn't capture the nuances of human emotions and connectios..AI-generate text may sem to be technically accurate but fen lacks the human touch that makes a narraive resonate." Related to both subthemes, Shireen observed, \*through my first read-through of the Al-generated esay, many unnatural words stuck out. A major tip-of that signifies AI-generation..like bound by a shared love for music' and discovering the artist within me."

Shireen's reflections are consistent with Kevin's remarks on AI's uncommon use of language. She further aded that such unnatural satterns are obvious since,

People would probably cringe hearing this AI's expressve language aloud because it is so cliche. These extra corny and sentimental phrases are uncommon in people's usual vocabulary, making the writing stand out as forced and unnatural. While

AI is trying to expressreal human emotion, its overuse of cliches makes it sound disingenuous and fake, as no human would seriously write or say similar things.

Shireen's observations efectively highlight an example of the stark contrasts in language use between Al and human writing, which can create excellent learning opportunities for the teaching of human expression. In the same vein, Amy shared that:

Rhetorical questions have become a staple of many Al-generated works, particularly when multiple appear sequentially. For example, Would I be able to forge meaningful connections in this digital realm? Could I navigate the intricacies of coursework without the reassuring presence of a teacher by my side?' Not only is the presence of questions unusual but the questions themselves are phrased awkwardly.

These insights bring to light the educational value of the DETEcT approach. By training students to notice and analyze such liscrepancies, instructors can leverage DETET as a tool for distingushing AI and human writing mainly to deepen students appreciation for authentic, nuanced expression in writing.

The second major theme, the pedagogical potential of DETECT, emerged from two subthemes: (1) integration of AI in education and (2) ethical considerations for using Al. Mary provided a detailed justification of how the DETECT approach can contribute to teaching and learning with AI stating that:

The human minds hold much more complexities than what can be coded into an AI detection tool. Thus, after using the methodology of DETECT, it does seem like there is greater opportunity for accurate detection when people are given the tols to do so. Such that the best solution based on the experience of this exercise, in using the DETECT approach, is that it would be best to expose both teachers and students to Al and allof its capabilities and lack thereof by employing them with the skill necessary to identify the differences between it and human-made works. That way, all of us can learn how to use it ethically and take advantage of its benefits as an aid in education and not as a shortcut.

In line with Bedington et al. (2024), Cummings et al. (2024), and Veter et al.'s (2024) implications to engage students with AI-generatedtext rathr than policing its use, Mar reconized the challenges in deloping long-term, ffective l-detection strategies and highlights the limitations of current methods as this technology evolves. She pointed out the unique complexities of human cognition that surpass Al's capabilities, which supports the potential of the DETECT approach in enhancing educational practices. Mary advocated for exposing both instructors and students to Al, teaching them to recognize is strengths and limitations. In other words, she proposed supplementing human intelligence augmentation (IA) through AI. Mary envisioned a future where generative AI is used ethicall in education, not as a shortcut, but as a valuable tol to augment learning. These insights emphasize the mportancef integrating generative Al into theeducational processthoughtfull, ensuring it complements rather than relaces the crtical thinking and creativity inherent in human intelligence. Similarly, Isabella added that:

If someone is using ChatGPT to help them write assignments, it i good to know what to change in order to \*tweak' the essay to sound more human written. For example, if the student notice, using the DETECT method, that the AI essay uses unnatural anguage, they could manuall edit the language so that i is les likely to be flagged" as an AI generated essy. Obviously, this has the potential to be damaging if students are using the DETECT method in order to \*fly under the radar', so to speak. However, if students are genuinely using ChatGPT as an asstant and just want to put ChatGPT's ideas into more human-like language, the DETECT method could also assist students in this way as well.

Isabell's points cover two important aspects of integrating generative Al in education: the ethical use of generative AI astance and the development of critical editing sills. She acknowledged the potential misuse of generative Al tols like ChatGPT in academic settings, where students might employ the DETECT method to conceal AI-generated work. On the other hand, she also highlighted a positive application of DETECT, where students use generative AI responsibly as alearning asstant/consultant. By using DETECT to refine AI-generated content to sound more human-like, students learn to discern between AI and human writing styles and enhance their own writing skill through editing and understanding language nuances. This dual perspective highlights the importance of guiding students to use generative AI and DETECT ethically to foster academic integrity and skill development.

The second subtheme emerged within the context of integrating generative Al in writing. On this note, Li stressed that AI can be a great tool for inspiration, and by ethiclly citing the Al, users can mprove the way they decribe experiences in the world, and present themselves more authenticlly." Shiren added that when combining human capabilities and generative AI together, I can serve as an important tool to help people refine and enhance their own writing. We should not shy away from AI and instead use it ethically to learn to expand our knowledge and capabilities as writers." Li and Shireen emphasized the importance of embracing the ethical use of AI, not just for asstance but also as a means to expand human knowledge and capabilitiesas writers. Their points highlighted the potential of generative AI in contributing constructively to the writing process, encouraging is use as a complement to human creativity and intellect rather than as a replacement. Additionally, Nicole remarked that,

As AI technology becomes rapidly integrated into every aspect of the modern world, it is important for modern consumers to learn how to critically understand, analyze, and draw conclusions about the ethical usage of Al in written texts. This skill is especially crucial in order t regulate bias and misinformation in a world that i very prone to such pitalls The DETECT method will no doubt prove to be an invaluable tool and resource to create informed populace.

Nicole stressed the importance of modern consumers learning to critically understand, analyze, and draw conclusions about the ethical use of generative Al in written texts, given the rapid integration of AI technology into every aspect of the modern world. She emphasized the crucial nature of this sill for regulating bias and misinformation. She added that the DETECT method could be an invaluable resource for such practies, indicating a ture where the ability to idtify lgenrate contet i not only tchnic kill but a vital part of digital literacy and responsible information handling. In the same domain, Yen-Yu explained that:

While it s tempting to deploy AI-generated content detection tols in educational setigs so that the influences of Al models are plausibly removed, educators must realize these tools cannot and will never stop students completely from using text-generation AI models. However, by using more reliable, accurate, and suitable teaching methods for ethical AI use, only then instructors can resolve the issues of generative AI models more effectively and educationally.

Aligned with current literature (Elkhatat et al., 2023; Weber-Wulff et al., 2023; Sadasivan et al., 2024; Warschauer et al., 2023) emphasizing the incorporation of generative Al in composition instruction instead of attempting to detect it, Yen-Yu's explanation provided a realistic perspective on the use of AI-generated content detection tools in educational environments. Yen-Yu cautioned that instructors would never be able to completely prevent students from using generative AI for text generation. Yen-Yu emphasized the necessit of training students on ethical generative AI practices. Yen-Yu suggested that the solution to the challenges posed by generative AI models lies not in absolute prevention but in education and ethical guidance, which coresponds with Dwivedi e al.'s (2023) perspective on the ethical use of generative AI.

The contribution of the DETECT steps is raising awarenes and stimulating critical thinking about the ethical use of generative AI in writing. In line with Warschauer et al. (2023), by highlighting the potential accuracy risks and limitations of AI-generated text, instructors can encourage more careful consideration of the ethical and social implications of this technology, which can lead to the evolution of human thinking and writing skill. This evolution, in turn, can contribute to the development of more responsible and effective uses of generative AI particularly in the instruction of composition and education, generally.

# Pedagogical implications for AI-assisted composition instruction

The identification of AI-generated text through the DETECT steps has pedagogical implications for transforming the instruction of writing, namely developing composition students' critical digital literacies (Li, 2024; Anderson, 2023). By systematically examining Al-generated texts, compositio instructors can elevate their learners writing capabilities to beter understand the possbilities and limitations of AI-generated texts. The way generative AI mimics human skills with a superficial version of written communication poses new challenges due to its powerful performance, but these challenges bring new opportunities, which lead to the conclusion that the implications of AI-assted composition instruction are futuristic and enormous. However, integrating AI toos into composition instruction has been an ongoing area of research before the advent of generative Al, particularly in automated written corrective feedback (AwCF) ools such as Grammarly (Li, 2021; Rall,2018; Ranalli & amashita, 2022; Ranall Hegelheimer, 2022) as wel as automated genre analysis through the Research Writing Tutor (RWT) (otos et al., 2015; Cotos & Pendar, 2016). These tols provide insights into enhancing written ccuracy, rhetorical writing, and understanding how automated feedack can influence writing styles.

Yet, while AwCF tools are relevant and powerful, generative AI tools go beyond automated feedback to augment, rather than replace, human intellectual and creative capabilite. For example, using generative AI as guided by DETECT, students can recognize the power of their own human voice in writing, which can be an eye-opening experience to enhance their ethical use of generative AI. More specifically, one way the DETECT steps can guide students generative AI use involves training students to identify in. consistencies in AI-generated text, which has the potential to hone students' analytical skill to think deely about text structure and rhetorical functions and moves.

Using the DETECT approach as a scaffold to enhance instructors' pedagogical practices can prepare students for a new era of writing. Instead of banning generative AI tools, instructors are encouraged to integrate DETECT to encourage a proactive approach towards Al-generated content. This approach empowers students to identify Al-generated content and to learn to maintain their authentic voice in writing in a post-plagiarism era. By analyzing Al-generated and human texts, students can beter understand different writing styles, structures and content development strategies. Such comparative analysis between AI and human writing can lead to improved writing skills as students learn to adopt best practices and avoid common pitallseen in Al-generated text.

The DETECT approach's overarching implications align with Cardon et al.'s (2023) perspectives on AI's capabilities for communication. Using DETECT, composition learners can apply their existing writing knowledge to spot anomalies in AI-generated text, enhance human composition by appreciating authenticity, learn about the importance of information verification, and maintain agency in their learning processes, thus avoiding overreliance on generative AI tools.

# Concluding remarks

This paper adresses the growing concern about academic integrity among composition educators with the emergence of ChatGPT and the anticipated use of additional generative AI tools in the composition classoom. The DETECT approach provides writing instructors with a new teaching approach to enhance their students' recognition of their own author voice and improve their understanding of texts, whether they are generated by AI or written by humans. This approach has the potential to highlight the quality of AIgenerated text in various contexts, which is crucial for understanding its impact on education generall and the teaching and learning of writing specificall. As there is no escape from generative Al's growing presence in education, detecting anomalies in AI-generated text alows students to critically engage with content, which empowers thir writing and analytical skill. Restricting students use of generative AI is not the solution; it is, in fact, a missed opportunity.

# Declaration of generative AI and AI-assisted technologies in the writing process

During the preparation of this work the author(s) used ChatGPT in order to improve language and readability, with caution. Afer using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.

# CRediT authorship contribution statement

Ali Garib: Writing - review & editing, Writing - original draft, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Tina A. Coffelt: Writing - review & editing, Writing - original draft, Methodology, Conceptualization.

# Declaration of competing interest

The authors declare there is no conflict of interests.

# Data availability

The data that has been used is confidential.

# Acknowledgement

We would like to thank the Editor, Dr. Kristine Blair, and the anonymous reviewers for their guidance, insightful comments, and constructive feedback throughout the multiple rounds of revising this manuscript. We are also thankful to the students who agreed to let us use their reflections and assignment submissions.

# Supplementary materials

Supplementary material associated with this article can be found, inthe online version, at doi:10.1016/j.compcom.2024.102869.

# References

Ali,  B  ,3tl  c r . Article 2023020513. https://doi.org/10.20944/preprints202302.0513.v1   
Allwright, D. (2003). Exploratory practice Rthinking practitioner rearch in language teaching. Lnguage Teaching Rearch, 7(2),113-141.   
Anen . 23P    s. i6 7...106/. compcom.2023.102778   
Anon  ).    1) 79./. eric.ed.gov/fulltext/EJ1361686.pdf.   
Ber, apprc for ong a sht x cier. ol J of hn g d c, 141, 15-150. hp/.g017/s13042-02 01553-3   
gton ,    024 h ih  ns ai from faculty and students. Computers and Compositin, 71, Article 102833. https:/doi.org/10.1016/j.compcom.2024.102833   
Berer  024  ri sic   ./o. 10.1016/j.acorp.2023.100083 2159676X.2019.1628806   
Bryman, A. (2016). Social research methods (5th ed.). Oxford University Press.   
Cao ., Li,  e,   , . 023)  , d o  fr    r.  ren. arXiv.2304.08191.   
age. Business and Professional Communication Quarterly, 0(0). https://doi.org/10.1177/23294906231176517 detection-update-from-turnitins-chief-product-officer. 2023/08/16/guidance-on-ai-detection-and-why-were-disabling-turnitins-ai-detector/   
Cotos  r,  (2016) cicton  rhorica tins for  fck o, 3(1) 2-116. h/.g/0.1558/. v33i1.27047   
tos  f   215. rg a oe/ i min   t o  . Journal of English for Academic Purposes, 19, 52-72. https://doi.org/10.1016/j.jeap.2015.05.004   
Coton, o y, .n  t t.   g International, 61(2), 228-239. https://doi.org/10.1080/14703297.2023.2190148   
Creswell J W., & Crewell, J D. (2017).Rch desig ulittive qnttive, and med methos aproches (5th ed.) AGE Publications.   
Creswel, ., & h, .. 2018. Qtiv inquy nd h: hon n iv che (4th esan a   ublicatin.   
,   , 02i  rst- i y ai f f ons   k or t future. Computers and Composition, 71, Article 102827. https://doi.org/10.1016/j.compcom.2024.102827   
Ctlfe  c  (9ii the iit  qtie  iin:  k.  of  30 374380. https://doi.org/10.1046/j.1365-2648.1999.01090.x   
Dehnart, A. (1999. The web's plagiarism police. Salon.com. Retrieved from htp://www.salon.com/tech/feature/199/06/14/plagiarism.   
i,       , e, ,    23.  h   etie n pe,  a i  tie.  f I, 71, Article 102642.   
tat, d  me 223). h fi    dt o  dftii man ae. International Journal for Educational Integrity, 19(1), 17. https://doi.org/10.1007/s40979-023-00140-5   
Gao, CA d,  Mr, , r,  h ,    P,  22.a cic abtrts   ic inl dor lor d i  . , 2022-1122.5210   
Gar   2a       . he generative AI in the communication classroom. IGI Global. https://www.igi-global.com/chapter/chatgpt/339066.   
Glaser, B. G., & Strauss, A. L. (1967). The discovery of grounded theory: Strategies for qualitative research. Aldine Transaction.   
Glusa . 023. h rion g drdng wtin dtio wr.d Pgn fro/gw--tng detection-works/ (Accessed 5 March 2023).   
Hanks, J. (2017). Exploratory practice in language teaching: Puzzling about principles and practices. Springer.   
Hastie  h,  n  209. Te eo ss g g c ci . 1-758 r. ol. .   
Howard, R M. (2007). Understanding Intenet plagiaris. omputers and Compositin, 24(1, 3-15.htp:/doi.0rg/10.1016/j.compcom.206.12.005   
bad  23  th /0/ pedagogical-dangers-of-ai-detectors-for-the-teaching-of-writing/.   
Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv Preprint. arXiv.1408.5882.   
Kostoulas, Lammerer, A (2015). Classroom-based research: Materials created for ELT Connect 2015. Liebiggasse: University of Graz.   
Krigel, S., Ostermaier, A., & Uhl, M. (2023). The moral authority of ChatGPT. arXiv. Preprint. arXiv.2301.07098 (Accessed 4 March 2023).   
Krause, D. (2023). Proper generative AI prompting for financial analysis. sRN Preprint. 10.2139/ssn.4453664.   
mar i    c Reports, 9(1), 24-30. https://doi.org/10.5530/bems.9.1.5   
Li . (021) oding th onnce intion to use omated witing eltion ong hie L r. AG pen, 11(4) 113 htps:/oi.org/ 10.1177/21582440211060782   
Li,  2n t ie r  l 022. https://doi.org/10.1016/j.compcom.2024.102825   
Liang, W., Yuksekgonul, M., Mao, Y, Wu, E., & Zou, J. (2023). GPT detectors are biased against non-native English writer.Pattens, 4(7)   
Li, P    ,   h    2023) P t       m gae processing. ACM Computing Surveys, 55(9), 1-35.   
Mitrovic, , di,  ub 023). Ch  at ad xp. a disio f mh mdl o i ht ChatGPT-generated text. arXiv Preprint:2301.13852. arXiv.2301.13852.   
g . h  .1).iry  i ce, , Article 100041. https://doi.org/10.1016/j.caeai.2021.100041 written-text/   
Pavli, . (02.lti it i th icati  ti ia nl im d mi lism Mass Communication Educator, 78(1), 84-93. https://doi.org/10.1177/10776958221149577   
Pigg, . 2024). h win th ChP: divembd prtice rmrk s anitio, 71, Aicle 1030. /oi.org 10.1016/j.compcom.2024.102830   
Price M(200)  h ing rism  ic g. on nin, 541) -115.h/i./10.2307/ 1512103   
Rnali, J, e,  2). tin  h  i wig eo n, 2, 14, 105/73473.   
Ranalli,   2. ti t  i.y, 26 (1), 1-25. http://hdl.handle.net/10125/73465.   
Ranalli, J 018 n tie k  e n ts   t  ng 1 5367. /. org/10.1080/09588221.2018.1428994   
Sadasivan, mar, , Balabnin, ., Wag, W.  i, . 2024). Ca text e elaly de ai v.23.1156.   
Shaet ac.uk/impactofsocialsciences/2022/05/17/new-ai-toos-that-can-write-student-essas-requir-educators-to-rethink-teaching-and-asssment.   
Tat d    t  . 10.35542/osf.io/4mec3.   
abet,013 h     us Studies, 2013, 1-20. https://doi.org/10.5171/2013.895661   
Turing, A.M. (1950). Computing machinery and inelligence. Mind; A Quarterly Review of Psychology and Philosoph, 59(236), 433-460.   
1) Ascition r ioLsis  2021. 001-2016). P a can ublic mber sion for ttiol Lnsic.   
and t (02  6.e dctin d  erei   ie or 20, 23, r /ww. vanderbilt.edu/brightspace/2023/08/16/guidance-on-ai-detection-and-why-were-disabling-turnitins-ai-detector/.   
Ver,     it, and composing with ChatGPT. Computers and Composition, 71, Article 102831. https://doi.org/10.1016/j.compcom.2024.102831   
Vie, . (2013). A edggy f restance toward plarism detion techloe. omputers and Composition, 301), 3-15. htps:/do.org/10.1016/. ompcom.2013.01.002   
Wan, H, Dang, , Wu, Z & Mac, . 2024). Sing ChG throgh uniersties olie, rres and line. Aiv Prrint aiv.2312.05235.   
arer   r J  ish aa second or foreign language. SsRN Preprint. https:/doi.org/10.2139/ssrn.4404380.   
er-f  a a  e  b, ., l n  223t f dti s orIgenerated text. International Journal for Educational Integrity, 19(1), 26. https://doi.org/10.1007/s40979-023-00146-z   
hite, J    t      . 23  pr engineering with ChatGPT. arXiv Preprint. arXiv.2302.11382.   
lf   ,  i     ,u C. ca L gr sh  (20. nae tnfmer st-te- nrl ngg prosi  Prn v.910.0377.

Appl  t ts and language teacher TPACK and professional development and practices.

Tiafe i nt o is t tit. nh  r tht i an aBA fro t r tt.  i  c h   n  o k  o nication, and privacy management. She was a Fulbright scholar to Uzbekistan in 2020.