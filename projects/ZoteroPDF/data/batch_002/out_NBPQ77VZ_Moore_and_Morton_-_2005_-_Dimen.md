# Dimensions of difference: a comparison of university writing and IELTS writing

Tim Moorea,\*, Janne Mortonb

a Language and Learning Unit, Faculty of Arts, Monash University, Clayton, Vic. 3168, Australia b Language and Learning Skills Unit, University of Melbourne, Parkville, Vic., Australia

# Abstract

A challenge for many EAP teachers working on pre-sessional programs is to find ways to reconcile the dual aims of preparing students for university study and for the IELTS test. The study described here seeks to provide some guidance on this issue through an analysis of the type of writing required in the two domains. We compared the standard IELTS Task 2 rubric with a corpus of 155 assignment tasks collected at two Australian universities and found that whilst IELTS writing bears some resemblance to the predominant genre of university study-the essay, there are also some very important difference. Our findings suggest that the type of writing the test elicits may have more in common with certain public nonacademic genres, and thus should not be thought of as an appropriate model for university writing. We conclude that it is probably best to deal with test preparation and the broader EAP writing curriculum within separate programs.

$©$ 2004 Elsevier Ltd. All rights reserved.

Keywords: English language tests; Academic writing; Assignment tasks; Discourse analysis

With apologies to Marx—a spectre haunts the world of EAP, this is the spectre of the academic English test. To speak of such tests as ‘spectres’ is perhaps over-dramatising matters, but the image nevertheless seems to us an appropriate one. This is because, for those involved in the preparing of students for university study on pre-sessional EAP programs, the IELTS and TOEFL tests (and equivalents) often cast a long shadow over one’s deliberations and efforts.

The last 20 years has seen a burgeoning in the number of second language students studying at anglophone universities. This has led to a huge expansion in pre-sessional English language programs designed to prepare students for the linguistic and academic demands of their courses. A parallel development, equally significant, has been the rise of tests used for university selection. The IELTS test, for example, has experienced an increase in the number of test sittings from about 20,000 a year after its inception in 1989 to approximately 220,000 sittings in 2001 (UCLES, 2002a). The challenge that many EAP teachers and program designers face is that their programs have unavoidably a dual aim— a primary one of preparing students for their prospective studies, but an important secondary aim of helping students to get through the test.

A good deal of debate is carried out in language centres around the world about how these two curriculum aims are best reconciled. Deakin (1997) in a survey of EAP programs in Australia notes a number of models in use, including:

(i) integrated models, where IELTS preparation is incorporated into EAP courses;   
(ii) separated models, where IELTS preparation courses and EAP courses are handled in separately timetabled modules;   
(iii) exclusive models, where IELTS preparation courses only are run, with no option of EAP for students.

Deakin (1997) goes on to point out that decisions about which curriculum model is employed are often based on certain practical, administrative constraints within centres— the length of time given over to the program, the teaching resources available, the expertise of staff and so on. But what if these decisions are based on educational considerations alone, as ideally they should be? What meaningful connections can be drawn between the two curricula? How can the short-term aim of test preparation be accommodated within a broader EAP framework? And finally which curriculum model of those mentioned above seems to be the optimal one? The study described here sought to provide some preliminary answers to these questions by considering an important subcurriculum within these two teaching contexts—writing instruction. Specifically, we were interested to see how the main writing task on the IELTS test-for which students often receive a good deal of preparation-might relate to the writing demands they will encounter subsequently in their university study.

# 1. Previous studies of university writing tasks

Our study which compared the rubrics of the IELTS essay task (Task 2) with those of a corpus of assessment tasks collected at two Australian universities fits with an active strand of EAP research into the nature of university writing tasks. Liz Hamp-Lyons (1991) points out the importance of developing a comprehensive understanding of such tasks as a way to improve the validity of writing tasks used in testing contexts. As she notes, “we do know something about task validity, and we could do more than we have typically done so far to build greater task validity into the prompts on our writing assessments” (1991, p. 95).

Perhaps the most well-known empirical work on university assignments is Horowitz (1986) groundbreaking study of 50 tasks set for students at one US university. A major challenge in this study, as it is in all research of this kind, was to come up with a way of systematically classifying the data. The schema Horowitz opted for was based mainly on the type of information sources to be used by students in the preparation of the task, generating the following categories: (i) summary/reaction to reading; (ii) annotated bibliography; (iii) report on a specific participatory experience; (iv) connection of theory and data; (v) case study; (vi) synthesis of multiple sources and (vii) research project. Horowitz’s main finding was that almost all tasks collected involved research processes of some kind, requiring students to collect and reorganise some specified source material. Very few tasks, by contrast, required students to draw exclusively on personal experience. Horowitz draws on this finding in a later paper (1991) that speculates on general differences between university writing tasks and those that students face in test situations. He notes, for example, that a fundamental difference seems to lie ‘in the relation of the text produced to other texts’, such that in writing test assessments—what Horowitz calls ‘content-free writing’—the ‘crucial aspects’ of citation and textual plurality are ‘entirely absent’ (p. 74).

Where Horowitz’ and other subsequent studies (Braine, 1995; Canesco & Byrd, 1989; Carson, Chase, Gibson, & Hargrove, 1992) have been conducted mainly for EAP needs analysis and course design purposes, there is one task study we are aware of that has been conducted specifically for test-related purposes. This is Hale et al. (1996) large-scale study involving the collection and analysis of tasks from 162 undergraduate and post-graduate courses at eight US universities. This study was conducted for test validation purposes, specifically for the development of future versions of the TOEFL test. The classification system used was considerably more elaborate than that used by Horowitz (1986) involving six broad categories of difference: locus of task (i.e. in class; out of class); prescribed length of product; genre; cognitive demands; rhetorical task; pattern of exposition. Under each of these dimensions was a set of sub-categories. For example, included under cognitive demands were the following: retrieve/organise and apply/analyse/synthesise. Whilst this study is impressive in scope, its findings are a little inconclusive. This is due in part to the complexity of the classification scheme used, as well as the difficulty the researchers had in achieving interjudge agreement in interpreting of data on the project.

The study we conducted draws to some extent on these earlier works, especially for the development of the classification system used. However, it differs in several ways. First, it is a comparative study, with direct comparisons made between writing requirements in two distinct domains-university courses and on the IELTS writing test. Second, it is perhaps more linguistically based than those previous studies, drawing to a greater extent on the methods of discourse analysis. Finally, to our knowledge, it is the first wide-scale survey of this kind using Australian-based data.

# 2. Details of the IELTS writing test

The IELTS test has assumed increasing importance in university systems around the world. In Australia, for example, it is now the only test accepted by all universities and is often referred to as the ‘preferred’ test (Coley, 1999, p. 10). The IELTS is one of the few English language proficiency tests in which the listening, speaking, reading, and writing skills of examinees are assessed and reported on. The writing component is a direct test of writing requiring examinees to produce two samples of writing in an hour. In Task 1, examinees write a short description of information presented in the form of a diagram, table etc. Task 2 requires a composition in response to a proposition or question. According to the test’s designers, examinees in their responses to Task 2 items are asked ‘to provide general factual information, outline and/or present a solution, justify an opinion, and evaluate ideas and evidence’ (UCLES, 2002b). In both tasks, examinees are assessed on their ability to write with ‘appropriate register, rhetorical organisation, style and content’ (UCLES, 1996). In the present study a decision was made to focus only on Task 2, the extended composition. This was partly because this component carries a heavier weighting on the test and thus generally receives greater attention in test preparation classes. Also, our various experiences of teaching on IELTS test preparation programs suggest to us that Task 2 has a major influence on students’ emerging understandings of what academic writing in anglophone universities fundamentally entails.

# 3. The study

The data used in the study were writing tasks collected from the two domains. The IELTS corpus, consisting of 20 Task 2 rubrics, came from both publicly available IELTS specimen materials (UCLES, 1996) and a sample of recent, commercial materials used in preparation courses. The corpus of university assignments was obtained from academic staff from two Australian universities. A total of 155 tasks from first year subjects (undergraduate and postgraduate coursework) were collected from 79 academics $81 \%$ response rate) from a range of discipline areas across the two universities (Table 1).

As in the previous studies discussed above, a major challenge in our research was to develop a classification system that would be appropriate for the study’s quite specific purpose. The approach we used was based largely on the methods of discourse analysis to analyse whole written texts. Whilst there are obvious differences between these two types of written data (tasks vs. texts), we believe there are grounds for analysing them in similar ways. First of all, the rubrics of assessment tasks do constitute texts in themselves, even though of their nature they are much shorter than whole texts. The second reason relates to the special communicative function of assessment tasks, which is to prescribe the composition of another text i.e. an essay, report, etc. From the nature of the task in question, it is possible, to varying degrees, to make informed predictions about the type of text that will be produced in response to it. It needs to be acknowledged, however, that this predicting involves an act of interpretation on the part of the analyst, a point we shall take up later in the discussion of the study’s results.

The field of discourse analysis offers many different frameworks and taxonomies for analysing written texts including, for example, systemic functional linguistics (Halliday, 1994), Rhetorical structure theory (Mann and Thompson, 1989), genre analysis (Swales, 1990). In our study, we did not seek to employ any single taxonomic framework, believing that a syncretic approach would be more useful to deal with the specialist data used. Furthermore, it was thought sensible not to begin with any a priori set of theoretical categories, but to draw initially on the data to establish broad dimensions of difference and then to refer to relevant theoretical frameworks later to refine the classification scheme. The classification scheme was developed in the first place through analysis of a selection of IELTS Task 2 items and university assignment tasks. From this process, the following broad categories were generated:

Table 1 Number of tasks collected by discipline   

<html><body><table><tr><td>Subject areasa</td><td>Total</td><td>Under graduate</td><td>Post graduate</td></tr><tr><td>Accounting</td><td>5</td><td>3</td><td>2</td></tr><tr><td>Agriculture</td><td>3</td><td>3</td><td></td></tr><tr><td>Anthropology</td><td>2</td><td>2</td><td></td></tr><tr><td>Architecture</td><td>4</td><td>4</td><td></td></tr><tr><td>Biology</td><td>5</td><td>5</td><td></td></tr><tr><td>Business development</td><td>4</td><td>4</td><td></td></tr><tr><td>Chemistry</td><td>2</td><td>2</td><td></td></tr><tr><td>Communication</td><td>2</td><td>2</td><td></td></tr><tr><td>Computing</td><td>12</td><td>11</td><td>1</td></tr><tr><td>Economics</td><td>11</td><td>11</td><td></td></tr><tr><td>Education</td><td>7</td><td>7</td><td></td></tr><tr><td>Engineering</td><td>7</td><td>3</td><td>4</td></tr><tr><td>English literature</td><td>1</td><td>1</td><td></td></tr><tr><td>Geography</td><td>1</td><td>1</td><td></td></tr><tr><td>History</td><td>6</td><td>6</td><td></td></tr><tr><td>Law</td><td>16</td><td>15</td><td>1</td></tr><tr><td>Linguistics</td><td>8</td><td>5</td><td>3</td></tr><tr><td>Management</td><td>18</td><td>11</td><td>7</td></tr><tr><td>Marketing</td><td>3</td><td>-</td><td>3</td></tr><tr><td>Medicine</td><td>8</td><td>3</td><td>5</td></tr><tr><td> Philosophy</td><td>7</td><td>7</td><td></td></tr><tr><td>Physics</td><td>2</td><td>2</td><td></td></tr><tr><td>Politics</td><td>10</td><td>10</td><td></td></tr><tr><td>Psychology</td><td>1</td><td>1</td><td></td></tr><tr><td>Social work</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Sociology</td><td>3</td><td>3</td><td></td></tr><tr><td>Tourism</td><td>3</td><td></td><td>3</td></tr><tr><td>Visual arts</td><td>2</td><td>2</td><td></td></tr><tr><td>Total</td><td>155</td><td>125</td><td>30</td></tr></table></body></html>

a The classifying of subject areas (or disciplines) was an additional taxonomic challenge on the project The designated areas shown in the table reflect faculty and departmental structures at the two universities. It should be noted that some of the areas listed take in a number of sub-disciplines. For example under Law, tasks were collected in a range of units-Torts, Legal Process and Jurisprudence.

A genre;   
B information source;   
C rhetorical function;   
D object of enquiry.

![](img/08986ae46a458c503aef92c647c9a8b633f2bb5cdaae88d569df04c44fe58b80.jpg)  
Fig. 1. Sample IELTS Task 2 (1ELTS Handbook, 1996). a The rhetorical function here relates to students’ views of the extent of ‘compatibility’ between the two entities mentioned-‘tradition’ and ‘technology’. The relevant functions are ‘comparison’ and ‘evaluation’ (see analysis in Section 3.3.1).

Fig. 1 shows an example of an IELTS Task 2 item and also indicates in a preliminary way how each of these categories was derived.1 In the sections that follow, we provide explanations for each of the Categories A–D as well as the sub-categories included under each (see Table 2 for overall classification scheme). Under the four categories, we also provide results from the comparative analysis. Whilst the data presented are of a quantitative nature, it needs to be acknowledged that the analysis was not a strictly empirical one. As mentioned, the process of analysing tasks involved a degree of interpretation and inference. Thus, it is intended that the numerical data not be seen as a definitive set of results; but rather they are designed to provide a broad picture only of the types of writing required in the two domains.

# 3.1. Genre

Weigle (2002) in her recent work on writing test design suggests that the authenticity of writing tasks needs to be understood primarily in terms of ‘genre’ (p. 96). But whilst genre as a construct is important in discourse analysis work (like that in the present study), it is fair to say that after several decades of intensive treatment within applied linguistic research, there is still only limited consensus about what the concept fundamentally entails. One source of complexity is the variety of definitions that have been offered for the term (e.g. Martin, 1989; Swales, 1990), as well as disagreement about how it might relate to associated terms, such as ‘text-type’ and ‘speech event’ (Levinson, 1979; Paltridge, 1996). Other problems arise from the array of genre taxonomies that have been generated by analysts. For example, Martin’s (1984) categories of report, recount, explanation, etc. bear no obvious correspondence to the those used by other genre theorists such as Swales (1990), e.g. research article, reprint request, etc. In our study, we sought to avoid these theoretical difficulties. As the first category in our analytical framework, the concept of genre was used in an unproblematical, self-referential way-that is, the genre of a task was taken to be the name given to the required written product as outlined in the task rubric, i.e. whether students were asked to write an essay, a literature review, etc. In reference to the variable taxonomies above, it should be noted that such a methodology generates a set of categories with more obvious correspondence to those of Swales (1990) than to Martin (1984).

Table 2 The classification scheme   

<html><body><table><tr><td>A. Genre [G]</td><td></td></tr><tr><td colspan="2">By what name is the task described? (Select one category)</td></tr><tr><td>1. Case study report</td><td>[G-CaseR]</td></tr><tr><td>2. Essay</td><td>[G-Ess]</td></tr><tr><td>3. Exercise</td><td>[G-Ex]</td></tr><tr><td>4. Experimental report</td><td>[G-ExR]</td></tr><tr><td>5. Literature review</td><td>[G-LitR]</td></tr><tr><td>6. Research report (other)</td><td>[G-ResR]</td></tr><tr><td>7. Research proposal</td><td>[G-ResP]</td></tr><tr><td>8. Review</td><td>[G-Rev]</td></tr><tr><td>9. Short answer</td><td>[G-SAns]</td></tr><tr><td>10. Summary</td><td>[G-Sum]</td></tr><tr><td>11. Written argument or case</td><td>[G-Arg]</td></tr><tr><td>12. Other</td><td>[G-Oth]</td></tr><tr><td colspan="2">B. Information source []</td></tr><tr><td colspan="2">On what information source(s) is the written product to be based? (Select one category)</td></tr><tr><td>1. Prior knowledge</td><td>[I-pk]</td></tr><tr><td>2. Primary sources</td><td>[I-ps]</td></tr><tr><td>2.1. Provided in task</td><td>[I-ps-P]</td></tr><tr><td>2.2. Collected by student</td><td>[I-ps-c]</td></tr><tr><td>3. Secondary sources</td><td>[I-s]</td></tr><tr><td>4. Primary/secondary sourcea</td><td>[I-p/s]</td></tr><tr><td>5. No specification of source</td><td>[I-n]</td></tr><tr><td colspan="2">C. Rhetorical function [R]</td></tr><tr><td colspan="2">What is the task (or component of the task) instructing students to do? (Select one or more categories)</td></tr><tr><td>1. Epistemic [R-E]</td><td></td></tr><tr><td>1.1. Comparison</td><td>[R-E-co] [R-E-d]</td></tr><tr><td>1.2. Description</td><td></td></tr><tr><td>1.3. Explanation</td><td>[R-E-ex] [R-E-ev]</td></tr><tr><td>1.4. Evaluation 1.5. Prediction</td><td>[R-E-p]</td></tr><tr><td>1.6. Summarisation</td><td>[R-E-s]</td></tr><tr><td></td><td></td></tr><tr><td>2. Deontic [R-D]</td><td>[R-D-h]</td></tr><tr><td>2.1. Hortation</td><td></td></tr><tr><td>2.2. Instruction</td><td>[R-D-i]</td></tr><tr><td>2.3. Recommendation</td><td>[R-D-r]</td></tr><tr><td colspan="2">D. Object of enquiry [0]</td></tr><tr><td>With which type of phenomenon is the task mainly concerned? (Select one category)</td><td></td></tr><tr><td>1. Phenomenal</td><td>[O-p] [O-m]</td></tr><tr><td>2. Metaphenomenal</td><td></td></tr></table></body></html>

a Categories 2.1 and 2.2 were also applied to the primary source component of these tasks.

Genre analysis of tasks in the university corpus was mainly an empirical procedure, but not in all instances. In some cases, no genre term was specified in the task rubric, whereupon a category was assigned, if there was other contextual information that enabled a plausible judgment to be made about the genre type. To assist in the process of allocating unspecified tasks, the following rough definitions were devised.

<html><body><table><tr><td>Essay</td><td>Task requiring the presentation of an argument in response to a given proposition or question</td></tr><tr><td>Review</td><td>Task requiring the summary and appraisal of a: single text (including non-verbal texts e.g. film, painting)</td></tr><tr><td>Literature review</td><td>Task requiring the identification, summary and appraisal of a range of texts relevant to a specific</td></tr><tr><td>Experimental report</td><td>field of knowledge Task requiring the description and analysis of data obtained from an empirical research procedure</td></tr><tr><td>Case study report</td><td>Task involving identification and discussion of a problem(s) arising from a given situation, along with suggested ways for solving the problem</td></tr><tr><td>Research report (other)</td><td>Task similar in many respects to the Experimental Report, but requiring the description and analysis of information of a more qualitative nature e.g. that</td></tr><tr><td>Research proposal</td><td>obtained from interview or participant observation Task requiring the description of an intended research project, including a statement of its</td></tr><tr><td>Summary</td><td>rationale Task requiring the representation of the main contents of a text or texts.</td></tr><tr><td>Exercise</td><td>Task requiring the application of some discipline- specific tool or model to a given situation</td></tr><tr><td>Short answer</td><td>Task requiring mainly the reproduction of previously provided items of knowledge e.g. from lectures or textbooks</td></tr></table></body></html>

# 3.1.1. Genre: university assignments vs. IELTS

The analysis of the university corpus found a great diversity in the genres of university assignments (Table 3). Of these types, clearly the essay was the most common, accounting for almost $60 \%$ of tasks. This assignment type appeared most frequently in subjects in the humanities and social sciences, but was also prescribed in a range of other disciplines, including biology, computing and medicine. As a generic form, the essay was characterised in a variety of ways in assignment handouts; common to most accounts, however, was the requirement that students argue for a particular position in relation to a given question or proposition.

The next most common genre was the case study report ( $10 \%$ of tasks), but confined to subjects in certain applied disciplines: management, accounting, law, computing and engineering. The next category, exercise $8 \%$ of tasks), included a range of minor tasks often set as a first piece of work in subjects and usually requiring students to demonstrate their understanding of a particular concept or technique by applying it to an exemplary situation. The only other genre to appear with any frequency was the Research Report (non-experimental). In these tasks, students were required to collect their own data and to describe and explain them.2

Table 3 Genres of university assignments   

<html><body><table><tr><td>Genre</td><td>No.</td><td>(%)</td></tr><tr><td>Essay</td><td>90</td><td>58</td></tr><tr><td>Case study report</td><td>15</td><td>10</td></tr><tr><td>Exercise</td><td>12</td><td>8</td></tr><tr><td>Research report (other)</td><td>10</td><td>6</td></tr><tr><td>Review</td><td>7</td><td>5</td></tr><tr><td>Experimental report</td><td>6</td><td>4</td></tr><tr><td>Literature review</td><td>2</td><td>1</td></tr><tr><td>Research proposal</td><td>2</td><td>1</td></tr><tr><td>Summary</td><td>2</td><td>1</td></tr><tr><td>Short answer</td><td>2</td><td>1</td></tr><tr><td>Othera</td><td>7</td><td>5</td></tr><tr><td>Total</td><td>155</td><td>100</td></tr></table></body></html>

a Annotated bibliography, letter, project brief, resume, homepage, computer program, educational program proposal.

In the IELTS corpus, the genre specifications were standard for all items. In each case, students were instructed to present a written argument or case on a given topic, taken from the rubric used in official versions of the test. The topic part of all items consisted either of a question or a proposition often followed by a prompt asking students to indicate the extent of their agreement or disagreement with the proposition (see Fig. 1). Whilst the written argument nomenclature does not correspond exactly to any of the genre categories identified in the university corpus, it would seem to resemble most closely the university essay. Indeed, on earlier versions of the official test, Task 2 was referred to as an essay (UCLES, 1990). The avoidance of the essay label in more recent versions suggests, however, that test developers have perhaps been aware of certain differences between the university and IELTS versions of this form. The differences we observed are discussed below under the remaining categories.

# 3.2. Information source

The second dimension of difference, information source, was concerned with the type of information that was to be used in the completion of a task; for example, whether students were required to read from a list of prescribed readings or to analyse data obtained from an experimental procedure, to examine case material and so on. The following subcategories were used, derived in part from the classification of Taylor (1989):

1. prior knowledge;   
2. primary sources;   
2.1. provided in task;   
2.2. collected by student;   
3. secondary sources;   
4. primary3 /secondary source;   
5. no specification of source.

The first category—prior knowledge—was applied to tasks which did not require students to draw on any external sources of information. For tasks in this category, the contents of the piece were to be based exclusively on the writer’s pre-existing knowledge, experience, beliefs, intuitions and the like. The two categories primary sources and secondary sources were applied to those tasks which required the use of external sources of information; in other words, tasks that involved research of some kind. The category primary sources, denoted those sources which might otherwise be called data. Examples of such sources in our sample tasks were:

(i) the documents provided for analysis in a history assignment;   
(ii) the details of a case given in a law assignment;   
(iii) the experimental data to be collected and analysed in a chemistry practical.

The category primary sources was further divided into two types: those provided in the task itself and those to be collected by students via some prescribed research procedure. Of the sample sources above, (i) and (ii) would be classified as provided and (iii) as collected. The category secondary sources was used for those tasks which required students to engage with and incorporate in their writing works of an interpretative nature– monographs, research articles and so on. The combined category primary/secondary sources was assigned to tasks which prescribed sources of both varieties. Examples from the corpus here were various research tasks which required students to collect and analyse their own data (primary source), but also to situate their work within previous research (secondary sources). Similarly, in a number of case study tasks, students needed to analyse case material (primary sources), but also to draw on relevant theoretical frameworks to help resolve issues raised in the case (secondary sources).

# 3.2.1. Information source: university assignments vs. IELTS

The information sources prescribed in the university tasks are shown in Table 4. Notably, almost all tasks involved a research component of some kind, requiring the use of either primary or secondary sources or a combination of the two, a finding consistent with Horowitz (1986) discussed earlier. The most frequently prescribed sources were secondary sources $5 5 \%$ of the corpus), usually described in tasks as references. These included monographs, journal articles and textbooks. The use of secondary sources was required in tasks from a broad range of disciplines, but with a higher aggregation in disciplines from the humanities and social sciences. There was a good deal of variation in the amount of information provided about the secondary sources to be used, ranging from tasks which included a simple exhortation for students to base their work on wide reading to those which provided a specific list of references to be incorporated in the written product. One feature common to most tasks prescribing the use of secondary sources was the inclusion of information about citation practices in the discipline, along with warnings about plagiarism.

Table 4 Information sources prescribed in university assignments   

<html><body><table><tr><td>Information sources</td><td>No.</td><td>%</td></tr><tr><td>Secondary</td><td>85</td><td>55</td></tr><tr><td>Primary/secondary</td><td>33</td><td>21</td></tr><tr><td>Primary</td><td>28</td><td>18</td></tr><tr><td>Prior knowledge</td><td>5</td><td>3</td></tr><tr><td>No specification of sources</td><td>4</td><td>3</td></tr><tr><td>Total</td><td>155</td><td>100</td></tr></table></body></html>

Tasks prescribing the use of primary sources were also from a wide range of disciplines, but especially in the more research-oriented, as opposed to theoretical, disciplines. There was much variety in the types of primary sources prescribed, ranging from quantitative data in the natural and social sciences, to case study material used typically in the disciplines of law, management and economics. As mentioned, a distinction was made between primary sources that needed to be collected by students and those that were provided in the task itself. In the latter type, students were not required to collect data but only to be engaged in their interpretation. The results from this analysis are shown in Table 5. It is of some interest that the majority of prescribed primary sources were of the provided-type, both at undergraduate and postgraduate (coursework) level. The likely explanation for this is that in certain discipline areas, lecturers may not have wanted their first year students to be conducting their own research without having first received some grounding in research methods in the discipline.

Notably, the category prior knowledge was the least prescribed information source. The very small number of tasks falling under this category $( 3 \% )$ tended to be minor pieces of work in the overall assignment requirements of subjects, including, for example, the following task set as the first piece of writing in a history subject:

Table 5 Primary source types prescribed in university assignments   

<html><body><table><tr><td>Primary sourcea</td><td>No.</td><td>%</td></tr><tr><td>Provided in task.</td><td>36</td><td>59</td></tr><tr><td>Collected by student</td><td>25</td><td>41</td></tr><tr><td>Total</td><td>61</td><td>100</td></tr></table></body></html>

a Sources from both the categories primary source and primary/secondary source were considered in this analysis.

Write a paragraph explaining what you know about your own family’s experience of World War II.

Unlike the assignments in the university corpus, IELTS Task 2 items, it was noted, were not framed around the use of external sources. All items in the IELTS sample included the following instruction to students, taken from the standard rubric in the official versions of the test.

You should use your own ideas, knowledge and experience and support your arguments with examples and relevant evidence.

Thus, all IELTS tasks in the sample were allocated to the category prior knowledge.

These findings point to a major difference in the nature of writing in the two domains; even if it is one for which we can readily account. The purpose of a test of writing is to elicit a written sample which is then assessed primarily in terms of its linguistic proficiency. In university study by contrast, writing is assessed according to far broader criteria, including a student’s understandings of key knowledge in a discipline, as well as understanding of the discipline’s modes of analysis and discursive practices. All of this, of necessity, will come from an engagement with sources. In short, in the university context, the content of a piece of writing is primary; in a language testing context it is often incidental. As Horowitz (1991) summarises the difference: “the concern of those who create writing assessments-to mitigate examinees differences in background knowledge-contrasts sharply with the concern of designers of academic writing tasks, whose precise purpose in creating questions is to place on the examinee the burden of proving mastery of a specific body of knowledge and a specific disciplinary approach to that knowledge” (p. 74).

The reasons for the differences in prescribed information sources are understandable enough. It needs to be acknowledged, however, that preparation for the IELTS writing test (Task 2) may not give students an entirely accurate view of the nature of academic argumentation, especially concerning the issue of what constitutes appropriate evidence in a piece of writing. In the IELTS test, students learn that it is sufficient to base their assertions on ‘their own ideas, knowledge and experience.’ In the university contextwhere valid evidence is usually seen as the findings of research or the authoritative pronouncements of disciplinary scholars—a student who relies exclusively on prior knowledge will usually be criticised for being anecdotal and for not having read adequately for the task. It is also worth noting that the IELTS Task 2, as it is framed, does not suggest any need for students to be taught about the conventions for citing the ideas of other writers. Weigle (2002) sees this as a general problem in writing in test conditions, noting that whilst university writing is always based on some prior reading, in test writing assessment, it is simply not possible ‘to simulate this degree of interaction with other texts’ (p. 95).

# 3.3. Rhetorical function

The concept of rhetorical function has been used widely in our field (e.g. Hoey, 1983; Lackstrom, Selinker, & Trimble, 1973; Meyer, 1975) and has led to the generation of an array of functional categories e.g. comparison/contrast; cause/effect; definition; problem/solution. By one definition, the rhetorical function of a text is ‘that which a given unit of discourse is trying to do’ (Trimble, 1985), e.g. comparing entities, explaining the cause of an entity. Applied to the study of academic writing tasks, the concept can be modified to mean ‘that which a task (or unit of a task) is instructing students to do’.

Table 6 Sub-categories of rhetorical functions   

<html><body><table><tr><td colspan="2"></td></tr><tr><td>Epistemic categories</td><td>Deontic categories</td></tr><tr><td>Comparison Applied to tasks (or components of tasks) which required students to identify the similarities and/or differences between two or more entities or phenomena. Prototypical &#x27;comparative&#x27; question:</td><td>Hortation Applied to tasks (or components of tasks) which required students to make a judgement about the desirability of a given entity or phenomenon, especially those concerned with actions and states</td></tr><tr><td>What are the similarities and/or differences between X and Y? Description Applied to tasks (or components of tasks) which required students to give an account of the nature of</td><td>of affairs. Prototypical hortatory&#x27; question: Should X happen/be done?</td></tr><tr><td>a given entity or phenomenon. Prototypical &#x27;descriptive&#x27; question: What are the features of X? Evaluation Applied to tasks (or components of tasks) which required students to make a judgement about the value of a given entity or phenomenon with respect</td><td>Instruction Applied to tasks (or components of tasks) which required students to outline a sequence of procedures for a given entity or phenomenon. Prototypical &#x27;instructional&#x27; question: What must be done to achieve X?</td></tr><tr><td>relevant, etc. is X? Explanation Applied to tasks (or components of tasks) which required students to give an account of the causes for a given entity or phenomenon. Both non- volitional causation (e.g. cause, reason) and</td><td></td></tr><tr><td>question: What is the cause of X? Prediction Applied to tasks (or components of tasks) which required students to speculate about the future state of a given phenomenon or entity. Prototypical</td><td>Recommendation Applied to tasks (or components of tasks) which required students to suggest ways of dealing with a given entity or phenomenon, usually presented in the form of a problem. The prototypical &#x27;recom-</td></tr><tr><td>Summarisation Applied to tasks (or components of tasks) which</td><td>recommendatory&#x27; question was in the form: What can be done about X?a</td></tr></table></body></html>

Our attempts to develop a systematic set of rhetorical categories began with an initial distinction drawn between tasks that involved a more ‘analytical’ rhetoric and those with a more ‘practical’ orientation. This difference can be illustrated in the following two tasks, the first from the pure discipline of sociology and the other from one of its applied counterparts, social work:

(i) Write an essay on the following topic: Do young people from different class backgrounds experience the world differently?   
(ii) Discuss some of the problems currently facing youth in Australia. Using a social theory, discuss how the situation of youth could be improved in Australian society.

The first task requires the writer to analyse a situation and to assert whether something does (or does not) happen—in this case whether class has a bearing on young people’s experience of the world. The focus of the second task, at least the second part of it, is not on what does happen, but rather on what could be done to change what happens—by way of a solution to the problems identified.

The rhetorical difference noted in these two tasks is captured in the distinction traditionally drawn in semantics between epistemic and deontic modality. An epistemic clause, as Huddleston (1982) explains, has the status of a proposition; it asserts whether something is true, partly true, false, etc. A deontic clause, in contrast, has the character of an action: ‘what is at issue is not whether something is true but whether something is going to be done’ (Huddleston, 1982, p. 168).4 The distinction between the deontic and epistemic was used in the study to establish a first level of rhetorical categories. Under these two broad categories, the following sets of sub-categories were generated, with an explanation provided for each (Table 6).

# 3.3.1. Rhetorical function: university assignments vs. IELTS

In interpreting the results for this category, it is important to note that assignments were generally found to prescribe more than a single rhetorical function. For example, the sample sociology essay question below would take in the following functions as a plausible sequence of moves (Swales, 1990):

† summarisation, in the summarisation of each of the two approaches;   
comparison, in explicit comparisons of two approaches;   
evaluation, in evaluation of the relative ‘validity’ of each. Essay question (sociology)   
Compare and contrast Scientific Management with the Human Relations approach to work. Which in your view is the more valid approach?   
Essays should be approximately 2000 words. You are encouraged to read more widely than the references provided. Also do not forget to read the ‘Departmental Policy on Plagiarism’ in this booklet.

Table 7 shows the results from the analysis of rhetorical functions in tasks from the university corpus. A total of 393 functions were identified in the corpus of 155 tasks (Table 7, column 3), with an average of 2.5 functions per task. The numbers in the final column indicate the percentage of the 155 tasks that were found to incorporate the specified function.

The first result to note is that the epistemic functions were considerably more common than the deontic. In general terms, tasks specifying exclusively epistemic functions tended to be from the more pure disciplines, e.g. the physical and social sciences. In contrast, those tasks that included deontic elements were clustered around disciplines of a more applied nature, e.g., agriculture, computing, engineering, education, law, management. This, of course, is not a surprising result, given that it is the nature of the applied disciplines to be concerned as much with practical knowledge as theoretical knowledge; the knowing how in addition to the knowing that, as knowledge in these fields is sometimes characterised (Becher, 1989).

The epistemic category of evaluation was found to be the most common, with about   
two-thirds of tasks in the corpus adjudged to involve this function. Evaluation was found   
to be characteristic of tasks across a wide range of disciplines in the corpus. The following   
are two sample evaluative questions taken from tasks set in sociology and management: How plausible do you find Marx’s account of social inequality? (Sociology)

To what extent can people be regarded as the most important resource of an organisation. (Management)

The next most common functions were also epistemic in nature: description, summarisation, comparison, explanation. A sample task under each of these categories is given below.

Table 7 Rhetorical functions in university assignments   

<html><body><table><tr><td>Rhetorical function</td><td>Modality (E, epistemic; D, deontic)</td><td>No. of tasks incorporat- ing function</td><td>Percentage of tasks incorporating function</td></tr><tr><td>Evaluation</td><td>E</td><td>104</td><td>67</td></tr><tr><td>Description</td><td>E</td><td>71</td><td>49</td></tr><tr><td>Summarisation</td><td>E</td><td>55</td><td>35</td></tr><tr><td>Comparison</td><td>E</td><td>54</td><td>35</td></tr><tr><td>Explanation</td><td>E</td><td>43</td><td>28</td></tr><tr><td>Recommendation</td><td>D</td><td>35</td><td>23</td></tr><tr><td>Hortation</td><td>D</td><td>15</td><td>15</td></tr><tr><td>Prediction</td><td>E</td><td>11</td><td>7</td></tr><tr><td>Instruction</td><td>D</td><td>5</td><td>3</td></tr><tr><td>Total functions</td><td></td><td>393</td><td></td></tr></table></body></html>

# Description

What is the biology of toxoplasmosis? (Biology)

Summarisation

What are the main points Christine Halliwell is making about the status of women in society in her chapter ‘Women in Asia: Anthropology and the study of women’? (Anthropology)

# Comparison

What differences and what similarities emerge from a comparison of Egyptian and Mesopotamian temples? (Architecture)

# Explanation

Adolescent mental health is a growth industry. Discuss factors which have contributed to this growth. (Medicine)

Of the deontic functions, recommendation was clearly the most common and was especially prominent in the more applied disciplines. In tasks involving recommendation, the entity to be analysed was presented as being problematic in some sense and students were required to suggest ways in which it could be resolved, as in the following example:

Recommendation

How can the land degradation problems of the Parwan Valley be overcome? (Agriculture)

The other deontic category that appeared in the data, though to a much lesser extent than recommendation, was hortation. In hortatory tasks, students were asked to comment on the desirability of a given course of action or state of affairs. These tasks were framed around the notion of necessary action (or should-ness) and were most characteristic of disciplines with an ethical or polemical element to their contents, including law, medicine, politics, philosophy, as in the following example:

Hortation

People subject to the power of the state need the protection of a bill of rights. Discuss. (Law)

The results for the IELTS tasks are shown in Table 8. As can be seen, all involved evaluation of some kind. In the following example, taken from IELTS specimen materials (UCLES, 1996), the quality to be evaluated is compatibility. (This task also takes in the function of comparison).

It is inevitable that as technology develops so traditional cultures must be lost. Technology and tradition are incompatible—you cannot have both together. To what extent do you agree or disagree with this statement?

Whilst all tasks involved some form of evaluation, in many instances this was found to be accompanied by another function, namely hortation. As mentioned, hortatory elements in tasks were those framed around the notion of necessity (or should-ness). The following two tasks are representative of the 14 tasks which were found to incorporate this function:

Table 8 Rhetorical functions in IELTS items   

<html><body><table><tr><td>Rhetorical function</td><td>Modality (E, epis- temic; D, deontic)</td><td>No. of IELTS items incorporating function (n=20)</td><td>Percentage of items incorporating func- tion</td></tr><tr><td>Evaluation</td><td>E</td><td>20</td><td>100</td></tr><tr><td>Hortation</td><td>D</td><td>14</td><td>70</td></tr><tr><td>Prediction</td><td>E</td><td>3</td><td>15</td></tr><tr><td>Comparison</td><td>E</td><td>3</td><td>15</td></tr><tr><td>Explanation</td><td>E</td><td>3</td><td>15</td></tr><tr><td>Recommendation</td><td>D</td><td>2</td><td>10</td></tr><tr><td>Description</td><td>E</td><td>-</td><td>-</td></tr><tr><td>Summarisation</td><td>E</td><td>-</td><td>-</td></tr><tr><td>Instruction</td><td>D</td><td>-</td><td>-</td></tr></table></body></html>

Higher mammals such as monkeys have rights and should not be used in laboratory experiments.

A government’s role is only to provide defence capability and urban infrastructure (roads, water, supplies, etc.) All other services (education, health and social security) should be provided by private groups or individuals in the community.

Other rhetorical functions—prediction, comparison, explanation and recommendation—also showed up in the analysis, but were confined to a total of only two or three tasks, as in the following examples:

Prediction

The idea of having a single career is becoming an old fashioned one. The new fashion will be to have several careers or ways of earning money and further education will be something that continues throughout life.

# Comparison

. Which subjects can be better taught using computers?

# Explanation

News editors decide what to broadcast on television and what to print in newspapers.   
What factors do you think influence these decisions?.

# Recommendation

What are the most effective ways of reducing population growth?

The patterns of rhetorical functions identified in the IELTS Task 2 items were clearly different from those in the university corpus. The more notable differences can be summarised thus:

(i) The functions of summarisation and description, which were common in the university corpus, did not appear in the IELTS samples.   
(ii) The functions of comparison, explanation and recommendation were much less common in the IELTS samples.   
(iii) The function of hortation, which was relatively rare in the university corpus, was, along with evaluation, the predominant rhetorical mode in the IELTS samples.

Of these findings, the last is perhaps the most significant. Indeed, it is interesting to speculate about why hortation should figure—so prominently in IELTS items. We can posit one explanation here—this is that writing in a hortatory mode, of its nature, may not require the same amount of background knowledge that is needed to engage with topics of an epistemic nature. As has been noted by a number of writers, it is essential that any test prompt be designed so that subject matter is accessible to examinees and that where possible all have an equal chance of success (Reid & Kroll, 1995; Weigle, 2002). To take the topic area of animal experimentation as an example, it seems fair to assume that students in a testing context would be able to write more readily about the moral desirability (or not) of this practice (hortation), rather than, for example, about the reasons why the practice is employed (explanation) or about its scientific validity (evaluation) or about the views of various animal rights proponents (summarisation). Whilst the prominence given to hortation in IELTS Task 2 items is probably attributable to certain test-specific exigencies, this feature nevertheless represents a clear difference in the nature of writing in the two domains, one that is likely to have implications for students whose pre-sessional English language instruction has a substantial focus on test preparation.

# 3.4. Object of enquiry

We have called the final dimension of difference object of enquiry. This dimension was concerned with probing the nature of the variable $X$ referred to in the discussion of rhetorical function categories above, a relationship that corresponds roughly to Hamp-Lyons (1986) distinction between ‘topic’ and ‘perspective’. The need for this additional category arose from our observation that some tasks, of their nature, required a more abstract form of writing than others, as illustrated in the following two topics from a first year management subject:

(i) Discuss the role of the manager in Australia in the 1990s.   
(ii) Are there significant differences between ‘systems’ and ‘classical’ views of management?

These topics would appear to deal with two distinct domains. In the first, the object of enquiry might be regarded as the real world of the manager (i.e. what managers do or need to do, in their real world activities). The second topic, in contrast, is concerned less with the world of managers and more with the abstract or metaphenomenal world of management theorists (i.e. how these theorists view the world). This difference in our view is not trivial; we would argue that the pattern of discourse elicited by each topic is likely to be of a different kind. In Hallidayan terms, the first is likely to elicit a preponderance of ‘material process’ verbs (e.g. Managers do.) and the second more mental process verbs (e.g. Management theorists believe...) (see Moore, 2002 for further discussion of this point). In the classification scheme, this difference in the objects of enquiry was captured in the following two categories, using terms from Halliday (1994, p. 252):

Phenomenal Metaphenomenal

The phenomenal category was used for those tasks which directed students to consider primarily such real world entities as events, actions, processes, situations, practices and the like. The metaphenomenal category, in contrast, was applied to tasks concerned mainly with the abstract entities of ideas, theories, methods, laws, etc.5

# 3.4.1. Object of enquiry: university assignments vs. IELTS

In the university corpus, a majority of tasks were concerned with topics of a phenomenal nature, but there was also a fair proportion of what we have termed metaphenomenal tasks (Table 9). The latter were particularly characteristic of humanities disciplines, some of which may be said to be concerned exclusively with the metaphenomenal e.g. philosophy and literature. Examples of metaphenomenal tasks, however, were found in a range of disciplines, including surprisingly the following quite demanding task set for first year agriculture undergraduates:

Present a critical review of literature relating to a scientific topic which interests you. Summarise the principal ideas presented in a collection of scientific papers, highlighting the validity of the claims made, the conclusions and other important features.

No attempt was made to analyse the objects of enquiry further within each of the two broad categories; the topics were found to be too diverse and of too discipline-specific a nature to allow for such an endeavour. A sense of the diversity of topics covered in the corpus is captured in the following list under the phenomenal and metaphenomenal categories (Table 10).

In contrast to the university tasks, the objects of enquiry in all IELTS items were found to be of a phenomenal nature. A complete list of these is provided in Table 11. The lack of metaphenomena in IELTS tasks can again be related to the issue of necessary background knowledge. Clearly, the sorts of metaphenomenal topics from the university corpus given (Table 10) would be unsuitable in a language testing context. For example, an account of ‘Barthes’ theoretical model’ could only be attempted after a careful reading of Barthes’ text (and even then, in this particular instance, there may be no guarantee of success). Similarly, it would not be possible to discuss different methods for calculating household incomes with-out first being familiar with these accounting methods.

Table 9 Objects of enquiry in university assignments   

<html><body><table><tr><td>Object of enquiry</td><td>No. of tasks</td><td>Percentage of tasks</td></tr><tr><td>Phenomenal</td><td>95</td><td>61</td></tr><tr><td>Metaphenomenal</td><td>60</td><td>39</td></tr></table></body></html>

Table 10 Sample objects of enquiry in university tasks   

<html><body><table><tr><td>Phenomenal objects of enquiry</td><td>Metaphenomenal objects of enquiry</td></tr><tr><td>1. Land degradation (Agriculture)</td><td>1. Barthes&#x27; theoretical model (Literature)</td></tr><tr><td>2. The Roman arch (Architecture)</td><td>2. Methods for calculating household incomes (Economics)</td></tr><tr><td>3. Atmospheric pollution (Biology)</td><td>3. Theoretical approaches to child&#x27;s play (Education)</td></tr><tr><td>4. Graphical user interfaces (Computing)</td><td>4. The Aboriginal Protection Act (Law)</td></tr><tr><td>5. Children&#x27;s acquisition of speech (Education).</td><td>5. Speech act theory (Linguistics)</td></tr><tr><td>6. Public water supply systems (Engineering).</td><td>6. Freud&#x27;s views of the feminine (Medicine)</td></tr><tr><td>7. The Vietnam war (History)</td><td>7. Systems and classical views of management (Management)</td></tr><tr><td>8. Adolescent mental health (Medicine)</td><td>8. Utilitarian and retributive theories of punishment (Philosophy)e</td></tr><tr><td>9. The vibration of strings (Physics)</td><td>9. Machiavelli&#x27;s political thought (Politics)</td></tr><tr><td>10. Developments in international tourism (Tourism)</td><td>10. The Chicago school of sociology (Sociology).</td></tr></table></body></html>

Table 11 Objects of enquiry in total IELTS corpus (phenomenal)   

<html><body><table><tr><td>1. The relationship between technology and tradition</td></tr><tr><td>2. Government regulation of motor car usage</td></tr><tr><td>3. Retirement age</td></tr><tr><td>4. Telecommuting</td></tr><tr><td>5. Studying abroad</td></tr><tr><td>6. Studying abroad</td></tr><tr><td>7. Government regulation of new technology</td></tr><tr><td>8. Government provision of health care</td></tr><tr><td>9. The use of animals in scientific experiments</td></tr><tr><td>10. Paternal responsibilities in child care</td></tr><tr><td>11. Government funding of tertiary education</td></tr><tr><td>12. Editorial policies of newspapers</td></tr><tr><td>13. The future of work</td></tr><tr><td>14. Provision of aid by wealthy nations</td></tr><tr><td>15. Patient attitudes to medical treatment</td></tr><tr><td>16. Government provision of social services</td></tr><tr><td>17. Computers in education</td></tr><tr><td>18. Capital punishment</td></tr><tr><td>19. Parental regulation of children&#x27;s television habits</td></tr><tr><td>20. Population growth</td></tr></table></body></html>

Whilst the objects of enquiry in the IELTS items shown in Table 11 are of a diverse nature they were found to be more amenable to further analysis than the university tasks. If there is any recurring theme to be discerned among these items, it is that of the social responsibilities of various agents of authority, especially with respect to: (i) the provision of services and (ii) the regulation of behaviours. On our analysis, the following items would fall within this overarching theme-items 2,7,8,9,11,12,14,16,18,19. In most instances, the agent in question is the government; other agents include wealthy nations (14), the scientific community (9), parents (19), fathers (6). This focus on the responsibilities of certain authorities is clearly connected to the rhetorical function of hortation and can be adduced here as additional evidence for the quite specific nature of Task 2 writing.

# 4. Conclusions

There are two broad conclusions to be drawn from the results we have described. The first is that there appears to be great diversity in the type of writing required of students in university coursework study, or at least in the types of tasks they are likely to encounter. Such diversity constitutes a challenge not only for students, but also for the EAP teacher and program designer who are involved in preparing students for these demands in presessional EAP programs. There is not the space here to outline in any systematic way how we think the patterns identified in our university corpus might translate into a coherent writing curriculum. On this issue, we shall go no further than to proffer the following minimal advice-namely if an EAP teacher is looking for a written genre to make central to their program, they could do no harm in opting for the common and garden university essay-that is, one written on the basis of a range of readings, and concerned possibly with a content of a more abstract, metaphenomenal nature. We note too that the persistence of the essay as the principal generic currency across the academy has been identified in other non-Australian studies (Dudley-Evans, 2002).

The second conclusion, the one most pertinent to the theme of this paper, is that there seem to be important differences between the writing required at university and that required to pass the IELTS test. To take the findings from the IELTS analysis together, we might characterise the conception of academic literacy under-lying this format as follows:

† writing as a spontaneous activity;   
† writing as opinion-giving;   
† evidence as anecdote, experience; writing as hortation (Should X be done?);   
† Real world phenomena as proper subject of writing; writing as an activity separate from reading.

These features are certainly at odds with the nature of first year university assessment tasks-in which writing is rarely spontaneous; opinions are acceptable in some disciplines, but not if they are supported only by personal experience and anecdote; valid evidence is usually seen as the findings of research or the authoritative pronouncements of established scholars; writing tends to be framed more around analytical rather than practical rhetorical modes (like hortation); abstract entities like ideas, theories and laws are as much the focus of writing as situations or actions in the real world; and perhaps most significantly, writing is an activity intimately related to processes of reading. It is our view that the form of writing being prescribed by the IELTS, on analysis, may have more in common with certain public forms of written discourse than with those of the academy. In particular, the emphasis placed on the spontaneous expression of opinion suggests such public nonacademic genres as the letter to the editor or newspaper editorial. Whilst practise in this type of writing will certainly contribute in a general way to students’ literacy development (how to write coherently, grammatically, etc.), it would be a mistake in our view to see it as an appropriate model for writing in a university context.

To return to the issue raised at the start of this paper-how teachers can reconcile the dual aim of EAP and IELTS preparation on pre-sessional programs-our findings lend greater support to the ‘separated’ model outlined by Deakin (1997) - that is, for EAP and IELTS to be dealt with in separately timetabled modules. Whilst the alternative model—an ‘integration’ of the two curricula—may be a worthwhile aim, and indeed for timetablers may seem like a more efficient allocation of time, without systematic attention given to the differences discussed above, such programs run the risk of presenting students with a confusing model of university writing. Finally, there is one point about which there can be no disagreement—this is, that it would be most unwise to view test preparation on its own as an adequate form of EAP writing instruction. In this regard, the ‘exclusive’ model is clearly the least satisfactory of the models mentioned.

A final question worth considering here briefly is whether the IELTS Task 2 could be redesigned somehow to bring it more into line with university writing demands. This takes in the broad issues of test validity and authenticity. As Bachman and Palmer (1996) point out, when large-scale public tests (like IELTS) have a significant washback effect on teaching programs, it is important that test tasks are as authentic as possible. However, as Wigglesworth and Elder (1996) note, in test development there is often a trade-off between validity and reliability. Recent developments to the IELTS test would suggest that reliability considerations have been more to the fore in the thinking of the test’s designers. Interestingly, in an earlier version of the test, in use up until the midnineties, tasks were constructed so that there was a thematic link between reading passages in the reading test and Task 2 items. Examinees thus, had the option of drawing on reading material to support their writing. But this feature, which among other things, provided a basis for the teaching of the skills of citation in preparatory courses, was abandoned unfortunately-mainly because it was thought too difficult to monitor for plagiarism in examinees’ responses, thus affecting test reliability. With the high stakes nature of IELTS, and with its anticipated growth as the test of choice for many institutions, the test is likely to come under additional accountability pressures. Thus, a continued emphasis on improving the reliability of the test is likely (UCLES, 2002a). Regrettably, such an emphasis may be at the expense of future improvements to the test’s validity.

# 5. Further research

The present study has considered only the Task 2 format of the IELTS writing test. Clearly, there is also a need to investigate the nature of the Task 1 format with respect to university writing requirements, and to see whether this task fills some of the rhetorical and linguistic gaps identified in the present study. The rather restricted format of this first component (written duplication of information in a graphical form) suggests, however, that the curriculum challenges of incorporating this writing format into an EAP program may be no less difficult than those for the Task 2.

In the broader area of writing research, the present study has made some contribution to the field of discourse analysis concerned with the classification and analysis of writing tasks. One limitation, however, of the taxonomic procedures used is that our dimensions of difference were all considered independently of each other. Clearly, there is a need to investigate in what ways these dimensions might relate to each other systematically; and in particular, to find out the extent to which categories of genre can be understood in terms of specific configurations of the other dimensions used: information sources; rhetorical functions; objects of enquiry. A better understanding of the nature of academic tasks will have some benefits for the field of language testing-to improve the way in which students are selected for university study. But such understandings are likely to have even greater benefits for the field of EAP teaching-to help students to be better prepared for their studies.

# Acknowledgements

We are grateful to IELTS, Australia for their generous funding of this project. An expanded version of this paper was first published in IELTS Research Reports No. 2. (1999) Canberra: IELTS Australia pp. 64–106.

# References

Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice. Oxford: Oxford University Press.   
Becher, T. (1989). Academic tribes and territories: Intellectual enquiry and the cultures of disciplines. Buckingham: Open University Press.   
Braine, G. (1995). Writing in the natural sciences and engineering. In D. Belcher, & G. Braine (Eds.), Academic writing in a second language: Essays on research and pedagogy (pp. 113–134). Norwood, NJ: Ablex.   
Canesco, G., & Byrd, P. (1989). Writing requirements in graduate courses in business administration. TESOL Quarterly, 23, 305–316.   
Carson, J., Chase, H., Gibson, S., & Hargrove, M. (1992). Literacy demands of the undergraduate curriculum. Reading Research and Instruction, 3, 25–50.   
Coley, M. (1999). The English language entry requirements of Australian universities for students of non-English speaking backgrounds. Higher Education Research and Development, 18, 7–18.   
Deakin, G. (1997). IELTS in context: Issues in EAP for overseas students. EA Journal, 15, 7–28.   
Dudley-Evans, T. (2002). The teaching of the academic essay: Is a genre approach possible?. In A. Johns (Ed.), Genre in the classroom: Multiple perspectives (pp. 225–236). London: Lawrence Erlbaum.   
Hale, G., Taylor, C., Bridgeman, B., Carson, J., Kroll, B., & Kantor, R. (1996). A study of writing tasks assigned in academic degree programs. (Research Rep. no. 54). Princeton, NJ.   
Halliday, M. A. K. (1994). An introduction to functional grammar (2nd ed.). London: Edward Arnold.   
Hamp-Lyons, L. (1986). The product-before: Task-related influences on the writer. In P. Robinson (Ed.), Academic writing: Process and product (pp. 35–46). Oxford: Modern English Publications (in association with the British Council).   
Hamp-Lyons, L. (1991). Pre-text: Task-related influences on the writer. In L. Hamp-Lyons (Ed.), Assessing second language writing in academic contexts (pp. 87–110). Norwood, NJ: Ablex.   
Hoey, M. (1983). On the surface of discourse. London: George Allen and Unwin.   
Horowitz, D. (1986). What professors actually require of students: Academic tasks for the ESL class-room. TESOL Quarterly, 20, 445–462.   
Horowitz, D. (1991). ESL writing assessments: Contradictions and resolutions. In D. Hamp-Lyons (Ed.), Assessing second language writing in academic contexts (pp. 71–86). Norwood, NJ: Ablex.   
Huddleston, R. (1982). Introduction to the grammar of English. Cambridge: Cambridge University Press.   
Lackstrom, J., Selinker, L., & Trimble, L. (1973). Technical rhetorical principles and grammatical choice. TESOL Quarterly, 7, 127–136.   
Levinson, S. (1979). Activity types and language. Linguistics, 1, 356–399.   
Lyons, J. (1977). Semantics, Vol. 2. Cambridge: Cambridge University Press.   
Mann, W., & Thompson, S. (1989). Rhetorical structure theory; a theory of text organisation. In L. Polanyi (Ed.), The structure of discourse (pp. 85–96). Norwood NJ: Ablex.   
Martin, J. (1984). Language, register and genre. In F. Christie (Ed.), Language studies: Children’s writing reader (pp. 21–30). Geelong: Deakin University Press.   
Martin, J. (1989). Factual writing: Exploring and challenging social reality. Oxford: Oxford University Press.   
Meyer, B. (1975). The organisation of prose and its effects on recall. New York: North Holland.   
Moore, T. (2002). Knowledge and agency: A study of metaphenomenal discourse in textbooks from three disciplines. English for Specific Purposes, 21, 347–365.   
Paltridge, B. (1996). Genre, text type, and the language learning classroom. ELT Journal, 50, 237–243.   
Reid, J., & Kroll, B. (1995). Designing and assessing effective classroom writing assignments for NES and ESL students. Journal of Second Language Writing, 4, 17–41.   
Swales, J. (1990). Genre analysis: English in academic and research settings. Cambridge: Cambridge University Press.   
Taylor, G. (1989). The student’s writing guide for the arts and social sciences. Cambridge: Cambridge University Press.   
Trimble, L. (1985). English for science and technology: A discourse approach. Cambridge: Cambridge University Press.   
UCLES (1990). IELTS specimen materials. Cambridge: UCLES Publications.   
UCLES (1996). The IELTS handbook. Cambridge: UCLES publications.   
UCLES (2002a). IELTS annual review 2001/2002. Cambridge: University of Cambridge. Local Examination Syndicate.   
UCLES. (2002b). International English language testing system. Available at: http://www.cambridgeesol.org/ exam/academic/bg_ielts.htm.   
Weigle, S. (2002). Assessing writing. Cambridge: Cambridge University Press.   
Wigglesworth, G., & Elder, C. (1996). Perspectives on the testing cycle: Setting the scene. Australian Review of Applied Linguistics Series S, 13, 13–32.

Tim Moore works in the Language and Learning Unit, Faculty of Arts, Monash University where he provides programs of language and academic support to students. He is currently doing PhD research on critical thinking and the disciplines.

Janne Morton works at the Language and Learning Skills Unit, University of Melbourne. She has published in the field of language testing and discourse analysis. Her current research interests are disciplinary discourses, and spoken academic genres.