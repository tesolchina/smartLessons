# Online individualized corrective feedback on EFL learners’ grammatical error correction

Chao-Jung Ko

To cite this article: Chao-Jung Ko (2024) Online individualized corrective feedback on EFL learners’ grammatical error correction, Computer Assisted Language Learning, 37:7, 1449-1477, DOI: 10.1080/09588221.2022.2118783

To link to this article: https://doi.org/10.1080/09588221.2022.2118783

# Online individualized corrective feedback on EFL learners’ grammatical error correction

Chao-Jung Ko

Department of English, National University of Tainan, Tainan, Taiwan

# ABSTRACT

This study aimed to examine the impact of an online writing system (OWS) providing individualized corrective feedback (CF) on learners’ self-correction of grammatical errors (GE). It consisted of two phases: the pilot and the formal phases. Four EFL (English as a Foreign Language) Taiwanese university students participated in the study at the pilot phase and 22 at the formal phase. An OWS was developed to provide individualized CF in text modes. The data was collected from learners’ four written works, their interview transcriptions, and the teacher’s observation at the pilot phase, and from learners’ pre- and post-written works, three online works, and a questionnaire at the formal phase. The findings of the study showed that individualized CF provided by the system seemed more beneficial to self-correction of GE for those learners at a higher proficiency level. They applied feedback to self-correction and problem-solving, which enhanced their autonomy development. By contrast, the system seemed less beneficial to lower-level learners’ self-correction of GE. In addition, individualized CF seemed to be most beneficial to subject-verb agreement error self-correction, and least beneficial to verb tense correction. Different feedback types and provision modes should be considered when using the developed system to deliver corrective feedback.

# KEYWORDS

Individualized corrective feedback; text feedback; grammatical error correction; self-correction; writing

# 1.  Introduction

Researchers (e.g. Ferris & Roberts, 2001) have indicated that learners expect to receive feedback and might feel disappointed when it is not forthcoming. However, how teachers should give written corrective feedback (CF) has been debated for years. Han and Hyland (2015) suggested that language teachers should carefully plan their strategies of written CF to enhance learners’ engagement with feedback. During recent years, technology has been widely used as a strategy to engage language learners with written CF provided by their teachers or with the software.

Using technologies as new learning tools has enabled changes in writing practices (Edwards-Groves, 2012; Warschauer, 2007), and in the ways instructors provide feedback. Although the emergence of digital technologies has transformed both the ways that instructors teach writing and researchers investigate it, many related studies have investigated the effects of technology on L2 writing using existing systems or social networking websites (Chen & Cheng, 2008; Elola & Oskoz, 2017; Li, 2021; Lim & Phua, 2019; Nazari et  al., 2021, Zhai & Ma, 2021; Zheng & Warschauer, 2017). Among those existing systems or websites, very few allowed learners to receive individualized feedback because that feedback was usually generated by the software, which did not detect some mistakes in learners’ written work due to their limited capability (Lim & Phua, 2019; Perdana & Farida, 2019), or allow instructors to provide quality feedback that ‘attends to meaning, respond to the writer’s thoughts, and give specific, personal comments’ (Chen & Cheng, 2008, p.107).

Some studies (e.g. Shintani & Aubrey, 2016) have indicated that despite the immediate or instant feedback provided by the software, some students did not use or trust the feedback they received to improve their writing. Some of them even indicated their preference for receiving feedback from their teachers and for interacting with them. Such social interaction is an important consideration for feedback provision (Hyland & Hyland, 2019; Ware & Warschauer, 2006). For some researchers (e.g. Vojak et  al., 2011), learners’ writing proficiency should be improved through interacting with a teacher who provides feedback that allows learners to develop independent strategies for revising their work (Cavaleri et  al., 2019).

However, few of the current existing systems can allow teachers to provide individualized feedback and to interact with students. Although teachers may provide individualized feedback through some social networking programs, the impact of that feedback on learners’ self-correction is still under-investigated. To fill these gaps, an online writing system (OWS) was developed to engage learners with individualized CF to improve their self-correction of grammatical errors (GEs). It could allow learners to receive individualized CF from their teachers via the system rather than from the system itself. This study aims to examine the impact of the OWS in providing individualized CF on learners’ self-correction of GE.

# 2.  Feedback in online media

Research has shown that teachers’ CF could facilitate students’ writing improvement, but the time which needs to be invested for error correction (EC) may overwhelm the teachers (Perdana & Farida, 2019), particularly when they have to deal with students’ repeated errors that might cause teachers’ frustration (Wihastyanang et  al., 2020). As technology develops and provides new capabilities to enhance personalized instruction (Chen & Liu, 2011), the use of computers to deliver and mediate feedback is becoming more and more common in practice and research. The provision of computer-networked connectivity, either in synchronous or asynchronous contexts, allows learners to become more active and autonomous (Nazari et  al., 2021; Warschauer, 2010;) when seeking feedback due to the ability to raise questions and take the initiative in discussions.

Feedback delivery via technology has several advantages. For example, teachers can store comments for later retrieval and get paper printouts for in-class discussion (Hyland & Hyland, 2006). They can use transcripts to increase learners’ autonomy in EC and reflection on their writing. Moreover, learners can receive feedback not only from their teacher (Guénette & Lyster, 2013) but also from other peers (Cho, 2017), or software-generated error feedback (Lim & Phua, 2019; Nazari et  al., 2021), which may reduce teachers’ marking time. The potential for delivering feedback in diverse ways also allows learners to receive feedback tailored to their learning speed.

# 2.1.  Feedback delivery modes and technology

The use of technology offers more possibilities for language teachers to provide individual feedback and EC, which has been a significant challenge for teachers in a traditional classroom since providing feedback is a time-intensive task for most of them (Conijn et  al., 2019; Lim & Phua, 2019; Warschauer & Ware, 2006). A growing body of literature has looked into the use of technology to deliver feedback during recent years. Feedback can be delivered through computers, or humans via technology use. Each delivery type has its advantages as well as limitations.

Computer-generated feedback influenced the writing of learners in different contexts differently. In some contexts, it is considered not helpful to writing improvement. For example, Chen and Cheng (2008) found that computer-generated feedback was not helpful for upper-intermediate level Taiwanese college students to communicate effectively in writing in terms of form and meaning and could not address their individual writing problems. Many students, particularly higher ability ones, thought form-focused feedback was of no help in revision, especially when teachers’ feedback was lacking. The students reported that the feedback was only beneficial to their writing in the early drafting and revising stage, followed by later feedback from teachers and peers. Some students complained about writing for a computer program and interacting with the software.

In other contexts, computer-generated feedback has been considered beneficial to improve writing. For example, Lim and Phua (2019) examined the provision of computer-generated feedback on mistakes of grammar, spelling, and punctuation in students’ composition in Singapore schools. The results showed that both teachers and students were receptive to the use of the software to improve students’ English composition and that it was time-saving for teachers. However, $4 4 . 7 \%$ of students found that some of their grammatical mistakes were not corrected and would have liked to receive more accurate feedback from the software. Two of the teachers also found that the software was not always accurate in detecting errors. Moreover, most teachers observed that the software was helpful to low-ability students in reducing their errors, but not to high-ability students.

The above researchers claimed that computer-generated feedback could not replace teachers, but only supplement writing instruction. Similar opinions have been expressed in other studies (Vojak et  al., 2011; Warschauer & Ware, 2006). Computer-generated feedback was suggested as useful to identify and improve learners’ language errors in early draft writing (Chen & Cheng, 2008; Klobucar et  al., 2012; Lim & Phua, 2019; Tang & Rich, 2017), and therefore likely to reduce teachers’ marking time (Lim & Phua, 2019). However, computer-generated feedback might increase teachers’ workload due to its providing little information for learners’ later revision (Chen & Cheng, 2008) and its inability to detect some students’ errors (Lim & Phua, 2019). Therefore, it does not seem helpful for high-ability students’ writing.

In addition to computer-generated feedback, feedback can be given by humans via technology. The teacher’s use of delivery modes greatly influenced learners’ perception of feedback. Wihastyanang et  al. (2020) delivered teachers’ feedback and peer feedback through Edmodo in EFL university-level writing classes and found that the inclusion of ICT use did not allow their learners to perform better in writing than those receiving teachers’ feedback in the traditional mode due to unclear feedback, poor internet connection and the absence of teacher-student interaction. Also, the simplicity of feedback forms didn’t provide enough clues for the learners to improve their writing.

However, Thompson and Lee (2012) delivered video feedback in college-level writing courses and found that their students perceived the feedback combining auditory explanations with visual representation of their work could give them enough information and therefore allowed them to make more meaningful revisions. Meanwhile some students were frustrated by technological difficulties that posed obstacles to understanding how to apply feedback to their revision, and therefore stated their preference for traditional written feedback due to its ease. Also, delivering video feedback was time-consuming for teachers.

In addition, delivery modes seem to affect the interpersonal aspects of feedback. Cunningham (2019) investigated the use of video and text feedback in three university- level ESL writing classes and found that the teacher was considered as the only authority while delivering text feedback, but as only one of many possible opinions while giving video feedback. Video feedback allowed the learners to have more control over their learning.

Regardless of delivery modes, teachers’ feedback was delivered through the existing programs in the above studies. However, it seems the selection of the program for feedback delivery influenced the teachers’ use of resources to give feedback. It appeared the programs for video feedback delivery seemed to possess more functions compared to those for text feedback delivery. In the study of Wihastyanang et  al. (2020), Edmodo only allowed teachers to provide text feedback with limited clues. In Cunningham’s (2019) study, text feedback was delivered through MS Word, which provided limited space and ability for teachers to edit feedback, whereas the spontaneous spoken aspects of video allowed teachers an abundance of expanding resources that enhanced the interpersonal aspects of teacher-student communication. Therefore, it is reasonable to assume that using a program with richer functions for text feedback delivery might have different impacts on learners’ writing improvement, which is one of the motives for the researcher of this study to develop the current system.

# 2.2.  Feedback types

Although studies have shown that feedback provision and reception in technology-facilitated writing environments have many advantages (Zheng & Warschauer, 2017), how different types of feedback influence learners’ revision and writing skills has been the subject of debate and remain under-examined. Ellis (2009) identified some basic types for providing written CF, involving direct CF, indirect CF, metalinguistic CF, and the focus of the feedback.

Some writing researchers argued that most learners favor indirect feedback that involves them in guided learning and problem-solving (Hyland & Hyland, 2006). Moreover, it leads them to reflect on linguistic forms, which may promote long-term acquisition (James, 2013). However, others (Chandler, 2003; Elwood & Bode, 2014) suggested that learners preferred teachers’ direct correction because it is more explicit and immediate and best for producing accurate revision and subsequent writing.

However, for EC researchers, indirect feedback is more helpful for learners to make progress in accuracy in the long term (Ferris & Helt, 2000) or both have positive effects (Chandler, 2003; Tursina & Chuang, 2016). Although the students in Chandler’s study (2003) showed their preference for direct corrections because it was the fastest and easiest way for them to correct errors, they perceived that self-correction allowed them to learn more.

Metalinguistic CF is another strategy for providing CF. In the case of metalinguistic CF, teachers provide CF by using error codes and brief grammatical descriptions (Ellis, 2009). This feedback strategy can help learners to understand the errors they have committed by appealing to their explicit knowledge (Ellis et  al., 2008). Some researchers suggested that this strategy could improve learners’ accuracy (Sheen, 2007) and was more effective than direct CF in the long term (Ellis, 2009).

In addition to the above strategies of CF, researchers also made distinctions between focused and unfocused corrections. Researchers such as Ferris and Helt (2000) argued that different types of error should be treated differently and therefore more recently, their attention has turned from broad correction of all errors (unfocused correction) to focused correction of errors. They believed that learners may fail to check all their errors if EC is not focused and suggested that teachers’ focused feedback is more effective than unfocused feedback on learners’ improvement of grammatical accuracy in L2 writing (Ellis, 2009; Farrokhi & Sattarpour, 2011, 2012; Sheen et  al., 2009). However, others (Frear, 2010; Karimi et  al., 2012) found that written CF, whether focused or unfocused, could equally contribute to L2 writers’ grammatical accuracy.

For Nicol and Macfarlane‐Dick (2006), good feedback practice includes providing learners with high-quality information and encouraging teacher-student conversations about their learning. Good external feedback involves offering not only timely feedback with corrective advice, but also information that can help learners trouble-shoot their performances and self-correct. For them, timely feedback refers to feedback given ‘before it is too late for students to change their work (i.e. before submission)’ (p.10). However, feedback delivered by most current existing systems seems unable to meet the above goals.

Computer-generated feedback provided by existing systems cannot address learners’ individual errors (Chen & Cheng, 2008). As to teachers’ feedback delivered through application software, some provide little or unclear information for learners to improve writing (Wihastyanang et  al., 2020) due to limitations of the software. Other feedback is delivered through software that causes frustrating tech-related problems to learners and makes teachers feel overburdened (Thompson & Lee, 2012). Moreover, some software doesn’t offer opportunities for teachers to give explanations for learners’ self-corrections, which is a skill that must be taught and trained (Guénette, 2012). As studies indicated that language learners hope to receive teachers’ feedback (Tursina & Chuang, 2016; Zheng et  al., 2015) or they may misunderstand feedback without teachers’ explanations (Cavaleri et  al., 2019), the researcher has developed a system that allows learners to receive feedback with explanations from teachers and help them develop self-correction skills in writing.

This study examined whether and how individualized CF delivered via an OWS could be beneficial to EFL learners’ self-correction of GEs. The examined research question was:

Can individualized CF delivered through the developed system assist EFL learners’ self correction of grammatical errors? if so, how?

# 3.  Description of the system

Many writing assistance systems are not designed for users to learn skills independently (Napolitano & Stent, 2009) and don’t treat writers’ learning problems individually. Although writers can receive assistance from those systems, feedback is often provided in batch rather than interactive modes. Therefore, learners cannot use the systems to interact with writing tutors or other writers. Also, their use of a rigid and context-independent set of writing rules may lead users to produce a mechanical style of writing rather than personalizing their writing.

Hence, an OWS was developed to improve the current existing systems. The OWS allows teachers to provide CF for individual students to become aware of their error patterns and independent writers who can self-correct their own mistakes (Li, 2016). It can record learners’ errors, and thus enable teachers to provide feedback accordingly in hopes of giving them a sense that feedback is provided for them personally (Hyland & Hyland, 2019).

The developed system has some unique features that distinguish it from current existing systems. First, it allows teachers to view students’ uploaded writing, edit or point out errors and provide CF in text modes. CF could be given in learners’ original texts as well as in the ‘Correction’ section, which enables teachers to interact with learners individually (Figure 1).

If students do not make an error in any of their subsequent writing, teachers can hide the related feedback, which can be reactivated once the same error appears. For Aljaafreh and Lantolf (1994), scaffolded help should be offered only when needed and removed as soon as learners can function on their own. With teachers’ scaffolded interaction, learners can move towards autonomous learning (Williams, 2004) (Figure 2).

![](img/52898a10553fe22298908c2095b0b30ebabe6d02b5f7d977b6be5728a67ace99.jpg)  
Figure 1. Feedback provision in the text and the ‘correction’ section.

In addition, the teacher can view all the corrections they have been provided for a student’s written works and therefore quickly know his/ her learning situation. They can also provide general comments regarding the organization and structure of a piece of writing (Figure 3).

As for students, the system allows learners’ online writing and submission. They can view teachers’ feedback displayed at the top of the webpage and self-correct their errors based on the feedback while or after writing. The displayed feedback, replacing the teacher, serves a reminder function to assist learners’ reflection on their errors and therefore encourage independent learning (Figure 4).

Additionally, the system records all the corrections and feedback that the teacher gave to learners’ writing. Such feedback could provide visual samples for learners’ future writing and thereby encourage learning skills (Napolitano & Stent, 2009) (Figure 5).

# 4.  Methodology of the study

This study adopted sequential exploratory mixed methods, which ‘is characterized by an initial phase of qualitative data collection and analysis followed by a phase of quantitative data collection and analysis’ (Creswell et  al., 2003, p.178), and involves the integration of the data at some stages in the process of research. The use of mixed methods can neutralize some of the disadvantages of using a single method, and can therefore strengthen a study (Greene & Caracelli, 1997). The study consisted of two phases: a pilot phase (qualitative) and a formal phase (quantitative).

![](img/ef3c13a252b8daa326155e15f2dfeec75b18508d83ff1806c30ea620f8a0f284.jpg)  
Figure 2. T he hidden function for errors no longer made by students.

![](img/bdc81da432f4e26c703406bf9fe2fb6951642247b0c513fa34a0bfde0b470dfb.jpg)  
Figure 3. T he space for teachers to view previous corrections and to provide general feedback.

The pilot study was conducted to pre-test the system and to give advance warning of problems that might occur in the formal study (Baker, 1994). Moreover, it offered flexibility for the researcher to adjust data collection and analysis to the emerging findings (Creswell, 2012). At the pilot phase, the qualitative method was mainly used to help develop the quantitative instrument and complement the quantitative findings of the formal study (Steckler et  al., 1992). At the formal phase, the quantitative method was mainly used by combining some qualitative data in order to get better insight into the explored questions. Qualitative data in the pilot study were collected before quantitative data in the formal study with the intention of exploring the questions and then

# Your Previous Mistakes/Errors

![](img/ea52f30e08f818d050be5f0e447e2ef73e74f4a3fc3957f1e2e4aa3794f50a8a.jpg)  
Figure 4. Feedback displayed at the top of the editing webpage.

![](img/51b0378986ff1a0773ca119327a1548784b8fac9c817f3b811436ba73445b76e.jpg)  
Figure 5. T he space for students to see previous feedback.

following up on this exploration with quantitative data, which were amenable to studying a large sample so that results might be inferred to a broader population (Creswell et  al., 2003).

# 4.1.  Participants

At the pilot phase, four EFL university learners in Southern Taiwan were recruited from the researcher’s general English classes to participate voluntarily in the study. Two of them were advanced level students majoring in English, and the other two were upper-intermediate level students majoring in Environmental Engineering. Their language proficiency level was determined by the General Scholastic Ability (GSA) Test, which is the university entrance exam in Taiwan. They all had been learning English for ten years and possessed basic computer skills. Four students were studied in the hope that an in-depth understanding of each student about the explored questions could be reached (Creswell et  al., 2007). All students signed written informed consent forms before the study.

At the formal phase, 22 English major freshmen from a writing class in a Taiwanese university were recruited to participate voluntarily in the formal study. They received two-hour instruction from the researcher in a classroom every week. Their language proficiency level as determined by the GSA test varied from upper-intermediate to advanced. All of them had been learning English for over eight years and possessed basic computer skills. All the participants signed written informed consent forms before the study.

# 4.2.  Feedback

In this pilot study, the learners received unfocused CF for each piece of writing. The reason why unfocused feedback was selected was that the teacher/researcher believed that if only some focused GEs were addressed, the learners might consider those uncorrected GEs correct and therefore continue making the same errors. Thus, she perceived that she did not ‘have the luxury of focusing exclusively on a single error’ (Ellis et  al., 2008, p.367).

Unfocused feedback was given for all GEs that learners made in their written works because most Taiwanese students hoped to receive the teacher’s comprehensive feedback on all types of errors, but mostly on GEs (Berg & Lu, 2017). For Perdana and Farida (2019), teachers’ grammar CF can help learners advance their writing. Moreover, the impact of technology on writing, especially for grammatical feedback, has not been widely studied (Nazari et  al., 2021).

The feedback type selected for the study was metalinguistic explanations with underlining, which some previous studies considered effective (Chandler, 2003; Ferris & Roberts, 2001) and able to contribute to accuracy improvement (Karim & Nassaji, 2020). It is suggested that the use of such indirect feedback type could encourage learners to self-correct GEs by highlighting their errors and giving explanations without providing correct forms, since they are involved in cognitive problem-solving (Guénette, 2012). The feedback was delivered in text modes, which some learners considered more time-efficient (Henderson & Phillips, 2015; Thompson & Lee, 2012) compared to other types of feedback.

# 4.3.  Data collection and analysis

The data of the pilot study involved multiple information sources (Yin, 2003): the learners’ four written works, their interview transcriptions, and the teacher’s observations. Interviews and observations were selected as the instruments because interviews allow access to data that researchers cannot observe (Merriam, 1990), and observations allow researchers to see what people do instead of what they say they do (Walshe et  al., 2012).

The participants’ writing was analyzed, and GEs appearing in the writing were counted. The frequency of error appearance and rates of repeated errors were calculated so that the teacher could perceive the actual impact of technology-enhanced feedback on the participants’ use of that feedback, which has been seldom examined in the majority of the research (Cavaleri et  al., 2019). Including numbers at this phase was ‘a legitimate and valuable strategy’, as it was used as ‘a complement to an overall process orientation’ to qualitative research (Maxwell, 2010, p.480).

After the study, a thirty-minute semi-structured individual interview (Appendix A) was conducted with each participant. Content analysis was used to examine the interview transcriptions. Inductive category formation, in which only those parts relevant to the research questions were considered, was processed to categorize interview data based on the category definition, which served ‘as selection criterion to determine the relevant material from the texts’ (Mayring, 2014, p.82). Since the scope of analysis was more explorative, there was no pre-formulated set of categories. After the categories were formulated and revised, they were interpreted in the context of the research questions.

At the formal phase, the data were collected from the learners’ preand post-written works, three online works, and the results of the questionnaire regarding their perception of this experience. The questionnaire, developed based on the themes identified in the qualitative data of the pilot phase (Creswell et  al., 2003), used a five-point Likert scale ranging from $5 =$ strongly agree to $1 =$ strongly disagree with a $3 =$ neutral type of response (Jamieson, 2004). Offering a neutral response might reduce the chance of response bias, and allowed respondents not to feel forced to favor one response over others (Fernandes & Randall, 1991) if they didn’t favor any. The questionnaire consisted of four sections.

The questions in the first section asked about learners’ background information. The items in the second section asked about learners’ perception of the feedback. The third section included the items investigating learners’ perception of the system. The items in the fourth section asked about learners’ perception of interacting with the teacher via the system. The learners were required to evaluate their level of agreement or disagreement with each item in Sections 2,3, and 4, where both positively- and negatively- worded items were included to minimize response bias (Croasmun & Ostrom, 2011). The negatively worded items measured the same concept that the positively worded items did.

Moreover, seven open-ended questions were asked in the fifth section of the questionnaire to allow the learners to reply in their own terms and opinions about the feedback, the system, and the interaction with the teacher. Such questions were considered as attractive for small-scale research due to their possibility of inviting honest and personal comments from the learners (Cohen et  al., 2007).

After questionnaire completion at the end of the study, the learners’ responses were keyed into the computer and analyzed statistically using Statistical Package for the Social Sciences (SPSS). The mean score of each item was calculated and frequency, item, and reliability analyses were undertaken. The Cronbach’s Alphas for Sections 2, 3, and 4 of the survey were 0.84, 0.76, and 0.74, which suggested acceptable internal consistency among the questions. As to the open-ended questions, the answers were coded, some main trends were identified and themes were determined.

Moreover, a comparison of the pre- and post-written works, as well as their three online works was done to see if the experience would impact their self-correction of GEs positively. The participants’ pre- and post- written works were analyzed. They were required to describe a summer vacation in the pre-written work, and to describe a winter vacation in the post-written work. GEs appearing in those works were categorized based on the error types suggested by Hewings (2013), whose typology has been used in many Taiwanese university EFL classrooms. Those errors were counted and compared, repeated GEs were identified and the rates of repeated errors were calculated.

# 4.4.  Procedure

Before the pilot study, all the participants created accounts to log into the OWS. Then they were introduced to its functions and trained to use them properly. During the study, each participant produced one new written work every two weeks for four works in total. They wrote without any time pressure. The topics of writing were selected based on consideration of their interests and were identical for all of them.

After the participants’ work submission, the teacher logged into the system and provided unfocused grammatical feedback. The errors were underlined in the text section, and related descriptive feedback was provided and recorded in the correction section. An example was given as follows:

‘Whenever I was available, I visit (4) my little Samuel in his baby buggy’. (in the text) (4) ‘The use of the verb tenses is not consistent’. (in the correction section)

When students produced the next written works, the teacher’s feedback would appear at the top of the editing webpage to remind them not to repeat the same errors. By reminding the learners of their errors through the system, they were expected to be aware of those errors, self-correct them and therefore become autonomous learners.

The procedure of the formal study was similar to that of the pilot study. However, the writing topics were selected based on the types of writing that the students were learning and were identical for all participants, who produced one new written work every three weeks for three works in total. The genres of the three written works were description, comparison, and definition respectively. After their work submission, the students received the teacher’s feedback through the system. If they failed to understand the feedback, they could ask the teacher related questions during the class time, although feedback consultation meetings were not provided.

# 5.  Findings

# 5.1.  Findings of the pilot study

In response to the research question as to whether individualized CF provided by the OWS assisted EFL learners’ self-correction of GEs, the findings from the written works analysis show that the numbers of errors made by Participants A, B, and C were about the same (Table 1).

Repeated errors were identified and calculated. Participants A and B tended to be more capable of self-correcting errors according to the feedback. Their rates of correction were 60 and 75 respectively (Table 2).

Participant C seemed unable to self-correct errors based on the feedback. Her rate of correction was only 16.7. Although Participant D’s number of errors was the lowest due to her shortest production (around 150 − 200 words per work), the teacher found that she failed to self-correct any error despite the feedback. She continued repeating the same errors, the feedback on which had been provided after the submission of her first written work.

Table 1. N umbers of GE s in learners’ four written works.   

<html><body><table><tr><td>Participants</td><td>Errors</td></tr><tr><td>A</td><td>20</td></tr><tr><td>B</td><td>22</td></tr><tr><td>c</td><td>21</td></tr><tr><td>D</td><td>15</td></tr></table></body></html>

Table 2. R ate of repeated GE s corrected by each participant.   

<html><body><table><tr><td>Participants</td><td>Repeated errors</td><td>Corrected errors</td><td>% of correction</td></tr><tr><td>A</td><td>5</td><td>3</td><td>60</td></tr><tr><td>B</td><td>4</td><td>3</td><td>75</td></tr><tr><td>c</td><td>6</td><td>1</td><td>16.7</td></tr><tr><td>D</td><td>2</td><td>0</td><td>0</td></tr></table></body></html>

To look into how individualized CF assisted the learners to self-correct GEs, the findings from the interview data analysis were presented under the main categories that were interpreted in the direction of the research questions (Mayring, 2014). Those categories were the system and its display function, the teachers’ feedback and its type, and the interaction with the teacher.

In terms of the system, they all considered it user-friendly. However, there was room for improvement.

D: I think it is not so convenient because I cannot make space by clicking the ‘Tab’ button like I usually do while using Microsoft Office Word.

Although the system allowed online editing, Participants A and B preferred using Microsoft Office Word to edit writing before submission because the software could inform them of their writing errors (Tafazoli et  al., 2014).

As to the feedback display function, Participants A, B, and C were satisfied with the visual text function. But Participant D suggested it could be replaced by a pop-up window, which might better serve as a reminder for her to do self-correction.

T: In what way do you prefer being reminded? D: Maybe a pop-up window… Because I have to read pop-up messages before submission.

The above participants’ feedback about the system was collected and would be considered for system updates, which would be carried out after feedback collection from a larger number of system users in the formal study.

Regarding the feedback, all the participants except Participant D expressed their ability to understand it.

T: You made this mistake twice. … You have to add ‘ing’ to a verb after a preposition. D: A preposition? T: So you did not understand my feedback?

D: I guess so.

In addition, it was noticed that Participants A and B were able to self-correct errors based on the teacher’s feedback. They used the feedback displayed at the top of the editing page to help them self-check errors after writing.

A: After I wrote a new text, I usually checked if I made the same errors in my previous texts… I usually checked after writing. If I checked before writing, I would be nervous and feel the pressure.

B: I would go through my texts and check them based on the feedback displayed at the top.

Although Participant C said she could understand the teacher’s feedback, few successful error corrections were seen in her written works. When asked about the way she used the feedback, she said she went through the feedback before writing but did not apply it to self-correction.

C: I usually looked through the feedback before I started writing.

As for Participant D, when asked why she didn’t correct errors after feedback provision, she expressed that she sometimes could not understand the teacher’s metalinguistic explanations. And it was noticed that even when she could, she was unable to do corrections and ignored the feedback completely while writing.

D: Sometimes I don’t know how to correct the errors. T: Did you notice the feedback at the top of your editing page? D: I didn’t pay attention to it.

In her case, if the feedback had been given in Chinese, it might have been more helpful for corrections.

D: I would check feedback out if it was in Chinese.

In terms of the feedback type, Participants A and B expressed their satisfaction with the type. However, Participant D was not used to it and expected to receive direct explicit feedback.

D: I hope there could be direct corrections.

For Participant C, the current feedback type was acceptable, but it could be given more explicitly with examples.

C: If there were examples, I would not have to look up right answers. I could understand the feedback immediately.

When being asked about their perception of the interaction with the teacher, Participants C and D felt distant from the teacher due to the lack of face-to-face(f2f) meetings. Participant A had a different perception. He enjoyed interacting with the teacher via the computer and thought it was less stressful to receive feedback through the system.

A: With the assistance of this system, students don’t need to meet the teacher f2f. … It can reduce students’ pressure of meeting the teacher.

However, they all considered it necessary to have f2f meetings with the teacher to discuss their writing.

# B: Meeting teachers f2f would be more effective for me to correct errors.

# 5.2.  Findings of the formal study

To examine whether online individualized CF assisted EFL learners’ self-correction of GEs, we looked closely into the participants’ GEs made in the pre- and post- tests and three written works, and found that the feedback affected the students’ self-correction to different extents. In the post-test, six among 22 students stopped making the errors appearing in the pre-test. Three stopped making the errors appearing in the pre-test and the three online written works. Seven had an error repeat rate of less than 50 percent, while five had an error repeat rate of up to 50 percent. Only one student could not stop repeating the same errors throughout the study (Table 3).

To examine error types closely, the students made twelve error types $( \mathrm { N } = 6 7 )$ in the pre-test. The most frequent error types in the pre-test were subject-verb agreement ( $\mathrm { N } = 1 3$ , $1 9 . 4 \%$ ), singular and plural nouns $( \mathrm { N } = 1 0$ , $1 4 . 9 \%$ ), and articles $\left( \mathrm { N } = 9 , 1 3 . 4 \% \right)$ respectively. During the study, four more error types appeared in their three written works. The subsequently appearing error types consisted of conjunctions, lack of verbs in the sentence, phrasal verbs, and relative clauses, but their amount comprised only a small percentage $( 5 . 8 \% )$ of all the errors in the post-test $\left( \mathrm { N } = 1 3 7 \right)$ ). Since the students produced longer and more sophisticated sentences in the post-test, the number of errors in the post-test was almost twice as many as those in the pre-test.

Regarding the post-test, the most frequent error types were verb tenses $\left( \mathrm { N } = 3 7 , \ 2 7 \% \right)$ , singular and plural nouns (23, $1 6 . 8 \% )$ , and articles (14, $1 0 . 2 \% )$ as well as prepositions (14, $1 0 . 2 \% )$ ) respectively. Although all 12 error types found in the pre-test also appeared across the study, the reminding function seemed beneficial for reducing the number of subject-verb agreement errors (13 to 5, $1 9 . 4 \%$ to $3 . 6 \%$ ). The impact on the other two frequent error types (singular and plural nouns, and articles) seemed limited, and the system did not seem helpful for verb tense corrections. The number of this error type increased from 7 $( 1 0 . 4 \% )$ to 37 $( 2 7 \% )$ throughout the study (Table 4).

Table 3. N umbers of error types and percentages of repeated errors (RE ).   

<html><body><table><tr><td>Ss</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td></tr><tr><td>A</td><td>2</td><td>0</td><td>3322</td><td>24218</td><td>3</td><td>17</td><td>1210</td><td>0320</td><td>3 </td><td>4</td><td>4</td><td>4</td><td>3</td><td>5</td><td>2</td><td>320</td><td>2</td><td></td><td></td><td>5</td><td>1</td><td>1</td></tr><tr><td>B</td><td>4</td><td>1</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td>6</td><td>3r</td><td>3r</td><td>3</td><td>6</td><td>4</td><td>1</td><td></td><td>3</td><td>332</td><td>2531</td><td>4</td><td>21 </td><td></td></tr><tr><td>c</td><td>1</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td><td></td><td>4</td><td>1</td><td>1</td><td>1</td><td>5</td><td>4</td><td>1</td><td></td><td>2</td><td></td><td></td><td>4</td><td></td><td>53 </td></tr><tr><td>D</td><td>1</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td><td></td><td>2</td><td>1</td><td>1</td><td>1</td><td>2</td><td>1</td><td>1</td><td></td><td>1</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>RE%=D/A</td><td>50</td><td>0</td><td>67</td><td>50</td><td>33</td><td>1</td><td>0</td><td>0</td><td>67</td><td>25</td><td>25</td><td>25</td><td>67</td><td>20</td><td>50</td><td>0</td><td> 50</td><td>33</td><td> 50</td><td>20</td><td>0</td><td>0</td></tr></table></body></html>

(\* $\mathsf { A } =$ numbers of error types in the pretest, $\mathtt { B } =$ numbers of error types in the posttest, ${ \mathsf C } =$ numbers of repeated error types in the 3 online written works, $\mathsf { D } =$ numbers of repeated error types in the posttest).

Table 4. N umbers of errors for each error type.   

<html><body><table><tr><td></td><td>Pretest</td><td>%</td><td>Posttest</td><td>%</td></tr><tr><td>S-V agreement</td><td>13</td><td>19.4</td><td>5</td><td>3.6</td></tr><tr><td>Verb tense</td><td>7</td><td>10.4</td><td>37</td><td>27</td></tr><tr><td>Word form</td><td>6</td><td>8.9</td><td>7</td><td>5.1.</td></tr><tr><td>Article/Determiner</td><td>9</td><td>13.4</td><td>14</td><td>10.22</td></tr><tr><td>Punctuation</td><td>3</td><td>4.5</td><td>6</td><td>4.3</td></tr><tr><td> Preposition</td><td>7</td><td>10.4</td><td>14</td><td>10.22</td></tr><tr><td>Gerund/Infinitive</td><td>1</td><td>1.5</td><td>3</td><td>2.2</td></tr><tr><td>Vowel</td><td>1</td><td>1.5</td><td>1</td><td>0.7</td></tr><tr><td>Singular/plural noun</td><td>10</td><td>14.9</td><td>23</td><td>16.8</td></tr><tr><td>Noun compound</td><td>1</td><td>1.5</td><td>5</td><td>3.6</td></tr><tr><td>Capitalization</td><td>8</td><td>11.9</td><td>4.</td><td>2.9</td></tr><tr><td>Spelling</td><td>1</td><td>1.5</td><td>10</td><td>7.3</td></tr><tr><td>Conjunction</td><td></td><td></td><td>2</td><td>1.5</td></tr><tr><td>Relative clause</td><td></td><td></td><td>3</td><td>2.2</td></tr><tr><td>Lack of verb</td><td></td><td></td><td>2</td><td>1.5</td></tr><tr><td>Verb phrasal Total</td><td>67</td><td>100</td><td>1 137</td><td>0.7 100</td></tr></table></body></html>

Concerning the research question about how the learners’ self-correction of GEs benefitted from individualized CF delivered via the system, the students showed their satisfaction with the feedback $( \mathrm { M e a n } = 3 . 9 )$ and were more slightly satisfied with their interaction with the teacher (Mean $= 4 )$ ), but they held neutral opinions about the system $( \mathrm { M e a n } \ : = \ : 3 . 1 )$ ).

As to the students’ opinions about the feedback, the results showed that most students perceived that the feedback was beneficial to their EC (Mean $\mathit { \Theta } = \ 3 . 9$ ), and could pay attention to the displayed feedback serving a reminder function while producing new written works (Mean $= 4 )$ ). Moreover, most of them could understand the feedback (Mean $=$ 3.8) and considered it helpful to self-correct errors $\mathrm { ( M e a n = 4 ) }$ ). However, some expressed the hope of receiving feedback in other modes (Mean $= 3 . 4 \AA$ ).

In terms of the system, the students slightly agreed that the system was easy to use $( \mathrm { M e a n } \ : = \ : 3 . 3 )$ and expressed some satisfaction with its specifications $\mathrm { ( M e a n } = 3 . 2 \mathrm { ) }$ and interface $\mathrm { ( M e a n } = 3 . 3 )$ . As to the interaction with the teacher, most of the students held the opinion that they could perceive the teacher’s presence by reading her feedback (Mean $=$ 4), which was considered as enhancing their interaction $\mathbf { \langle M e a n \mid } = \mathbf { \langle 4 \rangle } ,$ ). Also, they felt comfortable receiving the teacher’s feedback through the computer $( \mathrm { M e a n } = 4 )$ .

Regarding the qualitative data of the questionnaire, 16 students indicated they could understand the teacher’s feedback and 18 could also pay attention to the feedback displayed at the top of the editing page while producing new works. However, understanding and paying attention were not sufficient for them to self-correct errors. Four admitted they sometimes did not know how to correct errors, three could not stop repeating errors, and two had to read feedback many times not to repeat errors. Moreover, six expressed their hopes about receiving more explanations or examples in addition to the feedback.

About their preference for feedback delivery modes, four of them reported they preferred the system because of its recording function. Also, they considered receiving feedback through the system to be more efficient and comfortable. Seven said they expected having f2f discussions with the teacher additionally. Three indicated they would accept either online or f2f feedback. Five stated they preferred the teacher’s f2f feedback because they could ask once having some questions and the online feedback took a longer time for them to catch. As to the selected feedback type, only one expressed unhappiness with it. She considered direct feedback more explicit and immediate as well as making it easier for her to understand and correct errors.

In terms of the system, nine of them thought the system was user-friendly, and nine considered it acceptable. Five indicated that the system could be improved, and one expressed unhappiness with it because of its instability. When asked about their suggestions about the system, five hoped that its stability could be improved. Seven said more specifications could be added. Five indicated that it could give better instructions. One expressed his hope that it could be intelligent enough to help him detect repeated errors, which was currently done by writers themselves.

# 6.  Discussion

The findings of the qualitative pilot study suggest that individualized CF provided through the OWS seemed more beneficial to self-correction of GEs for Participants A and B, whose language proficiency level was higher. They could understand the feedback, apply it to self-correction and problem-solving by using the feedback reminder function of the system, and therefore become autonomous learners, which was the purpose of the system development.

Although Participant C understood how the feedback related to her errors, she ignored the feedback reminder function and did not use the feedback to address errors by self-checking new pieces of writing. Therefore, the system was less beneficial to her self-correction of GEs. As for Participant D, she failed to understand several pieces of feedback. Even when she did, she was unable to correct errors on her own. Also, she completely ignored the feedback reminder function. Her inability to understand the feedback and correct errors indicates her need for f2f meetings. Although such a need was expressed by the other three participants too, the frequency of needed meetings with the teacher seemed to vary by participants.

Participants A and B took advantage of the feedback and devised their learning strategies, for example, printing out a paper version of corrections and comparing it with the original version, to improve subsequent writing accuracy. For them as advanced learners, self-correction based on reflection was a necessary skill. Therefore, their need to meet the teacher for feedback clarification, an unrealistic expectation of classroom writing teachers (Ferris, 2010), seemed lower than the need expressed by Participants C and D, whose writing accuracy might improve further if they had been regularly exposed to oral and explicit feedback (Bitchener et  al., 2005).

The findings of the quantitative formal study suggest that individualized CF delivered via the system seemed able to assist most participants’ self-correction of GEs, the level of which varied according to their understanding of online feedback and memory retention capacity. Almost all of them stopped repeating the same errors to different extents over the study thanks to the reminding function of the system. But the system did not seem helpful to one student who continued repeating errors throughout all the study. Moreover, in the post-test, three students were found to repeat the errors, which they had made in the pre-test, but did not appear in their three written works.

In terms of error types, it seemed that the reminding function of the system was only useful for correcting some error types. The least amount of repeated errors was subject-verb agreement errors, followed by capitalization errors. Nevertheless, the online feedback did not seem helpful to verb tenses error correction. The amount of this error type in the post-test was five times greater than in the pre-test. This is probably because of their failure to understand the feedback, or their inability to use appropriate verb tenses for each writing situation. It is also likely that because this study adopted unfocused feedback, the learners failed to check all their errors (Ellis, 2009; Farrokhi & Sattarpour, 2011, 2012; Sheen et  al., 2009).

The findings of the questionnaire also show that the students perceived their EC could benefit from online feedback. They took advantage of the reminding function of the system to self-correct their new written works before submission, which is in line with Warschauer’s (2010) finding that online feedback motivates autonomous writing and revision. Nevertheless, some of them needed more assistance to better understand the feedback for self-correcting errors. This assistance could be given by providing more explanations or examples of their GEs in the current feedback delivery mode, or by delivering feedback with other modes or f2f in addition to the current mode, which needs to be examined in further research.

Feedback delivery through the system seemed suitable for most students, who considered such feedback more time-efficient and less stressful than feedback delivery in f2f situations. Moreover, the system allowed for error recording that could help them reduce cognitive load; therefore, they could retrieve their previous errors and try not to repeat them. Such advantage was also indicated in Hyland and Hyland (2006) study. Despite this advantage, some students were not used to this feedback delivery mode and preferred having f2f opportunities to clarify their understanding of online feedback with the teacher, even though receiving feedback through the system could allow them to feel the teacher’s presence. It appeared that the use of media in this study affected the learners’ perception of social presence and feedback (Thomas et al., 2017).

Although the system seemed able to assist the learners’ EC, there is great room for improvement of the current system. The students showed dissatisfaction with it because of its instability, current specifications, and interface. For them, more specifications could be added and the interface could be improved so that users could ‘communicate’ better with the system. However, their dissatisfaction did not seem to impact negatively on perceiving the teacher’s presence, understanding the feedback, or correcting GEs. Those negative impacts have been found to hinder learners’ writing improvement in other studies (e.g. Thompson & Lee, 2012; Wihastyanang et  al., 2020).

What seems to have affected their EC negatively is their language proficiency level. Although all the participants were English majors, their language proficiency level varied from Grade 9 to 13 (the highest score $= 1 5$ ) in terms of the GSA Test. Such difference seemed to influence their feedback understanding and abilities to self-correct errors, which supports some researchers’ (Ferris et  al., 2013; Kang & Han, 2015) findings that learners’ proficiency may influence the efficacy of CF. In addition to proficiency, the learners’ characteristics such as motivation and beliefs could be other reasons that influence their engagement with CF (Han, 2017; Hyland & Hyland, 2019).

In sum, the current system seemed beneficial to EFL learners’ self-correction of GEs, which was mostly influenced by the learners’ language proficiency level and feedback delivery mode. The CF delivered through the system could not only reduce the higher-level learners’ errors before submission and therefore reduce the teacher’s workload, but also encouraged reflection and independent learning. In contrast, the lower-level learners seemed to benefit less from the system. They either received the feedback but failed to use the reminder function to help self-correction, or failed to understand the feedback, so could not self-correct their GEs. Their cases evidence Ferris et  al.’s claim (2013)

that lower-proficiency students may fail to correct errors even when they are pointed out to them. They may need more assistance from the teacher, whose support can enable them to develop their text and writing abilities (Hyland & Hyland, 2019).

Also, although some students could self-correct errors by feedback reception, they preferred having f2f meetings with the teacher for confirming their understanding of the feedback on their errors. Their opinions confirm some researchers’ observations that learners prefer receiving feedback along with other resources such as oral conferences to better understand and interpret teachers’ feedback due to having greater opportunities for clarification, instruction, and negotiation (Ferris, 2011; Hyland & Hyland, 2019). They also echo some researchers’ conclusions (Shintani & Aubrey, 2016; Ware & Warschauer, 2006) that some students prefer teachers’ feedback due to their social interaction needs.

# 7.  Conclusion

This study investigated whether individualized CF on EFL learners’ errors provided through an OWS in text modes could be beneficial to their GE self-correction. The findings revealed that the OWS seemed to have the potential to benefit higher-level students’ self-correction by allowing teachers to provide ‘good quality external feedback’ in the text mode (Nicol & Macfarlane‐Dick, 2006), without the occurrence of technological difficulties causing learners’ frustration, as was the case in Thompson and Lee’s study (2012). Although computer-mediated feedback seemed to enhance learners’ grammatical accuracy (Tafazoli et  al., 2014), lower-level students needed more assistance from the teacher to understand the feedback to carry out self-correction. This suggests that teachers’ selection of feedback type could be tailored to students’ language proficiency level and supports Ferris’s (2010) assumption that ‘a one-size-fits-all approach to error treatment is unlikely to be effective or appropriate’. (p. 197)

The lower-level participants’ preference for direct explicit feedback support some researchers’ suggestions (Chandler, 2003; Elwood & Bode, 2014; Ferris & Roberts, 2001) that lower-level learners prefer teachers’ direct and explicit feedback on their writing. On the other hand, it has been suggested that teachers may consider providing feedback in learners’ native language or along with other modes to help lower-level learners understand indirect or metalinguistic CF while using the current system to assist self-correction of GEs. If learners can better understand teachers’ feedback, they may pay more attention to feedback while writing.

It seems that the online CF benefitted not only the students’ self-correction of GEs, but also the teacher, who could spend less time providing feedback thanks to the system’s reminding and recording functions, which is contrary to the findings in some previous studies (Thompson & Lee, 2012; Wihastyanang et  al., 2020), in which teachers’ workloads increased due to the use of technology to deliver feedback. The benefit of recorded feedback for teachers has also been found in Conijn et  al. (2019) study. The use of the current system would be also time-saving for teachers who plan to offer online feedback along with f2f oral meetings. Since higher-level students of this study indicated their preference for CF provided by technology, the frequency of oral meetings, consuming considerable amounts of teachers’ time (Hyland & Hyland, 2019), could be reduced for them.

This study has its limitations. Firstly, the current system was newly developed for the study purpose only rather than for commercial use. Although it allows whoever has an account to use it with their class, it has not been tested by any specialist. In addition, it displays feedback in the text format only. More functions could be added so that teachers could provide feedback in other modes, which seems to influence the learners’ perception of richness of mediums, quality of feedback, and correction of GEs.

Moreover, only underlining in combination with metalinguistic CF was examined and the participants’ GE types were not distinguished in this study; future research should examine the impact of different feedback types and particular GE types on correction efforts with a larger population of EFL learners. Finally, the length of time for the formal study was one semester, within which some learners repeated the errors they had made after a longer period of time than others. This suggests learners’ memory retention capacity differs, and that they possibly perform differently after a longer period.

# Disclosure statement

No potential conflict of interest was reported by the authors.

# Notes on contributor

Chao-Jung Ko is currently an assistant professor in the University of Tainan. Her research interests are in the fields of computer assisted language learning, cross-cultural communication, foreign language learning psychology, foreign language learners’ writing and speaking skills development.

# References

Aljaafreh, A., & Lantolf, J. P. (1994). Negative feedback as regulation and second language learning in the zone of proximal development. The Modern Language Journal, 78(4), 465–483. https://doi.org/10.2307/328585

(1994). Doing social research (2nd e   
Berg, D. R., & Lu, Y. (2017). Taiwanese student attitudes towards error correction and written corrective feedback in L2 writing classes. ASIAN TEFL Journal of Language Teaching and Applied Linguistics, 2(2), 149–161.   
Bitchener, J., Young, S., & Cameron, D. (2005). The effect of different types of corrective feedback on ESL student writing. Journal of Second Language Writing, 14(3), 191–205. https://doi.org/10.1016/j.jslw.2005.08.001   
Cavaleri, M., Kawaguchi, S., Di Biase, B., & Power, C. (2019). How recorded audio-visual feedback can improve academic language support. Journal of University Teaching & Learning Practice, 16(4), 6. 10.53761/1.16.4.6   
Chandler (2003). The efficacy of various kinds of error feedback for improvement in the accuracy and fluency of L2 student writing. Journal of Second Language Writing, 12(3), 267–296. https://doi.org/10.1016/S1060-3743(03)00038-9   
Chen, C. F. E., & Cheng, W. Y. E. C. (2008). Beyond the design of automated writing evaluation: Pedagogical practices and perceived learning effectiveness in EFL writing classes. Language Learning & Technology, 12(2), 94–112.   
Chen, S. Y., & Liu, X. (2011). Mining students’ learning patterns and performance in Web-based instruction: A cognitive style approach. Interactive Learning Environments, 19(2), 179–192. https://doi.org/10.1080/10494820802667256   
Cho, H. (2017). Synchronous web-based collaborative writing: Factors mediating interaction among second-language writers. Journal of Second Language Writing, 36, 37–51. https://doi.org/10.1016/j.jslw.2017.05.013   
Cohen, L., Manion, L., & Morrison, K. (2007). Research methods in education. Routledge.   
Conijn, R., van Zaanen, M., & Van Waes, L. (2019). Don’t wait until it is too late: The effect of timing of automated feedback on revision in ESL writing. In European Conference on Technology Enhanced Learning (pp. 577–581). Springer.   
Creswell, J. W. (2012). Qualitative inquiry and research design: Choosing among five designs (3rd ed.). Sage.   
Creswell, J. W., Hanson, W. E., Clark Plano, V. L., & Morales, A. (2007). Qualitative research designs: Selection and implementation. The Counseling Psychologist, 35(2), 236–264. https://doi.org/10.1177/0011000006287390   
Creswell, J. W., Plano Clark, V. L., Gutmann, M. L., & Hanson, W. E. (2003). An expanded typology for classifying mixed methods research into designs. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (pp. 209–240). Thousand Oaks, CA: Sage.   
Croasmun, J. T., & Ostrom, L. (2011). Using likert-type scales in the social sciences. Journal of Adult Education, 40(1), 19–22.   
Cunningham, K. J. (2019). How language choices in feedback change with technology: Engagement in text and screencast feedback on ESL writing. Computers & Education, 135, 91–99. https://doi.org/10.1016/j.compedu.2019.03.002   
Edwards-Groves, C. (2012). Interactive creative technologies: Changing learning practices and pedagogies in the writing classroom. Australian Journal of Language and Literacy, 35(1), 99.   
Ellis, R. (2009). A typology of written corrective feedback types. ELT Journal, 63(2), 97–107. https://doi.org/10.1093/elt/ccn023   
Ellis, R., Sheen, Y., Murakami, M., & Takashima, H. (2008). The effects of focused and unfocused written corrective feedback in an English as a foreign language context. System, 36(3), 353–371. https://doi.org/10.1016/j.system.2008.02.001   
Elola, I., & Oskoz, A. (2017). Writing with 21st century social tools in the L2 classroom: New literacies, genres, and writing practices. Journal of Second Language Writing, 36, 52–60. https://doi.org/10.1016/j.jslw.2017.04.002 university EFL writing classes in Japan. System, 42, 333–343. https://doi.org/10.1016/j. system.2013.12.023   
Farrokhi, F., & Sattarpour, S. (2011). The effects of focused and unfocused written corrective feedback on grammatical accuracy of Iranian EFL learners. Theory & Practice in Language Studies, 1(12), 1797–1803. https://doi.org/10.4304/tpls.1.12. 1797-1803   
Farrokhi, F., & Sattarpour, S. (2012). The effects of direct written corrective feedback on improvement of grammatical accuracy of high-proficient L2 learners. World Journal of Education, 2(2), 49–57. https://doi.org/10.5430/wje.v2n2p49   
Fernandes, M., & Randall, D. (1991). The social desirability response bias in ethics research. Journal of Business Ethics, 10(11), 805–807.   
Ferris, D. (2011). Treatment of error in second language student writing. University of Michigan Press.   
Ferris, D. R. (2010). Second language writing research and written corrective feedback in SLA: Intersections and practical applications. Studies in Second Language Acquisition, 32(2), 181–201. https://doi.org/10.1017/S0272263109990490   
Ferris, D. R., Hedgcock, J., & Hedgcock, J. S. (2013). Teaching ESL composition: Purpose, process, and practice. Routledge. https://doi.org/10.4324/9780203813003   
Ferris, D., & Roberts, B. (2001). Error feedback in L2 writing classes: How explicit does it need to be? Journal of Second Language Writing, 10(3), 161–184. https://doi. org/10.1016/S1060-3743(01)00039-X   
Ferris, D. R., & Helt, M. (2000, March). Was Truscott right? New evidence on the effects of error correction in L2 writing classes [Paper presentation]. Proceedings of the American Association of Applied Linguistics Conference, Vancouver, BC (pp. 11–14).   
Frear, D. (2010). The effect of focused and unfocused direct written corrective feedback on a new piece of writing. College English: Issues and Trends, 3, 59–71.   
Greene, J. C., & Caracelli, V. J. (1997). Defining and describing the paradigm issue in mixed-method evaluation. New Directions for Evaluation, 74, 5–17.   
Guénette, D. (2012). The pedagogy of error correction: Surviving the written corrective feedback challenge. TESL Canada Journal,  30(1), 117–126https://doi.org/10.18806/ tesl.v30i1.1129   
Guénette, D., & Lyster, R. (2013). Written corrective feedback and its challenges for pre-service ESL teachers. Canadian Modern Language Review, 69(2), 129–153. https:// doi.org/10.3138/cmlr.1346   
Han, Y. (2017). Mediating and being mediated: Learner beliefs and learner engagement with written corrective feedback. System, 69, 133–142. https://doi.org/10.1016/j.system.2017.07.003   
Han, Y., & Hyland, F. (2015). Exploring learner engagement with written corrective feedback in a Chinese tertiary EFL classroom. Journal of Second Language Writing, 30, 31–44. https://doi.org/10.1016/j.jslw.2015.08.002   
Henderson, M., & Phillips, M. (2015). Video-based feedback on student assessment: Scarily personal. Australasian Journal of Educational Technology, 31(1), 51–66. https:// doi.org/10.14742/ajet.1878   
Hewings, M. (2013). Advanced grammar in use with answers: A self-study reference and practice book for advanced learners of English. Cambridge university press.   
Hyland, K., & Hyland, F. (2006). Feedback on second language students’ writing. Language Teaching, 39(2), 83–101. https://doi.org/10.1017/S0261444806003399   
Hyland, K., & Hyland, F. (Eds.). (2019). Feedback in second language writing: Contexts and issues. Cambridge university press.   
James, C. (2013). Errors in language learning and use: Exploring error analysis. Routledge. https://doi.org/10.4324/9781315842912   
Jamieson, S. (2004). Likert scales: How to (ab)use them. Medical Education, 38(12), 1217–1218. http://eprints.gla.ac.uk/59552/   
Kang, E., & Han, Z. (2015). The efficacy of written corrective feedback in improving L2 written accuracy: A meta‐analysis. The Modern Language Journal, 99(1), 1–18. https://doi.org/10.1111/modl.12189   
Karim, K., & Nassaji, H. (2020). The revision and transfer effects of direct and indirect comprehensive corrective feedback on ESL students’ writing. Language Teaching Research, 24(4), 519–539. https://doi.org/10.1177/1362168818802469   
Karimi, M., Fotovatnia, Z., & Branch, N. (2012, August). The effects of focused vs. unfocused written teacher correction on the grammatical accuracy of Iranian EFL undergraduate [Paper presentation]. Professional Teaching Articles–CEBU Conference Issue August 2012 Volume 62 (p. 42).   
Klobucar, A., Deane, P., Elliot, N., Ramineni, C., Deess, P., & Rudniy, A. (2012). Automated essay scoring and the search for valid writing assessment. In C. Bazerman, C. Dean, J. Early, K. Lunsford, S. Null, P. Rogers, & A. Stansell (Eds.), International advances in writing research: Cultures, places, measures (pp. 103–119). Fort Collins, Colorado: WAC Clearinghouse/Anderson, SC: Parlor.   
Li, Y. Y. (2016). Written corrective feedback for L2 development, John Bitchener and Neomy Storch. Journal of Second Language Writing, 34, 25–27. https://doi.org/10.1016/j. jslw.2016.10.002   
Li, Z. (2021). Teachers in automated writing evaluation (AWE) system-supported ESL writing classes: Perception, implementation, and influence. System, 99, 102505. https:// doi.org/10.1016/j.system.2021.102505   
Lim, F. V., & Phua, J. (2019). Teaching writing with language feedback technology. Computers and Composition, 54, 102518. https://doi.org/10.1016/j.compcom.2019.102518   
Maxwell, J. A. (2010). Using numbers in qualitative research. Qualitative Inquiry, 16(6), 475–482. https://doi.org/10.1177/1077800410364740   
Mayring, P. (2014). Qualitative content analysis: Theoretical foundation, basic procedures and software solution. https://nbn-resolving.org/urn:nbn:de:0168-ssoar-395173   
Merriam, S. B. (1990). Case study research in education: A qualitative approach. Jossey-Bass.   
Napolitano, D. M., & Stent, A. (2009). TechWriter: An Evolving System for Writing Assistance for Advanced Learners of English. CALICO Journal, 26(3), 611–625. https:// doi.org/10.1558/cj.v26i3.611-625   
Nazari, N., Shabbir, M. S., & Setiawan, R. (2021). Application of Artificial Intelligence powered digital writing assistant in higher education: Randomized controlled trial. Heliyon, 7(5), e07014. https://doi.org/10.1016/j.heliyon.2021.e07014   
Nicol, D. J., & Macfarlane‐Dick, D. (2006). Formative assessment and self‐regulated learning: A model and seven principles of good feedback practice. Studies in Higher Education, 31(2), 199–218. https://doi.org/10.1080/03075070600572090   
Perdana, I., & Farida, M. (2019). Online grammar checkers and their use for EFL writing. Journal of English Teaching, Applied Linguistics and Literatures (JETALL), 2(2), 67–76. https://doi.org/10.20527/jetall.v2i2.7332   
Sheen, Y. (2007). The effect of focused written corrective feedback and language aptitude on ESL learners’ acquisition of articles. Tesol Quarterly, 41(2), 255–283. https:// doi.org/10.1002/j.1545-7249.2007.tb00059.x   
Sheen, Y., Wright, D., & Moldawa, A. (2009). Differential effects of focused and unfocused written correction on the accurate use of grammatical forms by adult ESL learners. System, 37(4), 556–569. https://doi.org/10.1016/j.system.2009.09.002   
Shintani, N., & Aubrey, S. (2016). The effectiveness of synchronous and asynchronous written corrective feedback on grammatical accuracy in a computer‐mediated environment. The Modern Language Journal, 100(1), 296–319. https://doi.org/10.1111/ modl.12317   
Steckler, A., McLeroy, K. R., Goodman, R. M., Bird, S. T., & McCormick, L. (1992). Toward integrating qualitative and quantitative methods: An introduction. Health Education Quarterly, 19(1), 1–8. https://doi.org/10.1177/109019819201900101   
Tafazoli, D., Nosratzadeh, H., & Hosseini, N. (2014). Computer-mediated corrective feedback in ESP courses: Reducing grammatical errors via Email. Procedia-Social and Behavioral Sciences, 136, 355–359. https://doi.org/10.1016/j.sbspro.2014.05.341   
Tang, J., & Rich, C. S. (2017). Automated writing evaluation in an EFL setting: Lessons from China. JALT CALL Journal, 13(2), 117–146. https://doi.org/10.29140/jaltcall. v13n2.215   
Thomas, R. A., West, R. E., & Borup, J. (2017). An analysis of instructor social presence in online text and asynchronous video feedback comments. The Internet and Higher Education, 33, 61–73. https://doi.org/10.1016/j.iheduc.2017.01.003   
Thompson, R., & Lee, M. J. (2012). Talking with students through screencasting: Experimentations with video feedback to improve student learning. The Journal of Interactive Technology and Pedagogy, 1(1), 1–16. https://jitp.commons.gc.cuny. edu/2012/02/17/   
Tursina, P., & Chuang, M. T. (2016). Direct and indirect corrective feedback on EFL students’ writing performance. Proceedings of EEIC, 1(2), 209–214.   
Vojak, C., Kline, S., Cope, B., McCarthey, S., & Kalantzis, M. (2011). New spaces and old places: An analysis of writing assessment software. Computers and Composition, 28(2), 97–111. https://doi.org/10.1016/j.compcom.2011.04.004   
Walshe, C., Ewing, G., & Griffiths, J. (2012). Using observation as a data collection method to help understand patient and professional roles and actions in palliative c are s ett ings. Palliative Medicine, 26(8), 1048–1054. https://doi. org/10.1177/0269216311432897   
Ware, P. D., & Warschauer, M. (2006). Electronic feedback and second language writing. In K. Hyland & F. Hyland (Eds.), Feedback in second language writing: Contexts and issues (pp. 105–122). Cambridge: Cambridge University Press.   
Warschauer, M. (2007). Technology and writing. In C. Davison & J. Cummins (Eds.), The International handbook of English language teaching (pp. 907–912). Norwell, MA: Springer.   
Warschauer, M. (2010). New tools for teaching writing. Language Learning & Technology, 14(1), 3–8.   
Warschauer, M., & Ware, P. (2006). Automated writing evaluation: Defining the classroom research agenda. Language Teaching Research, 10(2), 157–180. https://doi. org/10.1191/1362168806lr190oa   
Wihastyanang, W. D., Kusumaningrum, S. R., Latief, M. A., & Cahyono, B. Y. (2020). Impacts of providing online teacher and peer feedback on students’ writing performance. Turkish Online Journal of Distance Education, 21(2), 178–189. 10.17718/tojde.728157   
Williams, J. (2004). Tutoring and revision: Second language writers in the writing center. Journal of Second Language Writing, 13(3), 173–201. https://doi.org/10.1016/j. jslw.2004.04.009   
Yin, R. K. (2003). Case study research: Design and methods (3rd ed.). Sage. https://doi. org/10.1037/13620-009   
Zhai, N., & Ma, X. (2021). Automated writing evaluation (AWE) feedback: A systematic investigation of college students’ acceptance. Computer Assisted Language Learning. https://doi.org/10.1080/09588221.2021.1897019   
Zheng, B., Lawrence, J., Warschauer, M., & Lin, C. H. (2015). Middle school students’ writing and feedback in a cloud-based classroom environment. Technology, Knowledge and Learning, 20(2), 201–229. https://doi.org/10.1007/s10758-014-9239-z   
Zheng, B., & Warschauer, M. (2017). Epilogue: Second language writing in the age of   
computer-mediated communication. Journal of Second Language Writing, 36, 61–67.   
https://doi.org/10.1016/j.jslw.2017.05.014

# Appendix A  interview guided questions (translated from chinese by the researcher)

1. Do you think the teacher’s feedback is helpful to your self-correction of grammatical errors?   
2. How do you perceive those feedback?   
3. Do you hope to receive feedback in other ways? why?   
4. Can you suggest other ways that are helpful to your self-correction of grammatical errors?   
5. How do you perceive the developed system? What other functions do you hope that the system could possess?   
6. How do you perceive your interaction with the teacher through the online writing system?   
7. How do you perceive this online writing learning experience?

# Appendix B  questionnaire

The questionnaire is a five - point Likert scale (Strongly Disagree/Disagree/Neutral/ Agree/Strongly Agree) questionnaire that consists of three sections. The questions in the first section ask about learners’ background information. The second section includes the items asking about learners’ perception about the feedback provided by teachers through the system. The third section includes the items that investigate learners’ perception of the system. And the questions in the fourth section ask about learners’ perception of interacting with teachers.

Section one. Background information

1. Sex: Male Female   
2. Grade: First Second Third Fourth   
3. Are you taking any other English writing course now? Yes No   
4. How do you perceive your computer skills? poor so so good   
5. How do you perceive your English typing skills? poor so so good   
6. How long have you been learning English? $< 5$ years $6 \sim 1 0$ years $> 1 0$ years

Please indicate your level of agreement on the following statements: 1 (strongly disagree), 2 (disagree), 3 (neutral), 4 (agree), and 5 (strongly agree). Section 2. About the feedback

1. I am satisfied with the feedback provided by the system.   
2. The feedback provided by the system is beneficial to my error correction.   
3. I can always understand the feedback.   
4. I always pay attention to the feedback displayed at the top of the website while producing new written work.   
5. The feedback displayed at the top of the website can help me self-correct errors while producing new written work.   
6. I hope to receive feedback in other modes.   
7. The feedback provided by the system is not helpful to my error correction.   
8. I ignore the feedback displayed at the top of the website while producing new written work.   
9. It’s hard for me to understand the feedback.   
10. The feedback displayed at the top of the website is not helpful to my self-correction while producing new written work.

# Section 3. About the system

1. I think the system is easy to use.   
2. I am satisfied with the interface of the system.   
3. I am satisfied with the specifications of the system.   
4. Some specifications of the system are not necessary.   
5. I hope the system can provide more specifications.   
6. The interface of the system is not attractive.   
7. The system is not user-friendly.

Section 4. Interaction with the teacher

1. I can feel the teacher’s presence by reading his/her feedback.   
2. The teacher’s feedback enhances our interaction.   
3. I feel comfortable to receive the teacher’s feedback through the computer.   
4. In addition to the online feedback, I hope to receive the teacher’s feedback   
face-to-face.   
5. The teacher’s feedback is not helpful to our interaction.   
6. I don’t feel the teacher’s presence with his/her online feedback.   
7. I hope to receive feedback from the teacher, rather than through the computer.

Section 5. Open-ended questions

1. Do you understand the feedback? If not, why?   
2. Do you pay attention to the displayed feedback on the top of the webpage? why or why not?   
3. Do you hope to receive feedback in other modes? Any suggestions?   
4. Do you think the system is easy to use? If not, why? Any suggestions?   
5. Are you satisfied with the interface and the specifications of the system? Why? Any suggestion?   
6. Do you prefer receiving feedback through the system or the teacher, or both? Why?   
7. Do you hope that the teacher provides any additional help with self-correcting grammatical errors?