# Is CJ a valid, reliable form of L2 writing assessment when texts are long, homogeneous in proficiency, and feature heterogeneous prompts?

Peter Thwaitesa, Charalambos Kolliasb, Magali Paquota,c,\*

a Centre for English Corpus Linguistics, Institut Langage et Communication, UCLouvain, Belgium   
b Polytomous Limited, United Kingdom   
c Fonds de la Recherche Scientifique - FNRs, Belgium

# ARTICLEINFO

# ABSTRACT

Keywords:   
Comparative judgement   
Assessment   
Writing   
Learner corpora   
Crowdsourcing

Comparative judgement (CJ) is a method of assessment in which judges perform paired comparisons of pieces of student work and decide which one is "better'. CJ has many potential benefits for the writing assessment community, including its reliability, flexibility, and efficiency. However, by reviewing the literature on CJ's application to L2 writing assessment, we find that while existing studies have established the plausibility of using CJ in this context, they provide little indication of the conditions under which the method is most likely to prove useful. In particular, by focusing on the assessment of relatively short texts, covering a wide proficiency range, and using a single essay prompt, they leave unresolved the question of how such textual factors affect CJ's reliability and validity. To address this, we conduct two studies exploring the reliability and validity of a community-driven form of CJ for evaluating L2 texts which were longer, featured a narrower proficiency range, and were more topically diverse than earlier studies. Our results suggest that CJ remains reliable under these conditions. In addition, comparison with rubric-based assessment using CEFR scales suggests that the CJ approach also has an acceptable level of validity.

# 1. Introduction

In comparative judgement (CJ), judges are shown pieces of student work side-by-side and asked to decide which of the two is "better'. Many such paired comparisons are conducted by a team of judges, who usually (but not always) have experience and/or expertis in the target domain. A scale can then be constructed by inputting these judgements to the Bradley-Terry model (Bradley & Terry, 1952), which assigns a score to each item such that the texts which \*win' the greatest number o comparisons recie the highest scores, and those that win the fewest receive the lowest.

In the research literature, CJ is frequently presented as an alternative to more traditional methods of assessment such as mark schemes or rubric-based marking, over which it i seen as having several advantages. Firstly, as Jones & Davie (2023) haveargued, J is well-suited t the evaluation of competencies and constructs that are hard to pin down. For example, Jones & Inglis (2015) describe the use of CJ to asess students' mathematical problem-solving skils. This is a much more difficult thing to evaluate than traditional math tests, in which test-takers simply supply a corrct answer, because of the wide variety of approaches that test-takers can take. Nonetheles, Jones & Inglis reported that CJ-based assessments of these skills were highly reliable, and correlatd strongly both with scores of the same exam generated through rubric-based assessment, and with students' predicted GCsE scores.

Jones & Inglis study also illustrates a second key benefit of CJ, which is that it removes the need both for the development of complex marking schemes or rubrics, and for the reource-intensive proces of training examinersto use them. Rather than instructing judges on how to evaluate test items, J instead assumes that they already possess the necessary skills and knowledge. CJ tasks therefore typically include only very simple instructions on how decisions should be made, such as "Chose the better translated ersion' fr atranslation assessment (Han et al., 2022), or \*The eter writing?" for an 1 writing assesment (Wheadon et al., 2020).

Some scholars have argued that CJ's lack of reliance on rubrics or mark schemes also provides benefits in terms of validity. For example, Han (2021, 2022) argues that CJ is not vulnerable to issues afecting rubric-based marking, such as halo efects (ie a tendency for a rater's judgement of one aspect of a text to inluence their judgement of all others or ifferences in rater serity ie. When some raters tend to ie higher or lower scores than thers), sin l that is required of judgei t chose the eter of  texts. Han also found that judges considered robustnessagainst rater bias to be an advantage of CJ, compared to rubric-based marking. This is presumably because, in CJ, final grades are based on many judgements performed by many judges, reducing the influence of any single judge on any single text. This same proces also heps to ensure that C-based valuations refer to broad range f aspects ftext quality - since each judge brings their own knowledge, preferences, and experience, CJ can capture many perspectives on what constitute strong piece f work (Lesterhuis e l., 2022; van Daal et al, 2019). The absence of a rubric is central to this principle of allowing judges to draw on their own personal perspective, ensuring that they "are not tied to predefined criteria (van Daal et al., 2019, p. 61) imposed by what Pinot de Moira et al. (2022) have called the \*hyper-specificity" of some rubric-based approaches.

Much of the reearch providing evidence of Cs reliabilit, validit, and efficiency has been conducted in the areas of mathematics and L1 writing asessment. Much les i known about how CJ performs in the assessment of second language writing. Below, we review research exploring CJ's efficacy for both L1 and L2 evaluation, and argue that L2 studies to date have explored to narrow a range of assesment conditions, leaving much to be assumed about how the method wil perform in other, more challenging, contexts. We then describe two studies which seek to address this ssue by using CJ to assess a set of texts with more complex characteristics.

# 1.1. CJ for L1 writing assessment

Studies on the efficacy of CJ for L1 writig aessment can be divided into two general types. The first focuses on Cs rliability and concurrent validity. For example, a study by Wheadon et al. (2020), which described the CJ assessment of more than 5,000 English 1 texts written by UK primary school students, reported a mean reliabilit of between .85 and .91 acros various year groups (measured using the Rasch-based measure of scale separation reliability (ssR), which can be considered comparable to Cronbach's alpha; Ver. havert et al. (2018, and e Section 2.2, below). oncrrent valit is usuly onctalisd in thee stdie as correlation wth ther measures of the same construct such as rubric-based asessments of the same texts. Most studies report moderate to strong correlations. For example, Steedle & Ferrara (2016) reported an ssR of around .89 and correlations of around $r = . 6 6$ with rubric-based assessments in a study of argumentative essays written by American high school students, and a series of studies by Heldsinger and Humphry (2010, 2013), Humphry & Heldsinger (2019), McGrane et al. (2018) reported reliability levels above .95 and correlations with rubric-based assessment of .90 or higher for the assessment of narrative, descriptive, and argumentative essays.

A second type of study into CJ for L1 writing assessment investigates CJ's construct validity. Such studies are designed to address the perceived opacity of CJ, relative to rubric-basd marking, which tems from the fct that itis not ossibl to know what judges hae taken into consideration while making judgements. In order to explore the extent to which judges consider asufficiently broad range of context-relevant textual features, Lestrhuis e al. (2018) asked judgesto make notes explaining their decisions during J. The authors reported that $9 3 . 5 \%$ of comments were "construct-relevant'. While most of these comments pertained to text argumentation and organisation, all areas of their target competence description were mentioned to some extent. Similarly, Chambers & Cunningham (2022) used think-aloud to track judge attention, and again found that judges mostly paid attention to construct-relevant text features but also noted some influence of construct rrelevant features such as handwriting and the presence of missing responses on judge decisions. Lasly, Lts e l. 2022) dified four dfrt judgin profile narrowly foused, brody fused, source-foused and anguage-foused), l of which involved a principal fous on argumentation and organisation, but iffered in secondary focus. The authors argued that CJ is valid for the asesment of L1 writing, and that a diversity of judging approaches results in the highest validity, since \*combining the judgments of different types of assesors into score leds to more informed text scores, representing the full complexity of text quality" (p. 8).

# 1.2. CJ for L2 writing assessment

Research into CJ for L2 writing assessment has been slower to emerge. Most studies to date have sought to establish the basic reliaility of the method, and tested its concurrent validity via comparisons with rubric-based grades, considered the gold standard in language assessment. For example, Sims et al. (2020) compared CJ with rubric-based marking for aset f 60 L2 English argumentative essays wrtten by students in an intensive English program. They reported a reliability of around .90 for both novice judges (undergraduate TESOL students) and experts (trained raters) afer around 11 comparisons per text. Corrlations with rubric-based grades produced by the same judges, were also strong $( r > . 9 0 )$ . Similar levels of reliability and validity were reported by Paquot et al., (2022) in a study evaluating the potential for CJ to be used to generate proficiency ratings for texts in learner corpora. They used a community-driven approach in which they recruited judges from the applied linguisic community, who provided judgements of 50 argumentative essays taken from the ETS corpus of TOEFL exam responses (Blanchard et al., 2014). They reported areliabilit of.95 after 20 comparisons per text. A linear model showed asignificant relationship between CJ and pre-existing rubric-based grades (adj $\mathrm { R } ^ { 2 } = . 5 1$ $p < . 0 0 1 $ , with texts rubric-rated as "low" proficiency having a lower mean rank than those rated "medium", and the "medium" texts a lower rank than those rated as having "high" proficiency.

These studies suggest that CJ holds potential s a method for L2 writing ssessment. However, a rather weaker set of findings was reported in a study by Sahin (2021) on the CJ-based asessment of 35 L2 English writing samples. After around 20 comparisons per text, conducted by English instructors Sahin reported a rliabilit of .72 and correlations with pre-existing rubric-based scores of.51. These levels of reliability and concurrent validity provide some counterpoint t the more positive findings reported above, and sugest that a better understanding is required of the factors that contribute to CJ's reliability and validity in the context of L2 assesment.

One potential source of variance in CJ's eficacy is the characteristics of the texts used for comparison. In seeking to establish baseline measures of the eficacy of Jfor  wing sssment, stdes to date have tendd toward relatively simple sts f text, with broad proficiency spans, mean text lengths of 250 words or fewer, and relating to a single essay prompt. Each of these variables could affect the rliaiity and valiity of  tk Bennng with varianc n wor ount, judes are more likely to suffe from fatigue when assessing longer texts than shorter ones. Tired judges may be more likely to make inattentive or inconsistent decisions than less fatigued ones, potentially having an impact on reliability and validity (Verhavert et al., 2019, 2022)

Regarding proficieny span, conducting comparisons i siest where items differ widely in proficiency/quality, since i is easiest to decide which of two texts is better when they occup dffrent locations onthe proficieny sptrm. As a result, item sets wth broad proficiency spans will lead to greater agreement between judges and, in consequence, higher reliability (Crompvoets et al., 2021, 2022). Judge statements tend to support ths - for example one judge in a CJstudy by Han (2021,p.352) reported that it was difficult to make CJ decisions when two renditions were of similar quality".

Lastly, Cs efficacy may e affctedwhen item sets include reonses t severaldifferent prompts, rather than only one. In general. CJ s viewed as being robust to heterogeneity in item types (Jones et al., 2014, 2016; Jones & Davies, 2023). Nevertheless a substantial body of reearchtestifie to the potency f promp effctin2 assment. For example, inasynthesis study Innami andKizumi (2016) reported that cros 17 L2 writing aessment stdies taskeffts and tsk-examinee intractions acounted fr ustantil variance in L2 writing scores. In addition, ther is evidence to suggest that rater perform differentl in assesing different essay prompts. For example, Weigle (1999) reported that raters were significantly more severe in their assessments of some prompts than others.

In summary, while there is some evidence of J's potential in the area ofL2 writing assessment, lttlei known about why studies to date have varied in the levelf reliaility and validty they have reorte. As such, further rech is required to clarify the condtions under which CJ can be expected to perform effctively. In particular, further research is needed on how CJ performs in the evaluation of complex texts. Such research would put L2 writing asessors in a bettr position to decide whether CJ might be suitable in their context.

# 1.3. The current study

This paper reports on two studies in which CJ is used to asss relatively complex sets of L2 texts. The studies are part of a wider project aiming to assess CJ's potential to enrich learner corpora with accurate, reliable assessments of text proficiency. As Carlsen (2012) and Park et al. (2022) have both noted, few first-generation learner corpora contain satisfactory proficiency measurements, limiting their usefulness for studies investigating proficiency-related aspects of textual variation. While rubric-based rating has been used for this purpose, it tends to be considered too inefficient (Carlsen, 2012). This has led to call for alternative methods. CJ's flexibility and efficiency is well-suited to this role, but must be shown capable of yielding reliable grades for the ften complex texts stored in learner corpora.

In the first study, we test the reliabilit and concurrent validity of CJ for evaluating texts with a narrow proficiency range $^ { ( \mathrm { B } 1 + }$ to C1 on the Common European Framework of References (CEFR), and with long word counts (around 570 words). In the second study, weadditionall explore the influence of topical diverst on Cs efficacy by collecting judgments of reponse to five diffrent essay prompts, rather than only one.

For each of the two studies, there were two research questions:

1. Does comparative judgement of this set of texts result in a reliable scale?   
2. How valid are the asessments i terms of their overlap with scores generated through gold standard writing asessment methods (i.e., their concurrent validity)?

Existing research allows us to make two hypotheses regarding these studies. Firstly, studies on the assessment of long, complex items and those with relatively narrow proficiency spans/differences in quality, suggests that a larger number of comparisons willbe required to reach satisfactory reliability. Since the L2 assessment literature provides no reason to assume that written L2 texts will perform any differently in this regard to any other type of item, our first hypothesis is as follows:

1. Both studies will result in acceptable reliability $_ { ( > . 8 0 ) }$ and concurrent validity (correlations with rubric-based scores $> . 6 0 \AA$ ); but reaching these levels willrequire more comparisons per item than in studies e.g. Sims et al., 2020) featuring shorter, more heterogeneous texts.

Secondly, given evidence of prompt-rater interactions in L2 writing (e.g. Weigle, 1999), we hypothesise that:

2. Study 2 will yield lower levels of reliability and concurrent validit to Study 1, or rquire a larger number of comparisons to reach the same levels.

# 2. Method

# 2.1. Study design

The first stage in developing these two studies was to select two sets of texts of the desired length, proficiency range, and topical diversty. Given the wider project goal of L2 leaner corpus enrichment, alltexts were taken from the International Corpus of Learner English (ILE; Granger et al., 2020). This resource contains around 9000 L2 English argumentative essays of a wide range of lengths (range $\mathsf { \Omega } = 6 9 { - } 4 2 3 9$ words, mean $= 5 9 8$ , written by language learners from 25 different L1 groups, and constituting responses to a wide range of prompts.

From this corpus, an intial selection of 50 texts was made. These texts formed the basis of asub-project to generate triple-marked rubric-based grades for a subsection of ICLE texts which were (a) representative of the ICLE corpus in terms of length, (b) consisted of twenty responses for each of the corpus' 25 L1 groups, and (c) also contained a varied set of the most popular esay prompts in the corpus. The process for the selection and grading f these texts i decribed in detal in Thwates et al. (inprearation) and wll be only briefly described here. The following three steps were performed:

1. Filter the corpus by text length, leaving only texts whose length is within one standard deviation of the mean (i.e. 381-815 words)   
2. Filter by esay prompt, leaving only texts whose prompt occurred at least 50 times and contained responses from at least 3 L1 groups;   
3. Select exactly 20 texts for each of the 25 1 groups from this filtered selection. Ideally, there would be four texts on each of five prompts per L1 group; in practice, however, this was not always possible.

Each text in this sub-corpus was graded by three raters using CEFR writing rubric C4 (Council of Europe, 2009, p. 187). Grading was conducted online and was preceded by two hours of CEFR descriptor refamiliarization, led by the second author. Al raters had a minimum of 5 years of experience teaching or evaluating learners at CEFR levels B1-C2, and at least an undergraduate degree in TESOL or Applied Linguistics. To analyse the data, the many-Faceted Rasch measurement (MFRM) model (Linacre, 1989), an extension of the Rasch model (Rasch, 1960), was use. The MFRM modelallows for additional facets such as rater severity and task difficulty to be part of the analysis model. Judges will vary i systematic ways and MFRM allows for the modelling of these diffrences (McNamara, 1996). 67 texts had to be dropped from the analysis as they either had a negative point measure correlation and/or were not i accordance with the MFRM model (i.e, misiting). Note that none of these texts showed unusual propertie, such as being clssified as outliers or misfitting items, in the later CJ stage. The inter-rater reliability of the rubric-based assessments (overall Rasch-Kappa $= - . 0 3$ judge Rasch-Kappa from -.13 to -.09) indicated that judge agreement was within expected ranges. Separation reliabilit - comparable to CJ's SSR measure -- was .94.

The rubric-based scores were used to test the concurrent validity of the CJ-derived scores. Despite its shortcomings see.g. Panadero & Jonsson, 2020, for a summary), rubric-based asessment i well established as the benchmark against which ther forms of L2 writing assessment are tested, including CJ (e.g. Sims et al., 2020) and automated essay scoring methods (Powers et al., 2015, Vajjala, 2018).

After compiling the larger sub-corpus, the texts for CJ were selected. For both studies, we aimed to sample around 50 texts according to three citeria: 1) that they had a similar mean length to that of the ICLE as a whole; (2) that their writers came from a range of L1 backgrounds; and (3) that no L1 group provided a disproportionately high number of essays.

Study 1 concerns the comparative judgement of texts on a single topic. We chose the essay prompt \*Most universty degrees are theoretical and doot preare stdens for the real world. They are therfore of ey ite value. his i the most common prompt in the ICLE and features responses by 20 of the 25 L1 groups. Four essays on this topic were chosen for each of twelve L1 groups, resulting in a selection of 48 texts.

For Study 2, ten essays were chosen for each of five essay prompts. No L1 was represented more than once in each group of ten essays, nor more than four times overall Table 1 presents the characteristics of the texts in both studies, alongside data on the texts in the ICLE as a whole; details of the more easily evaluated texts used in Paquot et al., (2022) are provided for comparison. Table 2 provides details of the proficiency levels generated by the rubric-based MFRM model. Further descriptive data for the essay sets in the

Table 1 Details of the texts used in each experiment; ICLE and Paquot et al., (2022) data included for comparison.   

<html><body><table><tr><td></td><td>ICLE argumentative</td><td>Experiment 1</td><td>Experiment 2</td><td>Paquot et al. (2022)</td></tr><tr><td>n.</td><td>8965</td><td>48</td><td>50</td><td>50</td></tr><tr><td>Source</td><td>ICLE</td><td>ICLE</td><td>ICLE</td><td>ETS corpus</td></tr><tr><td>Text length mean</td><td>598 words</td><td> 569 words</td><td>610 words</td><td>272 words</td></tr><tr><td>Text length range</td><td>69-4239</td><td>400-771</td><td>399-813</td><td>71-421</td></tr><tr><td>n. Essay topics</td><td>Approx. 1500</td><td>1</td><td>5 (10 essays each)</td><td>1</td></tr><tr><td>Text proficiency</td><td>Unknown</td><td>Approx. B1-C2</td><td>Approx. B1-C2</td><td>&quot;Low&quot; to &quot;High&quot;; roughly A1-C2</td></tr><tr><td>n. L1 groups</td><td>25</td><td>12</td><td>22</td><td>1 50</td></tr><tr><td>Essays per L1 group</td><td>1-982</td><td>4</td><td>1-4</td><td></td></tr></table></body></html>

two studies is available in the supplementary materials.

Judges for both CJ studies were recruited using a community-driven approach based on Paquot et al., (2022). Members of the linguistic community were invite to participate via applied linguistic mailing lists on social media, and through personal invitation. For the second study, participants also included a group of fourth-year undergraduate students in TEsOL. The two sets of judges were very similar on all measured variables (see Table 3).

For both studies, participants first completed a brief demographic survey and consent forms. They were then directed to the No More Marking (NMM) CJ platform. This platform was selected over alternatives because it allows crowdsourced participants to begin submitting comparisons immediately upon learning of the study.

In the CJ task itelf, judges were shown two essays side-by-side and asked to decide \*Which of the two texts is a more advanced piece f second-language writig" (see the screnshot in ig. 1). This wording reflects atext-centred approach to proficiency-i.e., it asks judges to consider the text itself rather than the proficiency of the learner who produced it (Carlsen, 2012). It also explicitly mentions the target construct - i.e., "second-language writing", and in this sense is more specific than the wordings chosen by Sims et al. (2020) (Choose the better response; p. 35) and Paquot et al. (2022) (Which text was written by a more advanced speaker?; p. 11). Nevertheles, the task definition is minimal by comparison with rubric-based forms of writing asessment. This is in line withthe view that CJ's validity is increased when judges are allowed to make use of their own experience and understanding of the target construct, rather than being constrained by excessive guidance. (Lesterhuis et al., 2022).

Each judge was asked to complete a minimum of five comparisons, and permitted to complet a maximum of 10. Judges who failed to complete the minimum number of comparisons were excluded from further analysis. The number of judges meeing this citerion was 70 in Study 1 and 66 in Study 2.

Pairs of texts were chosen for comparison using NMM's pseudo-random algorithm, which first selects whichever text in the dataset has the fewest comparisons, and then randomly chooses a second text Prior to the commencement of the data collection, we selected stopping conditions and judge inclusion criteria for the two studies. We decided to end each data collection upon the completion of600 total comparisons. This equates to a total of 25 comparisons per text for the 48 texts in Study 1 (since each individual comparison constitutes a decision about each of two texts; $( 6 0 0 \times 2 ) / 4 8 = 2 5$ ; and ( $( 6 0 0 \times 2 ) / 5 0 = )$ 24 decisions per text for Study 2. This was based on Verhavert et al.'s (2019) finding that acceptable reliability (i.e. $\ge . 8 0 \dot { }$ can generally be achieved after around 20 comparisons per item. Datacollection for Study 1 took two months to complete, while Study 2 to six months. The longer period required for the second datacollection was because we wanted to avoid exhausting the goodwill of the aplied linguistic community by publicising the study too heavily.

# 2.2. Analysis

Once all data had been collected, it was transformed into a rating scale in R (R Core Team, 2022 v.4.2.2), using the btm function of sirt package (v3.12-66; Robitsch, 2022) with additional functions by Jones (2022). This package fits the judgement data to the Bradley-Terry model (Bradley & Terr, 1952), which yields a scale containing a score for each item which reflcts the probabilit of that item \*winning" a comparison against another item. Across the scaleas whole, these scores have a mean value of and a standard deviation of around 2.5 (Bramley, 2007). The scale is then used as the basis for further statistical tests.

The model was run twice for each study. The first run was used to identify and remove judges whose data suggested that their judgements may be unreliable. This istypically done using infit scores, which are calculated alongide the rating cale and indicate the extent to which a judge's decisions were consistent with those of other judges. Larger infit scores indicate greater inconsistency. In existing reearch, an infit score of two or more standard deviations above the mean is often taken as evidence of misfit - i.e. unacceptably weak consistency with other judges. However, high infit does not always indicate unprincipled judge behaviour but can insted stm from a udge taking a different (but valid) perspctive on certain texts. As such, removing judges based on high infit alone risks losing the diverse judge perspectives which Lesterhuis et al., (2022, p. 8) argue are key to the validity of CJ. We therefore employed a cautious approach to judge removal which focused on gathering sfficient evidenceof inexpert performance. We specified three \*red flags" f inexpert performance. Judges who met two of the three were removed. Infi cores of two standard deviations aove the mean were the first red flag. The other two were:

Table 2 Grades derived from MFRM model of rubric-based assessment.   

<html><body><table><tr><td></td><td>Experiment 1 texts</td><td>Experiment 2 texts</td><td>All rubric-rated texts</td></tr><tr><td></td><td>N = 48</td><td>N = 50</td><td>N = 500</td></tr><tr><td>CEFR level</td><td></td><td></td><td></td></tr><tr><td>A2</td><td>0 (0%)</td><td>0 (0%)</td><td>1 (0.2%)</td></tr><tr><td>A2+</td><td>0 (0%)</td><td>0 (0%)</td><td>1 (0.2%)</td></tr><tr><td>B1</td><td>0 (0%)</td><td> 0 (0%)</td><td>14 (3.2%)</td></tr><tr><td>B1+</td><td>6 (16%)</td><td>7 (15%)</td><td>46 (11%)</td></tr><tr><td>B2</td><td>15 (39%)</td><td>8 (17%)</td><td>102 (24%)</td></tr><tr><td>B2+</td><td>11 (29%)</td><td>13 (28%)</td><td>104 (24%)</td></tr><tr><td>C1</td><td>6 (16%)</td><td>16 (35%)</td><td>125 (29%)</td></tr><tr><td>C2</td><td>0 (0%)</td><td>2 (4.3%)</td><td>40 (9.2%)</td></tr><tr><td>Removed</td><td>10</td><td>4</td><td>67</td></tr></table></body></html>

Table 3 CJ judge demographics.   

<html><body><table><tr><td rowspan="2"></td><td>Experiment 1</td><td>Experiment 2</td></tr><tr><td>N = 69</td><td>N = 65</td></tr><tr><td>Education Level</td><td></td><td></td></tr><tr><td>Doctorate</td><td>33 (48%)</td><td>21 (33%)</td></tr><tr><td>Master&#x27;s degree</td><td>31 (45%)</td><td>27 (43%)</td></tr><tr><td>Bachelor&#x27;s degree</td><td>5 (7.2%)</td><td>8 (13%)</td></tr><tr><td>High School</td><td>0 (0%)</td><td>7 (11%)</td></tr><tr><td>English proficiency</td><td></td><td></td></tr><tr><td>C2</td><td>49 (71%)</td><td>44 (68%)</td></tr><tr><td>C1</td><td>16 (23%)</td><td>16 (25%)</td></tr><tr><td>B2</td><td>3 (4.3%)</td><td>4 (6.2%)</td></tr><tr><td>B1</td><td>1 (1.4%)</td><td>1 (1.5%)</td></tr><tr><td>Experience as L2 examiner</td><td></td><td></td></tr><tr><td>Frequently</td><td>24 (35%)</td><td>26 (40%)</td></tr><tr><td>Occasionally</td><td>13 (19%)</td><td>12 (18%)</td></tr><tr><td>Rarely</td><td>8 (12%)</td><td>5 (7.7%)</td></tr><tr><td>Never</td><td>24 (35%)</td><td>22 (34%)</td></tr><tr><td>Experience as L2 instructor</td><td></td><td></td></tr><tr><td>Frequently</td><td>36 (52%)</td><td>37 (57%)</td></tr><tr><td>Occasionally</td><td>14 (20%)</td><td>9 (14%)</td></tr><tr><td>Rarely</td><td>7 (10%)</td><td>6 (9.2%)</td></tr><tr><td>Never</td><td>12 (17%)</td><td>13 (20%)</td></tr><tr><td>Experience as content instructor</td><td></td><td></td></tr><tr><td>Frequently</td><td>30 (43%)</td><td>21 (32%)</td></tr><tr><td>Occasionally</td><td>14 (20%)</td><td>13 (20%)</td></tr><tr><td>Rarely</td><td>7 (10%)</td><td>11 (17%)</td></tr><tr><td>Never</td><td>18 (26%)</td><td>20 (31%)</td></tr><tr><td>Training in writing evaluation.</td><td>45 (65%)</td><td>48 (74%)</td></tr></table></body></html>

There is not much knowledge about feminism among young women. It is considered a kind of \* revolution \* of women against men, but it is not simply so .   
This social + political movement developed during the last years of the sixties and it believed + aimed that women should have the same rights, power and opportunities as men. However this movement cannot be understood without referring to the situation of society in those years. The late sixties and early seventies were characterized by a deep crisis, which involved, politic, education, social relationships and in this context feminism set up a protest against an old-structured society ruled by men .   
Feminism has old and noble origins which trace back to the French revolution. Women at that time claimed the right of being considered an important part of society, asking for civil and political rights .   
In Italy during the Risorgimento, the question of feminism wasill of lite value, but at the end of the 19th century, due to the great number of women working in factories, the \* female problem \* became very important. Nevertheless what women really claimed was not only the right of working in good conditions, but they aimed at the emancipation of women as a movement of renewal of the heart of society. However women had to wait until 1945 to obtain an important suecess, indeed in that year they gained the right to vote. From 1945 italian population, like many others in Europe, was \* busy \* in \* starting again \*, there was a lot to do in a country destroyed by the war. This period sees the rise of the middle-clss, and witnesed the so called economic \* boom \* This well established middle-class sill didn't give women the same opportunities as men, so in the seventies due to the ferments developed feminism broke out .   
In those years women expressed their anger showing a strong rebellious aspect. They stood up for their rights in a society which didn't give them the same chances as men. They fought against male power, thus trying to destroy his role in society. They were against the old structured society where women could be mainly housewife, whereas man could study, work and chose with freedom. Unfortunately their noble ideas degenerated into a sort of extremism which spoiled their ideals, thus reducing, in some situations, their insurrection in a sterile fight between man + women for who had the duty to do the washing up, or the houseworks in the family .   
In my opinion feminism was an important movement that marked a necessary break in Italian society. Thanks to those fights, now there are women free to dedicate themselves to successful careers in management, politics, science, before denied.   
However these glorious conquests are only one aspect of the results obtained. The new and imposing role of women created a lack of balance in our society. Families are more and more abandoned by women interested in working many

The problems have started by the use of money-which is much more practical instead of setling method. As time passed, with the developing of societies and with the increase of needs, that is, when the living conditions have got harder, money's become the root of all evils. At this position, it has gained importance how money is eamed and for which purposes it is used. If a person gives harm not only to himself but also to the others and neglect social and ethic rules, it means that he eams money in wrong ways. When we think like this, money has become the reason of several evils. This is because of that money is a magical key which opens many doors. This is also because of the conditions of today. In today's conditions, they aren't nearly given any right to live like human-beings to the poor. However, people living in a society have the same basic rights such as to be educated, to be cared by hospitals and like these, money Spoiled this equality and cause inequality.That is, in every stage of social life the one who pays the piper calls the une. People give bribe in order to be the owner of many things and to conceal their offense.If there's a need to give an example, a person who commitsa traffic crime and gives bribe to conceal his crime will be a good example. This is only a small example. This kind of impropriety has taken place even in the govemment, which should be the most respectful institution. At the same time, people are murdering each other, they are smuggling narcotic and weapon. This means the complete ignorance of social and ethic rules. In other words, money has taken the place of humanistic values. For the sake of money, people sell their bodies, children and wives. Moreover, in order to survive, they sell their organs, such their kidneys, which has a vital role for their survival. These are organized by groups which are named as Mafia. In other words, this has become a sector .

When people aren't able to achieve the necessary conditions to survive as human beings, that i, when they aren't able to bring bread to their homes, when they can't shelter in appropriate conditions, when they couldn't send their children to the school, when they couldn't respond to his familys needs of health and when they couldn't respond to other needs of their family and most important, when they havent got a job to achieve all these kind of needs, in societies people can be dragged to suicide. People even commit theft. I want to state that the sanction of social rules isn't important when people find themselves in these conditions

Because of that money is an important commerce tool, it is an important element of our lives. In other words, in todays hard conditions, people venture everything in order to ean money and survive. They can easily make concession. As I stated before, it is important how money is eamed and for which purposes it is used. So, it will hinder people to suffer if it isn't used in the way of its purpose .

Fig. 1. A screenshot of a text comparison from Experiment 2, in the No More Marking platform (nomoremarking.com).

1. An average decision time of less than twenty seconds per comparison;   
2. A left-text selection rate of ${ \bf 1 0 \% }$ or less, or $9 0 \%$ or more (this measure indicates a strong preference for texts on any one side of the presentation).

For each of the two studies, the btm function was re-run after removing judges on this basis, resulting in the final models used to answer the two research questions. To answer RQ1, pertaining to the reliability of the judgements in the two studies, the scale separation reliability (SR) was extracted from these models. This measure provides an indication of the extent to which each item is separated from other items on the rating scale. An alternative to SR is split halves reliabilit (Bisson et al., 2016), which involves assigning all judges to one of two groups,calculating a rating scal for each group, and then calculating the correlation between them. The process s then reated round 100 times, with the median correlation cofficient being taken as the final reliability core (Jones & Davies, 2023). This method is more transparent than sSR, but requires twice as many comparisons to be conducted. Since SSR provides the greatest comparability with existing 2 J studies, and due to the practical diffclty of recruiting so many judges through a community-driven approach, sSR was preferred for this study.

To answer RQ2, pertaining to the concurrent validit of the two CJ-derived rating scale, we used two methods. Firstly, to test the fit of the CJ rank order with the categorical CEFR grades derived from the MFRM analysis of our rubric-based asessments, we fit a linear model with CJ rank order as the dependent variable and CEFR grade as the independent variable. Since the rubric-based graders were allowed to use "plus' levels to assist them in the evaluation of borderline texts (e.g. $^ { \mathrm { B 1 + } }$ grades could be assigned to texts on the border between B1 and B2 standards), these plus levels were included in the linear model. Secondly, since MFRM analysis returns a rank-order of each text in the same way that CJ doe, we ran correlation tests to compare the rank order of each text from the CJ and rubric-based methods. Spearman tests were used due to their compatibility with rank-ordered data.

Finall, because it was important for usto understand the extent of the difference between CEFR grades and CJ ranks, we identified as an outlier any texts whoseJ rank order was 1.5 times higher or lower than the median rank of texts of the rubric-rated as belonging to the same CEFR band. This alows us to identify and explore texs which generated significantlydifferent response from the CJ and rubric-based judges.

# 3. Results

# 3.1. Study 1

# 3.1.1. RQ1 reliability of CJ for texts on a single topic

For the set f esays on a single prompt, a total of 659 comparisons were collected from 70 judges. Of these, 649 decisions from 69 judges (mean decisions per judge $= 9 . 4 \dot s$ ) passed the acceptability checks described in Section 2.2. These decisions were entered into the btm model, as described above. The sSR value for the resulting model was .82. A reliabilit of .7 was definitively achieved after 18 comparisons per text, and the .8 reliability threshold was crossed at 25 comparisons. The final value of .82 represents an asymptote, since the reliability level increased by less than .01 between rounds 27 and 29.

# 3.1.2. RQ2 concurrent validity for texts on a single topic

The MFRM model derived from rubric-based asessment showed that the texts in Study 1 alloccupied the proficiency range from $^ { 8 1 + }$ to C1 (see Table 4). Note that no rubric-based grades are available for ten of the texts used in ths study due to the data loss which occurred during the MFRM analysis, described in Section 2.1. A linear model was buil to test the fit between the CEFR levels of the 38 remaining texts and their meanrank order fromthe CJ evaluation. No model asumptions wereviolated. The model showed significant fit between the two assessments $( F ( 3 , 3 4 ) = 1 4 . 2 3 , p < . 0 0 1 )$ ; the adjusted $\mathrm { R } ^ { 2 }$ value was .52. Paired comparisons of the mean rank of texts at each CEFR level are presented in Table 4. The difference in mean rank of texts at $^ { \mathrm { B 1 + } }$ and B2 levels was non-significant $( t = . 2 4 1 , p = 1 )$ as was that between texts at $^ { \mathrm { B 2 + } }$ and C1 levels $( t = - 1 . 3 6 , p = 1 )$ . All other comparisons were statistically significant.

Fig. 2 plots each text according to its CJ rank order and its CEFR level. Exact and adjacent agreement between CJ-rank and rubricbased grades were calculated by comparing each text's rubric-based CEFR level and CJ-based rank with the number of other texts rubric-rated at the same CEFR level. For example, six texts were rubric-rated at $^ { \mathrm { B 1 + } }$ level in the current study. Therefore, CJ judgements of these texts were judged to show exact greement if thir J rank fell within the first i texts. Adjacet agrement was calculated by summing the number of texts at adjacent CEFR levels before testing the CJ rank in the same way. For example, the six $^ { 8 1 + }$ texts were summed with 15 B2 texts to get 21; CJ judgements of these texts were therefore considered to show adjacent agreement with rubric-based grades if their CJ rank fell within the span 1-21. This is ilustratd in Fig. 2 by horizontal lines corresponding to the number of texts rubric-rated at each CEFR level. If CJ rank perfectly corresponded to CEFR level, all $^ { \mathrm { B 1 + } }$ texts (for example) would appear below the corrsponding horizontal lie. I reality, each CEFR level contains texts with quite a wide range of CJ ranks. Exact agreement was $3 9 \%$ , while adjacent agreement was $9 5 \%$ Only two texts showed neither exact nor adjacent agreement between CJ rank and CEFR grade. These were Text Q081, which was the highest ranked text in the $^ { \mathrm { B 1 + } }$ level, and the outlier Q238, CEFR graded as B2 but CJ-ranked as the fourth strongest text. Outliers are briefly discussed below.

To explore the relationship between the CJ- and MFRM rubric-based datasets with greater granularity, a Spearman corelation showed that the CJ and MFRM rank-orders were significantly correlated $( r = . 6 6 8$ $p < . 0 0 1 $ . Levshina (2015) suggests that correlations between .3 and .7 indicate moderate strength. The correlation is plotted in Fig. 3.

# 3.2. Study 2

# 3.2.1. RQ1 reliability of CJ for texts on multiple topics

For the set f texts comprising responses to five different esay prompts rather than only one, 610 comparisons were collcted from 66 judges. Of these, one judge was removed, leaving 602 valid comparisons $\mathrm { ( m e a n = 9 . 3 }$ decisions per judge). The btm model created from these comparisons had an sSR of .82. Though the final reliability of the two studies is almost identical, threshold values were reached slightly more quickly in Study 2: a level of .7 was passed at 13 comparisons per text, and .8 at 20 comparisons. An asymptote was reached at round 26.

Table 4 Summary f linar model comparing J ran order t rubric-a CEFR rade for Expeimet 1test as n41 de f frdm onferron correction has been applied to p values. EMM $=$ Estimated Marginal Means.   

<html><body><table><tr><td></td><td>-</td><td></td><td></td><td colspan="2">vs. B2</td><td colspan="2">vs. B2+</td><td colspan="2">Vs C1</td></tr><tr><td>CEFR grade</td><td>n.</td><td>EMM</td><td>95% CI</td><td></td><td>p</td><td></td><td>p</td><td>t</td><td>p</td></tr><tr><td>B1+</td><td>6</td><td>14.83</td><td>6.98-22.68</td><td>0.241</td><td>1</td><td>-3.46</td><td>&lt;.01</td><td>-4.24</td><td>.001</td></tr><tr><td>B2</td><td>15</td><td>13.73</td><td>8.77-18.70</td><td></td><td></td><td>-4.72</td><td>&lt;.001</td><td>-5.31</td><td>&lt;.001</td></tr><tr><td>B2+</td><td>11</td><td>31.45</td><td>25.66-37.25</td><td></td><td></td><td></td><td></td><td>-1.36</td><td>1</td></tr><tr><td>C1</td><td>6</td><td>38</td><td>30.15-45.85</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

![](img/9f419bbf9a3cc1b8e3a1655efe9a029a9271555fdf67041caea3cb6d74b6bdd8.jpg)  
Fig.2. Set showing ank od f texts  eel for Eprim.ml cilear tts; l cie shw he me kf xs at each CEFR level. Hoizontal lines repreent the number ftexs at each CEFR level,according to MFRM rubric-based grades. "Equal or adjacent texts ar thoe se rank is hi t ound  n hih  owr thn txs thr  th seR el, or a dant  el. Non-adjacent" texts have unexpectedy high or low CJ grades,comparable to texts two CEFR bands higher or lower. utliers are texs whose CJ rank order was 1. times higher or lower than the median rank of texts of the same EFR level. Non-adjacent texts and outliers ar aelled with their Text ID.

# 3.2.2. RQ2 concurrent validity for texts on multiple topics

The MFRM rubric-based model revealed that the texts in Study 2 covered a slightly broader proficiency range than those in Study 1, with two texts at C2 level in addition to those at levels $^ { \mathrm { B 1 + } }$ to C1 (see Table 5). Four texts were removed because they were dropped from the MFRM model. The linear model comparing the CJ rank order and CEFR level of the remaining 46 texts met all model assumptions, and was again significant $( F ( 4 , 4 1 ) = 9 . 5 5 5 , p < . 0 0 1 )$ ; the adjusted $\mathrm { R } ^ { 2 }$ value was .43. This is slightly lower than in Study 1. Paired contrasts showed that in most cases, rank orders were not significantly diffrent between adjacent CEFR levels (e.g. $^ { \mathrm { B 1 + } }$ vs B2 $t = - 2 . 9 6$ $p = 1$ ; B2 vs ${ \bf B } 2 + t = - 2 . 6 0$ $p = . 1 3 )$ , but were significant for contrasts spanning two CEFR levels (e.g. $^ { \mathrm { B 1 + } }$ vs $\mathsf { B } 2 + t = - 2 . 9 6$ $p = . 0 5$ . The exact and adjacent agreement levels were $3 7 \%$ and $8 9 \%$ , respectively. Five texts received CJ ranks corresponding to a CEFR level two bands higher or lower than would be expected based on their actual CEFR level. Three of these texts are the outliers Q030, Q237 and Q437; the other two were $^ { \mathrm { B 1 + } }$ texts whose CJ rank placed them alongside texts ranked at $^ { \mathrm { B 2 + } }$ level (see Fig. 4).

![](img/204dfcf893741b126ee6552d562451aebbc707b8f67b08aa7422b2ac9659faab.jpg)  
Fig. 3. Correlation plot showing relationship between CJ- and MFRM rubric-based rank orders for Experiment 1.

Table 5 Sumary of inear model comparing  rank order o EFR grad or Experiment 2.-ests based on 42 degrees f freedo. Bonferrni correction has been applied to p values. EMM $=$ Estimated Marginal Means.   

<html><body><table><tr><td></td><td></td><td></td><td></td><td colspan="2">vs. B2</td><td colspan="2">vs. B2+</td><td colspan="2">Vs C1</td><td colspan="2">Vs C2</td></tr><tr><td>CEFR grade</td><td>n.</td><td>EMM</td><td>95% CI</td><td>t</td><td>p</td><td>t</td><td>p</td><td>t</td><td>p</td><td>t</td><td>p</td></tr><tr><td>B1+</td><td>7</td><td>11.43</td><td>3.00-19.86</td><td>.43</td><td>1</td><td>2.96</td><td>.05</td><td>4.74</td><td>&lt;.001</td><td>-3.68</td><td>&lt;.01</td></tr><tr><td>B2</td><td>8</td><td>13.88</td><td>5.99-21.77</td><td></td><td></td><td>2.60</td><td>.13</td><td>4.45</td><td>&lt;.001</td><td>-3.45</td><td>&lt;.05</td></tr><tr><td>B2+</td><td>13</td><td>26.77</td><td>20.58-32.96</td><td></td><td></td><td></td><td></td><td>2.04</td><td>.48</td><td>2.05</td><td>.47</td></tr><tr><td>C1</td><td>16</td><td>35.19</td><td>29.61-40.77</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1.06</td><td>1</td></tr><tr><td>C2</td><td>2</td><td>44</td><td>28.22-59.78</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

![](img/f3ac99ec5f05c98b4bda2a2b8a2568588e61d07e857f46bbd0d1b6a791a96acc.jpg)  
Fig. 4. Scatterplot showing rank order of texts by CEFR level for Experiment 2. See Fig. 1 for further information.

Lastly, we again ran a Spearman correlation test to identify the extent of overlap between the CJ and MFRM-based rank orders. The correlation was again of upper-moderate strength, almost identical to that of Study ! $\mathrm { ~ l ~ } ( r = . 6 7 7 , p < . 0 0 1 )$ . The correlation is shown in Fig. 5.

# 4. Discussion

The two studies tested two hypotheses regarding the reliability and concurrent validit of CJ for assessing written L2 texts which were longer and covered a narrower proficiency span than those of existing studies (Study 1), and which included responses to a greater diverst of topics (Study 2). The first hypothesis, that reliailit and validit levels for these more challenging texts would be lower than those in earlier studies which featured simpler sets of texts, was supported. In both studies, eliaility levels were around ${ \bf S } { \bf R } = . 8 1$ , much lower than the values above .90 in Sims et al. (2020) and Paquot et al. (2022), and after more comparisons per item (around 25, compared with around 11 in Sims et al., (2020) and around 14 in Paquot e al., (2022). Similarly, the oncurrent validty levels were also lower than in the earlier studies - correlations with rubric-based assessments equalled around .67, compared with around .90 in Sims et al. (2020). This is likely best explained by the complexity of the text characteristics in the present study.

In spite of these lower values, however, both studies retuned acceptable levels of reliability and validity: sR levels of .81 are midway between the thresholds of .70 and .9 for low- and high-stakes asessment, respectivel, sugested by Verhavert et al. (2019), and beyond the Cronbach's alpha threshold of .80 sugested by Hoyt (2018) to indicate "good dependability of scores". Rank-order correlations in the upper-moderate range additionally suggested a close relationship between CJ and rubric-based evaluations.

The second hypothesis was that the reliability and concurrent validity of Study 2, which involved comparison of texts on five different prompts, would be lower than that of Study 1, in which al texts were responses to a single prompt. This hypothesis was not supported: the twostudies had near-identical rliability and concurrent validity cores, suggesting that J was no more difficult when judges were required to evaluat texts on a range of topics than on just one. This supports the view that J is robust to heterogeneity in item sets (Jones et al, 2014, 2016; Jones & Davies, 2023) even in the field f 2 writing asesment, wher variance in writing tasks has een shown to affect rater performance Innami & Koizumi, 2016; Weigle, 199). This is n encouraging result from the perspective of using CJ for leaner corus enrichment, since t suggests that reliable reults can be achieved evenwhere corpus contains responses to a variety of prompts. One caveat is that although a variet of prompts were used in Study 2, all items were neverthelessthe same textual genre (i.e. argumentative esays). Further reearch will be required to establish whether CJ remains eliable for comparing L2 texts of different genres (e.g. diagram description tasks and narrative passages).

![](img/e42ead16ba92d2ee1fe39e7db674745cc85e6dba0f095123de7079a3c11bfb75.jpg)  
Fig. 5. Correlation plot showing relationship between CJ- and MFRM rubric-based rank orders for Experiment 2.

Which specifictextual characteristic(s) is the likeliest cause of the weaker reliability found in the present study? The data seem to rule out topical diversity, since Study 2 returned reliability and validity levels nearidentical to those of Study 1. The influence of the two remaining factors - text length and proficiency span - cannot be definitively separated from the present data, since our study did not examine texts which were long but covered a broad proficiency span. However, there is more mpirical and theoretical support for an influence of proficiency pan than for text length. As discussed in the introduction, several studies have identified an influence of proficiency span on CJ eficacy (Crompvoets et al. 2022; Han, 2021, 2022). Two such studies have even located a potential cognitive basis for this efect: van Daal e l. (2017) suggest that judges experience greater complexity when comparing text of similar quality than when comparing those which differ more clearly, while Gijsen et al. (2021) find evidence of this experienced complexity in judges' eye-tracking data, and demonstrate its ssociation with reduced accuracy in decision-making. By contrast, few studies have systematically investigated the influence of text length on CJ efficacy. Intuitively, whil it seems likely that longer texts would be harder to judge than shorter ones dueto the amount of reading required, itis also possile that longer texts contain more evidenceupon which judges can base their decisions, facilitating decision-making.

One question which clearly arises from the present data is what accounts for remaining differences between the CJ- and rubric. based assessment. One possilit is that judges attend to different aspects of text quality when using the two methods. Research on the content validit of CJ in L1 writing aessment suggests that J judges pay closer attention to higher-order aspects such as organisation and argumentation than to surface-level aspects such as observance of language conventions and proper use of referencing (Lteruis et al., 2018, 2022; van Daal e al., 2019. n ther L2 J study, Paquot t al., (202) examined several texts whose CJ rank was higher than would be expected based on their rubric-based grade, and found some evidence of a similar pattern. They noted that some of the texts which were CJ-ranked higher than their rubric-based grade would suggest were strong at the discourse-lel (e. aspects such as argumentation and coherence),but also contained lower-level weaknesses such as basic spelling errors.

In both Studies 1 and 2, there were ocasional texts whose CJ rank was markedly diffrent from what would be expected based on their rubric-based grade (extreme example of this are labelled outiers in Figs. 2 and 4). However, examination of these texts does not offer clear support for the pattern ugested by Paquo e al., (202). oillustrate, two examples of diverging texts are provided in the supplemental materials. Text Q030 was CJ-ranked much higher (48th - higher ranks indicate stronger texts) than would be expected from its rubric-based grade (Q030 was graded as a $^ { \mathrm { B 2 + } }$ text; the mean CJ rank for this grade was 26th). Based on the above, we might expect this tex to be strong at the lel of discourse but weaker in terms of acuracy. This is not obviously the case: the text is arguably stronger in it ower-level characteritc (excellent spelling, very inrequent lexico-grammatical errors) than at the level of discourse (it makes relevant points and use clear linking expressions, ut the author's position does not become clear until very lat in the text).

This can be compared with text Q437 ( $\mathbf { C J } \mathbf { r a n k } = 1 1 \mathbf { t h }$ , rubric-based grade C1 - the mean CJ rank for this grade was 38th) which is similarly accurate to Q030 at the level of lexico-grammar, and also contains a mix of strong and weak discourse-level aspects it contains aclear thesis statement and astraightforward conclusion, but its body paragraphs are somewhat unfocused and not always clearly relat  the toc. timatly, itis fficlt to kow xaly wh the woexts were jdd  differ owidly y judge. It may be that judges prefer texts with one or other of the discourse-level features in the texts, but a systematic investigation into the decisions made during CJ and rubric-based assessment will be necessary before any conclusions can be drawn.

Lastly, another potential avenue for future research on the comparison of CJ and rubric-based asssment of L2 writing is to explore which of the two processes yielded the higher internal accuracy and consistency. In the present study, rubric-based grading with MFRM was treated as a gold-standard approach to L2 writing assessment, because this facilitated the evaluation of CJ's concurrent validity. However, even given the corrections that MFRM ofers to the potential shortcomings of rubric-based asessment (McNamara et al., 2019), rubric-based assessment inevitably includes an element of measurement error. For this reason, we should be cautious about assuming that rubric-based scores are ncessaril more accurate than CJ ones. A recent study by Pinot de Moira e l. 2022) suggests an alternative approach. The authors compared CJ and rubric-based assessment of L1 writig in terms f lssification accuracy (i.e. how often each individual judgement matched what would be expected from each text's "definitive" score) and classfication consistency (i."a measure of the probabilit that ascript will be given the same grade under reeated administrations of an asesment" p32). The study found CJ to have higher accuracy than rubri-based asssment, and to be more consistent than single, double, and triple rubric-based assessment. Such an approach might offr a way to validate studies, such as the present one, that explore CJ's efficacy for L2 writing assessment.

# 5. Conclusion

This study sought to investigate whether CJ can be used to effectively assess sets of relatively long L2 texts which covered a relatively narrow proficiency range $\left( \mathbf { B } \mathbf { 1 } + \mathbf { - C } \mathbf { 2 } \right)$ , and which contained responses to a variety of essay prompts. The results suggest that these text characteristics do appear likely to affect reliability and validity levels as wel as the number of comparisons required to reach satisfactory reliability), but it nevertheless remains possible to achieve satisfactory evaluations.

These findings broaden the range of potential contexts in which CJ might effctively be used. Section 2.1 already discussed the idea of using CJ to enrich learner corpora with proficiency information. Similarl, the method could be used to asses and accurately reporting on the proficiency of the texts/learners involved in rearch studie. This would answer recent call within applied linguistics for the development of fthe-shelf assessment methods for research purpose e.g. Norrs, 2018; Norris & Ortega, 2003; Park et al., 2022). CJ might also be used to assestexts in contexts such as schools or university departments, where texts of similar qual. ity/proficiency frequently ned to be assessed. One potential limitation is the essentiall norm-referenced nature of CJ scores: if alignment with an extnal standard such as a CEFR band isrequired, additional steps will need to be taken. One ossile pproach is to include anchor items reflecting minimum standards for each band of the target proficiency scale (Jones & Davies, 2023.

The insights provided by this study regarding the reliability and concurrent validity of CJ for L2 assessment now need to be supplemented by rearch into construct validity. f CJ is to ecome n acceted tool for L writing asment, it will be neessary to demonstrate that the decisions from which CJ scores derive are being made on a principled basis, without excluding important aspects of text quality. We hope that this study will encourage such research.

# CRediT authorship contribution statement

Magali Paquot: Conceptualization, Funding acquisition, Investigation, Methodology, Project administration, Supervision, Writing - review & editing. Charalambos Kollias: Data curation, Formal analysis, Project administration, Software, Writing - review & editing. Peter Thwaites: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Resources, Software, Visualization, Writing - original draft.

# Data Availability

Data is available at https://osf.io/u28r4/files/osfstorage

# Acknowledgements

The authors wish to thank allthose who participated inthe CLAP project as CJ judges. This research was supported by grant numbe 40008459 awarded by the Le Fonds de la Recherche Scientifique (FNRs) to the third author.

# Appendix A. Supporting information

Supplementary data associated with this article can be found in the online version at doi:10.1016/j.asw.2024.100843.

# References

Bisso  J e., is, Jo  (016   n g ie me i J of h n Undergraduate Mathematics Education, 2(2), 141-164. https://doi.org/10.1007/s40753-016-0024-3   
lad,       01  i  s si  /.g 10.35111/7ez0-x912. (https://catalog.ldc.upenn.edu/LDc2014T06)   
Bradey,  r,  (1952 k asi o e bk  f r oriso.  393/) 324345. /o.g 10.2307/2334029   
Bramley, . 207).  marn m.  . n id, i, H Pric,  s.) qs o minhe oi examination standards (pp. 246-300). Qualifications and Curriculum Authority.   
Carse,  (012). P  viab n r r d nsic, 3(2 61-18/./.109/pin/a047   
Chamer .  i    in, . (https://www.frontiersin.org/articles/10.3389/feduc.2022.802392).   
R Core Team. (2022. R: A Language and Environment for Stistical Computing. R Foundation for Statisical omputing. htp://ww.R-proect.org.   
Cuc f 0   f R      R): A Manual. Council of Europe Language Policy Division.   
C   1)   i i . . org/10.31234/osf.io/32nhp.   
C        i   /. frontiersin.org/articles/10.3389/feduc.2021.788202).   
Gije,  n l  is,  js, Mr, . 2021)The t f ti m ing iv w An ee tracking study. Frontiers in Education, 5. (https://www.frontiersin.org/articles/10.3389/feduc.2020.582800).   
ager,  , , r,  at, 00)  f  (.2). P t d a ae.   
Han,. 2021). ltic rbic cs prive m mparis rc asn ke- intig  Joes Traducteurs / Meta: Translators' Journal, 66(2), 337-361. https://doi.org/10.7202/1083182ar   
Han, C. (2022). Aessng spoken-language interpreting: The method of comparative judgement. Interpreting, 24(1), 59-83.   
Han,   , Fa  i,  (2. g rie ive  to ti   e, 31, 5674. https://doi.org/10.1556/084.2022.00001   
Heingr   0  the   i  c . hcher, 372, 1-19. https://doi.org/10.1007/BF03216919   
Heldinger,  mpy, 013). sin e s the c f ing Anmprica . onl, 3 219235. https:/doi.org/10.1080/00131881.2013.825159   
Hoyt w. . (2018). nratr reliailit and gment. he rviewers guid to quanive metods in h sci scie (pp. 132-14). te.   
mphry,  n,. 019m   o w  on 563, 520. doi.org/10.1111/jedm.12223 doi.org/10.1177/0265532215587390   
Jone  (22). o me . h/inti.p3.nsirt %2Fblob%2Fmain%2FR%2Fsirt functions.R&data=05%7C01%7Cpeter.thwaites%40uclouvain.be%7C63005d2103e44f64362108db792c2b22% 7C7ab090d4fa2e4ecfbc7c4127b4d582ec%7C0%7C0%7C638237002831653403%7CUnknown% 7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C% 7C&sdata=yI2oKl7iGJmT3ncrc9Zmo5RPYnhublrngnHe9fH9LV0%3D&reserved=0.   
oe    .i    . f    o 0  . 10.1080/1743727X.2023.2242273   
Jones I, & Iis,  (015). The probe f asng probl oling Can cmpartiv udement helti des inhic, 89,337-355.   
Jones ,  it  (014 A mtil po  usin tive  i  of  his Education, 13(1), 151-177. https://doi.org/10.1007/s10763-013-9497-6   
one    reis, 016). y  -     oh , 424, 543-560. https://doi.org/10.1002/berj.3224 when comparing argumentative texts. Frontiers in Education, 7. (https:/www.frontiersin.or /articles/10.3389/feduc.2022.823895)   
Lethis,   a , Va e, , te , he,  e e . 018.  ch me ativ  o ord y mutiple cpl ass f x quait. 1-tion de in  n ir, 18 1-2 h/.g/10.17239/1-2018.18.01.02   
Levshina, N. (2015). How to do linguistics with R. Data Exploration and Statistical Analysis, Amsterdam-Philadelphia.   
Linacre, J.M. (1989). Many-faceted Rasch measurement [PhD Thesis, The University of Chicago]. https:/earch.proquest.com/openview 9947a2b1a43f10331c468abcca2ba3e6/1?pq-origsite=gscholar&cbl=18750&diss=y.   
cane  y, r 018)ys t thd r n Education, 31(4), 297-311. https://doi.org/10.1080/0895734] .2018.1495216   
McNamara, T. (1996). Measuring Second Language Performance (First Edition). Pearson Educational,.   
McNamara, T., Knoch, U., & Fan, J. (2019). Fairness justice and language assessment. Oxford University Press.   
Norris, J. M. (2018). Developing C-tests for estimating proficiency in foreign language research. Peter Lang,.   
Norris J. M., & Ortega, L. (2003). Defining and measuring SLA. The Handbook of Second Language Acquisition, 716-761.   
dr i te t th e0 1.106. edurev.2020.100329   
Paquot,  bi   Vwrd  (02r ptie artive Jment t- io fr i g, 5e Learning, 72(3), 853. https://doi.org/10.1111/lang.12498.   
Park   hoia  )  ini h  i de ny progress? Language Learning, 72(1), 198-236. https:/doi.org/10.1111/lang.12475   
Piot de  W,  hristou .2). e citn y d st f ctive m f wig o  ruribased teacher assessment. Research in Education, 113(1), 25-40. https://doi.org/10.1177/00345237221118116   
owers, . ki,  015  t  o   dd wrn in Education, 28(2), 130-142. https://doi.org/10.1080/08957347.2014.1002920   
Robitzsch, A. (2022).sirt: Supplementary Item Response Theory Models. https://CRAN.R-project.org/package=sirt.   
Sh 01t  u i     t i f    f i Research, 5(4), 140-154. https://doi.org/10.33902/JPR.2021474154   
Sims, M. E, ox  L ti ., Hrho, J, Wx, MP.  Hrt, JM. (2020) Rbric ati wh  vrus aly ue omarative judment rin f  p d- wi t.   d Prcice 94), 30-40. /o.g 10.1111/emip.12329 org/10.1080/08957347.2016.1171769   
aja 0 i t t   e Education, 28(1), 79-105. https://doi.org/10.1007/s40593-017-0142-3   
an Dal, , s,   ,  d m  ch   Mer, . 017) The omxt f ag  rk usig oaive judme g.0004   
anal 0i  iati of its holistic chter a udng n a shre o. in tin: ricie, Poic  Ptice 61) 5974. /o.g/10.1080/ 0969594X.2016.1253542   
re1 & Practice, 26(5), 541-562. https://doi.org/10.1080/0969594X.2019.1602027   
Vvert, , e Mr, , ,   018  t  d i   the   i pld Psychological Measurement, 42(6), 428-445. https://doi.org/10.1177/0146621617748321   
Vvert  e  d   ie r ti www.frontiersin.org/articles/) Frontiers in Education, 6. https:/doi.org/10.3389/feduc.2021.785919.   
egle .. t iii 5. doi.org/10.1016/S1075-2935(00)00010-6   
hn  0i     l. Assessment in Education: Principles, Policy & Practice, 27(1), 46-64. https://doi.org/10.1080/0969594x.2019.1700212

ter  t-   a p    h r     h e inguistics from Cardiff University and has published in the areas of L2 writing pedagogy and lexical knowledge.

(vital) stnd i rs to  inr to i dg rmk.  i   n f hric analysis metds, iding tes eqting, it analyis and test-taer emae, ad item bank cation though Rsch Meurement Thry (RMT.