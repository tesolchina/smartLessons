# Validity of automated essay scores for elementary-age English language learners: Evidence of bias?\*

Joshua Wilson1,\*, Yue Huang2,3

University of Delaware, School of Education, United States

# ARTICLEINFO

# ABSTRACT

Keywords: Automated writing evaluation Automated essay scoring Writing assessment English language learners Language assessment

Given increased prevalence of automated writing evaluation (AwE) systems in classroom settings, more research is needed to explore the potential for bias in automated scores with respect to English language learners (ELLs). Thus, this research study investigated and compared the predictive validity of automated and human scoring methods for elementary-age English ELLs on a writing test designed for ELLs and a state writing test designed for the general population. This study focused on the MI Write AWE system and sampled 2829 students comprising ELLs and nonELLs in Grades 3-5. Results of multilevel regression analyses and simple slopes estimation indicated that, for ELLs, the automated MI Write score had similar predictive validity to the human score for both writing tests. However, automated and human scores for ELLs were less closely related to the state writing test score than scores for non-ELL students. Findings suggest that MI Write's automated scoring was not uniquely biased relative to human scoring but does reproduce the same biases evident with human scoring. Implications and directions for future research are discussed.

# 1. Introduction

An increasing number of English language learners (ELLs) are educated in the United States (U.S.). According to 2017-2018 National Center for Education Statistics (NCEs) data, ELLs comprised $1 0 . 1 \%$ (5.0 million students) of public-school students in the US, up from $8 . 1 \%$ (3.8 millon students) in fall 2000. Nearly two-thirds of ELLs are found in elementary grades. However, there is an alarming gap in writing performance between elementary-age ELLs and their native-English-speaking peers. Results of the 2002, National Assessment of Educational Progress (NAEP) writing assessment indicated that only $8 \%$ of fourth-grade ELLs performed at or above the proficient level compared to $2 9 \%$ of non-ELLs.

ELLs' writing performance is the result of complex interactions between their native language proficiency (oral and written) and their developing English oral and writen language proficiency (Chenoweth & Hayes, 2001; Pasquarella, 2019), along with other factors known to contribute variance to writing performance such as rater and task variance (In'nami & Koizumi, 2016). ELLs may struggle with writing because composing in English places increased demands on key cognitive processes involved in composing, namely translation and transcription (Graham, 2018; Hayes, 1996).

Translation refers to the act of (a) generating ideas from multiple sources in long term memory, including linguistic information (i. e., words), images and sensory information, or abstract concepts, and (b) translating that information into appropriate language to be transcribed into text. ELLs often have difficulties developing vocabulary skill in English (Zhang et al., 2021), which may hinder the translation process. Transcription refers to the processof transcribing ideas fluently and accuratel into text (Graham, 2018; Hayes, 1996). ELLs may have difficulties learning the orthography of English and thus may struggle with spelling and applying appropriate written conventions fluently and accurately (Olson et al., 2015). ELLs also may need to acquire relevant metacognitive knowledge related to the witing process(i.., procedural knowledge) and how to compose in different genre (.., gnre/discourse knowledge) (see Wilson & Wen, 2022 for a discussion of metacognitive knowledge in writing).

Consequently, there is interest in identifying methods and tools that may help improve writing outcomes for ELLs (Olson et al., 2015; Pasquarella, 2019). One potential tool is automated writing evaluation (AWE). AWE is web-based software that leverages natural language procesing (NLP) and machine learning to generate automated feedback and automated essay scoring (AES) for formative asessment purposes (Grimes & Warschauer, 2010; Wilson \* Czik, 2016). AWE's immediate and reliable feedback and scoring is intended to support students in developing writing proficiency and to aid educators in instructional decision making (Gerard & Linn, 2016; Wilson, Ahrendt et al., 2021). Given these affordances, AWE systems are increasingly being adopted for use in elementary general education settings that include ELLs (Wilson, Huang et al., 2021).

However, AWE systems marketed towards general education classrooms in the US were not specifically designed for ELLs (Hoang & Kunnan, 2016; Warschauer & Ware, 2006). That is the training data-authentic student writing scored by trained human raters-used to develop AWE automated scoring systems may lack representation of ELLs. Consequently, AWE systems may not accurately evaluate the writing of ELs, generating potentiall biased automated scores for this population. In that case, the validity of using AWE for formative assesment purposes, such as identifying ELLs at risk of low performance on a state test, i called into question. Thus, more research is needed to explore AwE's potential for bias.

Accordingly, the present study examined the validity of automated essay scores generated by a formative AWE system called MI Write for providing valid inferences about the writing proficiency of US elementary-age ELLs. Specifically, we investigated the potential of AWE automated scoring to display bias relative to human scoring when evaluating the writing of ELLs with respect to predicting performance on external standardized measures of writing ability (i.e., predictive validity).

# 1.1. Prior evidence of predictive validity of automated scoring

Predictive validty eidnce refrs t evidence ndicating how accurately test data collcted at one time can predict citerion cores that are obtained at alater time (American Educational Reeach Association, 2014, p. 22). It i one of several source of evidence to examine when validating educational asessments and automated scoring models (Ramineni & Williamson, 2013). Evidence of predictive validity is especialy important when using assessment data for the purposes f identifying struggling leaners at risk of por educational outcomes (i.e., screening) and monitoring students' progresstowards end-of-year competencies (i.e., benchmarking). Should a measure have ntaly diffeent predctive validt for iffer subroups, such as ELs, it woul ndcat the presee f bias and the potential for inaccurate decision making and negative consequences for that subgroup(s) (Ramineni & Willamson, 2013). Indeed, as Ramineni & Williamson (2013) explain, an important method of evaluating the bias of automated scoring i to compare how similarly automate cores and human scores of the same set f essays predict an external outcome for aparticular subgroup finterest, such as ELLs.

Prior research exploring AWE's predictive validity reports promising findings. For instance, automated scoring has been shown to accurately identify students in Grades 3-5 at risk of non-proficiency on state English language arts and writig tests with acceptable levels of sensitivity and specificit (Kellr-Margulis et a., 2021; Wilson, 2018; Wilson & Rodrigues, 2020). Reearch also uggests that when AWE is implemented within a writing curriculum, automated essay scores applied to curriculum-based writing prompts predict third, fourth, and fifth grade state test writing performance even after controlling for demographics, prior tate test writing performance, and school-level contextual factors (Wilson, Huang et al., 2021).

Against this background, recent research has explored the potential of predictive bias in automated scoring. For instance, within a sample of 609 students spanning Grades 2-5, Matta et al. (2022) evaluated predictive bias of human and automated scoring of curriculum-based measurement (CBM) probes for scoring the writing of students with different race and ethnicity characterisic. Their analyses indicated that there was no evidence of bias between Hispanic and African American students for predicting the Texas STAAR writing test scores from CBM probes scored either by hand or via the writeAlizer (Mercer, 2020) automated scoring system.

Another study by Reed & Mercer (2022) considered the issue of predictive bias using a sample of approximately 2600students spanning Grades 3-11 with interim writing assesment data scored either by teachers or researchers and summative writing assesment data scored viautomated scoring. Findings indicated that the relationship between the interim and summative assessments did not differ across subgroups, but that teacher ratings exhibited astronger tendency towards bias than reearcher ratings, coring females higher and scoring ELLs, students with disabilities, and students receiving free/reduced lunch lower than researchers.

Finally, Matta et al. (2023) studied 421 students in Grades 4 and 7 who took a state writing exam comprising composition and multiple choice revising and editing questions. Automated writing quality scores generated by writelizer were used to asess the composition section. The study found no evidence of predictive bias in the automated scores for student subgroups based on race and ethnicity. However, the study did identify statistically significant goup difrences in predicting overallstate est scores three years later, but not distal composition subtes cores, sugesting that construct-irrelevant factors in the multiple-choice sections may have influenced performance among different racial or ethnic groups.

# 1.2. Present study

The present study extends current research in several ways. Prior AWE research has not typically focused on ELLs in the U.S. (see Stevenson & Phakiti, 2014), and no prior WE reearch has focused on elementary-aged ELLs. Instead,studies of AWE with non-native English speakers have focused mainly on college-aged students who lean English as a foreign language (e.g., Bai & Hu, 2017; Dikli & Bleyle, 2014; Hassanzadeh & Fotoonejad, 2021; Waer, 2021), a population experiencing very different instruction and motivation than elementary-aged students.

Moreover, most prior AWE validity research with ELLs has focused on the weaker precision and recall of AWE written corrective feedback (i., spelling and grammar feedack) relative to such feedback provided by teachers (e.g., Chapell et al., 2015; Dikli & Bleyle, 2014; Hoang & Kunnan, 2016; Ranall et a., 2017). Those studies focused on the accuracy of AWE feedack, not the validit of score inferences related to AwE's automated essay scoring.

Lastly, more research is needed to evaluate the presence of bias in automated scoring, particularly for automated scoring directly intended to support instructional decision making, as is the case with AwE. Specifically, as called for by Ramineni & Williamson (2013), more research is needed to examine diffrential evidence of predictive validity for ELLs when using automated and human scores. Although recent studies have begun to focus on this area (e.g., Matta et al., 2022; Matta et al., 2023; Reed & Mercer, 2022), those studies have focused on one researcher-developed AWE system (WriteAlizer) designed primarily to support progress monitoring and automated scoring. Additional research is needed to examine the potential of predictive bias with commercially developed and marketed AWE systems that are more widely deployed in clasroom setings for the purpose of supporting teaching and learning because of their scoring and feedback capabilities.

Thus, the present study investigated the potential for bias in MI Write's automated scores for ELLs by comparing the predictive validit of automated and human scores relative to external standardized measures of writing peformance. Specifically, we sought to answer the following research questions:

RQ1: Do automated and human scores equally predict elementary-age ELLs' performance on a writing test designed for ELLs and on a state writing test designed for the general population?

RQ2: Do automated and human scores predict state test writing performance for elementary-aged ELLs and non-ELLs similarly?

Answering the first research question allowed for considering whether, fr an ELL-only sample, automated and human coring were similarly predictive of two separat external standardized measures of writing asessment and to consider how similar the assciations were between each outcome. Answering the second research question allowed for considering whether, relative to human scores, automated scores showed greater evidence of bias for ELLs relative to non-ELLs for predicting state test writing performance. The present study therefore involved a multifaceted examination of bias of a commerciall developed and marketed formative AWE system for accurately evaluating elementary-aged ELLs' writing performance.

# 2. Methods

# 2.1. Context and participants

The present study includes a sample of students from a school distrct in a mid-Atlantic state in the US that implemented MI Write (previously called PEG Writing) in school year 2017-18 with all students in Grades 3-5. MI Write (ww.miwrite.net), developed by Measurement Incorporated, is a web-based AWE system intended to help students in grades 3-12 improve their writing through practice, timely feedback, and guided support The school district implemented MI Writ together with a Common Core-aligned English language arts curriculum called ReadyGen to support the teaching and learning of writing for all students.

Table 1 Sample Demographics.   

<html><body><table><tr><td rowspan="2">Variable</td><td colspan="2">Sample A (N = 608)</td><td colspan="2">Sample B (N = 2829)</td></tr><tr><td>Number</td><td>Percentage</td><td>Number</td><td>Percentage</td></tr><tr><td colspan="5">Grade</td></tr><tr><td>3</td><td>212</td><td>34.9</td><td>874</td><td>30.9</td></tr><tr><td>4</td><td>268 128</td><td>44.1 21.0</td><td>1026 929</td><td>36.3 32.8</td></tr><tr><td colspan="5">5</td></tr><tr><td>Gender</td><td></td><td></td><td></td><td></td></tr><tr><td>Male</td><td>346 262</td><td>56.9 43.1</td><td>1415 1414</td><td>50.0 50.0</td></tr><tr><td colspan="5">Female Race</td></tr><tr><td>Black</td><td></td><td></td><td></td><td></td></tr><tr><td>Asian</td><td>14</td><td>2.3</td><td>572</td><td>20.2</td></tr><tr><td> Hispanic/Latino</td><td>34 522</td><td>5.6 85.9</td><td>176</td><td>6.2</td></tr><tr><td>White</td><td></td><td>90.6</td><td>899 2060</td><td>31.8</td></tr><tr><td>Special Education Status</td><td>551 110</td><td>18.1</td><td>369</td><td>72.8</td></tr><tr><td>English Language Learner</td><td>608</td><td>100.0</td><td> 652</td><td>13.0 23.1</td></tr></table></body></html>

Note. Racial categories were not mutually exclusive, therefore, the percentages total to more than $1 0 0 \%$

The present sample consisted of students who utilized MI Write in the period ranging from April 1 to May 31, 2018 (Spring 2018) and who had complete data available for the ELL language proficiency test (ELLs only) and the state writing test (ELLs and non-ELLs). We selected the data from Spring 2018 because this was the second semester that the students and teachers used MI Write, ensuring that students had gained some familiarity with using MI Write. Also, this was the same period in which ELLs completed an ELL-specific language assessment and all students completed the state writing assessment.

Because the research questions focused on not only ELLs, but also the comparison between ELLs and non-ELLs, two samples were created. The firs sample, used for answering research question one, included only ELLs (Sample A) and consisted of 608 students, 131 teachers, and 14 elementary schoos. The second sample, used for answering research question two, included both ELLs and non-ELLS (Sample B) and consisted of 2829 students and 165 teachers in 14 schools. Demographics of both samples are presented in Table 1.

# 2.2. Measures

# 2.2.1. Outcome measures

2.2.1.1. English language proficiency test. The study used the writing scale score of a standardized K-12 test of ELLs' language proficiency named Asessing Comprehension and Communication in English State-to-State (ACcEss) to measure ELLs' written language proficiency. The ACcEss test i adminisrated by the World-Clas Instructional Design and Assessment (WIDA) Consortium and is used by states fr meeting the requirements for Every Student Sceeds ACT (EssA) (https:/wida.wisc.edu/assess/cces). The writing subtest includes constructed response items that are scored by human rater. The present study used the AccesS writing scale score as a measure of ELLs' written language proficiency. The writing scale score ranges from 100 to 600. According to WIDA's annual technical report, the reliability of the writing scale score (i.e, Cronbach's alpha) was 0.90 for grades 2-3 and 0.88 for grades 45 (World-Class Instructional Design and Assessment, 2018).

2.2.1.2. Standardized state writin assesment. The study also used students writing scalescore from the Spring 2018 administrationof the summative Smarter Balanced English Language Art (ELA) test (htt://www.smarterbalanced.org/about/). A computer-based assessment used for accountability purposes, the Smarter Balanced ELA test evaluates writing proficiency via six selected-response items and one extended performance task. The selected-response items were scored automatically, and the essay task was scored by human raters according to a rubric focusing on three traits of writing quality: organization/purpose, evidence/elaboration, and conventions. Raw scores on these items are converted to a verticallyscaled standard score ranging from 200 to 300. The 2017-18 ELA test's marginal rliaility (i.e, one minus the ratio of mean error variance to observed score variance) was.93for Grades 3-5 (Smarter Balanced Assessment Consortium, 2019).

# 2.2.2. Predictor measures

2.2.2.1. Writing prompts. The research team in collaboration with the chool district developed three nformative writing prompts per grade level (i., nine prompts total), which were then integrated into MI Write to asses student witing quality across three grades. Each prompt required students to read two short texts with matched Lexile levels and use the information to craft a comparative response Topics were chosen in collaboration with the district, aligned wth its EA curricula, and the complexit of the texts increased with each grade level. The prompts were administered in the spring of the school year in a counterbalanced fashion acrosschools and grade levels. Teachers administered the prompts using a set of standardized instruction, directing students to engage in planning, drafting, and revising within MI Write. All analyses used measures of the writing quality of students'final drafts.

2.2.2.2. Automated essay scoring: MI Write holisic score. MI Write employs the PEG (Project Esay Grade) automated essay scoring system to evaluate writing qualit across dfferent genres informatie, argumentative, narrative) and grade bands (3-4, 5-6, 7-8, 9-10, and 11-12). Thus, MI Write's automated scoring algorithms are genre- and grade-band-specific, but prompt independent, meaning that they are intended to work with any prompt aligned to a given genre. MI Writ provides automated cores for ix traits of writing-idea development, organization, style sentence fluency, word choice and conventions--with each trait scored on a 1.0 to 5.0 scale. Trait scores are very highly correlated (range $r = . 9 3 4 \mathrm { - } . 9 9 6$ , have very high internal consistency $( \mathsf { \pmb { \alpha } } = . 9 9 5 )$ , and represent the same latent trait- exploratory factor analysis (EFA) using principal axis factoring identified a single factor accounting for $9 7 . 7 2 5 \%$ of the variance. Thus, for purposes of data analysis, theM Write holistic score was used, a score generated by summing the ix individual trait scores (range $\mathsf { a } = 6 . 0 { - } 3 0 . 0 \bigr )$ . The MI Write holistic score is highly reliable, both in terms of its generalizability (see Chen et al., 2022; Wilson et al., 2019) as well as regarding machine-human agreement--quadratic weighted kappa averages in the low. ${ \boldsymbol { 8 0 } } { \boldsymbol { s } }$ $\scriptstyle { M = . 8 4 }$ , SD $= 0 . 0 3 \mathrm { \ : }$ across the 15 different grade-band\*genre scoring algorithms MI Write employs. For data analysis the MI Write holistic score was centered by subtracting 6 from the original scores (centered range $= 0 { - } 2 4 \dot s$

2.2.2.3. Human scoring. A total of nine trained raters scored the corpus of 2829 essays using the same Six Trait rubrics within the MI

Write system to ensure commensurate measurement of writing quality with the automated scoring. Raters on this project were full time MI staff with decades of experience scoring writing responses and managing writing projects, both holisic and trait based. This experience allowed the raters to reliably train themselves using the existing MI Write materials for grades 3-4 and grades 5-6. Raters scored prompts within their assigned grade band, and individuals were capped as to the percentage of responses they could score per prompt, so as not to allow any single rater to have outsized influence in the scoring of any given item.

Raters were encouraged to ask questions and instructed to place responses about which they had questions On Hold' for the Project Managers to review and provide fedback. Raters' inter-rater reliability and overall scoring trends were monitored, and feedback was provided if ratr seemed to be drifting from the criteria.  small sampl of responses for each prompt were pot-checked if the Project Managers noticed scoring discrepancie, and in some cases, non-exact scores on individual traits were adudicated by the Project Managers if they felt one rater was clearly inaccurate in their scoring for that trait on that response. Thus, the project manager review would have provided read-behind data in addition to the IRR.

Raters scored an average f 320 responses, assigning integr cores for trait using a cale of 1.0 to 5.0. An overall(i.e., holistic) score was calculated by summing the scores for each trait and ranged from 6-30. Raters randomly selected approx. ${ \bf 1 0 \% }$ of the corpus $( n =$ 312 essays) to double score as a reliability check. Inter-rater rliability, calculated as Pearson's correlation, was .926 based on the holistic score. Trait scores asigned by the human raters were highly correlate, though not as high as the automated trait ratings (range $r = . 6 8 8 \mathrm { - } . 8 1 0 $ and had very high internal consistency ( $( \mathsf { \pmb { \alpha } } = . 9 5 0 )$ . The human scored six trait ratings represented the same latent trait-- exploratory factor analysis (EFA) using principal axis factoring identified a single factor accounting for $8 0 . 2 6 3 \%$ of the variance. Thus, for purpose of data analysis the raters' summed holistic score was used, which matched the range of the MI Write holistic score (range $= 6 { - } 3 0 _ { \circ } ^ { \circ }$ . For purposes of data analysis, the human holistic score was centered by subtracting 6 from the original scores (centered range $= 0 \AA { - 2 4 } )$

# 2.2.3.Covariates

Student demographics were included in the statistical models as covariates because empirical studies have found that student demographics are important predictors of writing proficiency independent of ELL status (see Deane, 2023). Controlling for such demographic factors in the predictive models enabled achieving a more nuanced conclusion regarding the ELL group after consideration of other factors that could influence the outcome variable. Specifically, grade level was recoded as a three-level ordinal variable ${ \boldsymbol { 0 } } =$ Grade 3; $1 = \mathrm { G r a d e } 4$ $\mathbf { 2 } = \mathbf { G r a d e } \ 5$ . All other covariates were dummy-coded: Male, Hispanic, Black, Asian, Special Education status, and English language learner (ELL) status.

# 3. Data analysis

# 3.1. Prompt and rater effects analyses

Exploratory analyses examined the potential of prompt efects or rater efects influencing the validity of the automated and human scoring, respectively.

"Prompt efects" in the context of measuring writing performance refer to the influence that the characteristics of a writing prompt have on a student's written response, which can affect the reliabilit and validity of the asessment (In'nami & Koizumi, 2016). A one-way analysis of variance ANovA) with prompt assgnment as the main factor was conducted for each grade to explore the prompt effects for both Sample A(i., the EL-only sample) and Sample B(the ELL and non-ELL sample). The mean differences among group MI Write holistic and human holistic scores, respectively, were then compared by a post-hoc test called Tukey's honestly significant differece (H). Pmpt efcts we demed prent if there we tatisticy sificn dfferece in pfomae across the three prompts assigned by grade level.

"Rater effects" refer to the variations in the evaluation of student' writing performance attributable to individual differences among raters, such as their perceptions, biases, and interpretative frameworks, which can impact the coring consistency and farness of the assessment (Innami & Koizumi, 2016; Wind, 2019). To tes for rater effct, first, descriptie stisics including the mean and variance were calculated for each of the nine raters for ll six traits and the holistic score. Then, Levene's test was used to test for homogeneity of variance across the nine raters for the six traits and the holistic score. A statistically significant result indicates that rater variances are not equal, and the potential presence of rater efects manifested as varying degrees of leniency, strictness, or inconsistency in rating behavior.

# 3.2. Research question 1

Hierarchical linear modeling (HLM) was used with the ELL-only data (Sample A) to account for the nesting of students within schools (Raudenbush & Bryk, 2002). Teacher-level variance was not considered because with an average of only five units at the student level for teacher-level prediction it would lead to unreliable estimation of the model (Raudenbush & Bryk, 2002). Hence, for each dependent variable (i., the Access and Smarter Balanced writing scale scores), we designed an equivalent set of two-level models with students nested within schools.

We first specified the unconditional random-intercepts model and calculated the intraclasscorelations (ICC) at each level. The ICC refers to the ratio of the between-cluster variance to the total variance (Raudenbush & Bryk, 2002). ICCs allow for a better under. standing of the within-cluster variance and between-cluster variance in HLM. A high ICC value indicates large proportion of variance that is explained by clustering in the total variance, therefore showing that a hierarchical model (compared to the simple linear regression model is needed. In this study, ICCs were calculated in both the unconditional model and the conditional model with predicters added in to understand the variancestructure. We also compared the ICCs from the unconditional model and the conditional model to see how the ICCs changed with the addition of variables to the model, providing a measure of overallexplanatory power similar to $R ^ { 2 }$

The final conditional model included demographic covariates and the MI Write holistic score added at level one. Since the exploratory analyses did indicate the presence of prompt effects and rater efect (see detailed results in Section 4.2), two dummy variables representing two prompt groups were added to the model as controls. No predictors were added at level two because all predictors of interest were at the student level. The specific equations of the final two-level mixed models for each outcome variable are listed below:

ACCESsWritingScore ${ \mathrm { i j } } = \gamma _ { 0 0 } + \gamma _ { 1 0 } \ { } ^ { * } { \mathrm { G r a d e } } _ { \mathrm { i j } } + \gamma _ { 2 0 } \ { } ^ { * } { \mathrm { M a l e } } _ { \mathrm { i j } } + \gamma _ { 3 0 } \ { } ^ { * } { \mathrm { H i s p a n i c } } _ { \mathrm { i } }$ j + Y40 \*SpecialEdj + y50 \*Asianij + Y60 \*Blackij + Y70 \*MIWriteScoreij $^ +$ Y8o \*Pr $\mathrm { \ o m p t D u m m y 1 _ { i j } + \gamma _ { 9 0 } \mathrm { \# P r o m p t D u m m y 2 _ { i j } + u _ { 0 j } + \mathrm { r _ { i j } } } }$

SmarterBalancedWriting $\mathrm { S c o r e _ { i j } } = \gamma _ { 0 0 } + \gamma _ { 1 0 } \mathrm { { \Psi ^ { * } G r a d e _ { i j } } + \gamma _ { 2 0 } \mathrm { { \Psi ^ { * } M a l e _ { i j } } + \gamma _ { 3 0 } \mathrm { { \Psi ^ { * } H i s p a r } } } }$ icij + y40 \*SpecialEdj + Y50 \*Asianij + Y60 \*Blackij Y70 \*MIWriteScoreij $^ +$ Y80 $\mathrm { ^ { * } P r o m p t D u m m y 1 _ { i j } } + \gamma _ { 9 0 } \mathrm { ^ { * } P r o m p t D u m m y 2 _ { i j } } + \mathrm { u _ { 0 j } } + \mathrm { r }$

In the equations, s refer to the estimated coefficients for either the random intercepts or the fixed slopes. The notation $\mathrm { \mathbf { u } _ { 0 j } }$ refers to the random error at the school level, and the notation ${ \bf r _ { i j } }$ refers to the random error at the student level.

We further specified another nearly equivalent set of two-level conditional models for each dependent variable using the human holistic score, instead of the MI Write holistic score, with all other covariates remaining the same to compare the predictive power between human scoring and M Write scores. n addition, we added eight dummy variable representing the nine different raters.We only added the rater dummy variables to the models that considered human score as the main predictor because rater effcts were only meaningful when human scores were considered. Using the same notations as above, the equations for the additional two models with human scoring are shown below:

ACCEssWritingScoreij = y00 + y10 \*Gradeij + y20 \*Maleij + y30 \*Hispanicij $^ +$ y40 \*SpecialEdij + y50 \*Asianij + y60 \*Blackij + y70 \* HumanScoreij $^ +$ y80 \*PromptDummy1ij $^ +$ 790 \*PromptDummy2ij $^ +$ 7100 \*RaterDummy1ij $^ +$ Y110 \*RaterDummy2ij $^ +$ ... + Y170 \*RaterDummy8ij $+ \mathrm { \ u o j ^ { + } }$ rij.

SmarterBalan $\mathrm { c e d W r i t i n g S c o r e } _ { \mathrm { i j } } = \gamma _ { 0 0 } + \gamma _ { 1 0 } { } ^ { \star } \mathrm { G r a d e } _ { \mathrm { i j } } + \gamma _ { 2 0 } { } ^ { \star } \mathrm { M a l e } _ { \mathrm { i j } } + \gamma _ { 3 0 } { } ^ { \star } \mathrm { H i s p a n i c } _ { \mathrm { i j } } + \gamma _ { 4 0 } { } ^ { \star } \mathrm { S p e c i a l E d } _ { \mathrm { i j } } + \gamma _ { 5 0 } { } ^ { \star } \mathrm { A s i a n } _ { \mathrm { i j } } .$ nij + Y60 \*Blackij + Y70 \* HumanScoreij $^ +$ 780 \*PromptDummy1ij + Y90 \*PromptDummy2ij $^ +$ Y100 \*RaterDummy1ij + Y110 \*RaterDummy2ij + ...+ Y170 \*RaterDummy8ij + uoj+ rij.

As a measure of effect size, standardized coefficients $\left( B \right)$ were created by multiplying the unstandardized coefficient by its standard deviation and then dividing by the standard deviation of the dependent variable (Lorah, 2018; Snijders & Bosker, 2012). According to Cohen's (1988) interpretation of standardized coefficients, we identify.05as a smalleffect,.10 as a medium effect, and.25 as a large effect These tandardized coefficients were used tocompare the degree to which the MI Write holistic score equally predicted ELLS performance on ACCEsS and Smarter Balanced.

# 3.3. Research question 2

We utilized three-level HLM to acount for the nesting of students within teachers within schoos with the data that included both ELLs and non-ELLs (Sample B). The Smarter Balanced writing scale score was the outcome variable in this model. Since there were sufficient units at both levels 2 (teacher) and 3 (school) and the ICCs or al thre levels indicated sufficient variance, we estimated a three-level model for this data set. The unconditional random-intercepts model was specified first. The final conditional models-one using the MI Write holistic score and the other using the human holistic scoreincluded student-level demographic covariates and the following focal predictors aded at level one: ELL status, scoring method (i.e, MI Write or human), and the interaction between ELL status and scoring method (e.g., ELL status $\bf \Psi ^ { * } M I$ Write holistic score). The interaction effect explained how the relationship between scoring method and the standardized state writing test may have diffred if students were identified as either ELLs or non-ELLs the interaction efect was added uncentered. Like the HLM analyses for the ELL-only sample, two dummy variables representing two prompt groups were added to the model considering MI Writ score as the main predictor. No predictors were added at level two or three because all predictors of interest were at the student level. The equation of the final mixed three-level model including the MI Write holistic score is presented below:

SmarterBalancedWritingScore $\mathrm { \ddot { \ z i } _ { \mathrm { j k } } = \gamma _ { 0 0 0 } + \gamma _ { 1 0 0 } * _ { G r a d e _ { i j k } } + \gamma _ { 2 0 0 } * M a l e _ { i j k } + \gamma _ { 3 0 0 } * _ { H i s p a n i c _ { i j k } } + \gamma _ { 4 0 0 } * _ { S p e c i a l E d _ { i j k } } + \gamma _ { 5 0 0 } * _ { A s i a n _ { i j k } } + \gamma _ { 7 0 0 } * _ { A s i a n _ { i j k } } + \gamma _ { 9 0 0 } * _ { A s i a n _ { i j k } } }$ 7600 \*Blackijk + Y700 \*ELLjk+ Y800 \*MIWriteScoreijk $^ +$ 7900 \*ELL\*MIWriteScoreijk $^ +$ 71000 \*PromptDummy1ijk $^ +$ 71100 \*PromptDum$\mathrm { m y 2 i j k + r _ { 0 j k } + u _ { 0 0 k } + e _ { i j k } }$

In this equation, s refer to the estmated cofficients for either the random intercets or the fied slopes. The notation rok refers to the random error at the teacher level, the notation $\mathbf { u } _ { \mathrm { 0 0 k } }$ refers to the random error at the school level, and the notation. $\mathrm { \ e _ { i j k } }$ refers to the random error at the student level.

Like the analyses for research question 1, a subsequent model replaced the MI Write holistic score with human scoring to predict the students' state test writing performance, with ll other covariates remaining the same and eight dummy variables representing different raters being added. Using the same notations as the prior equation, the equation for the new model is as following.

SmarterBalancedWritingScor $\mathrm { \Delta \ e _ { i j k } = \gamma _ { 0 0 0 } + \gamma _ { 1 0 0 } \ ^ { * } G r a d { e _ { i j k } } + \gamma _ { 2 0 0 } \ ^ { * } M a l { e _ { i j k } } + \gamma _ { 3 0 0 } \ ^ { * } H i s p a n i c _ { i j k } + \gamma _ { 4 0 0 } \ ^ { * } S p e c i a l { E d _ { i j k } } + \gamma _ { 5 0 0 } \ ^ { * } A s i a n _ { i j k } + \gamma _ { 7 0 0 } \ ^ { * } C a l l { e _ { i j k } } }$ Y600 $\mathrm { ^ { * } B l a c k _ { i j k } } + \gamma _ { 7 0 0 } \mathrm { ^ { * } E L L _ { i j k } } + \gamma _ { 8 0 0 } \mathrm { ~ ^ { * } ~ }$ $\mathrm { \Delta _ { \mathrm { p } } ^ { * } E L L _ { i j k } + \gamma _ { 8 0 0 } ^ { * } \ H u m a n S c o r e _ { i j k } + \gamma _ { 9 0 0 } ^ { * } E L L ^ { * } ~ F }$ IumanScoreijk $^ +$ Y1000 \*PromptDummy1jk $^ +$ 71100 \*PromptDum$\mathrm { \Delta \mathrm { ^ { 2 } i | k + \gamma \mathrm { 1 2 0 0 } \ ^ { * } R a t e r D u m m y { 1 } _ { i j k } + \gamma \mathrm { 1 3 0 0 \ ^ { * } R a t e r D u m m y { 2 } _ { i j k } + \ldots + \gamma \mathrm { 1 9 0 0 \ ^ { * } R a t e r D u m m y { 8 } _ { i j k } + \mathrm { \bf { r } } _ { 0 j k } + \mathrm { \bf { u } } _ { 0 0 k } + \mathrm { e _ { i | k } } } } } }$

If the interaction effc i either of these models proved to e statistically significant, the effect was probed using Bauer & Curran's (2005) and Curran et al. (2006) simple slope method to estimate the simple intercept and simple slope for the thre-level HLM using

the observational values of the predictors.

# 3.4. Statistical software

All ANOVA analyses and post hoc Tukey's HSD analyses were conducted by PROC GLM statement in SAS 9.4. Correlations, EFA and Levene's Test were conducted using SPS v.29. All HLM analyses were conducted using PROC MIXED statement in SAS 9.4 using restricted maximum likelihood estimation. Regular standard erors are reported in the resuts. For all analyses the percentage of variance explained in the outcome was calculated by comparing the variance components of the final conditional model to those of the unconditional model.

# 4. Results

# 4.1. Descriptive statistics

ELLs underperformed their non-ELL peers on their spring essays as scored by both MI Write (ELLs: $M = 1 6 . 7 2$ $S D = 4 . 7 4$ ; non-ELLs: $M = 1 8 . 0 9$ $S D = 4 . 8 7 \mathrm { \Omega }$ and human scoring (ELLs: $M = 1 4 . 7 0$ $S D = 4 . 4 1$ ; non-ELLs: $M = 1 6 . 6 4$ $S D = 4 . 7 4 _ { . }$ . They also underperformed on Smarter Balanced (ELLS: $M = 2 4 0 6 . 8 2$ $S D = 9 7 . 0 6 $ ; non-ELLs: $M = 2 4 9 1 . 0 1$ $S D = 1 1 0 . 1 5$ . ELLs' mean performance on the writing portion of ACCESS was 344.40 $( S D = 3 2 . 1 3 $ . The correlation between ELLs' performance on Smarter Balanced and ACCEsS was $r =$ .53. In addition, although for ELLs the correlation between MI Write and ACCESS $( r = . 3 3 )$ was virtually identical as the correlation between MI Write and Smarter Balanced $\left( r = . 3 5 \right)$ I, the correlation for non-ELLs between MI Write and Smarter Balanced was greater $( r$ $= . 4 7 )$ . Furthermore, for ELLs the correlation between human scoring and Smarter Balanced $( r = . 3 9 )$ was similar to the correlation between human scoring and ACCESS $\left( r = . 3 6 \right)$ ; however, the correlation for non-ELLs between human scoring and Smarter Balanced was greater $( r = . 5 2 )$

# 4.2. Results from exploratory analyses

# 4.2.1. Prompt effects

Tables 2 and 3 report the mean, standard deviation, one-way ANOVA tests, and the post-hoc mean difference comparisons in MI Write holistic cores and human scores for the ELL sample (Sample A) and the ELL and non-ELL sample (Sample B), respectively It is important to note that the labels "Prompt 1," "Prompt 2," and "Prompt $3 "$ in both tables are simply identifiers used to differentiate the three unique prompts assigned to each grade and are not meant to mply that prompts with the same label are the same across ifferent grades.

Table 2 demonstrates significant prompt efects on MI Write cores for third and fourth graders regarding Sample A, with variances in mean scores across the thee assgned prompts for each grade. No significant prompt efects were observed in fift grade or in the human score for any grad in Sample A Specifically thrd graders exhibited a significant mean score difference btween Prompt 2 and Prompt 3, and fourth graders between Prompt 1 and Prompt 2. These findings suggest the nee for further analysis of either Prompt 2 or Prompt 3 for third graders, and Prompt 1 or Prompt 2 for fourth graders in Sample A.

Table 3 presents data for both ELL and non-ELL students (Sample B), revealing significant prompt efects in MI Write and human scores for third and fourth graders, with differences in mean scores across the three prompts for each grade. No prompt efect were noted for fith grade. For third graders, significant differences were observed between Prompt 1 and Prompt 2, and between Prompt 2 and Prompt 3, both in MI Writ and human scores. Similarly, fourth graders showed significant differences between Prompt 1 and Prompt 2, and between Prompt 2 and Prompt 3. These results suggest that Prompt 2 merits further analysis for both third and fourth

Table 2 Means, Standard Deviations, and One-Way Analysis of Variance in Human Scores and MI Write Scores by Prompt for Sample A.   

<html><body><table><tr><td> Measure</td><td>Prompt 1 M (SD)</td><td>Prompt 2 M (SD)</td><td>Prompt 3 M (SD)</td><td>F (df)</td><td></td><td>Prompt 1 vs. 2</td><td>Prompt 1 vs. 3</td><td>Prompt 2 vs. 3</td></tr><tr><td colspan="9">MI Write Score</td></tr><tr><td rowspan="3">Grade 3</td><td>16.00</td><td>14.71</td><td>17.13</td><td>4.13*</td><td>0.04</td><td>1.30</td><td>-1.12</td><td>-2.42*</td></tr><tr><td>(5.07)</td><td>(4.04)</td><td>(5.15)</td><td>(2, 209)</td><td></td><td></td><td></td><td></td></tr><tr><td>17.10</td><td>19.60</td><td>18.41</td><td>4.48*</td><td>0.03</td><td>-2.49*</td><td>-1.31</td><td>1.19</td></tr><tr><td rowspan="3">Grade 4 Grade 5</td><td>(4.88)</td><td>(4.37)</td><td>(4.76)</td><td>(2, 265)</td><td></td><td></td><td></td><td></td></tr><tr><td>16.14</td><td>18.21</td><td>15.96</td><td>2.58</td><td>0.04</td><td>-2.07</td><td>0.18</td><td>2.25</td></tr><tr><td>(5.88)</td><td>(2.77)</td><td>(4.32)</td><td>(2, 125)</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9">Human Scores</td></tr><tr><td rowspan="2">Grade 3</td><td>13.34</td><td></td><td>14.72</td><td>1.76</td><td>0.02</td><td>0.29</td><td></td><td></td></tr><tr><td></td><td>13.04</td><td></td><td></td><td></td><td></td><td>-1.38</td><td>-1.67</td></tr><tr><td rowspan="3">Grade 4</td><td>(5.12)</td><td>(3.82)</td><td>(5.02)</td><td>(2, 209)</td><td>0.02</td><td></td><td></td><td></td></tr><tr><td>15.24</td><td>16.87</td><td>16.23</td><td>2.73</td><td></td><td>-1.63</td><td>-0.99</td><td>0.64</td></tr><tr><td>(4.05)</td><td>(4.36) 15.87</td><td>(4.52)</td><td>(2, 265) 1.82</td><td>0.03</td><td>-2.48</td><td></td><td></td></tr><tr><td rowspan="2">Grade 5</td><td>13.38</td><td></td><td>14.37</td><td>(2, 125)</td><td></td><td></td><td>-0.95</td><td>1.53</td></tr><tr><td>(4.65)</td><td>(3.42)</td><td>(4.20)</td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

Note. $^ { * } p < . 0 5$ $^ { \ast \ast } p < . 0 1$ $^ { \ast \ast \ast } p < . 0 0 1$

Table 3 Means, Standard Deviations, and One-Way Analysis of Variance in Human Scores and MI Write Scores by Prompt for Sample B   

<html><body><table><tr><td> Measure</td><td>Prompt 1 M (SD)</td><td>Prompt 2 M (SD)</td><td>Prompt 3 M (SD)</td><td>F (df)</td><td>r?</td><td>Prompt 1 vs. 2</td><td>Prompt 1 vs. 3</td><td>Prompt 2 vs. 3</td></tr><tr><td colspan="9">MI Write Score</td></tr><tr><td>Grade 3</td><td>16.79 (4.67)</td><td>15.26 (4.66)</td><td>17.09 (5.03)</td><td>(2, 871) 12.05***</td><td>0.03</td><td>1.53**</td><td>-0.29</td><td>-1.82***</td></tr><tr><td>Grade 4</td><td>17.40</td><td>19.82</td><td>17.54</td><td>28.47***</td><td>0.05</td><td>-2.42***</td><td>-0.15</td><td>2.27***</td></tr><tr><td>Grade 5</td><td>(5.09) 18.70</td><td>(4.77) 18.07</td><td>(4.75) 18.18</td><td>(2, 1023) 1.80</td><td>0.00</td><td>0.63</td><td>0.52</td><td>-0.10</td></tr><tr><td>Human Scores</td><td>(4.50)</td><td>(4.31)</td><td>(4.50)</td><td>(2, 926)</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9">Grade 3</td></tr><tr><td></td><td>14.80 (5.07)</td><td>13.60 (4.39)</td><td>15.11 (4.87)</td><td>(2, 871) 8.26***</td><td>0.02</td><td>1.20*</td><td>-0.31</td><td>-1.51***</td></tr><tr><td>Grade 4</td><td>15.82 (4.57)</td><td>17.90 (4.64)</td><td>16.43</td><td>20.14*** (2, 1023)</td><td>0.04</td><td>-2.08***</td><td>-0.61</td><td>1.47***</td></tr><tr><td>Grade 5</td><td>17.22</td><td>17.25</td><td>(4.61) 16.83</td><td>0.96</td><td>0.00</td><td>-0.03</td><td>0.39</td><td>0.42</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>(4.47)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>(4.36)</td><td>(3.77)</td><td></td><td>(2, 926)</td><td></td><td></td><td></td><td></td></tr></table></body></html>

Note. $^ { * } p < . 0 5$ $\stackrel { * * } { p } < . 0 1$ $^ { * * } p < . 0 0 1$

graders in Sample B.

Based on the identified prompt effcts, two dummy variables were established for the follow-up Hierarchical Linear Modeling (HLM) analyses. The first, Prompt Dummy 1, indicates whether Grade 3 students completed Prompt 2, identified as problematic. The second, Prompt Dummy 2, signifies if Grade 4 students completed Prompt 2, also deemed problematic. No dummy variables were created for Grade 5, as no prompt effects were observed in this grade.

# 4.2.2. Rater effects

Table 4 presents the mean score and score variance of the nine raters' evaluations for each of the six traits and the holistic score. Inspection of these values indicates variability among the raters' evaluations. Levene's test based on the mean-raters mean scores were normally dstributedndcatd ection f the ll yothsis of ual vriance across ratrs for each f theix trt as wel as the holistic score, confirming the presence of rater effects. All the Levene statistics $[ d f = 8$ 2820) were statistically significant at $p <$ .001, ranging from 6.05 for idea development to 16.03 for conventions. Hence, eight dummy variables were added to subsequent predictive models to account for rater effects among the nine raters.

# 4.3. RQ1: predicting language proficiency and state test writing scores among ELLs

The results of the two-level models predicting the ACcess writing scale score are shown in Table 5. The ICCs of the unconditional random-intercepts model showed that $8 5 . 8 8 \%$ of the variance in the ACCEsS writing score was at the student level and $1 4 . 1 2 \%$ of the variance was at the chool level. This indicates that schoollevel variance needs to e considered in the model, but most of the variance in ACCESS writing score was at the student level.

In the full conditional two-level model, theM Write holistic core was statisticall significanly asociated with the Acess writing scale score $( \beta = 1 . 4 3 , p < . 0 0 1 )$ after accounting for demographic covariates and prompt effects. Among the demographic covariates, there were statistically significant effects of grade $( \beta = 1 1 . 9 9 $ $p < 0 . 0 0 1 $ ) and special education status $( \beta = - 2 5 . 1 5$ $p < . 0 0 1 \ r$ . This model accounted for $2 8 . 6 9 \%$ of the variance in students' AccEss writing scale score.

Table 4 Mean and Variance of Raters' Evaluations for the Six Traits and Holistic Score.   

<html><body><table><tr><td></td><td>Rater 1</td><td>Rater 2</td><td>Rater 3</td><td>Rater 4</td><td>Rater 5</td><td>Rater 6</td><td>Rater 7</td><td>Rater 8</td><td>Rater 9</td></tr><tr><td>Mean</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Dev</td><td>2.49</td><td>2.65</td><td>2.71</td><td>2.68</td><td>2.88</td><td>2.63</td><td>2.25</td><td>2.50</td><td>3.07</td></tr><tr><td>Org</td><td>2.49</td><td>2.54</td><td>2.65</td><td>2.43</td><td>2.74</td><td>2.22</td><td>2.19</td><td>2.33</td><td>3.07</td></tr><tr><td>Style</td><td>2.49</td><td>2.59</td><td>2.68</td><td>2.56</td><td>2.84</td><td>2.47</td><td>2.06</td><td>2.67</td><td>3.00</td></tr><tr><td>Word Ch</td><td>2.49</td><td>2.85</td><td>2.79</td><td>2.64</td><td>3.09</td><td>2.69</td><td>2.25</td><td>2.50</td><td>3.09</td></tr><tr><td>Sent Str</td><td>2.49</td><td>2.72</td><td>2.66</td><td>2.50</td><td>2.75</td><td>2.59</td><td>2.19</td><td>2.17</td><td>2.99</td></tr><tr><td>Conv</td><td>2.49</td><td>2.81</td><td>2.92</td><td>2.55</td><td>2.72</td><td>2.70</td><td>2.06</td><td>2.33</td><td>2.89</td></tr><tr><td>Holistic</td><td>2.49</td><td>16.16</td><td>16.39</td><td>15.36</td><td>17.02</td><td>15.30</td><td>13.00</td><td>14.50</td><td>18.10</td></tr><tr><td>Variance</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Dev</td><td>0.50</td><td>0.79</td><td>0.81</td><td>0.98</td><td>0.82</td><td>0.85</td><td>0.87</td><td>0.27</td><td>0.80</td></tr><tr><td>Org</td><td>0.37</td><td>0.67</td><td>0.77</td><td>0.92</td><td>0.73</td><td>0.73</td><td>0.70</td><td>0.42</td><td>0.67</td></tr><tr><td>Style</td><td>0.46</td><td>0.65</td><td>0.77</td><td>0.87</td><td>0.71</td><td>0.78</td><td>0.73</td><td>0.42</td><td>0.61</td></tr><tr><td>Word Ch</td><td>0.33</td><td>0.56</td><td>0.83</td><td>0.77</td><td>0.83</td><td>0.80</td><td>0.87</td><td>1.00</td><td>0.56</td></tr><tr><td>Sent Str</td><td>0.46</td><td>0.77</td><td>0.70</td><td>0.87</td><td>1.07</td><td>0.87</td><td>0.96</td><td>0.70</td><td>0.64</td></tr><tr><td>Conv</td><td>0.61</td><td>0.85</td><td>0.82</td><td>1.03</td><td>1.23</td><td>0.86</td><td>1.13</td><td>0.79</td><td>0.60</td></tr><tr><td>Holistic</td><td>10.98</td><td>20.46</td><td>23.51</td><td>27.45</td><td>23.67</td><td>25.73</td><td>28.93</td><td>16.45</td><td>17.27</td></tr></table></body></html>

Note. Dev $=$ idea development. Org $=$ organization. Word Ch. $=$ word choice. Sent Str $=$ sentence structure. Conv $=$ conventions. Trait scores range 1-5. Holistic score range $= 6 { - } 3 0$

The full conditional two-level model using human scoring yielded comparable results. The human holistic score was statistically significantly associated with the ACCESS writing scale score $( \beta = 1 . 7 9$ $p < . 0 0 1 $ , after accounting for demographic covariates and prompt and rater effects. There were also statistically significant effects of grade $( \beta = 1 2 . 9 5 , p < . 0 0 1 )$ and special education status $( \beta =$ 24.42, $p < . 0 0 1 $ ). This model accounted for $2 9 . 6 9 \%$ of the variance in students' ACCESS writing scale score.

The results of the two-level models predicting the Smarter Balanced writing scale score are presented in Table 6. The ICCs of the unconditional random-intercepts model showed that $8 9 . 0 9 \%$ of the variance in the state test writing score was at the student level and $1 0 . 9 1 \%$ of the variance was at the schoo level. These results indicate that schoo-level variance needs to be considered in the model, but most of the variance in Smarter Balanced writing scale score was at the student level.

In the fllconditional two-level model, the Mi Write holistic score was statistically significantly asociated with the Smarter Balanced writing scale score $( \beta = 4 . 5 7$ $p < . 0 0 1 \ r$ after accounting for demographic covariates and prompt effects. There was a statistically significant effect of grade $( \beta = 2 2 . 6 2$ $p < . 0 0 1 )$ , male $( \beta = - 1 5 . 8 3 , p < . 0 5 )$ , Asian $( \beta = 4 2 . 4 1$ $p < . 0 5 )$ , and special education status $( \beta = - 7 7 . 3 2$ $p < . 0 0 1 \ r$ . This model accounted for $2 7 . 2 0 \%$ of the variance in students' Smarter Balanced writing scale score.

The full conditional two-level model using human scoring yielded comparable results. The human holistic score was statistically significantly associated with the Smarter Balanced writing scale score $( \beta = 6 . 1 8 , p < . 0 0 1 )$ after accounting for demographic covariates and prompt and rater effects. There was also a statistically significant effect of male $( \beta = - 1 3 . 8 0 , p < . 0 5 )$ , and special education status $( \beta = - 7 3 . 2 2 , p < . 0 0 1 )$ . This model using human scoring accounted for $3 0 . 5 5 \%$ of the variance in students' Smarter Balanced writing scale score.

# 4.3.1. Comparing predictive validity coefficients

Table 5 and Table 6 also present the standardized coefficients (denoted as B) used to compare theffect sizes of the predictors in the models for the two outcome variables. The efect ie for the MI Write holistic score for predicting the Accs writing scale core was considered large $( B = 0 . 2 1 , p < . 0 0 1 )$ , as was its effect size for predicting the Smarter Balanced writing scale score ( $\mathbf { \mathit { B } } = 0 . 2 3$ $p < . 0 0 1 \ r$ In addition, the effect size for the human holistic scoring for predicting the Access writing scale score was considered large $( B = 0 . 2 5$ $p < . 0 0 1 )$ , as was its effect size for predicting the Smarter Balanced writing scale score $\langle B = 0 . 2 9$ $p < . 0 0 1 \mathrm { \AA }$

Table 5 Results of HLM Analysis Predicting ACCESS Scores for ELLs.   

<html><body><table><tr><td></td><td colspan="9">ACCESS Writing Scale Score</td></tr><tr><td></td><td colspan="2"> Model 1: Unconditional Model</td><td colspan="3">Model 2: Conditional Model-MI Write</td><td colspan="3">Model 3: Conditional Model-Human Scoring</td></tr><tr><td>Fixed Effects</td><td>Coefficient  (S.E.)</td><td>t</td><td>Coefficient  (S.E.)</td><td>t</td><td>Effect Size B</td><td>Coefficient  (S.E.)</td><td>t</td><td>Effect Size B</td></tr><tr><td>Intercept</td><td>344.81*** (367) df = 13</td><td>93.96</td><td>325.24*** (6.04) df = 13</td><td>53.83</td><td></td><td>362.42*** (7.86) df = 13</td><td>41.51</td><td></td></tr><tr><td colspan="9">Level-1 Predictors</td></tr><tr><td>Grade</td><td></td><td></td><td>11.99*** (1.99)</td><td>6.01</td><td>0.28***</td><td>12.95** (2.88)</td><td>4.50</td><td>0.30***</td></tr><tr><td>Gender</td><td></td><td></td><td>-3.04(2.18)</td><td>-1.39</td><td>-0.05</td><td>-3.41 (2.18)</td><td>-1.56</td><td>-0.09</td></tr><tr><td>Hispanic</td><td></td><td></td><td>-0.52 (3.96)</td><td>-0.13</td><td>-0.01</td><td>-0.60 (3.95)</td><td>-0.15</td><td>0.00</td></tr><tr><td>Black</td><td></td><td></td><td>0.25 (7.33)</td><td>0.03</td><td>0.00</td><td>-0.22 (7.31)</td><td>-0.03</td><td>0.00</td></tr><tr><td>Asian</td><td></td><td></td><td>4.88 (6.57)</td><td>0.74</td><td>0.03</td><td>2.98 (6.59)</td><td>0.45</td><td>0.02</td></tr><tr><td> Special Education</td><td></td><td></td><td>-25.15*** (2.83)</td><td>-8.88</td><td>-0.31***</td><td>-24.42*** (2.83)</td><td>-8.64</td><td>-0.30***</td></tr><tr><td> MI Write Holistic Score</td><td></td><td></td><td>1.43*** (0.24)</td><td>5.95</td><td>0.21***</td><td>1.79** (0.26)</td><td></td><td></td></tr><tr><td colspan="9">Human Scoring</td></tr><tr><td>Level-1 Dummy Covariates</td><td></td><td></td><td>-7.61 (4.00)</td><td>-1.90</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9">Grade 3 Prompt Effect Dummy</td></tr><tr><td>Grade 4 Prompt Effect Dummy</td><td></td><td></td><td>3.01 (5.93)</td><td>0.51</td><td></td><td>-7.20 (4.17) 2.29 (5.91)</td><td>-1.73 0.39</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>-3.16 (5.55)</td><td>-0.57</td><td></td></tr><tr><td>Rater 2 Rater 3</td><td></td><td></td><td></td><td></td><td></td><td>-0.83 (6.31)</td><td>-0.13</td><td></td></tr><tr><td>Rater 4</td><td></td><td></td><td></td><td></td><td></td><td>-3.45 (6.70)</td><td>-0.51</td><td></td></tr><tr><td>Rater 5</td><td></td><td></td><td></td><td></td><td></td><td>-5.10 (5.59)</td><td>-0.91</td><td></td></tr><tr><td>Rater 6</td><td></td><td></td><td></td><td></td><td></td><td>2.15 (5.51)</td><td>0.39</td><td></td></tr><tr><td>Rater 7</td><td></td><td></td><td></td><td></td><td></td><td>6.71 (12.48)</td><td>0.54</td><td></td></tr><tr><td>Rater 8</td><td></td><td></td><td></td><td></td><td></td><td>4.00 (19.51)</td><td>0.20</td><td></td></tr><tr><td>Rater 9</td><td></td><td></td><td></td><td></td><td></td><td>-6.31 (5.34)</td><td>-1.18</td><td></td></tr><tr><td>Variance Components</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9"></td></tr><tr><td>r: Level-1</td><td>931.66</td><td></td><td>664.35 118.55*</td><td></td><td></td><td>655.01</td><td></td><td></td></tr><tr><td>o: Level-2</td><td>153.20*</td><td></td><td>84.86%</td><td></td><td></td><td>108.46*</td><td></td><td></td></tr><tr><td>ICC (Level 1)</td><td>85.88%</td><td></td><td>15.14%</td><td></td><td></td><td>85.79%</td><td></td><td></td></tr><tr><td>ICC (Level 2)</td><td>14.12%</td><td></td><td></td><td></td><td></td><td>14.21%</td><td></td><td></td></tr><tr><td>Variance Explained Level 1</td><td></td><td></td><td>28.69%</td><td></td><td></td><td>29.69%</td><td></td><td></td></tr><tr><td>Level 2</td><td></td><td></td><td>22.62%</td><td></td><td></td><td>29.20%</td><td></td><td></td></tr><tr><td>Goodness of Fit Statistics</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9"></td></tr><tr><td>Deviance</td><td>5903.2</td><td>df = 2</td><td>5655.3</td><td>df = 2</td><td></td><td>5596.3</td><td>df = 2</td><td></td></tr><tr><td>AIC</td><td>5907.2</td><td></td><td>5659.3</td><td></td><td></td><td>5600.3</td><td></td><td></td></tr><tr><td>BIC</td><td>5908.5</td><td></td><td>5660.6</td><td></td><td></td><td>5601.6</td><td></td><td></td></tr></table></body></html>

Note. $\mathfrak { s } _ { \mathtt { p } } < . 0 5$ $\ast \ast _ { \mathfrak { p } } < . 0 1$ $\ast \ast \ast _ { \mathfrak { p } } < . 0 0 1$

Table 6 Results of HLM Analysis Predicting Smarter Balanced Writing Scores for ELLs.   

<html><body><table><tr><td></td><td colspan="9">Smarter Balanced Writing Scale Score</td></tr><tr><td></td><td colspan="2">Model 1: Unconditional Model</td><td colspan="3">Model 2: Conditional Model-MI Write</td><td colspan="3">Model 3: Conditional Model-Human Scoring</td></tr><tr><td>Fixed Effects</td><td>Coefficient  (S.E.)</td><td>t</td><td>Coefficient  (S.E.)</td><td>t</td><td>Effect Size B</td><td>Coefficient  (S.E.)</td><td>t</td><td>Effect Size B</td></tr><tr><td>Intercept</td><td>2415.00*** (9.83) df = 13</td><td>245.72</td><td>2372.78*** (16.72) df = 13</td><td>141.95</td><td></td><td>2408.06*** (22.43) df = 13</td><td>107.34</td><td></td></tr><tr><td colspan="9">Level-1 Predictors</td></tr><tr><td>Grade</td><td></td><td></td><td>22.62*** (5.90)</td><td>3.83</td><td>0.17***</td><td>12.81 (8.44)</td><td>1.52</td><td>0.10</td></tr><tr><td>Gender</td><td></td><td></td><td>-15.83* (6.65)</td><td>-2.38</td><td>-0.08*</td><td>-13.80* (6.54)</td><td>-2.11</td><td>-0.07*</td></tr><tr><td> Hispanic</td><td></td><td></td><td>-8.91 (12.00)</td><td>-0.74</td><td>-0.03</td><td>-8.94 (11.77)</td><td>-0.76</td><td>-0.03</td></tr><tr><td>Black</td><td></td><td></td><td>31.18 (22.22)</td><td>1.40</td><td>0.05</td><td>35.37 (21.82)</td><td>1.62</td><td>0.06</td></tr><tr><td>Asian</td><td></td><td></td><td>42.41* (19.25)</td><td>2.20</td><td>0.10*</td><td>31.85 (19.10)</td><td>1.67</td><td>0.08</td></tr><tr><td> Special Education</td><td></td><td></td><td>-77.32** (8.61)</td><td>-8.98</td><td>-0.31***</td><td>-73.2** (8.46)</td><td>-8.66</td><td>-0.30***</td></tr><tr><td> MI Write Holistic Score</td><td></td><td></td><td>4.57** (0.73)</td><td>6.26</td><td>0.23***</td><td>6.18** (0.79)</td><td>7.86</td><td>0.29***</td></tr><tr><td colspan="9">Human Scoring</td></tr><tr><td>Level-1 Dummy Covariates Grade 3 Prompt Effect Dummy</td><td></td><td></td><td>-33.59** (11.75)</td><td>-2.86</td><td></td><td>-38.87** (12.05)</td><td>-3.22</td><td></td></tr><tr><td></td><td></td><td></td><td>-0.87 (16.72)</td><td>-0.05</td><td></td><td>-9.85 (16.65)</td><td>-0.59</td><td></td></tr><tr><td>Grade 4 Prompt Effect Dummy</td><td></td><td></td><td></td><td></td><td></td><td>-47.76** (16.51)</td><td></td><td></td></tr><tr><td>Rater 2</td><td></td><td></td><td></td><td></td><td></td><td>-36.23 (18.77)</td><td>-2.89</td><td></td></tr><tr><td>Rater 3</td><td></td><td></td><td></td><td></td><td></td><td>-39.91* (19.95)</td><td>-1.93</td><td></td></tr><tr><td>Rater 4</td><td></td><td></td><td></td><td></td><td></td><td></td><td>-2.00</td><td></td></tr><tr><td>Rater 5</td><td></td><td></td><td></td><td></td><td></td><td>-45.59** (16.74)</td><td>-2.72</td><td></td></tr><tr><td>Rater 6 Rater 7</td><td></td><td></td><td></td><td></td><td></td><td>-19.35 (16.36)</td><td>-1.18</td><td></td></tr><tr><td>Rater 8</td><td></td><td></td><td></td><td></td><td></td><td>-55.52 (37.27)</td><td>-1.49</td><td></td></tr><tr><td>Rater 9</td><td></td><td></td><td></td><td></td><td></td><td>-33.72 (58.22)</td><td>-0.58</td><td></td></tr><tr><td>Variance Components</td><td></td><td></td><td></td><td></td><td></td><td>-13.26 (15.93)</td><td>-0.83</td><td></td></tr><tr><td colspan="9"></td></tr><tr><td>r: Level-1</td><td>8485.06</td><td></td><td>6177.17</td><td></td><td></td><td>5892.56</td><td></td><td></td></tr><tr><td>o: Level-2</td><td>1038.92*</td><td></td><td>317.55</td><td></td><td></td><td>337.63</td><td></td><td></td></tr><tr><td>ICC (Level 1)</td><td>89.09%</td><td></td><td>95.11%</td><td></td><td></td><td>94.58%</td><td></td><td></td></tr><tr><td>ICC (Level 2)</td><td>10.91%</td><td></td><td>4.89%</td><td></td><td></td><td>5.42%</td><td></td><td></td></tr><tr><td>Variance Explained Level 1</td><td></td><td></td><td>27.20%</td><td></td><td></td><td>30.55%</td><td></td><td></td></tr><tr><td colspan="9"> Level 2</td></tr><tr><td>Goodness of Fit Statistics</td><td></td><td></td><td>69.43%</td><td></td><td></td><td>67.50%</td><td></td><td></td></tr><tr><td>Deviance</td><td>7241.1</td><td>df =2</td><td>6977.6</td><td></td><td></td><td>6882.9</td><td>df = 2</td><td></td></tr><tr><td colspan="9"></td></tr><tr><td></td><td></td><td></td><td>df = 2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AIC</td><td>7245.1</td><td></td><td>6981.6</td><td></td><td></td><td>6886.9</td><td></td><td></td></tr><tr><td>BIC</td><td>7246.4</td><td></td><td>6982.9</td><td></td><td></td><td>6888.2</td><td></td><td></td></tr></table></body></html>

Note. $\mathfrak { s } _ { \mathtt { p } } < . 0 5$ $\ast \ast _ { \mathfrak { p } } < . 0 1$ $\ast \ast \ast _ { \mathfrak { p } } < . 0 0 1$

Thus, this indicates that (a) the M Write holistic core had virtually identical predictive validit for the two outcome measures, (b) the human holistic core also had virtuall identical predctive validity for the two outcome measures, andc) the predictive validty of the MI Write score was a bit lower but comparable to that of the human holistic score for predicting ELLs' performance on two standardized measures of writing ability, indicating absence of evidence of predictive bias.

# 4.4. RQ2: Comparing the predictive validity of automated scores for ELLs and non-ELLs

The results of the threelevel models are shown in Table 7. The ICCs of the unconditional random-intercepts model showed that $6 8 . 8 9 \%$ of the variance in the Smarter Balanced writing scale score was at the student level, $1 4 . 7 4 \%$ was at the teacher level, and $1 6 . 3 7 \%$ was at the school level. This indicates that both teacher-level and school-level variances need to be considered in the model but most of the variance in Smarter Balanced writing scale score was at the student level.

The full conditional three-level model using MI Write holistic score revealed statistically significant efects of grade $( \beta = 3 9 . 0 5 $ $B =$ 0.2 $2 8 , p < . 0 0 1 )$ , male $( \beta = - 1 4 . 4 5$ $B = - 0 . 0 6$ $p < . 0 0 1 )$ , Black $( \beta = - 3 1 . 0 1$ $B = - 0 . 1 1$ $p < . 0 0 1 )$ , Asian $( \beta = 3 9 . 5 4 $ $B = 0 . 0 8$ $p <$ .001), special education status $( \beta = - 7 2 . 6 5 , B = - 0 . 2 2 , p < . 0 0 1 )$ and ELL status $= - 3 3 . 0 4 , B = 0 . 1 2 , p < . 0 0 1 )$ . After accounting for demographic covariates and prompt effects there was a statistically significant large efect of the MI Write holistic score $( \beta = 7 . 0 1$ $B =$ 0.30, $p < . 0 0 1 )$ . The interaction effect between ELL status and MI Write holistic score was a statistically significant smalleffect $( \beta =$ 1.79, $B = - 0 . 0 8$ $p < 0 . 0 5 $ . This model explained $2 9 . 2 9 \%$ of the variance in students' Smarter Balanced writing scale score.

The full conditional three-level model using human scoring revealed statistically significant effects of grade $( \beta = 2 9 . 7 5$ $B = 0 . 2 1 , p$ $< . 0 0 1 \AA$ , male $( \beta = - 1 4 . 8 9$ $B = - 0 . 0 7$ $p < . 0 0 1 \ r$ , Black $( \beta = - 2 8 . 6 5$ 3 $B = - 0 . 1 0$ $p < . 0 0 1 \ r$ , Asian $\beta = 3 9 . 1 3$ 3 $B = 0 . 0 8$ $p < . 0 0 1 \ r$ special education status $( \beta = - 6 8 . 6 5 $ $B = - 0 . 2 1$ $p < . 0 0 1 \ r$ , and ELL status $( \beta = - 3 0 . 4 9$ $B = - 0 . 1 1$ $p < . 0 0 1 \ r .$ . After accounting for demographic covariates and prompt and rater effects, there was astatistically significant large effect of the human holisti score $( \beta =$ 8.46, $B = 0 . 3 5$ $p < . 0 0 1 $ . The interaction effect between ELL status and human holistic scoring was a statistically significant small effect $( \beta = - 1 . 9 6$ $B = - 0 . 0 7$ $p < 0 . 0 5 $ . This model explained $3 2 . 4 1 \%$ of the variance in students' Smarter Balanced writing scale score.

Table 7 Results of HLM Analysis Predicting 2018 Smarter Balanced Writing Scale Scores: ELLs and Non-ELL.   

<html><body><table><tr><td></td><td colspan="2">Model 1: Unconditional Model</td><td colspan="3">Model 2: Conditional Model-MI Write</td><td colspan="3">Model 3: Conditional Model-Human Scoring</td></tr><tr><td>Fixed Effects</td><td colspan="2">Coefficient  (S.E.)</td><td colspan="3">Coefficient  (S.E.)</td><td colspan="3">Coefficient  (S.E.)</td></tr><tr><td>Intercept</td><td>2461.14*** (12.84)</td><td>t 191.65</td><td>2381.91*** (9.15)</td><td>t 260.32</td><td>Effect Size B</td><td>2411.93*** (11.06)</td><td>t 217.99</td><td>Effect Size B</td></tr><tr><td colspan="7">df = 13</td><td></td><td></td></tr><tr><td>Level-1 Predictors</td><td></td><td></td><td>39.05*** (3.22)</td><td>12.11</td><td>0.28***</td><td>29.75*** (4.13)</td><td>7.20</td><td>0.21***</td></tr><tr><td>Grade Male</td><td></td><td></td><td>-14.45*** (3.07)</td><td>-4.71</td><td>-0.06***</td><td>-14.89*** (2.99)</td><td>-4.98</td><td>-0.07***</td></tr><tr><td></td><td></td><td></td><td>-5.36 (4.53)</td><td>-1.18</td><td>-0.02</td><td>-2.78 (4.43)</td><td>-0.63</td><td></td></tr><tr><td>Hispanic Black</td><td></td><td></td><td>-31.01*** (4.62)</td><td>-6.71</td><td>-0.11***</td><td>-28.65*** (4.52)</td><td>-6.33</td><td>-0.01 -0.10***</td></tr><tr><td>Asian</td><td></td><td></td><td>39.54*** (6.91)</td><td>5.72</td><td>0.08***</td><td>39.13*** (6.76)</td><td>5.79</td><td>0.08***.</td></tr><tr><td>Special Education</td><td></td><td></td><td>-72.65*** (4.62)</td><td>-15.72</td><td>-0.22***</td><td>-68.65*** (4.53)</td><td>-15.14</td><td></td></tr><tr><td>English Language Learner</td><td></td><td></td><td>-33.04** (10.07)</td><td>-3.28</td><td>-0.12***</td><td>-30.49*** (8.84)</td><td>-3.45</td><td>-0.21*** -0.11***</td></tr><tr><td> MI Write Holistic Score</td><td></td><td></td><td>7.01*** (0.41)</td><td>17.30</td><td>0.30***</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>-1.79* (0.78)</td><td>-2.31</td><td>-0.08*</td><td></td><td></td><td></td></tr><tr><td>ELL* MI Write Human Scoring</td><td></td><td></td><td></td><td></td><td></td><td>8.46** (0.41)</td><td>20.87</td><td>0.35***</td></tr><tr><td>ELL * Human Scoring</td><td></td><td></td><td></td><td></td><td></td><td>-1.96* (0.80)</td><td>-2.45</td><td>-0.07*</td></tr><tr><td>Level-1 Dummy Covariates</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="7"></td><td></td><td></td></tr><tr><td>Grade 3 Prompt Effect Dummy</td><td></td><td></td><td>2.55 (9.27) -10.37 (7.37)</td><td>0.28 -1.41</td><td></td><td>-1.93 (9.00) -11.80 (7.14)</td><td>-0.21 -1.65</td><td></td></tr><tr><td>Grade 4 Prompt Effect Dummy</td><td></td><td></td><td></td><td></td><td></td><td>-33.34*** (7.06)</td><td>-4.72</td><td></td></tr><tr><td>Rater 2</td><td></td><td></td><td></td><td></td><td></td><td>-35.63*** (8.32)</td><td></td><td></td></tr><tr><td>Rater 3</td><td></td><td></td><td></td><td></td><td></td><td>-34.07*** (9.15)</td><td>-4.28</td><td></td></tr><tr><td>Rater 4</td><td></td><td></td><td></td><td></td><td></td><td>-27.84*** (6.56)</td><td>-3.72</td><td></td></tr><tr><td>Rater 5</td><td></td><td></td><td></td><td></td><td></td><td>-23.32** (7.29)</td><td>-4.24</td><td></td></tr><tr><td>Rater 6</td><td></td><td></td><td></td><td></td><td></td><td></td><td>-3.20</td><td></td></tr><tr><td>Rater 7 Rater 8</td><td></td><td></td><td></td><td></td><td></td><td>-40.10 (21.45)</td><td>-1.87</td><td></td></tr><tr><td>Rater 9</td><td></td><td></td><td></td><td></td><td></td><td>-15.48 (24.27)</td><td>-0.64</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>-26.06*** (6.07)</td><td>-4.29</td><td></td></tr><tr><td colspan="7">Variance Components e: Level-1</td><td></td><td></td></tr><tr><td></td><td>8820.90 1887.99***</td><td></td><td>6237.01 419.87***</td><td></td><td></td><td>5961.92</td><td></td><td></td></tr><tr><td>ro: Level-2</td><td></td><td></td><td>471.32*</td><td></td><td></td><td>375.67***</td><td></td><td></td></tr><tr><td> oo: Level 3</td><td>2095.47**</td><td></td><td>87.50%</td><td></td><td></td><td>438.11*</td><td></td><td></td></tr><tr><td>ICC (Level 1)</td><td>68.89%</td><td></td><td>5.89%</td><td></td><td></td><td>87.99%</td><td></td><td></td></tr><tr><td>ICC (Level 2)</td><td>14.74% 16.37%</td><td></td><td>7.56%</td><td></td><td></td><td>5.54% 6.47%</td><td></td><td></td></tr><tr><td>ICC (Level 3)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="8">Variance Explained</td><td></td></tr><tr><td>Level 1</td><td></td><td></td><td>29.29% 77.76%</td><td></td><td></td><td>32.41% 80.10%</td><td></td><td></td></tr><tr><td>Level 2 Level 3</td><td></td><td></td><td>77.51%</td><td></td><td></td><td>79.09%</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="8">Goodness of Fit Statistics</td><td></td></tr><tr><td>Deviance</td><td>34005.5</td><td>df = 4</td><td>32838.4 32844.4</td><td>df = 13</td><td></td><td>32650.7</td><td></td><td>df = 13</td></tr><tr><td>AIC</td><td>34011.5</td><td></td><td>32846.3</td><td></td><td></td><td>32656.7 32658.6</td><td></td><td></td></tr><tr><td>BIC</td><td>34013.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

Note. $\mathfrak { s } _ { \mathtt { p } } < . 0 5$ $\ast \ast _ { \mathfrak { p } } < . 0 1$ $\ast \ast \ast _ { \mathfrak { p } } < . 0 0 1$

# 4.4.1. Probing the interaction effect

According to the imple slope method, the fully conditional three-level model that included the interaction efect was considered as a conditional regression of the state writing test on the MI Write holistic score/the human scoring (denoted as the focal predictor) as a function of the students' ELL status (denoted the moderator). Hence, the original equations were transformed as follows:

SmarterBalancedWritingScoreij $\mathrm { k } = \left( \gamma _ { 8 0 0 } + \gamma _ { 9 0 } \right.$ o \*ELL)\* MIWriteScoreijk $^ +$ (yo00 +Y700 \*ELLj)+ Y100 \*Gradeijk + y200 \*Malejk + Y300 $^ { * } \mathrm  H i s p a n i c _ { i j k } + \gamma _ { 4 0 0 } * 5 p e c i a l E d _ { i j k } + \gamma _ { 5 0 0 } * \& s i a n _ { i j k } + \gamma _ { 6 0 0 } * B l a c k _ { i j k } + \gamma _ { 1 0 0 0 } * P r o m p t D u m m y \boldsymbol { 1 } _ { i j k } + \gamma _ { 1 1 0 0 } * \mathrm { P r o m e }$ mptDummy2ijk + rojk + uook + eijk.

SmarterBalancedWritingSco $\mathrm { r e i \vert k = ( \gamma _ { 8 0 0 } + \gamma _ { 9 0 0 } * \mathrm { E L L } ) ^ { * } H u m a n S c o r e _ { i \vert k } + ( \gamma _ { 0 0 0 } + \gamma _ { 7 0 0 } * \mathrm { E L L _ { i \vert k } } ) + \gamma _ { 1 0 0 } * G r a d e _ { i \vert k } + \gamma _ { 2 0 0 } * M a l e _ { i \vert k } + \gamma _ { 3 0 0 } * \mathrm { V o l u m a n } }$ \*Hispanicijk $^ +$ Y400 \*Sp $\mathrm { \ e c i a l E d _ { i j k } + \gamma _ { 5 0 0 } \mathrm { \ e ^ { * } A s i a n _ { i j k } + \gamma _ { 6 0 0 } \mathrm { \ e ^ { * } B l a c k _ { i j k } + \gamma _ { 1 0 0 0 } \mathrm { \ e ^ { * } } } } }$ PromptDummy1 jk $^ +$ 71100 \*PromptDummy2ijk $^ +$ Y120C \*RaterDummy1jk + Y1300 \*RaterDummy2ijk + ... + Y1900 \*RaterDummy8ijk + rojk + uo0k + eijk.

The estimation of $( \gamma _ { 0 0 0 } + \gamma _ { 7 0 0 } \mathrm { ^ { * } E L L _ { i j k } ) }$ is called the simple interaction and the estimation of $( \gamma _ { 8 0 0 } + \gamma _ { 9 0 0 } \textrm { } ^ { * } \textrm { E }$ LL) is called the simple slope. Regarding the model that used MI holistic score as the scoring method in the model, for non-ELLs, the simple intercept was 2381.91 $\ s e = 9 . 1 5$ $t = 2 6 0 . 3 2 , p < . 0 0 1 )$ and the simple slope was 7.00 $\ s e = 0 . 4 0$ $t = 1 7 . 5 0 , p < . 0 0 1 )$ . For ELLs, the simple intercept was 2348.87 ( $\mathbf { \Delta } s e = 1 1 . 8 7$ $t = 1 9 7 . 8 5$ $p < . 0 0 1 $ and the simple slope was 5.21 $s e = 0 . 7 1$ $t = 7 . 3 7$ $p < . 0 0 1 $ . Fig. 1 illustrates these differences, revealing that ELL students' MI Write holistic score was les closely related to the sate writing test score than non-ELL students. That is, ELLs who scored the same as non-ELLs on MI Write would be predicted to perform worse on the writing portion of Smarter Balanced.

Regarding the model that used human holistic scoring, for non-ELLs, the simple intercept was 2411.93 $\mathit { s e } = 1 1 . 0 6$ $t = 2 1 7 . 9 9$ $p < . 0 0 1 \mathrm { \AA }$ and the simple slope was 8.46 $\langle s e = 0 . 4 0$ $t = 2 1 . 1 5$ $p < . 0 0 1 \ r$ . For ELLs, the simple intercept was 2381.44 $\mathit { s e } = 1 2 . 9 0 $ $t = 1 8 4 . 6 1$ $p < . 0 0 1 \ r _ { \cdot }$ and the simple slope was 6.50 ( $s e = 0 . 7 2$ $t = 9 . 0 1$ $p < . 0 0 1 $ . Fig. 2 illustrates these differences, revealing that ELL students human scoring was les closely related to the state writing test score than non-ELL students. That is, ELLs who scored the same as non-ELLs on human scoring would be predicted to perform worse on the writing portion of Smarter Balanced.

# 5. Discussion

The purpose of this study was to evaluate whether AWE automated scores demonstrated evidence of bias when evaluating the writing of ELLs. We examined bias in two ways. First, we considered how similar the predictive validity of automated and human scoring was for ELLs on an ELL-specific writing test and a standardized state writing test. Automated and human scores generated from the same underlying rubric(s) should demonstrate comparable predictive validity when examining relationships with external measures. Second, we extended our inestigation of bias to consider the potential for subroup bias in predictie validity, that is whether the predictive relationship between the standardized state writing test and automated and human scores of curricular-based writing prompts was similar for ELLs and non-ELLs. Key findings are discussed below.

First, we found that the AWE automated scores comparably replicated the same relationships between human scores and per formance on an ELL language proficiency writing test and the standardized state writing test scores. As is best practice (American Educational Research Association, 2014), and consistent with prior research (Matta et al., 2022; Matta et al., 2023; Reed & Mercer. 2022), we went beyond simple zero-order correlations and used regression analyses, specificall HLM, to investigate predictie al. idity after accounting for rlevant demographic factors as well as the nested structure of the data. Finding comparable predictive validity evidence for automated and human scoring supports the validity of automated scoring (Ramineni & Williamson, 2013).

Further, findings indicated that MI Writ's holistic scores provide an equally valid inference of ELLs' writing skills on an ELL. specific writing test and astate writing test designed for the general population. Despite relatively weak zero-order correlations between M Writ and those assessments, the size of the standardized effect () was large for both tes. An implication of this finding is that educators can use AWE automated score to make inferences about their ELL students progress towardsattaining ELL-specific and general grade level expectations for writing proficiency. Given this finding, a potentially fruitful area of future research would be testing the suitabilit of MWrite ascreener for identifying ELLs at risk of non-proficiency onthe writig portions of the ACcEss and Smarter Balanced asessments. This would extend existing but limited research on the use of writing screeners with ELLs (e.g., Kel. ler-Margulis et al., 2016).

![](img/3d59f982cb14f447721b900b59e442f77b426312d718ce194b1d449959d5783b.jpg)  
Plot of the Interaction Effect between ELL Status and Mi Write Score   
Fig. 1. The Plot for the Interaction Effect of MI Write Score and ELL Status.

![](img/2eee50956f1c32a29665a65608bc9d068b08d4546e9dac0db35270cb4573b797.jpg)  
Fig. 2. The Plot for the Interaction Effect of Human Scoring and ELL Status.

Third, we found a statistically significant interaction effect between AWE automated scores and ELL status when predicting state test writing performance. The interaction effct was also evident when examining human holistic scoring, indicating different predictive relationships by subgroup. Importantly, the size of the automated scoring\*ELL and human scoring\*ELL interaction ffect was comparable. Prior research has not directly compared diffrences in predictive relationships by subgroup acrosscoring methods. Future research should utilize similar comparisons when examining evidence of predictive bias among other automated scoring methods because such comparisons can reveal whether automated scoring is uniquely biased or simply replicating biases present in human scoring. Indeed, one of the principal concerns regarding automated scoring is this potential to reproduce human biases (Johnson et al., 2022; Wind et al., 2017).

The present study found evidence of prompt and rater effects in the exploratory analyses. Although the HLM models accounted for these effets, thr pree sugt theonstruct-rreleant soures f variane xert an nlencen predtive vality anlyes. particular, the presence of rater efects among expert raters underscores the fact that AWE models trained on human data containing biases will replicate those iases. Indeed, study findings suggest that M Write's automated scoring was not uniquely biased relative to human scoring; rather it likely reflects biases within the human scoring used as training data. Specificall, both scoring methods showed a weaker predictive relationship with the state standardized writing test for ELLs than for non-ELLs. Rater biases are a persistent issue in writing asessment and current findings underscore the point that such biases are likely baked' into automated scores. Thus, a critical area for future research and development in AWE will be to characterize and make transparent the biases present in data used to train algorithms used in educational settings, if not completely remove them.

Given that prompt and rater efects did not influence the difference in predictive validity of both the human and automated essay score for ELLs and non-ELLs, what might explain this inding? One possbilit s that the curriculum-specific writing tasks used by the AWE system may not have aligned well with the extended performance task used on the Smarter Balanced writing test. This misalignment may have left ELLs unprepared to handle the more challenging task on the state assessment. ELLs have unique instructional needs when it comes to writing, including syntax, vocabulary, cohesion, and genre-specific conventions (Olson et al., 2015; Pasquarell, 2019; Weigle, 2013; Zhang et al., 2021). As a result, non-ELLs, who often have stronger writing skill and may be more familiar with the genre conventions on the state assessment, may have been better prepared to perform well on both the curriculum-specific tass and the state assessment. In contras, ELLs who rely more heavily on instruction to develop their writing kils may have been at a disadvantage on the state assessment if they were not given sufficient curricular preparation. Further research should be conducted to extend the present investigation with tasks pecifically designed to imitate those used on the Smarter Balanced state test. For instance, Reed & Mercer (2022) found no evidence of predictive bias for ELLs and other demographic subgroups when comparing the predictive validity of interim assessments scored either by teacher or expert raters for predicting performance on a summative state writing test.

# 5.1. Limitations and future research

First, there were diferent motivational contexts for completing classroom writing assignments within an AWE and taking the ACCESS and Smarter Balanced ELA tests. AWE is intended to be used as a lower-stakes, classroom-based formative assessment. In contrast, both the ACcEss and Smarter Balanced ELA tests were used for accountability purposes. Differences in the motivational contexts surrounding these assessments must be recognized.

In addition, we only looked at the students' writing performance in one specific commercially developed and marketed AWE system (MI Write) that utilized the ix Trait scoring model. It is not possible to distinguish between the effect of AWE automated scoring in general and the effect of MI Write, or the Six Trait model, specificlly in the current study. More reearch is needed with other AWE systems before making broad claims about the suitability of automated scoring with elementary-age ELLS.

# 6. Conclusion

Our findings indicate evidence that AWE automated scores are not uniquely biased relative to human scores with respect to predictive validity and predictive bias by language subgroup. Instead, it appears that AWE automated scores replicate the same predictive ias evident in human scoring. Thi is a legitimate concern for WE and artificial intlligce more broadly (Jonson t al. 2022). Yet, because the source of this predictive bias remains unclear, the remedybe it improvements in rater training, greater scrutiny to th training data used to develop AWE systems, or educators more mindfully developing curriculum-specific writing tasks for ELLsremains unclear as well. Future research should continue to explore the potential for predictive bias in automated scoring methods and examine the factors that contribute to differential predictive validity by subgroup. Such investigations, like the present study, aid sakeholders in making more informed decisions about the appropriateapplication of AWE with ELLs specificall, and the role of AwE in supporting classroom instruction broadly.

# CRediT authorship contribution statement

Wilson Joshua: Conceptualization, Formal analysis, Funding acquisition, Investigation, Project administration, Resources, Supervision, Writing - original draft, Writing - review & editing. Huang Yue: Conceptualization, Formal analysis, Investigation, Methodology, Writing - original draft, Writing - review & editing.

# Declaration of Competing Interest

None.

# Data Availability

Data will be made available on request.

# References

Amri i i01)mili n  i i and psychological testing. Washington, DC: AERA.   
Bai L,  H, . (2017). I the ae fllile W fdack Hw  studnts rond?catin Pchoy, 371, 67-81. hp/i.org10.1080/ 01443410.2016.1223275 373-400. https://doi.org/10.1207/s15327906mbr4003_5   
Chapele , o,   . 015.ty m o sti  sn m witingat.  et (3, 385 405. https://doi.org/10.1177/0265532214565386   
hn, ert,  , . 202. g  n t y  w lt  iviat lizait hory application. American Educational Research Journal, 59(6), 1122-1156. https:/doi.org/10.3102/00028312221106773   
henoweth, . A, Hye, J.R. 001). ly in witin ing tex i 1 and . win ion 181), 80-98. ps:/i.g/10.117 0741088301018001004   
Cohen, J. (1988). Statistical power analysis for the behavioral sciences. New York, NY: Routledge Academic.   
ran J Bae . , . 0  t h   .  .   .) The ote  rien ti       .-129 ci.   
ne, .     h    r   
Diki    0  k r      r  2, 1-17 https://doi.org/10.1016/j.asw.2014.03.006   
Gerad   6      cq.  f  cin 27 111-129. https://doi.org/10.1007/s10972-016-9455-6   
Graham, S. (2018). A rvised writer(s)-within-community model of writing. Educational Pschologist 53(4), 258-279. htps://doi.org/10.1080/ 00461520.2018.1481406   
re  i 8(6), 4-43. https://ejournals.bc.edu/index.php/jtla/article/view/1625   
Hash,   1).   k r  i  st Computer Assisted Learning, 37, 1494-1507. https://doi.org/10.1111/jcal.12587   
Hyes, . 9  nt ing   ,hn.12 NJ: Erlbaum.   
Hang, .016  o h     , 13(4, 359-376. https://doi.org/10.1080/15434303.2016.1230121   
In  4./. org/10.1177/0265532215587390   
Joson,   fre,  222. eric m ta mt d it    c  o tin Measurement, 59(3), 338-361. https://doi.org/10.1111/jedm.12335   
Keler-ais  Pa, Jaer, Bro . 016.alty astc ary  w o r-a m or students with diverse language backgrounds. Reading & Writing Quarterly, 32, 174-198. htps://doi.org/10.1080/10573569.2014.964352   
Kelle-ais, , er . a (2021).t f  t ein l  wexsoi- ent comparison study. Reading & Writing, 34, 2461-2480. https://doi.org/10.1007/s11145-021-10153-64 org/10.1186/s40536-018-0061-2   
Mat,  ,  ei,  2). vt    -   w xsi-ae measurement scores. Asessment in ducatio: Principe, Policy & Practic, 29(2), 200-218. htps://doi.org/10.1080/0969594x.2022.2043240   
Mattr is,      o  i, 38(3), 173-181. https://doi.org/10.1037/spq0000517   
err, ..ww qiwer./b.com/ shmercer/writeAlizer/)-   
o  l  015. s  g h    , 1(4 7052. /o.g 10.1086/681235   
asqur 1     e .,i n  5-05) New York, NY: Guilford Press.   
i013   r  pti 1853..101 asw.2012.10.004   
Ranal, J, Lin,, -lne, (017tig tin r ftie at of  aa witin tin the accuracy and usefulnes of feedback as part of argument-based validation. Educational Psychology, 37(1), 8-25. htps://doi.org/10.1080/ 01443410.2015.1136407   
Raudenbush, s. W., & Bryk, A. . (202) Hierarchical linear models: Aplicaions and data analysis metods (2nd ed.). Thousand Oaks, CA: Sage.   
Red, D.K, & Mer, .H. (2022. til scoig and prdctie bis i iner and stive writing amets Aance nin ulication chool Psychology. https://doi.org/10.1037/spq0000527.   
Smarter B  .01    2017-18i c e id fr /. smarterbalanced.org/library/en/2017-2018-interim-assessments-technical-report.pdf.   
Sniders, .  ker,  J. 012 eel is n tin to ic d d eel mdin.  k. a uli,   
Stson14f er kt qt  w 9 1-..0.106. asw.2013.11.007   
aer  1)  d Teaching. https://doi.org/10.1080/17501229.2021.1914062   
Warher .06t   4.g 10.1191/1362168806lr190oa   
Wegle, .. 013h    cor Ci idtiosn 18, 5. /.g/.16/. asw.2012.10.006   
Wison, . 08  h   t tn nr    f c, 8 193. https://doi.org/10.1016/j-jsp.2017.12.005   
Wion,     e ,    1). cr     g. Transfoming th chi ad g f witin usingoma wig tion. oer  ction, 168, l 104208. h/o.g/0.1016/. compedu.2021.104208   
Wion ,   e 1it f   win t n .  o, 111, 619-640. (https://doi.apa.org/doi/10.1037/edu0000311). quality. Computers and Education, 100, 94-109. https:/doi.org/10.1016/j.compedu.2016.05.004   
Wion,       21). k      s n ascii th win  rci tiofit. il  f i nio. /i. 10.1007/s40593-020-00236-w   
Wion, .   .itiny d ficie in  s   c  , 2, 123-140. https:/doi.org/10.1016/j.jsp.2020.08.008   
Wison .  2 t ie  a g as o  a . y School Journal, 123, 99-127. (https://www.journals.uchicago.edu/doi/10.1086/720562). 0146621618789391   
Wd .   01 ra   qt  mae scoring for writing asessments. International Journal of Testing, 18, 27-49. https://doi.org/10.1080/15305058.2017.1361426   
World-Cl t  . 018 c  for  f .insh  i t  40, 2016-2017 administration. Retrieved from (https://www.cde.state.co.us/assessment/accessforellsonlinetechreport).   
ag    r  (  s     i   Exg predictors of contextualized speling, witing fluency, and writing qualit. Ring and Writing https:/oi.org/10.1007/11145-021-10223-9