# The comparison of general tips for mathematical problem solving generated by generative AI with those generated by human teachers

Jiyou Jia, Tianrui Wang, Yuyue Zhang & Guangdi Wang

To cite this article: Jiyou Jia, Tianrui Wang, Yuyue Zhang & Guangdi Wang (2024) The comparison of general tips for mathematical problem solving generated by generative AI with those generated by human teachers, Asia Pacific Journal of Education, 44:1, 8-28, DOI: 10.1080/02188791.2023.2286920

To link to this article: https://doi.org/10.1080/02188791.2023.2286920

# The comparison of general tips for mathematical problem solving generated by generative AI with those generated by human teachers

Jiyou Jia a , Tianrui Wang $\oplus ^ { \mathsf { b } }$ , Yuyue Zhang $\oplus ^ { \mathsf { a } }$ and Guangdi Wang c

a Department of Educational Technology, Graduate School of Education, Peking University, Beijing, China; b School of Humanities, University of Chinese Academy of Sciences, Beijing, China; c Chinese Institute, Beijing International Studies University, Beijing, China

# ABSTRACT

In designing an intelligent tutoring system, a core area of the application of AI in education, tips from the system or virtual tutors are crucial in helping students solve difficult questions in disciplines like mathematics. Traditionally, the manual design of general tips by teachers is timeconsuming and error-prone. Generative AI, like ChatGPT, presents a new channel for designing general tips. This study utilized prompt engineering and Chain of Thought to summarize general tips for given mathematical problems (one geometry problem and one algebra problem) and their solutions. A Turing test was conducted to compare ChatGPT-generated general tips with human-designed ones. Results from 121 human evaluators, each assessing 6 ChatGPT-generated and 6 human-designed general tips for each of two mathematical problems, showed that the average score for ChatGPT-generated tips is less than that of human-designed tips at a statistically significant level $( p < 0 . 0 5 )$ , and Zero-Shot CoT achieved the best score. However, no evaluator could distinguish the tip types exactly. The average precision, recall and F-value of all ChatGPT-generated tips are less than $40 \%$ . AI-generated general tips can serve as a valuable reference for teachers to enhance efficiency and students’ mathematical learning.

# ARTICLE HISTORY

Received 3 August 2023   
Accepted 14 November 2023

# KEYWORDS

General tips; intelligent   
tutoring system;   
mathematical education; large language models;   
prompt engineering; turing test

# Introduction

In mathematics education, due to the higher student-teacher ratio, the ordinary and collective teaching and learning in the school and in the classroom cannot provide every student with individualized tips and feedback, especially students with learning difficulties and those in need. Intelligent tutoring system (in abbreviation ITS in the following text), a core application and research area of artificial intelligence in education, can help each student with learning a specific subject or difficult content by providing personalized diagnosis and feedback (Jia, 2015). The ITS can have almost the same effect on students’ learning performance as the one-to-one human tutor according to the meta-analysis by VanLehn (2011), and can be an effective approach to facilitate personalized learning according to the meta-analysis by Kulik and Fletcher (2016).

A general tip to solve a mathematical problem refers to the general but critical concept, axiom, theorem, idea or hint regarding this problem. As opposed to the direct answers or detailed explanations to this problem, a general tip can facilitate the students’ problem-solving, stimulate their independent thinking and exploration, motivate their mastery of knowledge, highly improve their comprehension of mathematical concepts, their flexibility in applying mathematical methods and their reflection on the rationality of the steps taken.

However, the current design of general tips relies on manual and great efforts from teachers and tutors, as they need to meticulously analyse the key steps and error-prone points of each problem and then design the content of the prompt, thus rendering manual writing exorbitantly costly and greatly limiting the large-scale production and application of general tips (VanLehn, 2011). Therefore, traditional methods of generating general tips have their constraints, emphasizing an urgent necessity to identify a more intelligent and efficient approach to design general tips that could improve the quality of mathematics teaching and learning, while the rapid development of large language models opens up the possibility for the process of generation of general tips to be more expedient and semi-automated.

In recent years, deep learning has advanced by leaps and bounds, bringing revolutionary changes to the field of artificial intelligence. Among the numerous AI technologies, large language models (LLMs), distinguished by their capability for language generation and extensive knowledge coverage, have garnered much publicity. ChatGPT is a representative of large language models, which are pretrained with a large corpus of text and have shown to have human-like linguistic abilities and domain-specific knowledge in multiple fields (Bubeck et al., 2023). Compared to previous conversational robots, ChatGPT is capable of generating smooth and logical content, dynamically adapting and optimizing it to different contexts and instructions, and responding to user queries in a highlevel manner. At present, large language models are incorporated in many fields, such as writing, translation, intelligent assistants and coding, and virtually any domain can be combined with large language models.

In the face of the AI era ushered in by ground-breaking AI technologies such as ChatGPT, large language models also have a broad application prospect and significant value in the field of education, such as assisting teachers in producing teaching content and delivering personalized learning materials and feedback to students. However, due to the fact that ChatGPT has not been released for a long time and still has many drawbacks, the research of application of large language models in school subjects such as mathematics is still scarce for now, not alone the tips generation for mathematical problem solving with ChatGPT.

To address this research gap, this paper attempts to apply the ChatGPT to generate general tips for helping students solve mathematical problems. At first, the related work about the tip design for ITS and the usage of LLM for mathematical education will be reviewed. Then the research question will be raised and the research method including prompt engineering and Turing test will be explained. After the findings of the experiment are introduced in details, they will be discussed from the pedagogical point of view. At last, the paper will be concluded.

# Related work

# Mathematical tips design

Current studies on mathematical tips mainly look into tips design, and students’ attitudes towards the use of tips and also assess the effect of tips on students’ learning of mathematics. Russo et al. (2020) examined students’ attitudes towards the use of tips in a student-orientated approach and conducted a study with students in eight classes in Australia, where students generally perceived tips as enhancing their understanding of mathematical topics and improving their performance in handling mathematical tasks, as opposed to viewing them as a social stigma. Lee (2016) also noted prompts applied in teaching, and based on Polya’s pedagogy to explore whether appropriate prompts have a significant positive impact on students’ learning of mathematics, and prompts were generated over different curricular situations and students’ maths achievements, and the categories of prompts contained keyword prompts, tip prompts, pictorial representation prompts and problemsolving instruction. The findings proved that the method can remarkably improve the learning efficiency of students in the low and middle achievement groups, thereby providing assistance for correctional education.

Besides, some studies focus on the effects of the application of tips to students through ITS. Jia et al. (2023) developed a WEB-based editor for general tips and an ITS for assessing and tutoring school students in mathematics, Mathematics Intelligent Assessment and Tutoring System (MIATS), which can provide students with effective feedback and facilitate their abilities to draw analogies. Besides, Jia et al. (2023) introduced the tips design principles, and carried out experiments in a middle school with the system, and the results indicated that MIATS significantly benefits students’ learning of mathematics, thus contributing to the implementation of the reform of teaching assessment and the policy of reducing students’ burden of education and increasing their efficiency. To achieve personalized tutoring for solving mathematical problems, Tang et al. (2023) also focused on tips and invented Guided and Adaptive Tutoring Tips (GATT) that provides intelligent assessment and personalized feedback to enhance students’ learning performance. Experimental results indicated that students using GATT through ITS showed significant improvements in their outcomes of mathematical learning.

# LLM and mathematics education

Currently, mainstream research focuses on probing into the use of large language models to solve mathematical problems independently. Dao and Le (2023) conducted a comprehensive analysis of ChatGPT’s abilities in mathematical reasoning and problem-solving based on the Vietnamese National High School Graduation Examination, and the results displayed that ChatGPT’s performance varied according to the difficulty and topic, and the study affirmed the potential of ChatGPT as a mathematical teaching tool but further advances are needed in areas such as handling graphical data and also the ability to solve very challenging topics. Wardat et al. (2023) carried out a qualitative study focusing on the perspectives of students and educational professionals regarding the use of ChatGPT in mathematical instruction. Most participants believed that ChatGPT, with its ability to engage in intelligent conversations, could assist in solving various mathematical problems and provide instructional support, but some respondents pointed out certain limitations of ChatGPT in mathematical education, such as a lack of in-depth understanding of geometry and potential impact on accuracy and effectiveness due to equation complexity and instructions. Lo (2023) provided a rapid review of 50 pieces of literature related to ChatGPT, elucidating its varying performance in different subjects and potential advantages as a teacher assistant and virtual mentor for students, but still pointed out that it performs unsatisfactorily in the field of mathematics. Kasneci et al. (2023) looked into the use of large language models like ChatGPT in education in the light of both students and teachers. They highlighted that ChatGPT can be employed to personalize the learning experience and promote student interaction and engagement in several courses such as Mathematics, Physics, and so on. Frieder et al. (2023) tested the mathematical capabilities of ChatGPT and GPT-4 and found that their performance in mathematics contradicts the positive media reports. Instead of being able to solve advanced mathematical problems, they are only capable of fact-checking or handling undergraduate-level mathematics. Bubeck et al. (2023) also assessed the capabilities of GPT4 in quantitative reasoning and expressing mathematical concepts. The results indicated that it could answer difficult high school maths problems, and even demonstrate some inference abilities in advanced mathematics, but it still exhibited basic errors in understanding certain problems.

In view of the above, many scholars have investigated the prospective of LLM in the teaching and learning of mathematics education. However, limited by difficulties such as the hallucination problem and weak multi-step reasoning ability, related research has failed to yield effective progress. Even the most advanced GPT-4 only achieved $1 6 . 1 \%$ on maths cloze questions and $4 7 . 0 \%$ on maths choice questions retrieved from authentic Chinese entrance exams to higher education, far away from the average human performance of $7 3 \%$ (Zhong et al., 2023), and achieved a modest accuracy rate of $5 3 . 9 \%$ on a special MATH dataset (Zheng et al., 2023). Hence, relying solely on LLMs to complete mathematical problems on their own, and expecting them to function as private tutors for students, poses difficulties and requires further technical breakthroughs.

# Summary

In summary, the aforementioned studies rely on manually designed general tips, with sporadic probes into intelligent methods of generating general tips. And some studies explored the use of LLMs such as ChatGPT to solve maths problems directly and have not addressed the use of them to generate tips to assist students in solving maths problems. To fill in this research gap, this study extends previous research examining the potential of large language models in generating general tips, proposes a semi-automated approach based on large language models for the generation of mathematical general tips, and experimentally compares the tips generated by the LLM with the tips generated by the human tutors, and provides a new idea for the application of large language models to the field of mathematics.

# Research questions

Based on the above literature review, three main questions will be investigated in this paper: 1.How can the general tips for helping mathematical problem solving be generated by LLM?

In previous studies, the benefits of general tips for mathematics instruction have been wellestablished. However, due to LLM’s unreliability in solving mathematical problems, it is not reasonable to expect LLM to provide accurate general tips when it cannot provide solutions to specific mathematical problems. Therefore, in order to utilize LLM for the generation of general tips, it is essential to address this question by providing specific steps for using LLM to generate general tips that facilitate mathematical problem-solving. Resolving this question is a prerequisite for addressing subsequent research questions.

2.Are the general tips generated by LLMs comparable with the tips designed by human beings?

Because there are different prompts for tip generation, this study aims to explore the optimal prompt that can help ChatGPT generate better tips. A sub question for this main question is: are the general tips generated by different prompts in LLM comparable with each other, and which prompt produces general tips with the highest quality? This research question aims to evaluate the quality of general tips generated by LLM while attempting to explore the most optimal method of tip generation.

3. Can human evaluators distinguish the general tips generated by LLMs from the tips designed by human beings?

Similar to research question 2, this question is also intended to assess the quality of general tips generated by LLM, but from a different perspective. Research question 2 directly derives the difference in quality between the two types of tips by means of scoring. Complementing research question 2, research question 3 is based on the assumption that if human evaluators are unable to differentiate between general tips of two sources, this is further evidence of the reliability of the quality of the AI-generated general tips, because if humans cannot discern between the sources, thus the AI-generated general tips have a certain level of quality assurance, and are not significantly inferior, and teachers can use these as a basis for modification without having to write them from scratch.

# Research methods

To answer the first question, prompt engineering for LLM will be applied. To answer the second and the third questions, a Turing test will be implemented. The methods are explained in details as the following.

# Prompt engineering to generate general tips

Although LLMs may not perform well in mathematical problem-solving tasks that demand highly accurate step-by-step reasoning, they have demonstrated remarkable superiority over traditional natural language processing models in the generation of text, such as summarization and translation. Therefore, this study adopts prompt engineering to generate general tips based on existing problems and solutions that are in fact all textual descriptions. Prompt engineering is a method that utilizes natural language to guide LLMs to learn and complete tasks. Its core idea is to embed the requirements of the task into the model’s input in the form of questions or instructions, rather than implicitly changing the model through parameter modifications, such as fine-tuning. The common approach is to transform different tasks into prompt-based input texts and employs multiple strategies to optimize the input texts, expecting the LLM to provide desired outputs (Liu & Chilton, 2022).

To generate general tips, this study adopts various prompt engineering methods, including Zero-Shot Learning, One-Shot Learning, Few-Shot Learning, and Chain of Thought (CoT). These methods have been widely used and have shown promising results in guiding LLMs to complete various tasks (Brown et al., 2020; Kojima et al., 2022; Liu & Chilton, 2022; Sanh et al., 2021; Wei et al., 2022). These three methods were chosen not only to provide comprehensive coverage of common prompt engineering strategies but also to explore the most appropriate prompt engineering methods for generating high-quality general tips. Zero-Shot Learning, One-Shot Learning, and Few-Shot Learning have been supported by previous studies as effective approaches in prompt engineering (Garcia et al., 2023; Song et al., 2023; Winata et al., 2021; Yang et al., 2023). By utilizing these three methods, we aim to evaluate and compare their effectiveness in generating general tips, specifically focusing on the quality of the generated tips.

Furthermore, recent research has highlighted the benefits of incorporating the Chain of Thought (CoT) strategy to enhance the multi-step reasoning capabilities of LLMs and improve their output quality (Kojima et al., 2022). In this study, CoT will be integrated as an additional strategy to further enhance the results of the prompt engineering methods. The following sections provide detailed explanations of each method.

Moreover, in the following text, the original mathematical formula will be shown for the reader’s better understanding instead of mathematical expressions in LATEX format that can be inputted into the LLM as prompts.

# Zero-shot learning

Zero-Shot Learning, also known as Zero-Shot Prompting, is a method of prompt engineering that is not reliant on task-relevant examples. The method draws on pre-trained language models and short prompts in the expectation that a large language model will reason to get the correct output based on its own knowledge alone (Kojima et al., 2022). In order to generate a general tip for a mathematical problem, a prompt constructed based on the strategy of Zero-Shot Learning can be the following Prompt 1:

Suppose you are a math teacher in a junior high school, and you are asked to write general tips for solving mathematical problems. General tips are key steps in the process or ideas that students can refer to as they work through the problem, giving them direction and guiding them through the process. The tips must be correct, detailed and inspiring. Here, given a problem and a solution, please provide a general tip for that problem. Answers should only contain the tip and no other descriptions are allowed. [Problem and solution attached].

It is worth clarifying that although the current large language model does not support a mixture of images and text as input, in this context, the essence of the task of LLMs is to induct and summarize based on existing textual content, but not to solve the problem based on the geometrical graph of the problem, and thus using a text-only form of the input will not affect the quality of the generated general tips.

# One-shot learning

One-Shot Learning, also referred to as One-Shot Prompting, is an approach to prompt engineering that utilizes a single task-related example, as well as a method of In Context Learning, in that it depends on the availability of one example in the context of the prompts to facilitate the learning of a large language model, without requiring modifications to the parameters of the large language model. With pre-trained language models, task descriptions, and a single task-related example, the approach expects a large language model to learn, generalize, and reason about the correct output of the task in comparison to the current user-given task input with only one example (Brown et al., 2020). In order to generate a general tip for this problem, a prompt constructed based on the strategy of One-Shot Learning can be the following Prompt 2:

“[The preceding text is the same as Prompt 1]. One example of a general tip that builds on the above introduction is:

Problem: The line $y = 2 x - 1$ is intercepted by a circle of radius $\sqrt { 5 }$ at the points $( 0 , 1 )$ and (2,1). Find the length of the chord.

Solution: The points $( 0 , 1 )$ and (2,1) are on the line $y = 1$ . The midpoint of the line segment formed with these two points as endpoints is $( 1 , 1 ) ,$ and the perpendicular bisector of this chord is $x = 1$ . Since the perpendicular bisector of any two points on the circle must pass through the centre of the circle, the centre of the circle is on the line $x = 1$ .

Therefore, assuming the coordinates of the center of the circle as $( 1 , x ) ,$ the distance from the centre to the point $\left( 0 , ~ 1 \right)$ is equal to the radius, implying ${ \sqrt { \left( 1 - 0 \right) ^ { 2 } + \left( \mathbf { x } - 1 \right) ^ { 2 } } } = { \sqrt { 5 } }$ . Work out that $x = - 1$ or $x = 3 ,$ which deduces that the centre of the circle is the point (1,3) or $\left( 1 , - 1 \right)$ .

$y = 2 x - 1$ is equivalent to $2 x - y - 1 = 0$ . The distance from the point (1,-1) to the line is

$$
d _ { 1 } = { \frac { | A x 0 + B y 0 + C | } { \sqrt { ( A ^ { 2 } + B ^ { 2 } ) } } } = { \frac { | 2 \times 1 - ( - 1 ) - 1 | } { \sqrt { 2 ^ { 2 } + ( - 1 ) ^ { 2 } } } } = { \frac { 2 { \sqrt { 5 } } } { 5 } }
$$

The distance from the point (1,3) to the line is:

$$
d _ { 2 } = { \frac { \left| A x 0 + B y 0 + C \right| } { \sqrt { \left( A ^ { 2 } + B ^ { 2 } \right) } } } = { \frac { \left| 2 \times 1 - 3 - 1 \right| } { \sqrt { 2 ^ { 2 } + \left( - 1 \right) ^ { 2 } } } } = { \frac { 2 { \sqrt { 5 } } } { 5 } }
$$

Because $d _ { 1 } = d _ { 2 } ,$ , the length of the cut chord is

$$
I = 2 \sqrt { r ^ { 2 } - d ^ { 2 } } = 2 \sqrt { ( \sqrt { 5 } ) ^ { 2 } - ( \frac { 2 \sqrt { 5 } } { 5 } ) ^ { 2 } } = \frac { 2 \sqrt { 1 0 5 } } { 5 } .
$$

The corresponding general tip for this problem is:

(1) From the two points on the circle given in the problem, determine that the centre of the circle is on the perpendicular bisector, and set the coordinates of the centre of the circle in one variable after calculating the equation of the perpendicular bisector;   
(2) Set out the equation and solve for the coordinates of the centre of the circle by using the mathematical relationship that the distance from the centre of a circle to a known point is equal to the radius;   
(3) Calculate the distance from the centre of a circle to a given line based on the distance of point to line;   
(4) Use the chord length formula to calculate the chord length and obtain the final answer.

Here, given a problem and a solution, please provide a general tip for that problem. Answers should only contain the tip and no other descriptions are allowed. [Problem and solution attached].”

# Few-shot learning

Few-Shot Learning is a method of prompt engineering that employs a small number (usually between two and five) of task-relevant examples. Similar to One-Shot Learning, the approach looks for large language models that can extract effective features and knowledge from a few examples to achieve high performance over new tasks (Brown et al., 2020). The prompt construction, derived from the strategy of Few-Shot Learning, can manifest as the following Prompt 3:

“[The preceding text is the same as Prompt 2].

As another example, given the arithmetic sequence $\left\{ a _ { n } \right\}$ with the initial terms $a _ { 5 } - 2 a _ { 1 } = 1 4$ and $a _ { 3 } = 1 0 _ { i }$ , you need to find the sum of the first 10 terms of the sequence $\left\{ { \frac { 1 } { a _ { n } a _ { n + 1 } } } \right\}$ Solution: Let’s assume that the common difference of the arithmetic sequence $\left\{ \widetilde { a } _ { n } \right\}$ is d. According to the formula for the general term of an arithmetic sequence, we can derive the equation as follows:

The result of solving this eq

$$
\begin{array} { c } { { \{ \begin{array} { l l } { { a _ { 1 } + 4 d - 2 a _ { 1 } = 1 4 } } \\ { { a _ { 1 } + 2 d = 1 0 } } \end{array}  } } \\ { {  u a t i o n \ i s \{ \begin{array} { l l } { { a _ { 1 } = 2 } } \\ { { d = 4 } } \end{array}  } } \\ { { a _ { n } = a _ { 1 } + ( n - 1 ) d = 2 + 4 ( n - 1 ) = 4 n - 2 } } \end{array}
$$

So

$$
\left\{ \frac { 1 } { a _ { n } a _ { n + 1 } } \right\} = \frac { 1 } { ( 4 n - 2 ) ( 4 n + 2 ) } = \frac { 1 } { 4 } \left( \frac { 1 } { 4 n - 2 } - \frac { 1 } { 4 n + 2 } \right)
$$

Therefore, the sum of the first 10 terms in the sequence is

$$
{ \frac { 1 } { 4 } } \left( { \frac { 1 } { 2 } } - { \frac { 1 } { 6 } } \right) + { \frac { 1 } { 4 } } \left( { \frac { 1 } { 6 } } - { \frac { 1 } { 1 0 } } \right) + \ldots + { \frac { 1 } { 4 } } \left( { \frac { 1 } { 3 4 } } - { \frac { 1 } { 3 8 } } \right) + { \frac { 1 } { 4 } } \left( { \frac { 1 } { 3 8 } } - { \frac { 1 } { 4 2 } } \right) = { \frac { 1 } { 4 } } \left( { \frac { 1 } { 2 } } - { \frac { 1 } { 4 2 } } \right) = { \frac { 5 } { 4 2 } }
$$

The corresponding general tip for this problem is:

(1) Based on the general term formula of arithmetic sequence and known conditions, set out a system of equations for the first term and the common difference of arithmetic sequence, and solve for the values of the first term and the common difference;   
(2) Use the general term formula of arithmetic sequence to find the general term of the number sequence;   
(3) Using the method of telescoping sum, find the sum of the first 10 terms of the number sequence and simplify the result.

Here, given a problem and a solution, please provide a general tip for that problem. Answers should only contain the tip and no other descriptions are allowed. [Problem and solution attached].”

# Chain of thought (CoT)

Chain of Thought (CoT) is a method that stimulates the reasoning ability of language models to solve complex problems by gradually guiding them. This method expects the LLMs to think step by step according to a certain chain of thought, transforming a difficult main problem into several simple sub-problems, solving one corresponding sub-problem at a time, and thus solving the whole problem (Kojima et al., 2022). For CoT prompts with examples, they can be called One-Shot CoT or Few-Shot CoT depending on the number of examples. For CoT prompts without examples, they are called Zero-Shot CoT. In this case, CoT prompting is achieved by adding guiding words, such as “Let’s think step by step”. The strategy of CoT has been shown to be effective in enhancing the quality of output from LLMs. Therefore, this study will add the strategy of CoT to further boost the results in this example.

Prompt 4 is formulated by adding the following guiding sentence to Prompt 1:

When generating a general tip, please think step by step from the perspective of solving the problem and write out the tip.

Prompt 5 and Prompt 6 are formulated by adding the same guiding sentence to Prompt 2 and Prompt 3 respectively. In this way, all 6 prompts can be formed and inputted to LLMs, and their outputs construct the 6 different general tips for one single mathematical problem.

# Examples of prompt engineering for two mathematical problems

The construction of prompts for generating tips introduced above can be illustrated by taking one geometry problem and one algebra problem as two examples, because both geometry and algebra are important but difficult contents in junior high school mathematics instruction. These are the two problems covered in our subsequent study, and an overview of the prompt engineering methods used in both problems is shown in Table 1.

The following are the two examples of prompt engineering for two mathematical problems.

Example 1: “As shown in Figure 1, within triangle ABD, $\angle A$ is a right angle, $\mathsf { A B } = 6 , \mathsf { A D } = 8 , \mathsf { B C } = 1 5$ , and ${ \mathsf { D C } } = { \sqrt { 3 2 5 } } ,$ , what is the area of this quadrilateral ABCD?”

The solution to the problem is given by a maths teacher as the following: “Since $\angle A$ is a right angle, $A B = 6$ and $A D = 8 ,$ $B D ^ { 2 } { = } A D ^ { 2 } { + } A B ^ { 2 }$ , which works out to $B D = 1 0$ . Within triangle BCD, $B D = 1 0 ,$ $B C = 1 5$ and $D C = \sqrt { 3 2 5 }$ . Since $1 0 ^ { 2 } + 1 5 ^ { 2 } = \left( { \sqrt { 3 2 5 } } \right) ^ { 2 } .$ , which means that $B D ^ { 2 } + B C ^ { 2 } { = } D C ^ { 2 } ,$ , triangle BCD is a right-angled triangle. Area of quadrilateral $A B C D =$ area of triangle $A B D + a r e a$ of triangle $B C D = \textstyle { \frac { 1 } { 2 } } A D \times A B + \textstyle { \frac { 1 } { 2 } } B C \times B D = 9 9 .$ ” For this example, the ChatGPT generated one tip based on Zero-Shot Learning as the following: $" ( 1 )$ Calculate the length of BD using the Pythagorean theorem from the triangle ABD given in the problem;

Table 1. Overview of prompt engineering methods used.   

<html><body><table><tr><td>Prompt Engineering Method</td><td>Number of Generated General Tips</td></tr><tr><td>Zero-Shot Learning</td><td>1 for algebra + 1 for geometry</td></tr><tr><td>One-Shot Learning</td><td>1 for algebra + 1 for geometry</td></tr><tr><td>Few-Shot Learning</td><td>1 for algebra + 1 for geometry</td></tr><tr><td>Zero-Shot CoT</td><td>1 for algebra + 1 for geometry</td></tr><tr><td>One-Shot CoT</td><td>1 for algebra + 1 for geometry</td></tr><tr><td>Few-Shot CoT</td><td>1 for algebra + 1 for geometry</td></tr></table></body></html>

![](img/889b2f047a8fcfb360eb7c5c2d255ec7096f81c6881080463a004fb4c22da2bc.jpg)  
Figure 1. Geometric graph of example 1.

(2) Apply the Pythagorean theorem to determine whether BCD is a right triangle based on the lengths of BD and BC; (3) Using the formula for the area of a quadrilateral, calculate the areas of triangles ABD and BCD respectively; (4) Add the areas of the two triangles to obtain the area of quadrilateral ABCD.” A human-designed general tip is listed as the following: “(1) Set the goal: The area of quadrilateral ABCD is the sum of the areas of triangles ABD and BCD; (2) According to the problem, $\angle A$ is known to be a right angle, AB is 6, and AD is 8, then the area of triangle ABD is ${ \scriptstyle { \frac { 1 } { 2 } } } \times A B \times A D = 2 4 ;$ (3) Do you remember what the Pythagorean theorem is? What about its converse theorem? (4) According to the Pythagorean theorem, $A B ^ { 2 } + A D ^ { 2 } = B D ^ { 2 }$ , then $B D = 1 0 ;$ : (5) Based on the converse theorem of the Pythagorean theorem, given $B C = 1 5 ,$ $D C = { \sqrt { 3 2 5 } }$ find $B D =$ 10. According to $B D ^ { 2 } + B C ^ { 2 } = D C ^ { 2 }$ , determine that triangle BCD is a right triangle with angle $B$ as the right angle; (6) The area of triangle $B C D \ i s \ \frac { 1 } { 2 } { \times } \ B D \times B C = 7 5 ;$ (7) Calculate the area of a quadrilateral. The area of quadrilateral ABCD is the sum of the area of triangle ABD and the area of triangle DBC, giving the area of quadrilateral ABCD as 99.”

Example 2: “Please calculate the result of $\frac { 8 } { a - 4 } + \frac { a + 4 \prime } { 4 - a }$

A maths teacher provides the following solution to the problem:

The original fraction is equal to $\begin{array} { r } { \frac { 8 } { a - 4 } - \frac { a + 4 } { a - 4 } = \frac { 8 - a - 4 } { a - 4 } = \frac { 4 - a } { a - 4 } = - 1 . } \end{array}$

For this example, the ChatGPT generated one tip based on Zero-shot learning as the following:

“(1) Observe the denominators of two fractions and find that they are opposite numbers. Using the property that the value of a fraction remains unchanged after the numerator and denominator of the fraction are multiplied by the same negative number at the same time, multiply the numerator and denominator of one of the fractions by $- 1$ to make the denominators of the two fractions the same; (2) Add two fractions together, using the formula of addition and subtraction of fractions with the same denominator to add the numerators and leave the denominators unchanged; (3) Simplify to get the simplest result”. A human-designed general tip is listed as the following: “(1) Goal setting: to calculate the result of a given fraction, the unknown numbers appear in the fraction, so it needs to be eliminated in the final value; (2) Simplification of fractions: according to the basic properties of fractions, the value of a fraction remains unchanged when both the numerator and denominator of the fraction are multiplied by the same non-zero number; (3) Observe the relationship between the numerator and denominator and whether the result can be obtained by the reduction of a fraction.”

# Turing test

To answer the second and the third questions, the Turing test will be implemented. The mathematician and one founder of artificial intelligence Alan Turing (1950) proposed the innovative theoretical test to measure whether a computer can have the same intelligence as a human being, which is called Turing-test in the history of artificial intelligence. Although this idea has been both influential and widely criticized, many practical tests have been implemented to test the intelligence of a computer system. Since the birth of ChatGPT, some researchers also have conducted Turing tests to assess the intelligence of this LLM from different points of view. Nov et al. (2023) conducted one survey to evaluate patients’ ability to distinguish between human provider and ChatGPT responses, i.e., in fact a Turing test, and patients’ trust in ChatGPT’s functions. A US representative sample of 392 study participants aged 18 and above participated in the survey. Participants were informed that five responses were provider-generated and five were ChatGPT-generated. Participants were asked and incentivized financially to correctly identify the response source, as well as their trust in ChatGPT’s functions in patient-provider communication, using a Likert scale of 1–5. The survey result analysis shows the correct classification of responses ranged between $4 9 . 0 \%$ to $8 5 . 7 \%$ for different questions. On average, ChatGPT responses were correctly identified $6 5 . 5 \% ,$ and provider responses were correctly distinguished $6 5 . 1 \%$ of the time. On average, responses towards patients’ trust in ChatGPT’s functions were weakly positive (mean Likert score: 3.4), with lower trust as the health-related complexity of the task in question increased.

Table 2. Evaluation indicators and scores for general tips.   

<html><body><table><tr><td>Primary Indicator</td><td>Secondary Indicator</td><td>Description</td><td>Score</td></tr><tr><td>Correctness</td><td>Correctness</td><td>The general tip correctly reflects the key steps or thinking process when solving the problem, without any logical or mathematical errors.</td><td>1</td></tr><tr><td>Expressiveness</td><td>Expressiveness</td><td>The general tip is clear, concise and coherent, without any grammatical errors, and uses the appropriate language and symbols.</td><td>1</td></tr><tr><td></td><td></td><td>Comprehensiveness Comprehensiveness If the content of general tip covers two aspects: method and knowledge that requires. It gets 1 point if it only covers one aspect, and 2 points if it</td><td>2</td></tr><tr><td rowspan="3">Proceduralness</td><td></td><td>covers both aspects. Clarify the problem The general tip can help learners understand what the problem is asking. The general tip can help learners take into account all the given conditions</td><td>1</td></tr><tr><td> Make a plan</td><td>and find the connection between the known conditions and the unknowns.</td><td>1</td></tr><tr><td></td><td>Implement the plan The general tip can help learners transform the original problem into an equivalent problem or a solved problem.</td><td>1</td></tr><tr><td rowspan="2">Heuristicness</td><td>Review</td><td>The general tip can summarize the knowledge points and methods used in this problem at the end, connecting common sense and theory. The general tip can reflect the students&#x27; subjectivity and initiative, guide</td><td>1</td></tr><tr><td>Heuristicness</td><td>them to think and explore independently, rather than directly provide the answer or too simple hints. It can help learners apply the method to</td><td>2</td></tr><tr><td>Total</td><td></td><td>other problems and build their own knowledge network..</td><td></td></tr></table></body></html>

In order to assess the quality of the ChatGPT-generated tips and to test the human user’s ability to distinguish between human-generated tips and ChatGPT-generated tips, a Turing test was conducted. Besides the 12 general tips for 2 mathematical problems generated by ChatGPT, 12 general tips also were designed by human tutors. Thus for each example problem, 6 general tips were designed by human tutors and 6 general tips were generated by ChatGPT. For each problem 12 tips were put together in the random order and without any indication of the authorship.

In order to evaluate the tips’ quality, a set of scoring criteria was developed according to a comprehensive GPT scoring guide proposed by Ouyang et al. (2022). Its content is shown in Table 2.

An online survey was designed with four questions:

(1) For the example question 1, 12 general tips are presented as the following. Please score their quality within the range [0,10] based on the scoring criteria.   
(2) For the example question 1, 12 general tips are presented as the following. Please check the ones which you find are generated by ChatGPT.   
(3) For the example question 2, 12 general tips are presented as the following. Please score their quality within the range [0,10] based on the scoring criteria.   
(4) For the example question 2, 12 general tips are presented as the following. Please check the ones which you find are generated by ChatGPT.

Totally 121 respondents answered this online survey. They were participants in the summer school “Frontiers in Educational Technology” from Peking University that was held from July 10 to 14 July 2023. They were asked to volunteer in this survey. Among them, 25 $( 2 0 . 6 6 \% )$ were males, and 96 $( 7 9 . 3 4 \% )$ were females. Most of the respondents have mathematical teaching or other teaching backgrounds, among which 3 $( 2 . 7 8 \% )$ have mathematical teaching experience, 114 $( 9 4 . 2 1 \% )$ have other teaching experience, and only 4 $( 3 . 3 1 \% )$ have no experience in these areas. In addition, the educational and occupational backgrounds of these respondents varied. The number of undergraduates, master students, doctoral students, teachers and others were 19 $( 1 5 . 7 0 \% )$ , 61 $( 5 0 . 4 1 \% )$ , 9 $( 7 . 4 4 \% )$ , 23 $( 1 9 . 0 1 \% )$ and $9 ( 7 . 4 4 \% )$ respectively. The demographic information of those respondents is listed in Table 3.

# Findings

# General tips summarized using problems and solutions through LLM and prompt engineering

To overcome the problem that LLMs are unreliable mathematical problem solvers, this paper introduces a change in the approach to problems that require LLM to solve: in the solution proposed in this paper, LLM is not expected to rely solely on its own capabilities to solve mathematical problems, but rather analyse the given problem and the correct solution in order to derive accurate general tips. This task aligns more closely with the text summarization, which LLM excels at.

To enhance the quality of general tips generated by LLM, this paper employs various advanced prompt engineering methods, including Zero-Shot Learning, One-Shot Learning, Few-Shot Learning, and Chain of Thought (CoT). Compared to directly instructing LLMs to complete a task, a welldesigned prompt combined with prompt engineering can effectively improve the quality of the generated general tips.

# Statistically significant difference between the scores of human-designed general tips and those of ChatGPT-generated general tips

To analyse the score difference between the scores of human-designed general tips and those of ChatGPT-generated general tips, the statistical software SPSS V24 was utilized to conduct statistical description and independent samples t-test for the difference between two groups of scores that were collected as the answers to the questions 1 and 3 in the survey.

# Findings of the survey result about the geometry problem

The frequency histograms of the scores for human-designed general tips and robot-generated general tips were shown in Figure 2, which showcases the frequency histogram of human ratings provided by human evaluators for robot-generated tips in a general context (left graph), as well as the frequency histogram of human ratings for human-designed tips in a general context (right graph). This figure together with Table 4 as the statistical summary table for both sets of scores demonstrates a predominantly normal distribution, implying the concentration and stability of the scores.

Table 3. The demographic information of the respondents.   

<html><body><table><tr><td></td><td>Demographic profile</td><td>Frequency</td><td>Percentage</td></tr><tr><td rowspan="2">Gender</td><td>Male</td><td>25</td><td>20.66%</td></tr><tr><td>Female</td><td>96</td><td>79.34%</td></tr><tr><td rowspan="4">Mathematical teaching experience</td><td> No teaching experience</td><td>4</td><td>3.31%</td></tr><tr><td>With mathematical teaching experience</td><td>3</td><td>2.48%</td></tr><tr><td>With other teaching experience</td><td>114</td><td>94.21%</td></tr><tr><td>Others</td><td>9</td><td>7.44%</td></tr><tr><td rowspan="4">Educational background or other occupation</td><td> Undergraduate</td><td>19</td><td>15.70%</td></tr><tr><td>Master student</td><td>61</td><td>50.41%</td></tr><tr><td>Doctoral student</td><td>9</td><td>7.44%</td></tr><tr><td>Teacher</td><td>23</td><td>19.01%</td></tr></table></body></html>

![](img/224d8643e779138a29fbaec29e14f568d60a83ac6b9e8cdcc047b3e713ce611c.jpg)  
Figure 2. Frequency histograms of human scores for two types of tips for the geometry problem.

Table 4. Statistics of human scores for two types of tips for the geometry problem.   

<html><body><table><tr><td></td><td></td><td></td><td colspan="2">95% Confidence Interval for Mean</td><td colspan="7"></td></tr><tr><td></td><td></td><td>Mean</td><td>Lower Bound</td><td>Upper Bound</td><td> Median</td><td>Var.</td><td>Std. Deviation</td><td>Maxi- mum</td><td>Mini- mum</td><td>Skewness Kurtosis</td><td></td></tr><tr><td>Robot- generated</td><td>Statistic 5.41 Std.</td><td>0.083</td><td>5.25</td><td>5.57</td><td>5.00</td><td>5.043</td><td>2.246</td><td>10</td><td>0</td><td>0.092 0.091</td><td>0.510 0.181</td></tr><tr><td>Human-</td><td>Error Statistic  6.38</td><td></td><td>6.18</td><td>6.57</td><td>7.00</td><td>6.978</td><td>2.642</td><td>10</td><td>0</td><td>0.499</td><td>0.771</td></tr><tr><td> designed</td><td>Std.</td><td>0.097</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.091</td><td>0.181</td></tr><tr><td></td><td>Error</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

The distribution of the histogram shows that human experts tend to score human-designed tips more highly compared to scoring robot-generated tips. As can be seen in Table 4, the mean of human experts’ ratings of robot-generated general tips is 5.41 and the mean of their ratings of human-designed general tips is 6.38, suggesting that the mean of human ratings of human-designed tips is higher. The variance of human ratings of robot-generated general tips is 5.043, and the variance of ratings of manually designed general tips is 6.978, indicating that the degree of dispersion in human ratings of human-designed tips is higher relative to the scores of robot-generated tips. In addition, the maximum and minimum values of human experts’ ratings of general tips from both types of sources appear to be 10 and 0, showing that different human experts show inconsistent preferences for human-designed and robotgenerated general tips.

Table 5. Independent samples t-test results comparing the scores for two types of tips for the geometry problem.   

<html><body><table><tr><td rowspan="3"></td><td colspan="2">Levene&#x27;s Test for Equality of Variances</td><td colspan="7">T-test for Equality of Means</td></tr><tr><td rowspan="2">F</td><td rowspan="2">Sig.</td><td rowspan="2">t df</td><td rowspan="2">Sig. ( 2-tailed)</td><td rowspan="2"></td><td rowspan="2">Mean Difference</td><td rowspan="2">Std. Error Difference</td><td colspan="2">95% Confidence Interval for Difference</td></tr><tr><td>Lower</td><td> Upper</td></tr><tr><td>Equal variances. assumed</td><td>32.861</td><td>0.000</td><td>7.525</td><td>1450</td><td>0.000</td><td>0.967</td><td>0.129</td><td>-1.219</td><td>0.715</td></tr><tr><td>Equal variances. not assumed</td><td></td><td></td><td>7.525</td><td>1411.934</td><td>0.000</td><td>0.967</td><td>0.129</td><td>1.219</td><td>0.715</td></tr></table></body></html>

Table 5 shows the results of the independent samples t-test for the two groups’ scores. As shown in Table 5, the homogeneity of variance test yielded an F value of 32.861, corresponding to a p-value less than 0.05, which rejects the null hypothesis of equal variances. Under the assumption of unequal variances, the two groups’ independent sample two-tailed test with a p-value less than 0.05 rejects the null hypothesis that there is no significant difference between human ratings of human-designed and robot-generated general tips, which means that humans are more satisfied with human-designed general tips than robot-generated general tips based on the geometric problem.

![](img/cf7c68db3cc68800d81a41547b5fbeb391c156c1a5f1640bf3652925e68573d8.jpg)  
Figure 3. Frequency histograms of human scores for two types of tips for the algebra problem.

Table 6. Statistics of human scores for two types of tips for the algebra problem.   

<html><body><table><tr><td colspan="3"></td><td colspan="2">95% Confidence Interval for Mean</td><td colspan="7"></td></tr><tr><td></td><td></td><td>Mean</td><td>Lower Bound</td><td>Upper Bound</td><td> Median</td><td>Var.</td><td>Std. Deviation</td><td>Maxi- mum</td><td>Mini- mum</td><td>Skewness</td><td>Kurtosis</td></tr><tr><td rowspan="2">Robot- generated</td><td>Statistic 5.76 Std.</td><td>0.078</td><td>5.61</td><td>5.92</td><td>6.00</td><td>4.433</td><td>2.106</td><td>10</td><td>0</td><td>0.257 0.091</td><td>0.361 0.181</td></tr><tr><td>Error</td><td>6.21</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Human- designed</td><td>Statistic Std.</td><td>0.088</td><td>6.03</td><td>6.38</td><td>7.00</td><td>5.647</td><td>2.376</td><td>10</td><td>0</td><td>0.386 0.091</td><td>0.637 0.181</td></tr><tr><td>Error</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

# Findings of the survey result about the algebra problem

The graphical representation of the frequency histograms for two sets’ scores is depicted in Figure 3. The frequency histogram displays the ratings provided by humans for robot-generated tips in a general context, while on the right side, it represents the frequency histogram of human ratings for human-designed tips in a general context. This figure together with Table 6 as the statistical summary table for both sets of scores demonstrates a predominantly normal distribution, implying the concentration and stability of the scores.

Furthermore, the shape of the histogram distribution indicates that human experts tend to assign higher ratings to human-designed tips compared to robot-generated tips. The mean of rating scored by human experts for robot-generated general tips is 5.76, while the mean of rating they give to human-designed general tips is 6.21, which suggests that the mean of rating for human-designed tips is comparatively higher. Moreover, the variability of ratings for robot-generated general tips is indicated by a variance of 4.433, whereas the variance for human-designed general tips amounts to 5.647. This finding highlights that there is a greater degree of dispersion in the ratings of humandesigned tips compared to those generated by robots. Additionally, it is worth noting that the maximum and minimum ratings provided by human experts for general tips, regardless of their source, range from 10 to 0. This demonstrates the varied preferences exhibited by different human experts towards both human-designed and robot-generated general tips.

Table 7 presents the outcomes of the independent samples t-test conducted for the two groups. It exhibits the results of the homogeneity of variance test, indicating an F value of 17.251, which associates with a p-value below 0.05 and invalidates the null hypothesis of equivalent variances. Assuming unequal variances, the independent sample two-tailed test for the two groups, with a p-value below 0.05, dismisses the null hypothesis suggesting no notable distinction in human ratings between human-designed and robot-generated general tips. Consequently, human ratings prove capable of discerning between human-designed and robot-generated general tips based on the algebra problem.

Table 7. Independent samples t-test results comparing the scores for two types of tips for the algebra problem.   

<html><body><table><tr><td></td><td colspan="2">Levene&#x27;s Test for Equality of Variances</td><td colspan="7">T-test for Equality of Means</td></tr><tr><td></td><td>F</td><td></td><td></td><td></td><td>Sig.</td><td>Mean</td><td>Std. Error</td><td colspan="2">95% Confidence Interval for Differencee</td></tr><tr><td></td><td>17.251</td><td>Sig.</td><td>t</td><td>df</td><td>( 2-tailed)</td><td>Difference</td><td>Difference</td><td>Lower</td><td>Upper</td></tr><tr><td>Equal variances assumed</td><td></td><td>0.000</td><td>3.766</td><td>1450</td><td>0.000</td><td>0.444</td><td>0.118</td><td>0.675</td><td>0.213</td></tr><tr><td>Equal variances not assumed</td><td></td><td></td><td>-3.766</td><td>1429.182</td><td>0.000</td><td>0.444</td><td>0.118</td><td>0.675</td><td>0.213</td></tr></table></body></html>

Table 8. The mean values and standard deviation of different prompts.   

<html><body><table><tr><td></td><td>Problem 1 mean</td><td>Problem 1 std</td><td>Problem 2 mean</td><td>Problem 2 std</td><td>Mean</td></tr><tr><td>Zero-Shot CoT</td><td>6.4</td><td>2:12</td><td>6.4</td><td>1.88</td><td>6.4</td></tr><tr><td>Zero-Shot Learning</td><td>4.6</td><td>2.16</td><td>6.9</td><td>2.07</td><td>5.7</td></tr><tr><td>Few-Shot CoT</td><td>5.7</td><td>2:11</td><td>5.4</td><td>1.97</td><td>5.5</td></tr><tr><td>One-Shot Learning</td><td>5.6</td><td>2.06</td><td>5.5</td><td>2.09</td><td>5.5</td></tr><tr><td>Few-Shot Learning</td><td>4.9</td><td>2.45</td><td>5.4</td><td>2.06</td><td>5.2</td></tr><tr><td>One-Shot CoT</td><td>5.3</td><td>2.11</td><td>5.0</td><td>1.98</td><td>5.2</td></tr></table></body></html>

To evaluate the different effects achieved by different prompts, human evaluator scores were calculated for the six tips formed from the six prompts for each question. The mean values and the standard deviation of the scores for the general tips formed by the different prompts are listed in Table 8 in descending order of the score means of two problems. Zero-Shot CoT achieved the highest score of 6.4, followed by Zero-Shot Learning with a score of 5.7. Next to it, the two prompts of Few-Shot CoT and One-Shot Learning were tied with 5.5, while Few-Shot Learning and One-Shot CoT obtained the lowest score of 5.2. All the average scores ranged from 5.2 to 6.4, indicating a relatively concentrated and stable performance of the ChatGPT-generated general tips. However, the quality of the general tips formed from different prompts shows subtle variations. Furthermore, the ranking of scores for geometry and algebra problems differs across the six prompts, highlighting the varying effectiveness of different prompts in generating general prompts for different types of questions.

To make more substantial claims regarding prompt effectiveness and enhance guidance for user interactions with ChatGPT, we conducted further in-depth statistical tests and assessed validity using SPSS. In regard to problem 1, to assess the validity of prompt design, KMO and Bartlett’s Test was performed. KMO and Bartlett’s sphericity test are commonly used to assess the suitability of variables for factor analysis. In general, when the value is closer to 1, it indicates a stronger correlation between variables, making them more suitable for factor analysis. Kaiser has provided the following common KMO measurement standards: 0.9 and above indicate very suitable; 0.8 indicates suitable; 0.7 indicates moderate suitability; 0.6 suggests less suitability; and below 0.5 indicates unsuitability. The obtained results, as depicted in Table 9, displayed a KMO value greater than 0.8 with a p-value of

Table 9. KMO and Bartlett’s test of problem 1.   

<html><body><table><tr><td colspan="2">Kaiser-Meyer-Olkin Measure of Sampling Adequacy.</td><td>.875</td></tr><tr><td rowspan="3">Bartlett&#x27;s Test of Sphericity</td><td> Approx, Chi-Square</td><td>271.313</td></tr><tr><td>df</td><td>15.</td></tr><tr><td>Sig.</td><td>.000</td></tr></table></body></html>

Table 10. Total variance explained of problem 1.   

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">Initial communalities</td><td colspan="3">Initial Eigenvalues</td><td rowspan="2">Extraction communalities</td><td colspan="3">Extraction Sums of Squared Loadings</td></tr><tr><td></td><td>% of</td><td></td><td></td><td>% of</td><td></td></tr><tr><td>Few-Shot</td><td>initial 1.000</td><td>Total 3.464</td><td>Variance 57.731</td><td>cumulative 57.731</td><td>Extraction .539</td><td>Total 3.464</td><td>Variance 57.731</td><td>cumulative 57.731</td></tr><tr><td>Learning Zero-Shot</td><td>1.000</td><td>.708</td><td>11.794</td><td>69.525</td><td>.521</td><td></td><td></td><td></td></tr><tr><td>Learning</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Few-Shot CoT</td><td>1.000 1.000</td><td>.589 .491</td><td>9.809</td><td>79.334 87.519</td><td>.696 .662</td><td></td><td></td><td></td></tr><tr><td>One-Shot CoT Zero-Shot CoT</td><td>1.000</td><td>.404</td><td>8.185 6.735</td><td>94.254</td><td>.434</td><td></td><td></td><td></td></tr><tr><td>One-Shot</td><td>1.000</td><td>.345</td><td>5.746</td><td>100.00</td><td>.613</td><td></td><td></td><td></td></tr><tr><td>Learning</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

Table 11. Component Matrix of problem 1.   

<html><body><table><tr><td>Prompt Engineering Method</td><td>Value</td></tr><tr><td>Few-Shot CoT</td><td>.834</td></tr><tr><td>One-Shot CoT</td><td>.813</td></tr><tr><td>One-Shot Learning</td><td>.783</td></tr><tr><td>Few-Shot Learning</td><td>.734</td></tr><tr><td>Zero-Shot Learning</td><td>.722</td></tr><tr><td>Zero-Shot CoT</td><td>.659</td></tr></table></body></html>

Table 12. KMO and Bartlett’s test of problem 2.   

<html><body><table><tr><td>Kaiser-Meyer-Olkin Measure of Sampling Adequacy.</td><td>.804</td></tr><tr><td>Bartlett&#x27;s Test of Sphericity Approx, Chi-Square</td><td>280.875</td></tr><tr><td>df</td><td>15.</td></tr><tr><td>Sig.</td><td>.000</td></tr></table></body></html>

Table 13. Total variance explained of problem 2.   

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">Initial communalities</td><td colspan="3"> Initial Eigenvalues</td><td rowspan="2">Extraction communalities</td><td colspan="3">Extraction Sums of Squared Loadings</td></tr><tr><td></td><td>% of</td><td></td><td></td><td>% of</td><td></td></tr><tr><td>Few-Shot</td><td>initial 1.000</td><td>Total 3.309</td><td>Variance 55.155</td><td>cumulative 55.155</td><td>Extraction .638</td><td>Total 3.309</td><td>Variance 55.155</td><td>cumulative 55.155</td></tr><tr><td>Learning Zero-Shot</td><td>1.000</td><td>.919</td><td>15.321</td><td>70.476</td><td>.310</td><td></td><td></td><td></td></tr><tr><td> Learning</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Few-Shot CoT</td><td>1.000</td><td>.713</td><td>11.882</td><td>82.358 89.589</td><td>.639 .651</td><td></td><td></td><td></td></tr><tr><td>One-Shot CoT Zero-Shot CoT</td><td>1.000 1.000</td><td>.434 .340</td><td>7.231 5.668</td><td>95.257</td><td>.449</td><td></td><td></td><td></td></tr><tr><td>One-Shot</td><td>1.000</td><td>.285</td><td>4.743</td><td>100.00</td><td>.623</td><td></td><td></td><td></td></tr><tr><td> Learning</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

0.000, indicating that Exploratory Factor Analysis can be applied to investigate the prompt design of the six forms for problem 1, demonstrating favourable outcomes.

Moreover, the “Cumulative $( \% ) ^ { \prime \prime }$ data in Table 10 indicates the percentage of the total variance that can be cumulatively explained by the first N components. As observed from Table 10, the first five components can explain $9 4 \%$ of the variance, while the first three explain $7 9 \%$ of the variance. This indicates that the prompts Few-Shot Learning, Zero-Shot Learning and Few-Shot CoT are already capable of explaining a significant portion of the information, thus representing the underlying variables in the context of generating general prompts for Problem 1 in the sense that when generating general tips for problem 1, these three forms of prompts effectively clarify most of the information in question 1.

Further examination of the Component Matrix reveals which variable characteristics should be included in the components. Table 11 signifies that the ranking in decreasing order of influence is Few-Shot CoT, One-Shot CoT, One-Shot Learning, Few-Shot Learning, Zero-Shot Learning and ZeroShot CoT. Few-Shot CoT plays a relatively pivotal role in generating general tips for the first category of mathematical problems. Therefore, prompts designed in this particular form can be deemed as critical features for subsequent analysis.

For problem 2, KMO and Bartlett’s Test was also developed. Table 12 indicates that the KMO value is greater than 0.8 and the p-value $= 0 . 0 0 0$ , illustrating that Exploratory Factor Analysis can be done on the prompt design of the six forms for problem 2 with satisfactory results.

As seen in Table 13, the first five components explain $9 5 \%$ of the variance, while the first three explain $8 2 \%$ of the information, suggesting that Few-Shot Learning, Zero-Shot

Table 14. Component Matrix of problem 2.   

<html><body><table><tr><td>Prompt Engineering Method</td><td>Value</td></tr><tr><td>Few-Shot CoT</td><td>.807</td></tr><tr><td>One-Shot CoT</td><td>.799</td></tr><tr><td>One-Shot Learning</td><td>.798</td></tr><tr><td>Few-Shot Learning</td><td>.789</td></tr><tr><td>Zero-Shot Learning</td><td>.670</td></tr><tr><td>Zero-Shot CoT</td><td>.557</td></tr></table></body></html>

Table 15. The evaluation indicators for ChatGPT-generated tips in percentage   

<html><body><table><tr><td>Example problem</td><td>Precision mean  std dev</td><td>Recall mean  std dev.</td><td>F-value mean  std dev.</td></tr><tr><td>1</td><td>35.04  25.77</td><td>31.54  27.54</td><td>33.29  25.23</td></tr><tr><td>2</td><td>39.31  23.58</td><td>34.44  24.03</td><td>36.87  22.49</td></tr><tr><td>Both</td><td>37.78  20.59</td><td>32.99  23.38</td><td>35.39  20.86</td></tr></table></body></html>

Learning, and Few-Shot CoT already explain most of the information, and these three forms of prompts are better able to capture the original variable and clarify most of the information from problem 2.

The Component Matrix was further viewed to determine which variable features should be included in the components. Table 14 suggests that the ranking in decreasing order of influence is Few-Shot CoT, One-Shot CoT, One-Shot Learning, Few-Shot Learning, Zero-Shot Learning, and Zero-Shot CoT. The One-Shot CoT form of prompt design has a relatively more important role in generating general tips for the second type of mathematical problems, and prompts designed in this form can be used as key features for subsequent analyses.

# Lower recognition precision and recall of ChatGPT-generated general tips

By analysing the answers to the question 2 and 4 in the survey, the precision, recall and F-value that is defined as the arithmetic mean of precision and recall of all evaluators’ recognition of the ChatGPTgenerated general tips for each of the two example problems, as well as the precision, recall and F-value for all ChatGPT-generated general tips were calculated. Their means and standard deviations in percentage are listed in Table 15. The means of precision, recall and F-values are all lower than $4 0 \%$ , thus the evaluators could not distinguish the ChatGPT-generated tips from the humandesigned tips.

# Discussion

Cognitivism emphasizes the active and selective acquisition of stimulation and its processing by students. When students receive general tips, these tips guide them to connect new mathematical knowledge with their existing knowledge, constructing effective cognitive structures in their minds. This process enhances their understanding and memory of mathematical concepts. Heuristic teaching method pays attention to student-centred learning, guiding students to discover and create knowledge autonomously. General tips provide key ideas that stimulate students’ interest in mathematics, and encourage them to actively explore and verify problem-solving methods. Existing research on how to use mathematical tips to promote effective learning for students has been undertaken, but most of them have used manually-written tips (Han, 2010; Hao & Zhang, 2006). In contrast, this study aims to propose an innovative method for generating general tips for mathematics based on LLMs, which can be inductively applied to different contexts based on existing problems and solutions, thus reducing the cost and time of manually writing general tips.

In this study, human evaluators were allowed to score the tips, and detailed scoring criteria and fine-tuning guidelines were developed to avoid the effects of scoring subjectivity on the survey. The findings of this study reveal a significant difference in the ratings of the general tips for the two types of problems, which may be attributed to the profound knowledge and teaching experience that human tutors possess in the field of mathematics, enabling them to design tips that better align with the learning patterns of students studying mathematics.

Which strategy is the best one for ChatGPT to generate general tips is also a small part of what this study explores. Theoretically, Few-Shot CoT should have scored the highest as it utilizes more examples than other strategies to help GPT better understand users’ intentions, learn how to generalize tips, and employ CoT to enhance the quality of generated content through incremental reasoning (Brown et al., 2020). However, its score is not the highest. One plausible explanation is that this experiment only involves two problems, making the credibility of measuring the effectiveness of different prompts less reliable compared to analysing tip generation across an entire dataset. And this prompt may cause GPT to overly rely on the given examples, overlooking the specific characteristics and variations of the given mathematical problems, resulting in tips that may not align well with the genuine needs of students. Zero-Shot CoT, which achieved the highest average score in this experiment, allows GPT to unleash its creativity and avoid overdependence on examples. This gives GPT the freedom to generate prompts based on the specific problem, rather than being restricted by the contents of the examples. Additionally, it generates more plausible tips through the process of incremental deduction. Some researchers are also delving into the reasons behind the suboptimal performance of Few-Shot Learning. In the case of ChatGLM and BLOOMZ models, the outcomes of Zero-Shot Learning outshine that of Few-Shot Learning. This disparity can be attributed to the fact that these models are trained without examples, which makes them more adept at handling problems devoid of examples. The precise intricacies underlying this finding necessitate further exploration (Li et al., 2023).

Furthermore, based on calculations of the means of precision, recall and F-values, many evaluators were unable to distinguish between tips generated by ChatGPT and those generated by human tutors, which may be owing to the extensive application of prompt engineering in this study, where the integration of these concepts greatly enhanced the quality of ChatGPT-generated tips, endowing them with inspiration and high accuracy.

To sum up, incorporating the current imperfect LLMs into Mathematics educational practices while ensuring satisfying results has become a key research problem, which is the focus of this study. Different LLMs can be tested in mathematics education in the future, comparing the effects of different model architectures, parameter settings, training methods, and other factors on the generation of results, with a view to the most appropriate LLMs for mathematics, and the use of technology to advance students’ mathematical performance.

# Conclusions

This study proposes a semi-automated method for generating general tips in mathematics based on a large language model. Moreover, the study employs the Turing test to compare the general tips for one geometry problem and one algebra problem generated by ChatGPT and human tutors. The results demonstrate a statistically significant difference in human ratings between human-designed general tips and ChatGPT-generated tips for both problems. The effectiveness of ChatGPT in generating general tips varies depending on the prompts used, and most evaluators were unable to distinguish between tips from the two sources. This suggests that the content generated by ChatGPT can be tweaked by human teachers to alleviate the burden of complete manual design of general tips for mathematical questions.

In conclusion, ChatGPT-generated general tips can serve as a useful reference for teachers when designing tips for use in ITS to assist students with mathematical problem-solving. This semiautomated approach improves efficiency by generating tips, breaks through the limitations of manual design and provides a new perspective for the deep integration of generative artificial intelligence and mathematics education.

# Limitations and future work

The quality of general tips generated by ChatGPT still needs to be improved, and several factors affecting them were also identified during the experiment in this study, which warrant further optimization in future studies. Firstly, higher-quality prompts may result in better general tips. As a key determinant in generating general tips, prompts of different types and content can impact the model’s understanding and output. Future studies should continue to explore adjusting the content of prompts to enhance the model’s clarity in understanding task requirements, thereby inspiring its reasoning and creative abilities, while minimizing the occurrence of the model’s hallucination and irrelevant content. Besides, we will explore more advanced prompt engineering methods to improve the quality of LLM’s generation of general tips. For example, one possibility is to combine newer prompt engineering methods such as Tree-of-Thoughts (Yao et al., 2023) and Graph-of-Thoughts (Besta et al., 2023) to build more comprehensive and flexible generation strategies that can guide LLMs to employ different thought paths and problem-solving methods. The difficulty of the examples in prompts also affects the results. Two highly difficult problems were used as input examples in this study, but examples that are either too simplistic or too complex can impede the model’s ability to generate general tips. Therefore, in future research, a more appropriate selection of examples is necessary to improve the model’s generalization ability. For instance, incorporating examples of varying difficulty and types could expose the model to different mathematical concepts and problem-solving methods, thereby enhancing the coverage and adaptability of the generated general tips. And more categories of problems as input may be needed to fully demonstrate LLM’s ability to generate general tips. In future work, a worthwhile research topic would be to construct large-scale problem-analysis datasets and explore automated (or semi-automated) methods of evaluating the quality of generated general tips, which would enable more comprehensive analyses.

To evaluate the quality of the ChatGPT-generated tips, the ITS equipped with such tips should be applied in mathematical education. Through quasi-experiments, the performance of the students in mathematical learning, as well as their feeling and satisfaction with such kinds of tips should be compared with traditional human-designed tips.

In summary, in the era of digital education, technology should be widely applied in education under the premise of security. Future research should discover how to utilize large language models to empower education, helping to stimulate students’ thinking and creativity, improving educational quality and efficiency, and accelerating the establishment of an innovative education system geared towards future development.

# Acknowledgements

This research is supported by the National Education Research Funding Project “Students’ Intelligent Assessment and Tutoring Research Based on Big-data Mining” (Number: BCA220208) granted by National Social Science Foundation, China. The authors extend heartfelt appreciation to all the teachers and students who have participated in this research, as well as gratitude to the reviewers and editors for their invaluable insights and suggestions.

# Disclosure statement

No potential conflict of interest was reported by the author(s).

# Funding

The work was supported by the National Social Science Fund of China [BCA220208].

# Notes on contributors

Jiyou Jia, male, professor and department head of Department of Educational Technology, Graduate School of Education, Peking University, director of International Research Centre for Educational Informatization at Peking University. He is mainly engaged in the research of educational technology and the application of artificial intelligence in education.

Tianrui Wang, female, postgraduate student of History of Science and Technology, University of Chinese Academy of Sciences. She is mainly engaged in the research of the history of artificial intelligence and educational technology.

Yuyue Zhang, female, postgraduate student in Department of Educational Technology, Graduate School of Education, Peking University. She is mainly engaged in the research of educational technology and the application of artificial intelligence in education.

Guangdi Wang, female, postgraduate student in Chinese Institute, Beijing International Studies University. She is mainly engaged in the research of Chinese information processing.

# ORCID

Jiyou Jia $\textcircled{1}$ http://orcid.org/0000-0002-8034-342X Tianrui Wang $\textcircled{1}$ http://orcid.org/0009-0005-1517-9589 Yuyue Zhang http://orcid.org/0009-0003-5802-1381 Guangdi Wang $\textcircled{1}$ http://orcid.org/0009-0006-1737-8239

# References

Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J. , and Hoefler, T. (2023). Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687.   
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Amodei, D. (2020). Language models are few-shot learners. Advances in Neural Information Processing Systems, 33, 1877–1901.https://arxiv.org/abs/2005.14165   
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., & Zhang, Y. (2023). Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.   
Dao, X. Q., & Le, N. B. (2023). Investigating the effectiveness of ChatGPT in mathematical reasoning and problem solving: Evidence from the Vietnamese National high school graduation examination. arXiv preprint arXiv:2306.06331.   
Frieder, S., Pinchetti, L., Griffiths, R. R., Salvatori, T., Lukasiewicz, T., Petersen, P. C., Chevalier, A., & Berner, J. (2023). Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867.   
Garcia, X., Bansal, Y., Cherry, C., Foster, G., Krikun, M., Johnson, M., & Firat, O. (2023). The unreasonable effectiveness of few-shot learning for machine translation. arXiv preprint arXiv:2302.01398.   
Han, L. (2010). The characteristics and application of guided tips in mathematics teaching. Teaching & Administration, 2010(10), 59–61.   
Hao, H., & Zhang, J. (2006). A study of the effect of strategic tips on the performance of mathematical underachieving students in word problems. Theory and Practice of Education, 26(11), 40–42.   
Jia, J. (2015). Intelligent tutoring systems. In M. Spector (Ed.), Encyclopedia of educational technology (pp. 411–413). Sage.   
Jia, J., Le, H., Zhang, Y., Liu, H., Chen, A., & Li, S. (2023). The design of an intelligent assessment and tutoring system based on big data mining. China Educational Technology, 3, 112–119. https://doi.org/10.3969/j.issn.1006-9860.2023.03.016   
Jia, J., Zhang, Y., Liu, H., & Li, S. (2023). Intelligent assessment and tutoring system helps to reform student assessment and alleviate burdens for efficiency. E-Education Research, 44(6), $7 4 \substack { - 8 0 + 8 9 }$ .   
Kasneci, E., Seßler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Günnemann, S., Hüllermeier, E., Krusche, S., & Kasneci, G. (2023). ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 103, 102274. https://doi.org/10.1016/j.lindif. 2023.102274   
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems, 35, 22199–22213. https://arxiv.org/abs/2205.11916   
Kulik, J. A., & Fletcher, J. D. (2016). Effectiveness of intelligent tutoring systems: A meta-analytic review. Review of Educational Research, 86(1), 42–78. https://doi.org/10.3102/0034654315581420   
Lee, C.I. (2016). An appropriate prompts system based on the Polya method for mathematical problem-solving. Eurasia Journal of Mathematics, Science and Technology Education, 13(3), 893–910. https://doi.org/10.12973/eurasia.2017.00649a   
Liu, V., & Chilton, L. B. (2022, April). Design guidelines for prompt engineering text-to-image generative models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, New Orleans, Louisana, USA (pp. 1–23).   
Li, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y., Duan, N., Baldwin, T., & Baldwin, T. (2023). CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212.   
Lo, C. K. (2023). What is the impact of ChatGPT on education? A rapid review of the literature. Education Sciences, 13(4), 410. https://doi.org/10.3390/educsci13040410   
Nov, O., Singh, N., & Mann, D. M. (2023). Putting ChatGPT’s medical advice to the (Turing) test. arXiv preprint arXiv:230110035, cs.HC   
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.   
Russo, J., Minas, M., Hewish, T., & McCosh, J. (2020). Using prompts to empower learners: Exploring primary students’ attitudes towards enabling prompts when learning mathematics through problem solving. Mathematics Teacher Education & Development, 22(1), 48–67.   
Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., & Rush, A.M. (2021). Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.   
Song, Y., Wang, T., Cai, P., Mondal, S. K., & Sahoo, J. P. (2023). A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities. ACM Computing Surveys.   
Tang, R., Zhang, Y., Cao, Y., Liu, H., & Jia, J. (2023, July). Design and effect of guided and adaptive tutoring tips for helping school mathematics problems solving. In: C. Li. (Ed.), Proceedings of International Conference on Blended Learning (pp. 273–284). Cham: Springer Nature Switzerland.   
Turing, A. (1950). Computing machinery and intelligence. Mind, LIX(236), 433–460. https://doi.org/10.1093/mind/LIX.236.433   
VanLehn, K. (2011). The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems. Educational Psychologist, 46(4), 197–221. https://doi.org/10.1080/00461520.2011.611369   
Wardat, Y., Tashtoush, M. A., AlAli, R., & Jarrah, A. M. (2023). ChatGPT: A revolutionary tool for teaching and learning mathematics. Eurasia Journal of Mathematics, Science and Technology Education, 19(7), em2286. https://doi.org/10. 29333/ejmste/13272   
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.   
Winata, G.I., Madotto, A., Lin, Z., Liu, R., Yosinski, J., & Fung, P. (2021). Language models are few-shot multilingual learners. arXiv preprint arXiv:2109.07684.   
Yang, C.H., Gu, Y., Liu, Y., Ghosh, S., Bulyko, I., & Stolcke, A. (2023). Generative speech recognition error correction with large language models. arXiv preprint arXiv: 2309.15649.   
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K. (2023). Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.   
Zheng, C., Liu, Z., Xie, E., Li, Z., & Li, Y. (2023). Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797.   
Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., & Duan, N. (2023). Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364.