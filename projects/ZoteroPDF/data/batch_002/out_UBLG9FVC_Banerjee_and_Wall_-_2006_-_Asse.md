# Assessing and reporting performances on pre-sessional EAP courses: Developing a final assessment checklist and investigating its validity

Jayanti Banerjee \*, Dianne Wall

Department of Linguistics and English Language, Lancaster University, Lancaster LA1 4YT, UK

# Abstract

As the number of non-native speakers of English studying at universities in the United Kingdom (UK) has grown, so has the need to provide pre-sessional English for Academic Purposes (EAP) courses. One of the challenges facing the organisers of such courses is deciding how to determine whether students have completed them successfully and whether they are ready to enter their academic departments. This paper reviews research into assessment on pre-sessional and other EAP support courses, and summarises the results of a survey of practice in a number of universities in the UK and other countries. It describes how assessment on pre-sessional courses has evolved at one British institution (Lancaster University), and outlines some of the problems that have led to changes in recent years. Explanations will be given of the methods used to pilot and refine a new assessment procedure and of the attempts that have been made to investigate its validity.

$©$ 2005 Elsevier Ltd. All rights reserved.

Keywords: English for academic purposes; Assessment; Validation; Pre-sessional EAP courses

# 1. Assessing and reporting performance on pre-sessional EAP courses

All English-medium universities stipulate a minimum level of English language proficiency for admission to their courses, but they set down different routes by non-native speakers of English can satisfy their requirements. We surveyed institutions in the UK and other countries in order to identify how different universities decided whether students were proficient enough to begin studying in their academic departments. The survey revealed that some institutions will only accept the results of external tests (e.g., IELTS, TOEFL, the test of English for International Communication—TOEIC) or tests which have been designed by external agencies and released for institutional purposes (e.g., the Institutional TOEFL). Some institutions, in addition to demanding a minimum level of performance on an external test, require students to take an inhouse test. The results of this test are used to determine whether the students need further language support before they begin their academic programme or, in some institutions, while they follow their programme. In other institutions (particularly those in the UK), students who meet all the other requirements for admission but whose performance on the external test falls short of that required for unconditional admission can be admitted provided that they successfully complete a pre-sessional EAP course.

There is no common approach to assessing and reporting performance on pre-sessional EAP courses. In the case of institutions that will only accept the results of external tests, students are required to re-take the tests at the end of their pre-sessional EAP courses. One recently reported example is that of Australian universities where pre-sessional EAP courses serve not only to prepare students for academic study but also for the IELTS test (Moore & Morton, 2005). At least one institution uses the results of an external test that has been piloted on its students; others design their own tests, which may or may not be modelled on tests created elsewhere for other purposes. Some institutions prefer to judge students on their in-course performance, combining internal test scores, performance on written assignments (projects and course assignments, sometimes collected together in portfolios), formal presentations, and classroom participation. The weighting given to these elements varies, but it is common to give more weight to tasks that are completed near the end of the course.

Most institutions are required to send individual reports to the central admissions office or the receiving department, either with a test result (from one or more of the tests described above), a grade determined by a combination of test results and other assessments, or a pass-fail judgement which is based either on tutors’ impressions or on a more straightforward criterion such as attendance. An estimated IELTS score is required in some institutions. A recent development is the use of ‘can-do’ scales, which consist of lists of performance objectives and columns where EAP tutors indicate whether students are able or not to achieve each objective.

The history of assessment on Lancaster University’s pre-sessional EAP courses reveals that we have adopted a number of these approaches at different times. In the early 1990s, we were asked to provide estimated IELTS scores. The admissions officers found these estimates useful as they could interpret them in the same way they had processed genuine test results when making their initial decisions. We, however, became increasingly concerned about the lack of validity and reliability in our judgements because:

(i) We did not have access to the official rating scales for speaking and writing.   
(ii) Estimating reading and listening abilities was problematic, since this depended on inferences about students’ ‘inner processes’ rather than analyses of products such as essays or oral presentations, and   
(iii) None of the teaching team had been trained as IELTS examiners.

We abandoned this practice in favour of writing a profile report for each student, commenting on their general ability in listening, oral presentations, group discussions, reading and writing. The report was written by the student’s main tutor, who had at least $^ { 8 \mathrm { h } }$ of contact with the student every week and also had access to feedback sheets and comments from the student’s other tutors. The main tutor would also give an overall judgement of the student’s ability. The report was discussed with the student before being passed on to the course co-ordinators. The course co-ordinators reviewed and edited it to standardise the style and to ensure that it was a summary of the student’s progress and ability rather than a collection of anecdotes. Copies were sent to the University’s admissions officers, the departments and to the students themselves.

An example of a completed profile report can be found in Appendix A.

# 1.1. Problems with the reporting system

This system continued until 2001 without too many objections or queries, but there were a number of problems:

(i) The reports went to two different audiences: the administrative audience, consisting of the central university admissions officers and the students’ departmental admissions officers, and the students themselves. It was difficult to write in a way which satisfied the needs of two such distinct audiences—in particular, to say what needed to be said without discouraging the students.   
(ii) Tutors had varying interpretations of what constituted readiness for entry into a university department and there were few opportunities in either pre-course or in-course staff meetings to discuss definitions of the key criteria. Table 1 contains a compilation of the types of features that tutors mentioned in the “Writing” section of a sample of 50 final reports produced in 2001. The left-hand column contains comments which relate to some notion of writing proficiency. It is interesting to note that features which often figure in textbooks on writing, such as cohesiveness, coherence, and checking drafts, occur rarely. The comments about linguistic performance (under “Language”) are general rather than detailed. It is a matter of debate whether the comments in the right-hand column (which relate to other facets of the students’ performance) reveal anything substantial about the students’ ability to express them in formal writing and to conform to standard academic conventions.   
(iii) The brevity of the courses, the need to give fair coverage to all skill areas, and a course approach which emphasised out-of-class preparation and in-class group discussions meant that there were many aspects of performance that were either not visible in the classroom or that manifested themselves when students were talking with one another rather than with the tutor. This affected the depth of knowledge any tutor had of a student’s ability.   
(iv) We have already explained that all the course reports were edited by the course coordinators in order to standardise the style and to lift the level of commentary from the more specific to the more general. It was inevitable that those doing the editing changed the meaning of some of the reports, either on purpose (toning them down) or inadvertently, through a lack of understanding of the original text. As a result of changes that made the language more diplomatic, admissions officers sometimes had problems with interpretation, which made their job harder rather than easier, thus undermining one of the purposes of the exercise.

It is clear from the survey of other institutions that Lancaster University’s practices before 2001 were not unusual. However, we remained concerned about the validity and reliability of our exit assessments. We wanted to ensure that our practice was well grounded in theory, that it was transparent to users (our students and admissions personnel), and that it was practical in terms of tutors’ skills and time.

Table 1 Final report 2001—the types of comments recorded in the section for writing   

<html><body><table><tr><td>Related to writing</td><td>Interesting, but are they related to writing?.</td></tr><tr><td>GENERAL</td><td>Confident</td></tr><tr><td>Excellent writing</td><td>Has improved</td></tr><tr><td>Thoughtful writing (3)</td><td>Feels has improved</td></tr><tr><td>Transmits ideas clearly</td><td>Will improve</td></tr><tr><td>Cohesive (4)</td><td>No improvement in first 3 weeks, then sudden improvement</td></tr><tr><td>Coherent (6)</td><td>Raised self-awareness</td></tr><tr><td>Fluent</td><td>Has become aware of the need to write well</td></tr><tr><td>CONTENT</td><td></td></tr><tr><td>Related to reading:</td><td>Aware of difficulties and trying to overcome (2)</td></tr><tr><td>Reveals she&#x27;s understood reading.</td><td>Has taken note of principles (3)</td></tr><tr><td>Can analyse data</td><td>Puts in effort (2)</td></tr><tr><td>Can respond critically to data (10)</td><td>Has applied self</td></tr><tr><td>Argumentation:</td><td></td></tr><tr><td>Good argumentation (11)</td><td>Responds to feedback (9)</td></tr><tr><td>Does not hesitate to give own views</td><td>Found module helpful (2)</td></tr><tr><td>Needs to read widely and critically.</td><td>Has enjoyed this module</td></tr><tr><td>Style:</td><td></td></tr><tr><td>Reader-friendly (9)</td><td>Settled down quickly</td></tr><tr><td>Academic</td><td>I have no worries</td></tr><tr><td>Concise expression</td><td>Should be able to cope with demands (2)</td></tr><tr><td>Verbose</td><td>I hope she will continue to improve.</td></tr><tr><td>Conventions:</td><td></td></tr><tr><td>Has explored conventions of academic writing</td><td>Weaknesses likely to disappear soon.</td></tr><tr><td>Can convey message in own words</td><td>Weak in xxx but once that improves I hope</td></tr><tr><td>Editing:</td><td></td></tr><tr><td>Developing editing skills</td><td>Will benefit from continued academic support</td></tr><tr><td>Less than rigorous when checking drafts (2)</td><td></td></tr><tr><td>ORGANISATION (31)</td><td></td></tr><tr><td>LANGUAGE</td><td></td></tr><tr><td>General comment about language (13)</td><td></td></tr><tr><td>Grammatical problems (5)</td><td></td></tr><tr><td>Good word choice</td><td></td></tr></table></body></html>

# 2. Research into EAP assessment

One obvious option when restructuring the EAP exit assessment was to require the students to re-take an external EAP test such as IELTS or TOEFL. However, there are a number of drawbacks in this approach. The first is practical in that there is a time delay between the taking of the test and the release of the official results. The pre-sessional EAP courses finish just before the academic year begins so this means that students would need to take the external test before the pre-sessional course ended. Since, the shortest (and largest) of Lancaster University’s presessional EAP courses is only four weeks, the students would probably have to take the external test mid-way through the programme, possibly before they were able to make measurable advances in their language proficiency.

The second, and stronger, argument is made by research that has analysed the demands of university writing tasks, particularly research that has compared these demands to those on external EAP tests. Horowitz (1986) and Canseco and Byrd (1989), the latter looking specifically at the writing demands in postgraduate degree programmes, report that university students need to be able to select relevant data from sources, reorganise the data to answer the question set, and also need to write their text using academic register. Other important writing skills include the ability to present information in tables and graphs, and to revise and edit drafts. Students are rarely required to refer to personal experience. This, as Moore and Morton (2005) point out, marks an important difference between the demands of the target language situation and the writing demanded by at least one major external test—the IELTS. IELTS Task 2 typically requires students to draw upon their personal experience for examples to support claims that they make rather than to draw on reading sources or primary data.

Other differences documented by Moore & Morton (2005) include:

(i) IELTS Task 2 falls within a restricted genre of a written argument or case whereas university writing tasks cover reviews, case study reports, research reports, research proposals, summaries, exercises and short answer tasks.   
(ii) The predominant rhetorical functions in IELTS Task 2 are evaluation and hortation. Though evaluation is the predominant rhetorical function of university writing tasks, other rhetorical functions such as description, summarisation and comparison are also common and hortation is relatively rare.

Consequently, Moore & Morton argue that it is not possible to combine preparation for the IELTS test with preparation for university study.

Another possible reason for lack of fit between EAP tests and the language demands students face at university may lie in the criteria used for assessment in EAP contexts. If the assessment criteria used in EAP tests do not reflect the criteria against which the students’ performance will be judged in academic contexts, then the scores achieved are less easily interpretable with reference to the students’ ability to perform tasks in those contexts. For instance, Thorp and Kennedy (2003) have reported that ‘idiomaticity’ rather than the ability to write within the academic genre is rewarded on the IELTS writing test. Jacoby and McNamara (1999) comment that the assessment criteria in specific purposes tests usually employ only language-focused categories whereas performances in a specific purposes context are not wholly language based. This last point is problematic because the main purpose of a pre-sessional EAP course is to boost students’ ability to perform in English in an academic environment so it can be argued that the focus should therefore be upon language. However, this would be to ignore the complexity of language for specific purposes contexts. It appears, therefore, that external EAP tests such as IELTS and TOEFL (in their current forms) [2005], while suitable as a pre-entry measure, are insufficiently representative of the construct of EAP (particularly EAP writing) to be used as an exit measure for pre-sessional EAP courses.

With some notable exceptions (e.g., Cho, 2003), there is little published on EAP-course finalassessment procedures. However, the construct of EAP assessment has been addressed from a number of other perspectives (see Blue, Milton, & Saville, 2000 for a compendium of recent work). Studies like Horowitz (1986), Canseco and Byrd (1989) and Moore and Morton (2005) have analysed the writing demands placed upon university students. Others have looked more broadly at the language and academic needs of university students. Weir’s (1983) groundbreaking work focused upon the needs of overseas students. More recently, as part of the development programme for the TOEFL internet-based test (TOEFL iBT), the Educational Testing Service has published a series of commissioned papers covering the reading, writing, speaking and listening demands on university students (see Bejar, Douglas, Jamieson, Nissan, & Turner, 2000; Butler, Eignor, Jones, McNamara, & Suomi, 2000; Cumming, Kantor, Powers, Santos, & Taylor, 2000; Enright, Grabe, Koda, Mosenthal, Mulcahy-Ernt, & Schedl, 2000; Hamp-Lyons & Kroll, 1997; Jamieson, Jones, Kirsch, Mosenthal, & Taylor, 2000; Waters, 1996). Though the primary purpose of these papers was to define the initial construct of the revised TOEFL test, they also form an excellent resource for the EAP construct in all four-language skills.

Table 2 Key factors in successful academic performance, taken from Ginther and Grant (1996)   

<html><body><table><tr><td>Reading</td><td>Writing</td></tr><tr><td>Understanding the main idea of their reading.</td><td>Organisation</td></tr><tr><td>Reaching valid conclusions</td><td>Summarisation</td></tr><tr><td>Making critical evaluations of content</td><td>Well-formed sentences</td></tr><tr><td>Comprehending significant detail</td><td>Vocabulary</td></tr><tr><td>Understanding explicitly stated information.</td><td>Usage</td></tr><tr><td>Detecting inferences between the lines</td><td>Research skills</td></tr><tr><td></td><td>Economy</td></tr><tr><td></td><td>Clarity</td></tr><tr><td></td><td>Providing sufficient evidence</td></tr><tr><td></td><td>Grammatical</td></tr><tr><td></td><td>Correctly punctuated</td></tr><tr><td></td><td>Ability to use &#x27;standard&#x27; academic discourse</td></tr><tr><td></td><td>Knowing what your tutor-examiner values</td></tr><tr><td></td><td>(and giving that to him/her).</td></tr></table></body></html>

Another useful resource is Ginther and Grant’s (1996) meta-analysis of research into the academic needs of native English-speaking university students in the US. This provides information about the language demands placed on all students (not only international students). They note that speaking and listening needs have not been investigated (possibly because these skills are taken for granted among native speakers or because they are rarely formally assessed in an academic context) and that reading is often commented on in relation to writing. Their metaanalysis suggests a number of reading and writing features that are key factors in good academic performance. These are listed in Table 2.

Yet other researchers have explored the criteria applied by subject tutors to university students’ performances. For instance, as part of a study to design suitable writing tasks for post-graduate students Wall (1981) analysed the criteria that Economics tutors applied to the students’ written work. She found that the tutors applied some or all of the following criteria when assessing students finished work: knowledge of subject matter, critical ability, structuring of the essay, answering the question, and the use of sources. Tutors responded negatively to verbosity/irrelevance and carelessness/lack of rewriting. They made relatively few comments about specific language features. Wall, Nickson, Jordan, Allwright and Houghton (1988) arrived at a similar finding when they compared the reactions of an academic tutor to a student essay with those of three EAP tutors.

Table 3 Factors influencing lecturers’ judgements of student writing, from Errey (2000, p. 156)   

<html><body><table><tr><td>Content</td><td>Layout</td></tr><tr><td>Use of sources</td><td>Punctuation</td></tr><tr><td>Coherence</td><td>Understanding of reading</td></tr><tr><td>Organisation</td><td>Appropriate register</td></tr><tr><td>Addresses topic</td><td>Appropriate expression.</td></tr><tr><td>Pre-writing strategies.</td><td>Accurate vocabulary</td></tr><tr><td>Development of ideas.</td><td>Essential vocabulary</td></tr><tr><td>Completion of task</td><td>Length</td></tr><tr><td>General linguistic accuracy</td><td>Paragraphing</td></tr><tr><td>Grammatical accuracy</td><td>L1 interference</td></tr><tr><td>Fluency/style</td><td>Legibility</td></tr><tr><td>Spelling</td><td>Cohesion</td></tr></table></body></html>

Although the EAP tutors differed in their approaches to helping the student they all seemed to share the opinion of the academic tutor that “a higher return to language teacher input can be achieved by a shift in teaching emphasis towards the wider questions of essay structure in general and the art of logical presentation in particular” (1988, p. 127). More recently, Hartill (2000) and Errey (2000) have investigated the assessment demands faced by students at two UK universities. Hartill reports that departments tend to value “communicative effectiveness,” but that this does not necessarily encompass linguistic accuracy. Errey’s introspective verbal report study with lecturers from five different departments at Oxford Brookes University has revealed 24 factors that influence the lecturers’ judgements of student writing. These factors are presented in Table 3.

Table 4 ‘Cost’ factors listed in Banerjee (2003, pp. 335 and 343)   

<html><body><table><tr><td>Language related source of &quot;cost&quot;</td><td>Course related source of &quot;cost&quot;&#x27;..</td></tr><tr><td>Inability to understand the lecturers&#x27; accents</td><td>The structure of the course</td></tr><tr><td>Inability to understand native-speaking colleagues</td><td>Course content</td></tr><tr><td>Inability to understand local residents</td><td>-Theoretical orientation.</td></tr><tr><td>Inability to understand other non-native speakers</td><td>-Inadequate coverage</td></tr><tr><td>Difficulty in following radio broadcasts.</td><td>-Rigid (insufficient allowance for student contributions)</td></tr><tr><td>Need to translate into L1 in order to understand concepts</td><td>-British centred examples</td></tr><tr><td>Need to translate from L1 when speaking or writing</td><td>-No focus on current affairs</td></tr><tr><td>Difficulty in making themselves understood</td><td>-Mathematics</td></tr><tr><td>Restricted/inadequate lexical resources</td><td>Course delivery</td></tr><tr><td>Inability to retrieve lexical resources quickly.</td><td>-Poor lecturing</td></tr><tr><td>(particularly problematic under examination conditions)</td><td></td></tr><tr><td>Negative perception of their ability to communicate.</td><td>Conflict between preferred study method and the demands of the course</td></tr><tr><td>Slow reading speed</td><td>Conflict between study aims and the demands of the course</td></tr><tr><td>Difficulty in/inability to identify key points in reading British academic writing conventions</td><td>Volume of assessment Lack of variety in assessment</td></tr><tr><td>Avoidance of resources in their L1 because of a</td><td>Lack of background in the subject</td></tr><tr><td>perceived incompatibility between the two languages</td><td></td></tr><tr><td>Preference for resources in their L1.</td><td>Relationship with supervisor</td></tr><tr><td>Inability to memorise easily (particularly problematic under examination conditions) Poor examination performance</td><td>Behaviour of course colleagues.</td></tr></table></body></html>

A few researchers (e.g., Geoghegan, 1983; Banerjee, 2003) have explored study needs from the students’ perspective by drawing on their on-going study experiences. Banerjee (2003) conducted a longitudinal study of 25 postgraduate students, interviewing them at a number of different points during their Masters degrees. She found that they experienced difficulties (she termed this “cost”) stemming from factors related to both language and the courses they were following (Table 4). She also found that students experienced difficulties in adjusting to the British system of education, particularly the lecture-seminar structure of their courses, the emphasis on learner independence, the demand for critical and independent thought, academic conventions, the criteria used for marking (particularly the way they were applied), the role of the teacher, and the emphasis in many courses on group work.

To summarise, our previous experience of reporting student performance on the pre-sessional EAP course and the studies reported above narrowed our options with respect to the reporting mechanisms that we could consider. It was clear that the use of an external EAP test would risk construct under-representativeness and would undermine the usefulness of the report for the admitting departments and the students. Our experience of using profile reports based on tutors’ personal criteria was similarly unsatisfactory. We wanted to devise a report form which would incorporate key features from the research we have reviewed and, because much of this focused on writing, would also incorporate features from scales such as those found in the English Speaking Union (ESU) framework (Carroll & West, 1989). We would complement this with our own experience, not only as pre-sessional and in-sessional tutors but also as members of an academic department.

In the sections that follow, we describe both the design of an exit assessment checklist and the initial stages in its validation.

# 3. Developing a final assessment checklist

We took a number of decisions in order to ensure that the new exit assessment procedure provided more useful guidance to tutors, students and the admissions officers; was more practical; and explicitly reflected theories of the EAP construct. These decisions were:

(i) Function and audience—The main function of the report would be to give the admissions officers an accurate picture of the student’s abilities. Students would receive a copy of the report, and it was hoped they would benefit from receiving a frank account of their strengths and weaknesses, but we would not edit the reports to soften the blow for students who were not performing adequately.   
(ii) Coverage—The report form would explicitly reflect current EAP theory. It would specify the skills and strategies that had emerged from our analysis of previous research, the report forms prepared in 2001, and our previous experience as EAP teachers and academic tutors. We decided that it would be inappropriate to comment on attitude, aptitude, motivation, awareness, or any other quality which was a feature of personality rather than of linguistic or EAP ability.   
(iii) Evidence—We wanted to make it clear that we were only commenting on features that we had evidence for. We also wanted to make clear the limits of our judgements.

For instance, we stated explicitly that the tasks that students performed during the presessional course approximated (but could not fully replicate) the demands students were likely to encounter in their study contexts. We would indicate how students had performed on the tasks set during the course rather than predicting whether they would do well in their future settings. Our intention was to provide our primary users—the admissions personnel, with an evidential basis for deciding whether or not a student was ready to begin studying on a particular programme. This allowed admissions tutors for different degree programmes to interpret the evidence differently, depending on the aspects of EAP proficiency they considered more important for their fields of study.

(iv) Format—It was clear that prose reports were difficult for the tutors to write and timeconsuming to edit. They were also difficult for the end-users (primarily university admissions personnel) to interpret. One way of eliminating the problems of composing, editing and interpreting was to devise a checklist where tutors would only have to place ticks in columns to indicate whether the student had or had not demonstrated certain abilities.

The procedure for devising the checklist was as follows:

1. We drew up an exhaustive list of the features of academic reading, writing, listening and speaking abilities.   
2. We went through an iterative process of combining some items that were similar and, in contrast, breaking down some broad descriptors into sub-categories. For instance, we split an item taken from Errey (2000)—“content’”—into two sub-categories, “can analyse the topic of the assignment” and “can produce relevant content”.   
3. We grouped items according to the language skill they represented. At times these decisions were complex. For example, “providing sufficient evidence” (Ginther & Grant 1996) is a writing skill but is also dependent on the student’s ability to understand and make use of reading. We resolved this particular problem by breaking the skill down into two components “can analyse argumentation in academic texts” (classified in the checklist as a reading skill) and “can reproduce others’ ideas using his/her own words” (classified as a writing skill).   
4. The checklist was formatted to include two columns for each can-do descriptor: “yes” (the student can do this) or “must pay attention to” (this skill or strategy).   
5. The draft checklist was then presented to the University’s postgraduate admissions officer and to the pre-sessional course tutors. Changes were made on the basis of their feedback.   
6. The revised checklist was piloted by the pre-sessional course tutors on three of their students (a strong student, a weak student, and one whose ability fell between these two extremes)—approximately 27 students. On the basis of this pilot procedure, further changes were made.   
7. Taking account of recommendations by Weigle (1994) for rater familiarisation with the adopted rating scale, there was extensive opportunity, at each point during the consultation and piloting phase, for discussion of what the can-do descriptors meant and how they might be applied. Indeed, the raters (the pre-sessional course tutors) contributed to the content, wording and final format of the exit assessment checklist. The details of this consultation are presented in Section 3.1 (below).   
8. We also met with the students in order to introduce the checklist to them as well as to elicit their feedback on the items included. Changes were made on the basis of this feedback.

9. The final form of the exit assessment checklist (Appendix B) was used during the last week of the pre-sessional course. Each student received their report and a copy was sent to the University’s postgraduate admissions officer and the admissions personnel in admitting departments.

3.1. Consulting the stakeholders and piloting the checklist

As has already been stated, the aims of the consultation and piloting phase were threefold:

† To provide some of our stakeholders (the University’s admissions personnel and the presessional course tutors) with an opportunity to comment on the initial draft and suggest changes or additions before the piloting stage (we consulted students during the piloting stage). † To pilot the revised instrument in order to ascertain its usability and coverage. † To maximise the course tutors’ opportunities to familiarise themselves with the checklist and to arrive at shared interpretations of each of the ‘can-do’ statements.

During the consultation and piloting phase, we received valuable feedback in a number of areas:

(i) Usability of the report form—The tutors found the report form quick to complete and, despite some suggestions for changes, they believed that they understood the can-do statements. The University’s Postgraduate Admissions Officer noted that the form provided judgements in four skill areas, commenting that this paralleled the report formats of language proficiency test scores like the IELTS. She thought this was a step forward, arguing that this form was more transparent than its predecessor and that the information was easy to retrieve because it was not presented in prose. She believed that course directors in the academic departments would be able to return to this form at a later date in order to compare the students’ on-going performance in their departments with what was reported at the end of the EAP course.

(ii) Format of the overall report—The tutors initially argued that if a student seemed to be satisfactory in all respects it would be sufficient to present a summary judgement of his/her performance on the course. They suggested that the form for such students should consist of a single page with a box to tick for each of the four skills (listening, speaking, reading and writing). We resisted this suggestion for two reasons. The first was that we needed to be sure (inasmuch as this is possible) that all the tutors shared the same understanding of the construct for each skill area. We felt that if they had to place a tick opposite each aspect listed under each skill they would be obliged to consider all the traits that we considered to be important. The second reason had to do with accountability and, ultimately, face validity: we felt it unlikely that the University would accept our judgements on the adequacy or otherwise of students’ performances unless we demonstrated how we had arrived at our decisions.

Nevertheless, the course tutors prevailed upon us to include a summary judgement page (this became the final page of the exit assessment form). They argued that they would feel more comfortable being “honest” in the checklist if they could also indicate how seriously they viewed a student’s particular difficulties. They would not worry about ticking individual boxes if they could indicate in the summary section that they were “generally satisfied with the student’s performance”. This would allow them to give their students specific feedback and guidance without suggesting that they had not “successfully completed” the course.

(iii) Wording of the judgement—The tutors argued that some of the diplomacy of the original reporting format had been lost and that, in order to correct the apparent baldness of the claims being made, the descriptors needed to be presented within the context in which the judgements took place. The outcome of this discussion was that all the judgements are prefaced with the phrase “Our evidence suggests that.” The tutors also argued that it should be possible to indicate when there was no evidence for a particular judgement. For instance, a skill may have been taught but the student may not have taken advantage of opportunities to demonstrate it. This resulted in the inclusion of the third column, “We have no evidence.” It should be noted at this point that the tutors’ request for this column proved somewhat problematic when the report was used in its final form (see below). Thirdly, in line with the spirit of the new exit assessment, the tutors rejected the formulations “will have difficulty” and even “has difficulty” as they implied a prediction of future performance. They preferred instead to locate their judgements within the period of the course, hence the formulation “has had difficulty”.   
(iv) Evidence available on which to base a judgement—Though the tutors considered the report form easy to use and transparent, they were concerned about the extent to which they could make detailed judgements about the students’ listening and reading abilities. They argued that since the processes involved in listening and reading are internal, they would be reliant on student self-report (which might not be reliable) or on their interpretation of students’ listening and reading abilities based on what they produced when speaking and writing. This problem has not been fully or satisfactorily resolved. Indeed, it remains one of the difficulties in any assessment of receptive skills.

The tutors also argued that students with good language skills might fail to give evidence of certain can-do statements such as strategies like “asking for clarification,” since they might never be in a position where they have misunderstood and have to ask for clarification. The solution we have adopted has been to orientate the students to the criteria by which they are to be judged. When the students register they receive a self-assessment checklist which requires them to make judgements about their language abilities at the start of the course. This checklist replicates the can-do statements used in the exit assessment checklist and has proved a useful consciousness-raising tool particularly since students discuss their self-assessments during their first consultation with their tutor. Nevertheless, we are still concerned that students might fail to fully demonstrate their abilities according to the “can-do” statements and we plan to explore this area in a later study.

# 4. Investigating the validity of the checklist

The final version of the checklist incorporated as much of the feedback as was feasible from the course tutors and the University’s Postgraduate Admissions Officer. It was then used to assess 86 students, all of whom completed the 4-week pre-sessional EAP course. In a further

stage of developing and reviewing this new assessment format, we investigated the validity of the exit assessment in two phases:

† Content relevance and coverage † Interpretation and use of the new instrument

# 4.1. Content relevance and coverage

The aims of this validation phase were to determine where evidence for items on the checklist might be available from assessment opportunities during the programme and what opportunities for collecting evidence were being missed. The first step in this process was to list all the assessment instruments that were used during the course, when they were used, what function they performed, what they consisted of, and where the results of the assessment were kept for reference. A table containing the details of 12 instruments can be found in Appendix C. We distributed this list and copies of all the instruments to a seminar of colleagues working in language testing, and asked them to analyse the instruments to determine whether there were features on the final checklist for which no evidence was being collected during the course and whether there was evidence being collected on the course for which there were no features mentioned in the checklist.

The main conclusion emerging from the discussion was that there was a great deal of assessment on the pre-sessional course, both formal and informal, but the overall system was not integrated. Each instrument had its own function and its own rationale but the instruments did not fit together in a coherent whole. The following types of problems were identified:

† Entry on checklist but no evidence available through instruments—for example, the checklist asks for a judgement concerning the student’s ability to participate in small group discussions, but there is no structured way of gathering this evidence.   
† Evidence available but no entry on checklist—for example, the tutor feedback form for oral presentations includes several features of oral performance which do not appear on the checklist.   
† Terminology: some of the terminology in the checklist and in the instruments was not defined so it was not clear whether they were referring to the same thing—for example, the TEEP (Test of English for Educational Purposes) attribute writing scale, which is used to assess the students’ first piece of writing, contains a criterion called “cohesion”. There is no equivalent term on the checklist. The nearest notion seems to be “Can make appropriate use of heading and subheadings,” but this may have been quite different from what the TEEP designers originally had in mind.   
† What do these things look like in the classroom? The checklist contained several notions such as “Can cope with heavy reading load” and “Can understand gist” but it was not clear what the teachers should look for to confirm whether students were able to do these things.

It was to be expected that there would be inconsistencies within the overall system, given that different instruments and practices were designed at different times in the history of the development of the programme. We have begun to harmonise all of the components so that evidence is available for all the features specifically mentioned in the checklist.

# 4.2. Interpretation and use of the checklist

In the second phase of our validity investigation, we explored how the pre-sessional course tutors used the checklist to make judgements about individual students. Since, the course tutors had been so closely involved in the development of the instrument, we were particularly interested in the extent to which the co-operative development process had benefited them in the way they used it.

Three tutors were selected for individual interviews (out of a teaching team of nine) based on their general teaching experience and their experience of teaching EAP (particularly, the Lancaster University pre-sessional course). One was a teacher with many years of experience in both teaching and course management. He had worked on the teaching team the previous year and had used the previous reporting format. The second was a tutor who had also worked on the teaching team the previous year but who was less experienced. The third tutor selected had no previous EAP teaching experience and was teaching on this course for the first time. She had therefore not used the old reporting format.

Each interview followed a three-part structure. In the first stage, the tutors were asked for their general impressions of the form and the process of using it. They were then shown a blank copy of the form and asked to work through it, pointing out any difficulties they faced in completing it. In the final stage, they were given the report forms they had previously completed for their class. They were asked to select three students—the top student, the bottom student, and one in the middle—and then asked to recount the process they had gone through when making judgements about them.

The interviews revealed three perceived strengths of the report form. First, the tutors were impressed by the amount of time saved in using this report form. The tutor who was teaching on the course for the first time commented that she could not imagine having to write prose reports in addition to the other pressures on tutors’ time. The tutors also appreciated the transparency of the report form. One tutor commented that he did not “have to be creative.” He also argued that it was clearer for departments who no longer had “to wade through . diplomatically couched language to find out what the tutor was really getting at.” Finally, the tutors were in favour of the disclaimer on the front page which delimited the judgements being made to the context of the course.

The interviews also revealed that the tutors were able identify the behaviours on which they had based their judgements and that they had used both the more formal opportunities for monitoring progress (such as essays and oral presentations) and the learning opportunities provided in class (such as group discussions and poster presentations) to gather evidence of students’ language ability. Clearly, they found it easiest to comment on language areas for which they had formal evidence. This was particularly true of writing because, as one tutor commented, “it is the skill we gather the most evidence for”. Listening and reading definitely presented problems, not least because the tutors were unable to observe their students in all the contexts for which they were making judgements. For instance, though the students attended lectures each week (delivered by invited members of academic departments), the tutors responsible for judging their progress were not necessarily present and so did not have an opportunity to observe them.

Where tutors did not have direct access to a particular ability such as “Can understand a variety of non-native speaker accents,” they found that they were able to consult with the students. This is a particularly interesting outcome because, during the consultation and piloting process the tutors had expressed some concern about how much they should rely on student self-assessment. Since, the exit assessment report represented high stakes for the students they might well have described themselves in a way that they thought appropriate in order to successfully complete the course. However, the tutors we interviewed felt that their students had largely been forthright in their self-assessments and individual tutorials. In fact, one tutor commented that the students “were more honest than perhaps we give them credit for.” Indeed many students encouraged their tutors to be “truthful” so that they could set learning goals for the rest of the year.

Nevertheless, a number of issues also emerged from the interviews. Most of them are clearly training issues which need to be addressed as soon as possible:

1. The checklist is criterion-referenced and each student is to be judged against the can-do statements. However, at least one tutor not only considered whether the student had the stated ability or had had difficulty but also tried to rank his students. For example, in the case of the writing item “Can produce grammatically correct text” he ticked that a student had this ability not because she performed well in this area but because he wanted to distinguish her from another good student who did not.   
2. During the consultation and piloting phase (see Section 3.1, above), the tutors specifically requested a column entitled “we have no evidence” to cover cases where students did not demonstrate a particular ability. The purpose of this column was discussed extensively at the time yet, during the implementation of the checklist, at least one tutor agonised over using the “no evidence” column, worrying that it would imply that the student did not have that language ability.   
3. The exit assessment checklist includes a summary judgement on the final page. This was included at the request of the course tutors (see Section 3.1, above). However, the interviews revealed that the route by which the tutors arrived at their summary judgements differed. One tutor said that she had a global sense of whether a student had made satisfactory progress or not and based her judgement on that rather than on the distribution of ticks across the report. Another tutor based her judgement on the number of ticks the student received in the “has had some difficulties” column as well as her judgement of the gravity of these ticks. This last practice is particularly interesting for it introduces a level of detail not explicit in the report form. This has implications for the relationship between the detailed checklist and the summary section of the report form and is again an issue to be dealt with in training.

Though there is still much to be done to help tutors adjust to the new reporting system and to orientate new users of the checklist, the remaining concern relates to how the tutors applied the “can-do” statements. It is important to remember that the tutors had discussed their interpretations of items during the consultation and piloting stage (described earlier) and were therefore familiar with the checklist. Indeed, they had contributed to its wording. Yet, as Lumley (2002) points out, when making their judgements all raters have to reconcile the rating scale (in this case the can-do statements) and their observation and interpretation of student performances. This proved to be an issue for the tutors we interviewed. For instance, one tutor commented on the word “satisfactory” in the statement “can take satisfactory notes.” Since, notes are generally for personal use, this tutor wondered whether she was the best judge of whether a student’s notes were “satisfactory” for the student’s own purposes. Much can be done to discuss with the course tutors the behaviours that constitute evidence of a particular ability (as well as the behaviours that suggest difficulty). However, the tutor is still the ultimate arbitrator and, as Lumley (2002) argues, it is not possible to cover all eventualities during rater training.

# 5. Planning future directions

In this paper, we have presented the rationale for a new assessment instrument and the process we went through to tailor the instrument to our needs and to investigate its validity. Our work so far has identified two areas for immediate action:

1. Integrating continuous assessment with final assessment—Our review of the instruments we use for continuous assessment (Appendix C) clearly shows that there is a need to review and rationalise our system, so that each formative judgement contributes to the summative checklist. This should also provide our students with sufficient opportunities to demonstrate whether they have mastered the abilities in the final checklist. This will, of course, have an impact upon syllabus design.   
2. Training the tutors—Despite our involvement of course tutors in the development of the checklist, it is clear that they still diverged in their interpretation and implementation of the instrument. This problem could be exacerbated in cases where tutors are teaching on the course for the first time. We must prepare materials and procedures that we can use with these tutors, both in our orientation work before teaching begins and during the rest of the course, to make sure that they understand the different meanings behind terms such as ‘coherence’ and to bring all tutor judgements into line. Our investigations so far have identified key elements to be included in the training material.

This study has also revealed a number of longer-term objectives. We are keenly aware that we are still in the early stages of design and that there is work to be done before we can be satisfied that this form of assessment is entirely appropriate for our situation. There are two further challenges which must be dealt with in the near future:

1. Interpretation and use of the instrument by admissions personnel—One of our aims for this checklist was to disambiguate the information that the pre-sessional exit assessment procedure provides to admitting departments. The preliminary feedback that we received (see Section 3.1, above) indicated that we had succeeded in this aim. However, a further phase of validating the checklist should explore the ways in which admissions personnel interpret the results of the checklist and arrive at a decision on whether or not to admit a conditional applicant.   
2. Refining the construct—The checklist that we have designed was based on our understanding of the issues present in the literature on EAP assessment, our own experience both as EAP tutors and as academic lecturers, and the experience of colleagues in other universities in Britain and other parts of the world. Despite the rich resources available, there is still much to learn about the demands placed on students at different levels in the university system (undergraduates and postgraduates) and in all the different departments that our students will be entering. In particular, we need to understand better what aspects of language ability are crucial in determining students’ success in meeting these demands. Investigating this will involve in-depth interviews with academic staff and both native and non-native English speaking students, bearing in mind that students have been found to construct ‘success’ rather differently from their tutors (Basturkmen & Lewis, 2002).

The issues involved in carrying out end-of-course assessment on pre-sessional courses have not been well covered in the literature, yet the assessment that takes place on these courses is high-stakes for the students involved. Failure to meet the standards set could result in the students being refused entry to the university of their choice. Our survey of assessment practices on pre-sessional courses has revealed that EAP professionals are aware of these pressures and the importance of getting it right. We hope that this paper will encourage this work to move into the public domain so that we can all benefit from the insights gained.

NAME: XX XXX

Appendix A. End-of-course profile, English for Academic Purposes course 2001   

<html><body><table><tr><td>Skill</td><td>Comments</td></tr><tr><td>LISTENING</td><td>She did not, to begin with, understand very much of what was said to her, and this probably accounted for the fact that she did not make as much progress as she should have during the first two weeks. However, in week 4 she suddenly</td></tr><tr><td>ORAL PRESENTATION</td><td>appeared to understand more of what was said, and showed more progress Her oral presentation was fine, and no doubt her presentations will get even better with practice. She needs to think carefully though about how much</td></tr><tr><td>GROUP DISCUSSION</td><td>information she can usefully fit on overhead transparencies. She tended to be on the quiet side for the first half of the course but her participation improved in the final week. She knows that she should talk</td></tr><tr><td>READING</td><td>more and says she will do her best to speak more in her undergraduate classes She initially had trouble understanding the materials she was supposed to read, but she is using better reading strategies and her reading is now getting quicker and better focussed</td></tr><tr><td>WRITING</td><td>For the first two weeks I did not see any improvement, but in the third week, when she word-processed her essay, there seemed to be a great improvement. She did not hesitate to give her own views, and she succeeded in making it clear that these were indeed her own views.</td></tr></table></body></html>

Additional comments:

XXX’s problem at the moment is mainly linguistic. She was a very conscientious student, but she had such difficulties with English syntax and vocabulary that she was not able, until week 4 of the course, to demonstrate her improvement in study skills. However, in this final week her comprehension suddenly improved; she spoke more and appeared to be more confident.

Academic tutor course. Course Co-ordinator’s signature. Date: 1st October 2001

Appendix B. Final assessment checklist (the full version is available at http://www.ling. lancs.ac.uk/groups/ltrg/docs/ssreport.pdf)   

<html><body><table><tr><td rowspan="2"></td><td rowspan="2"></td><td colspan="2">Our evidence suggests that the student</td><td rowspan="2">We have no evidence</td></tr><tr><td>Has this ability</td><td>Has had some difficulty</td></tr><tr><td rowspan="6">Listening</td><td>Speed</td><td></td><td></td><td></td></tr><tr><td>Can respond quickly to what is said to him/her</td><td></td><td></td><td></td></tr><tr><td>Note-taking</td><td></td><td></td><td></td></tr><tr><td>Can understand the gist of lectures.</td><td></td><td></td><td></td></tr><tr><td>Can understand the gist of seminar discussions Can identify the main points in a lecture.</td><td></td><td></td><td></td></tr><tr><td>Can identify the main points in a seminar discussion</td><td></td><td></td><td>(continued on next pag.</td></tr></table></body></html>

<html><body><table><tr><td>Our evidence suggests that the student.</td><td>We have no. evidence</td></tr><tr><td>Has this Has had</td><td></td></tr><tr><td>ability some</td><td></td></tr><tr><td>difficulty</td><td></td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Speaking</td><td colspan="3">Can take saislactory notes Varieties of English Can understand classmates who speak quickly Can understand a variety of native speaker accents Can understand a variety of non-native speaker accents Speed Can process the input and express his/her thoughts quickly Participation</td></tr><tr><td>Can participate in large group discussions (e.g. plenary sessions) Can participate in small group discussions Can participate in informal group discussions (e.g. a social event) Can participate in one-to-one discussions (pairs or with a tutor) Can make major contributions to group discussions Can express own point of view effectively Can debate the views of other group members</td><td></td></tr><tr><td colspan="3">Presentations Can select appropriate content Can establish rapport with the audience (e.g. eye contact) Can handle questions from the audience Strategies Can ask for clarification Can use turn-taking strategies to participate in debates/discussions Quality of language</td></tr><tr><td colspan="3">Can be easily understood (pronunciation) Can use appropriate vocabulary Can speak with few/no grammar mistakes Written skills</td></tr><tr><td colspan="3"></td></tr><tr><td colspan="3">Reading Speed Can skim texts for gist Can scan texts to locate specific information</td></tr></table></body></html>

Our evidence suggests that the We have no   

<html><body><table><tr><td rowspan="2"></td><td colspan="2">0 bu88obto tn student</td><td rowspan="2">evidence</td></tr><tr><td>Has this ability</td><td>Has had some. difficulty</td></tr><tr><td>Can understand visual displays of data and/or results of data analyses (tables, charts, diagrams)</td><td></td><td></td><td></td></tr><tr><td>Critical reading Can analyse argumentation in academic texts</td><td></td><td></td><td></td></tr><tr><td>Can evaluate argumentation in academic texts Can support or refute argumentation in</td><td></td><td></td><td></td></tr><tr><td>academic texts</td><td></td><td></td><td></td></tr><tr><td>Content</td><td></td><td></td><td></td></tr><tr><td>Can analyse the topic of the assignment</td><td></td><td></td><td></td></tr><tr><td>Can produce relevant content Can reproduce others&#x27; ideas using my own</td><td></td><td></td><td></td></tr><tr><td>words</td><td></td><td></td><td></td></tr><tr><td>Can write concisely</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Can respond critically to input material</td><td></td><td></td><td></td></tr><tr><td>(lectures, texts, discussions)</td><td></td><td></td><td></td></tr><tr><td>Can produce well-supported argumentation</td><td></td><td></td><td></td></tr><tr><td>Can produce effective conclusions</td><td></td><td></td><td></td></tr><tr><td>Organisation</td><td></td><td></td><td></td></tr><tr><td>Can write informative introductions</td><td></td><td></td><td></td></tr><tr><td>Can organise texts on the macro-level</td><td></td><td></td><td></td></tr><tr><td>Can make appropriate use of headings and</td><td></td><td></td><td></td></tr><tr><td>sub-headings</td><td></td><td></td><td></td></tr><tr><td>Can use paragraphs</td><td></td><td></td><td></td></tr><tr><td>Acknowledging sourcese Can use proper conventions to acknowledge</td><td></td><td></td><td></td></tr><tr><td>sources</td><td></td><td></td><td></td></tr><tr><td>When paraphrasing</td><td></td><td></td><td></td></tr><tr><td>When quoting verbatim</td><td></td><td></td><td></td></tr><tr><td>Can construct bibliography correctly</td><td></td><td></td><td></td></tr><tr><td>Language</td><td></td><td></td><td></td></tr><tr><td>Can produce grammatically correct text</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Can select appropriate vocabulary</td><td></td><td></td><td></td></tr><tr><td>Can use the appropriate academic style</td><td></td><td></td><td></td></tr><tr><td>Can use spell-checker</td><td></td><td></td><td></td></tr><tr><td>Can use appropriate punctuation</td><td></td><td></td><td></td></tr></table></body></html>

Appendix C. Inventory of course assessment instruments   

<html><body><table><tr><td>Instrument</td><td>When used</td><td>Function</td><td>Descriptione</td><td>Where kept</td></tr><tr><td>Initial grammar test</td><td>Day 1</td><td>Grouping, diagnosis</td><td>Grammar in context--mcq, short answer, gap-filling,.</td><td>Tutor&#x27;s file.</td></tr><tr><td>Initial listening test</td><td>Day 1</td><td>Grouping,</td><td>re-ordering TEEP-style *communicative</td><td>Tutor&#x27;s file</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td> diagnosis</td><td>dictation&#x27;-understanding of</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>key point</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>(continued on next page).</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

<html><body><table><tr><td>Instrument</td><td>When used</td><td>Function</td><td>Description</td><td>Where kept</td></tr><tr><td>Initial writing test</td><td>Day 1</td><td>Grouping, diagnosis</td><td>TEEP writing task-read 12-page article and write a summary</td><td>Tutor&#x27;s file</td></tr><tr><td>Criteria for marking writing</td><td>Day 1</td><td>Grouping, diagnosis</td><td>TEEP attribute writing scale (adapted)-content, organis- ation, cohesion, vocabulary, grammar, punctuation, spel-</td><td>Tutor&#x27;s file</td></tr><tr><td>Self-evaluation questionnaire</td><td>Week 1</td><td>Awareness- raising and goal-setting</td><td>ling, lack of plagiarism Five pages-questions on abilities in all skill areas, short composition</td><td>Tutor&#x27;s file</td></tr><tr><td>Tutorial form</td><td>Weeks 1-4</td><td>Record- keeping</td><td>Three pages-personal details, summary of self-evaluation, test results, student&#x27;s reflec- tions/concerns re classes, tutor&#x27;s evaluation/feedback re</td><td>Tutor&#x27;s file</td></tr><tr><td>Reading-review of critical reading and discussion</td><td>Week 4</td><td>Self- evaluation</td><td>all skills areas Student&#x27;s record of prep- aration for class, notes on important points, thoughts re</td><td>Student keeps only copy. Tutor never sees</td></tr><tr><td>Writing-Guidance to tutors on marking written assignments</td><td>Weeks 2-4</td><td>Focusing feedback</td><td>future action Features to comment on when marking each weekly assignment</td><td>Tutor&#x27;s file</td></tr><tr><td>Writing-feedback form</td><td>Weeks 2-4</td><td>Feedback to student on each writing assignment</td><td>Hamp-lyons formative feed- back profile-communicative quality, ideas and organis- ation, grammar and vocabu-</td><td>Student keeps one copy, and one kept in tutor&#x27;s file</td></tr><tr><td>Oral Presentations- tutor feedback form</td><td>Weeks 1-4</td><td>Feedback to student on each presentation</td><td>lary, surface features Short comments on content, organisation, vocabulary, grammar, pronunciation and other features related to public</td><td>Student keeps one copy, and one kept in tutor&#x27;s file</td></tr><tr><td>Oral presentations- peer feedback from</td><td>Weeks 1-4</td><td>Feedback from peers on each presentation</td><td>speaking Adapted from Lynch and Anderson (1992). Tick boxes to evaluate 15 aspects of</td><td>Student keeps all copies. Tutor never sees</td></tr><tr><td>Seminar discussion- self-evaluation</td><td>Weeks 2-4</td><td>Reflection, goal-setting</td><td>speaking Nine questions re student&#x27;s participation in seminar</td><td>Student keeps copy. Tutor never sees</td></tr></table></body></html>

# References

Banerjee, J. V. (2003). Interpreting and using proficiency test scores. Unpublished doctoral dissertation. Lancaster, UK: Lancaster University.   
Basturkmen, H., & Lewis, M. (2002). Learner perspectives of success in an EAP writing course. Assessing Writing, 8(1), 31–46.   
Bejar, I., Douglas, D., Jamieson, J., Nissan, S., & Turner, J. (2000). TOEFL 2000 speaking framework: A working paper. TOEFL Research Monograph Series MS-19. Princeton, NJ: Educational Testing Service.   
Blue, G. M., Milton, J., & Saville, J. (2000). Assessing English for academic purposes. Bern: Peter Lang, AG.   
Butler, F. A., Eignor, D., Jones, S., McNamara, T., & Suomi, B. K. (2000). TOEFL 2000 speaking framework: A working paper. TOEFL Research Monograph Series MS-20. Princeton, NJ: Educational Testing Service.   
Canesco, G., & Byrd, P. (1989). Writing required in graduate courses in business administration. TESOL Quarterly, 23(2), 305–316.   
Carroll, B. J., & West, R. (1989). ESU framework: Performance scales for English language examinations. Harlow, Essex: Longman.   
Cho, Y. (2003). Assessing writing: Are we bound by only one method? Assessing Writing, 8(3), 165–191.   
Cumming, A., Kantor, R., Powers, D., Santos, T., & Taylor, C. (2000). TOEFL 2000 writing framework: A working paper. TOEFL research monograph series MS-18. Princeton, NJ: Educational Testing Service.   
Enright, M., Grabe, W., Koda, K., Mosenthal, P., Mulcahy-Ernt, P., & Schedl, M. (2000). TOEFL 2000 reading framework: A working paper. TOEFL research monograph series MS-17. Princeton, NJ: Educational Testing Service.   
Errey, L. (2000). Stacking the decks: What does it take to satisfy academic readers’ requirements?. In G. M. Blue, J. Milton, & J. Saville (Eds.), Assessing English for academic purposes (pp. 147–168). Bern: Peter Lang, AG.   
Geoghegan, G. (1983). Non-native speakers of English at Cambridge University. Cambridge: The Bell Educational Trust in association with Wolfson College.   
Ginther, A., & Grant, L. (1996). A review of the academic needs of native English-speaking college students in the United States Research monograph series MS-1. Princeton, NJ: Educational Testing Service.   
Hamp-Lyons, L., & Kroll, B. (1997). TOEFL 2000: Writing: Composition, community and assessment. Princeton, NJ: Educational Testing Service.   
Hartill, J. (2000). Assessing postgraduates in the real world. In G. M. Blue, J. Milton, & J. Saville (Eds.), Assessing English for academic purposes (pp. 117–130). Bern: Peter Lang, AG.   
Horowitz, D. M. (1986). What professors actually require: Academic tasks for the ESL classroom. TESOL Quarterly, 20(3), 445–462.   
Jacoby, S., & McNamara, T. (1999). Locating competence. English for Specific Purposes, 18(3), 213–241.   
Jamieson, J., Jones, S., Kirsch, I., Mosenthal, P., & Taylor, C. (2000). TOEFL 2000 framework: A working paper. TOEFL research monograph series RM-00-3. Princeton, NJ: Educational Testing Service.   
Lumley, T. (2002). Assessment criteria in a large-scale writing test: What do they really mean to the raters? Language Testing, 19(3), 246–276.   
Moore, T., & Morton, J. (2005). Dimensions of difference: A comparison of university writing and IELTS writing. Journal of English for Academic Purposes, 4(1), 43–66.   
Thorp, D., & Kennedy, C. (2003, April). What makes a ‘good’ IELTS answer in academic writing?. Paper presented at the British Association of Lecturers in English for Academic Purposes (BALEAP) Conference, Southampton, UK.   
Wall, D. (1981). A pre-sessional academic writing course for postgraduate students in Economics. In G. Sealy (Vol. Ed.), Practical papers in English language education: Vol. 4 (pp. 31–105). Lancaster: Institute for English Language Education, Lancaster University.   
Wall, D., Nickson, A., Jordan, R. R., Allwright, J., & Houghton, D. (1988). Developing student writing: A subject tutor and writing tutors compare points of view. In P. Robinson (Ed.), Academic writing: Process and product (pp. 117– 129). London: Modern English Publications in association with The British Council.   
Waters, A. (1996). A review of research into needs in English for academic purposes of relevance to the North American higher education context. TOEFL research monograph series MS-6. Princeton, NJ: Educational Testing Service.   
Weigle, S. C. (1994). Effects of training on raters of ESL compositions. Language Testing, 11(2), 197–223.   
Weir, C. (1983). Identifying the language needs of overseas students in tertiary education in the United Kingdom. Unpublished doctoral dissertation, University of London, London, UK.

Dr Jayanti Banerjee is a Lecturer at Lancaster University. She has published in Language Teaching and has also contributed chapters to edited collections such as C. Elder et al. (eds.), Experimenting with uncertainty: Essays in honour of Alan Davies, Cambridge: CUP. Her main interests are Language Testing and Assessment and English for Academic Purposes.

Dr Dianne Wall is a Lecturer at Lancaster University. She is co-author (with Charles Alderson and Caroline Clapham) of Language Test Construction and Evaluation and author of The Impact of High-Stakes Testing on Classroom Teaching (both published by Cambridge University Press). She has published articles in Language Testing, Applied Linguistics and System.