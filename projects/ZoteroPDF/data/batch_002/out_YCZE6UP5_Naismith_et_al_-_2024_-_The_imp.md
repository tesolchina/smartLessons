# The impact of task duration on the scoring of independent writing responses of adult L2-English writers

Ben Naismith \*,1 , Yigal Attali 2 , Geoffrey T. LaFlair 3

Duolingo Inc, USA

# A R T I C L E I N F O

# A B S T R A C T

Keywords: Task duration Writing assessment Test-retest reliability Criterion validity

In writing assessment, there is inherently a tension between authenticity and practicality: tasks with longer durations may more closely reflect real-life writing processes but are less feasible to administer and score. What is more, given total testing time, there is necessarily a trade-off be tween task duration and number of tasks. Traditionally, high-stakes assessments have managed this trade-off by administering one or two writing tasks each test, allowing 20–40 minutes per task. However, research on second language (L2) English writing has not found longer task du rations to significantly improve score validity or reliability. Importantly, very few studies have compared much shorter durations for writing tasks to more traditional allotments. To explore this issue, we asked adult L2-English test takers to respond to two writing prompts with either 5-min ute or 20-minute time limits. Responses were then evaluated by expert human raters and an automated writing evaluation tool. Regardless of scoring method, short duration scores evidenced equally high test-retest reliability and criterion validity as long duration scores. As expected, longer task duration yielded higher scores, but regardless of duration, test takers demonstrated the entire spectrum of writing proficiency. Implications for writing assessment are discussed in relation to scoring practices and task design.

# 1. Introduction

On high-stakes English language proficiency (ELP) exams, test takers are typically asked to complete one or two writing tasks, each of which takes 20–40 minutes. These tasks are intended to provide test takers with the opportunity to write academic text types, often persuasive essays, while employing relevant cognitive processes (Cushing & Ren, 2022; R´ev´esv et al., 2017). Test developers assume that these durations allow test takers to produce texts containing features of different writing subconstructs that are relevant to a target language use (TLU) domain (e.g., lexical/syntactic complexity, cohesion, accuracy; Banerjee et al., 2007). The extent to which such tasks correspond to their intended counterparts in the TLU domain determines the degree of task authenticity (Bachman & Palmer, 2010).

For writing tasks of this nature, time limits are important for reasons of standardization and practicality (Bridgeman, 2020; Jurich, 2020; Kane, 2020; Powers & Fowles, 1996). Standardization ensures fairness by creating an assessment experience that is as similar as possible for all test takers. With respect to practicality, whatever the length of a test, time is of the essence, so it is imperative to gather information about test-taker proficiency as efficiently as possible. Longer durations for a single writing task limit the number of total tasks that can be included in a single assessment. For test takers, the longer the test, the more strenuous it is, which may impact individuals to different extents (Ackerman et al., 2010; Ackerman & Kanfer, 2009) and worsen the test-taker experience (Burstein et al., 2022). For writing raters, longer texts may also create greater cognitive load. With these considerations in mind, it remains unclear whether traditionally allocated time limits for writing tasks are in fact ideal from an assessment perspective, and there does not seem to be a clear, empirical motivation for traditional writing task durations.

“Examination writing” has long received criticism for its lack of authenticity (Braddock et al., 1963; Caudery, 1990; Cooper, 1984) because test takers may not have time to plan or revise, or they may not have access to common writing tools and resources. For example, in real academic contexts, students typically do not write essays in 40 minutes under test conditions, and they are often required to write texts for a range of rhetorical purposes (Moore & Morton, 1999). But what then is the optimal task duration that balances authenticity and practicality (Breland et al., 1999)?

To answer this question, it is critical that task durations be evaluated to ensure that they are as short as possible without introducing construct-irrelevant factors (e.g., typing speed) which would negatively affect the overall test validity (Bridgeman, 2020; Jurich, 2020; Kane, 2020; Lee et al., 2021). Therefore, to help test developers and researchers make informed decisions about writing task duration, this paper reports findings from a study investigating the effect of task duration on the ratings of writing quality awarded to responses of adult second language (L2) English test takers, focusing on under-researched task durations and scoring methods.

# 2. Previous research on task duration

The potential for time limits to impact test performance is not a novel issue (e.g., Yerkes, 1921). However, the effects of task duration on performance assessments, like essay writing, have been researched to a much lesser extent compared to other closed-ended response types (Margolis et al., 2020). In this section, we consider the relatively small number of studies which investigate the effects of task duration on writing scores, score reliability, and score validity. We also briefly describe typical differences between human and machine scoring.

Primarily, research on task duration has focused on independent writing tasks, which are designed to evaluate a test taker’s inherent writing ability, with the writer relying solely on their existing knowledge and experience (Crossley et al., 2014; Guo et al., 2013). Independent writing is also considered to be a complex performance task, suitable for assessment due to its authenticity as a real-world activity used for communicative purposes (Enright & Quinlan, 2010), despite the previously noted criticisms. Furthermore, such tasks have been shown to elicit construct-relevant writing processes, including planning, evaluating, and revising, which vary across proficiency levels (Barkaoui, 2011). In part, these attributes may be why this type of assessment is the most researched (Weigle, 2002) and why direct testing of writing is considered to have high face, construct, and content validity (Hamp-Lyons, 1990).

# 2.1. Impact of task duration on writing performance

A small number of studies across different contexts have compared the quality of timed, in-class writing tasks (30–60 minutes) to untimed, often at-home writing tasks (multiple days). As might be expected, in general, adult L2-English learners produce higher quality texts when they have unlimited time and access to resources, though even these findings are somewhat mixed. Holistic scores were higher for untimed writing in some cases (Kenworthy, 2006; Khuder & Harwood, 2015; Shin, 2011), though not all (Caudery, 1990; Kroll, 1990). Many of these studies also rated specific writing subconstructs, finding gains for “language” (Caudery, 1990; Kenworthy, 2006; Khuder & Harwood, 2015; Shin, 2011), “content” (Khuder & Harwood, 2015; Shin, 2011), “mechanics/spelling” (Kenworthy, 2006; Shin, 2011), and “organization” (Khuder & Harwood, 2015), but not for “grammar” (Khuder & Harwood, 2015; Kroll, 1990). As a whole, this body of research indicates that even when the differences between task durations are extreme, performance outcomes are not guaranteed to meaningfully increase in aggregate.

In contrast to timed versus untimed task durations, more research has focused on comparing different timed task durations, whether in the classroom or in a test setting (as in the current study). The findings of this body of work nevertheless mirror the same patterns as seen for timed versus untimed tasks: holistic writing scores typically, but not always, increase when adult L2-English students/test takers are provided with additional time (Bridgeman, 2020). For example, Hale (1992) found that the scores of inter national students $( n = 8 2 0 )$ on a mock Test of Written English (TWE) were ${ \sim } 1 / 3$ standard deviations (SD) higher for the 45-minute condition than the 30-minute condition. Under these same task durations, Lee et al. (2021) also observed significantly higher scores in the longer condition in every analytic rating category, especially for “rhetoric” (effect size $d = 1 . 1 9 _ { \cdot }$ ), for advanced language learners (n $= 3 2$ ) completing the writing section of the Michigan Language Assessment ECPE test.

Similar results have been found in other, non-ELP, assessment contexts with adults. In low-stakes contexts, Powers and Fowles (1996) allotted students $\left( n = 3 0 4 \right)$ ) taking a practice GRE test either 40 or 60 minutes to complete the writing section; the mean scores for responses in the longer condition were 0.9 higher (effect size $= 0 . 4 3$ ). Biola (1982) compared the essays of first-year students in a US university $( n = 9 6 )$ ) written in either 45 or 120 minutes. Again, the longer condition led to significantly higher scores (no other details reported). In high-stakes contexts, Younkin (1986) analyzed the effects of additional time allowances of 10 or 20 minutes for test takers $( n = 6 5 1$ ) taking the College Level Academic Skills Test, showing no significant difference between the $+ 1 0$ and $+ 2 0$ -minute conditions, but a significant increase between the base time and combined additional time conditions $( p < 0 . 0 0 1$ , $t = - 3 . 6 9 3 $ ). Klein (1981) studied test takers $( n = 1 7 2 9 )$ ) completing an optional additional section to the California Bar Exam consisting of two legal essays in either 55 or 90 minutes, with a small increase in mean scores reported.

Counter to the above results, other task duration studies (investigating both L1- and L2-English adults) did not find significant increases in performance with longer, timed durations. Knoch and Elder (2010) compared 30-minute and 55-minute conditions for first-year L2-English students $\left( n = 3 0 \right)$ ) completing an essay task for the Australian Diagnostic English Language Assessment, resulting in no significant differences between mean scores. Livingston (1987) analyzed essays written by test takers $( n = 5 1 2$ ) for the writing component of the New Jersey College Basic Skills Placement Test in 20 minutes, 30 minutes, and 20 minutes $^ { + 1 0 }$ minutes planning time. The author found no significant score increase for the longer duration conditions other than an interaction of topic and duration for mid- and high-ability test takers.

In all of the above studies, task durations have been within a range of 20–60 minutes writing time. A few other investigations of adult L2-English writers have been conducted with more varied task durations: Wu and Erlam (2016) timed the participants $( n = 2 3$ ) for the first essay and then allowed $7 0 \%$ of that time for the second essay (no significant difference in task achievement, accuracy, or complexity); Cho (2003) compared international graduate students’ $( n = 2 0 9$ ) 70-minute individual essays and 6-hour workshopped essays (higher writing quality for the longer condition); and Polio et al. (1998) compared essays written in 30 minutes to a subsequent draft of that same writing where students $( n = 6 4 )$ ) had 60 minutes for revision (small improvements in linguistic accuracy).

However, only three studies, to our knowledge, have examined open-response writing task durations of less than 20 minutes. In Davis and Norris (2023), TOEFL iBT’s new 10-minute “Writing for an Academic Discussion” task was compared to the 30-minute independent writing task that it was replacing (adult and teenager L2-English test takers). No meaningful difference in mean scores was found, and score distributions for the two tasks were similar, despite the differences in task duration and design. The linguistic features of responses in the two conditions were likewise very comparable. Also considering 10-minute durations, Lovett et al. (2010) asked US college students $( n = 1 4 0$ , L1s unknown) to write essays in 10 minutes or 15 minutes, either by hand or with a word pro cessor. There was no effect on writing quality though the typed responses were longer, especially in the longer duration condition. Focusing on even shorter durations, Lee and Son (2023) looked at fluency, complexity, and sophistication measures for Japanese university students $( n = 8 1$ ) completing TOEIC writing tasks with durations from 2.5 minutes to 5 minutes, increasing in 30 second increments. Generally, there were no linear increases with longer durations regardless of the task type other than a continual increase in fluency (i.e., number of words) for the argumentative tasks.

In addition to the effect of duration, task effects were observed in Klein (1981) and Livingston (1987), but not Biola (1982), indicating the need to monitor and potentially control for this variable. Overall, the above body of work on the relationship between task duration and performance is mixed, with comparison across studies made challenging by the differences in duration conditions, sample sizes, assessment stakes, and the tasks (prompts) themselves. Similarly mixed findings have been found in task duration studies focusing on young learner writing. This research is less relevant to the current study, but see Crawford et al. (2004) for an example and review of other studies in this area since testing for task duration effects may have different outcomes for younger students. For a more detailed review of timing considerations for performance assessments, including essay writing, see Margolis et al. (2020).

# 2.2. Impact of task duration on score reliability and validity

Compared to research on the relationship of task duration and writing performance, much less attention has been devoted to the impact of task duration on score reliability and validity. Hale (1992) made a comment to this effect 30 years ago, but little has changed in the interim. Nevertheless, understanding how these variables interact is essential as “time constraints need to be consistent with the intended interpretation and use of the scores” (Kane, 2020, p. 30).

# 2.2.1. Reliability

An important consideration is whether different task durations produce equally reliable scores. That is, if a test taker were to produce two pieces of writing with duration held constant, would these two responses receive similar scores? The general consensus is that score reliability is not greatly impacted by task duration, and if there is an effect, it is actually shorter durations which produce more reliable results. For example, part of Hale’s (1992) study had test takers write two essays in the same condition (30 minutes or 45 minutes). He found no significant difference in the rankings of essays, with test-retest reliability measures of 0.77 in the shorter condition and 0.75 in the longer condition. Likewise, in comparing test-taker rankings, Knoch and Elder (2010) found slightly higher, though not significant, correlations for the reliability of the rater judgments of shorter task conditions (0.93 for 30-minute condition, 0.89 for 55-minute condition). Other studies with no significant difference in reliability measures include Klein (1981), Lee et al. (2021), and Powers and Fowles (1996).

If reliability does not benefit from longer time limits, one implication is that using shorter tasks can greatly improve the gener alizability of scores. A common finding of generalizability studies of production for both independent and integrated writing tasks is that the variance component of the person-by-task interaction is large (Brennan et al., 1995; Brown & Bailey, 1984; Gebril, 2009, 2010; Lee & Kantor, 2005; Schoonen, 2005; Shavelson et al., 1993). In other words, scores may vary for reasons such as a test taker’s fa miliarity with topic or task type (In’nami & Koizumi, 2016), suggesting that reliability considerably benefits from multiple tasks per test taker. Therefore, if total testing time is a constant (say, 40 minutes), it seems plausible that measurement of writing would benefit from administering several prompts within this time limit (e.g., four 10-minute prompts).

# 2.2.2. Validity

Two notable examples of duration studies that looked at aspects of validity are Powers and Fowles (1996) and Knoch and Elder (2010). As mentioned previously, Powers and Fowles (1996) compared 40-minute and 60-minute duration conditions for a GRE writing test. In terms of validity, the authors calculated correlations of the test scores with various external measures of writing proficiency (i.e., criterion validity), namely test-taker self-assessments, independent writing sample grades, and grades for courses that required writing. These correlations were not significantly impacted by the difference in task duration.

In Knoch and Elder (2010), after finding no significant mean score differences, the researchers conducted subsequent discourse analysis of the texts written in 30-minute and 55-minute conditions. They found that test takers in the longer condition wrote longer texts and made more self-corrections, whereas test takers in the shorter condition had higher grammatical complexity and use of discourse markers. There were no significant differences in lexical complexity measures. These findings suggest that there may be differences in certain aspects of writing, but that these differences do not necessarily impact rater perceptions to a great extent.

Another aspect of validity is the potential for task durations to differentially impact subgroups of test takers, thereby reducing test fairness (Kane, 1982). These subgroups can be classified in terms of background characteristics such as educational status, age, race, and gender (Hale, 1992; Klein, 1981; Younkin, 1986), proficiency level (Hale, 1992; Klein, 1981), and writing speed (Powers & Fowles, 1996). In this regard, the different task durations have consistently not been found to favor different subgroups of writers. One non-adult exception is for young learners with special needs who do benefit from additional time (Crawford et al., 2004).

Face validity of task durations has also been previously investigated, operationalized as the extent to which test takers perceive the task duration to be adequate. Using questionnaires, various studies have found that writers believe longer durations to be useful (Hale, 1992; Knoch & Elder, 2010; Lee et al., 2021), allowing them to better reflect their competence (Kroll, 1990; Polio & Glew, 1996; Powers & Fowles, 1996; Shin, 2011). However, these perceptions do not significantly interact with writing scores (Hale, 1992; Knoch & Elder, 2010), indicating a mismatch between task duration preferences and necessities when it comes to developing valid writing assessments.

The concept of task duration can also be framed on the continuum of speeded and power tests (Jurich, 2020; Kane, 2020), with implications for the validity of score interpretation and use. Typically in speeded tests, time is part of the construct, as in a test of typing speed; however, most educational tests are power tests, meaning that performances are interpreted with respect to what test takers can do (in a reasonable amount of time; Kane, 2020). While most high-stakes standardized assessments have an element of speededness, the performances on these tests are intended to reflect test-taker ability to complete the task at a certain level of proficiency (Powers & Fowles, 1996).

# 3. Comparing human scoring to machine scoring

All of the previously discussed studies have one important commonality: they employed expert human raters to assess writing quality, whether through holistic scales (e.g., Hale, 1992; Kenworthy, 2006) or analytic scales (e.g., Knoch & Elder, 2010; Lee et al., 2021). While the use of human raters for scoring remains common, it is not without its drawbacks. Human raters have variable severity/leniency and differ in how they weight and interpret aspects of criteria (e.g., Eckes, 2012; Goh & Ang-Aw, 2018; Vaughan, 1991; Zhang & Elder, 2014), all of which can affect the validity, reliability, and accuracy of writing scores (Attali, 2016; Huang, 2012). For example, Breland et al. (1987) had adult L1-English students respond (in a low-stakes setting) to six prompts in three modes of writing, and each essay was rated by three highly experienced raters. Their reliability estimate for a single essay scored by one rater was 0.42. Attali (2012) reports that the cross-task reliability coefficient (the correlation between the two writing tasks) for a single human rater for GRE and TOEFL (adult L2-English writers) is 0.56 and 0.51, respectively. Breland et al. (1999) summarizes the evidence thus: “Writing assessments based on single essays, even those read and scored twice, have extremely low reliability—usually less than. 60.” Possibly as a consequence of this low reliability, many writing assessments are composed of (at least) two writing tasks.

Reported validity coefficients of essay writing assessments by humans are also generally low. Breland et al. (1999) reviewed earlier predictive validity studies for scores on a single essay. They report median correlations with English course grades of 0.36, with writing performance of 0.49, and with writing self-assessment of 0.24. Analyzing performance on a GRE writing test, Powers and Fowles (1996) found correlations of 0.12–0.19 with scores of academic course-related writing samples, and correlations of 0.30–0.37 with grades in academic courses requiring writing. Norris et al. (2006) reported a correlation of 0.24 between an SAT writing section, composed of an essay writing task and multiple-choice questions, and first-year English composition grade point average (GPA). Analyzing performance on TOEFL independent tasks, Weigle (2010) found correlations of 0.36 with writing self-assessment, 0.20 with instructor ratings of English writing ability, and 0.36 with scores of academic writing samples. Similarly, for performance on TOEFL independent and integrated writing tasks, Biber et al. (2017) obtained a median of 0.35 for correlations with a variety of scores of academic papers. Finally, Llosa and Malone (2019) compared performance on TOEFL writing tasks with instructor-assigned grades on two course assignments and found correlations of 0.16 (non-significant) for the independent task and 0.26 for the integrated task. These low validity coefficients may be driven by a multitude of reasons, including differences between how writing is assessed in the predictor and criterion, the amount of time elapsed between them, differing stakes, and low reliability.

In recent years, high-stakes ELP tests have also adopted tools for automated writing evaluation (AWE; e.g., Burstein et al., 2013; Cardwell et al. 2023), though there remain concerns, for example whether the same construct is truly being assessed (Deane, 2013) or how to best address the oversized impact of length-based features (Ben-Simon & Bennett, 2007). To date, no task duration studies have investigated the mediating variable of scoring method, though an extensive body of research has compared these two scoring methods more generally (Attali, 2013; Chen et al., 2022). For example, Attali (2012) found a much higher cross task reliability coefficient for AWE (with the e-rater system) for GRE and TOEFL, 0.75 and 0.70 respectively. Research has also found that in many cases the cor relation between a human rating and AWE scores of the same response are at least as high as the correlation between two human ratings of the same response (Page, 1966; Ramineni et. el, 2012). Finally, only one study (Weigle, 2010) compared the validity co efficients of human and machine (e-rater) scoring and found no differences between the two methods with respect to writing self-assessment, instructor ratings of English writing ability, or scores of academic writing samples.

# 4. Current study

The review above paints a fairly consistent picture of the effects of task duration on writing performance of both L1-English and L2- English adult writers: more time leads to increases in holistic scores for all subgroups, but not to increases in score reliability or validity (when these measures are included in the research design). This body of research has focused on task durations typically of 30–60 minutes (when timed), always with human scoring, and often without consideration of reliability and validity measures. To address these gaps, we conducted a research study investigating the effect of task duration on the scores awarded to those responses of adult L2-English writers.

In brief, participants were randomly assigned to one of two task duration conditions (5 minutes or 20 minutes) when completing an independent writing task. Subsequently, participants completed an additional writing task in response to a new prompt under that same duration condition (i.e., a between-subjects design). This repeater data was then used to calculate performance, reliability, and validity measures. All responses were scored by expert human raters and an AWE tool. To our knowledge, this is the first study of its kind explicitly targeting the difference between task durations in the range of 5–20 minutes, as well as the first to investigate the interaction of task duration and scoring method (human vs. automated). Specifically, we addressed the following research questions regarding adult L2-English writers:

1. Does writing performance improve when test takers are given more time to complete the task?   
2. Does the test-retest reliability of writing scores improve when test takers are given more time to complete the task?   
3. Does the criterion validity of writing scores improve when test takers are given more time to complete the task?   
4. Do these aspects of performance, reliability, and validity vary by scoring method (i.e., human scoring vs. machine scoring)?   
5. What aspects of writing did raters attend to across the two conditions and how were their ratings affected by task expectations?

Question 1 allows us to situate the current study in the research reviewed with respect to raw test scores. For questions 2 and 3, if the longer task duration results in significantly higher reliability and validity, it could suggest that longer tasks are worth the additional time. However, a lack of significant differences would suggest that the shorter duration provides better “bang for the buck”, maximizing the available time for assessment, especially when we consider how multiple short tasks can be administered in the time needed for one long task. Question 4 then drills down one level deeper by introducing a mediating variable – scoring method, and question 5 explores rater cognition in relation to their rating decisions and task duration. If human raters and machine scoring show different rating patterns for different task durations, these differences must then be explained and accounted for.

# 5. Methods

# 5.1. Data

The context of this study is the Duolingo English Test (DET) practice test. The certified DET is an online, high-stakes test of ELP for communication and use in English-medium settings, primarily used for making higher education admissions decisions (Cardwell et al., 2023). As part of their readiness materials, the DET offers a practice test that is a freely available, shortened, unproctored version of the DET; this practice test is completed over 10,000 times daily. The current study relied on a piloting platform that was developed as part of the practice test whereby test takers can opt in at the end of the practice test to complete additional experimental tasks (such as the ones in this study). Around $5 0 \%$ of practice test takers choose to complete these additional experimental tasks.

# 5.1.1. Responses

For the study design, data is needed from repeaters – test takers who complete the experimental task twice with the same task duration each time. To collect these data, test takers were therefore randomly assigned to one of two time conditions, and if they completed the practice test for a second time, they received the same duration condition again but with a different prompt. The study was open for 11 days, but most participants completed the two writing tasks within one day. A total of 3366 test takers met this criterion and also completed the certified DET, providing us with an additional measure of proficiency. Of the 3366 test takers, 1262 reported IELTS scores. Our target sample size was 600 repeat test takers: 100 test takers (two responses each) for each of the two task duration conditions (5 minutes or 20 minutes) crossed with three pairs of possible prompts (see Fig. 1). The resulting dataset therefore totaled 1200 responses.

![](img/2a6b45bb72dda622ea66bf4de38b92cb6b29f36b8110c1949144c5086f605cc7.jpg)  
Fig. 1. Experimental design.

This sample size of 600 test takers/1200 responses was motivated by a power analysis for the main analysis of comparing two independent correlational validity coefficients with a directional hypothesis. The effect size measure for this analysis is Cohen’s $q$ (Cohen, 1988). For this effect size measure, a value of 0.1 is considered small and a value of 0.3 is considered medium. Our goal was to achieve power of 0.9 with sensitivity of at least $q = 0 . 3$ or smaller (and alpha of 0.5). We consequently determined that with a sample size of 600 responses in each of the two duration conditions, our sensitivity would be $q = 0 . 1 7$ with power of 0.9. Even for our test-retest reliability comparisons, based on sample sizes of 300 responses, sensitivity was $q = 0 . 2 4$ .

To create this dataset of 600 test takers, two samples were assembled, one for adult test takers with reported IELTS scores and one without (in this context, “adult” typically signifies 16 or older.) Both samples were designed to be balanced across the six groups. First, the IELTS sample was assembled: the minimum number of test takers across the six groups was 38, so we randomly sampled 38 test takers with IELTS scores from each of the six groups. We then randomly sampled 62 more test takers without IELTS scores from each of the six groups. This sample of 600 test takers was varied in terms of reported age $M = 2 6$ years, $S D = 8$ ) and first languages (56 total, the most common are Indonesian, English, Telugu, Spanish, and Hindi), with $5 7 \%$ self-reported females. The majority of participants were interested in the DET for academic admissions purposes $4 6 \%$ undergraduate and $4 4 ~ \%$ graduate).

# 5.1.2. Prompts

Three writing prompts were selected from the item bank of the certified DET for the independent writing task type. For this task, the test taker responds to a written prompt in order to evidence overall writing proficiency. On the certified test, different prompts are intended to elicit different rhetorical purposes (e.g., narrative or descriptive), and by extension different linguistic features (e.g., cohesive devices; Abdi Tabari & Johnson, 2023). However, for consistency, the prompts selected for this study are all intended to elicit persuasive writing since additional time may be differentially beneficial for different rhetorical purposes (Applebee et al., 1990). The instructions for both prompts were identical other than the task duration, and no guidelines for expected response length were given. (See Cardwell et al. [2023] for further details about this task type.) These three prompts (Appendix A) were selected because the topics are commonly found on other high-stakes ELP tests: enjoyment of work (prompt 1), effect of technology on modern life (prompt 2), and the role of hard work versus luck in achieving success (prompt 3).

# 5.2. Scoring

To provide a comprehensive understanding of the effects of task duration, both human and machine scoring were employed to score the written responses. On the DET, all scoring is carried out using automatic scoring models specific for each task. These models are informed by human ratings during the training and evaluation process.

# 5.2.1. Human scoring

Two expert raters scored the responses, and $1 0 0 ~ \%$ of the responses were double rated. A third expert rater acted as an adjudicator by providing a deciding score in rare cases where there was a two-point discrepancy on the six-point rating scale. All three raters possessed graduate degrees in Applied Linguistics or English, the Cambridge DELTA teaching qualification, $^ { 1 5 + }$ years’ experience in the field of Teaching English as Second or Other Language (TESOL), and $^ { 1 0 + }$ years’ experience assessing writing for international, high-stakes ELP tests.

All ratings were based on public writing task rubrics developed by the DET 4 for training and evaluation of the AWE model; they were not created expressly for this study. These rubrics use a 6-point, holistic scale that was based on the six levels/descriptors from the CEFR and publicly available rubrics from testing organizations. A rating of 0 was given to bad-faith responses in which the test taker did not attempt to respond to the prompt.5 An initial two-hour training session and follow-up certification task were carried out for standardization purposes, as well as subsequent weekly monitoring and feedback. During the training, raters were informed that task duration was between 5 and 20 minutes, but exact conditions were not disclosed to make it impossible for raters to adjust their ratings based on expected length. Guidance was also given regarding assessing writing quality based on the presented length rather than a prescribed expected minimum.

Overall, the interrater agreement for the dataset (including removed bad-faith responses) was 0.62 exact agreement, 0.97 adjacent agreement, and 0.72 Quadratic Weighted Kappa (QWK), a standard measure to compare rater agreement which takes into account the closeness of agreement. These agreement results are consistent with previous research. For example, Ramineni et al. (2012) report (for TOEFL independent task scoring) 0.60 exact agreement, 0.98 adjacent agreement, and 0.69 QWK. The magnitude of QWK agreement rates can be interpreted like those of unweighted Kappa rates (Fleiss et al., 2003), with agreement of 0.61–0.8 considered substantial (Landis & Koch, 1977).

In addition, three months after completing the initial ratings, the same raters re-rated $2 0 \%$ $\scriptstyle { n = 2 4 0 }$ ) of the dataset. Whereas in the initial process the raters were not aware of the task duration, for this subset, the raters were explicitly told whether the test takers had 5 minutes or 20 minutes to respond. These additional data were intended to shed light on the impact of task duration expectations on rater behavior.

# 5.2.2. Machine scoring

The operational DET AWE tool was applied to each response in the sample using a task-specific model. This model was developed to assess essential theoretical subconstructs of writing proficiency. These subconstructs are described in the DET scoring rubrics and are operationalized for AWE through the measurement of a wide range of linguistic features:

• Content: Task achievement, Relevance, Effect on the reader, Appropriacy of style, Development • Discourse coherence: Clarity, Cohesion, Structure, Progression of ideas, Appropriacy of format • Lexis: Lexical diversity, Lexical sophistication, Word choice, Word formation, Spelling, Error severity • Grammar: Range of structures, Grammatical complexity, Error frequency, Error severity, Appropriacy

Numerical values for each feature are calculated, and the final score is derived as a weighted sum of these features using a com bination of two models, one based on CEFR-aligned expert rater data and another based on DET data. For further description of the AWE tool, see Cardwell et al. (2023). Since the operational DET AWE tool produces scores on a standardized scale, these machine scores were transformed to a score with mean 3 and SD 1 to make them approximately comparable to the human scores.

# 5.3. Reliability and validity measures

As a measure of reliability, we used test-retest reliability. The test-retest reliability coefficient is commonly interpreted as indicating the extent to which a test will produce the same results if taken by two people with similar proficiency levels. Criterion validity co efficients were based on the correlations of the writing performance scores with two external measures of writing proficiency: (1) the DET writing score from the participants’ certified test results and (2) the participants’ self-reported IELTS writing scores. Importantly, (1) is based on automatic scoring of seven short writing tasks6 totaling approximately 15 minutes, whereas (2) is based on human scoring of two longer writing tasks totaling 60 minutes.

The final dataset, named DET-duration, is publicly available in an open repository at https://osf.io/zy8fb/.

# 5.4. Semi-structured rater interviews

To better understand patterns of scores awarded to performances across the two conditions, we conducted post-hoc rater in terviews. This methodology aligns with the explanatory sequential study design (Creswell & Plano, 2011) in which quantitative data collection and analysis are followed up with qualitative data collection and analysis to support interpretation of the data. The in terviews were semi-structured in nature (Leavy, 2014), consisting of six set questions (Appendix B) about six test-taker responses, with follow-up questions to probe the raters’ initial answers. These questions and responses were selected to focus on higher-level responses (CEFR C1-C2) because it was noted that the data contained few short responses for these levels $\mathbf { \vec { C 1 } } = 1 9$ , $\mathrm { C } 2 = 4$ by at least one human rater or machine). Interviews over Zoom lasted approximately one hour each. The transcripts were then cleaned and coded for the matic analysis, using a combination of predetermined categories (the writing subconstructs from the rubrics) and emergent categories (Polio & Friedman, 2016). A second author then independently coded the extracted comments, resulting in a $9 0 ~ \%$ exact agreement rate.

# 6. Results

# 6.1. Research Question 1: Writing performance

Table 1 shows that response length is very strongly affected by task duration, as expected. However, there is not a linear rela tionship between time limit and length of output. Although test takers have four times as much time in the Long condition, they produce on average only slightly more than double the length (from 72 to 168 words), indicating diminishing returns on length with longer task duration. With respect to scores, Table 1 shows that the effect size $d$ of duration on machine scores is smaller than for human rater 2 and larger than for human rater 1. T-tests on each of the four measures shown in Table 1 were significant $( p < 0 . 0 0 1 )$ with the comparison between the Short and Long conditions. In line with previous research on task duration, these findings suggest that task duration increases performance and that these gains are independent of scoring method.

Subsequently, the re-rated subset of responses led to increased scores for the Short condition for both rater 1 $\mathbf { \mathcal { M } } = 0 . 3 0$ , $t = 4 . 5 8 , p$ $< 0 . 0 0 1 \dot { }$ ) and rater 2 $M = 0 . 1 4$ , $t = 2 . 0 1$ , $\begin{array} { r } { p = 0 . 0 4 6 \jmath } \end{array}$ ). These results indicate that responses in the Short condition were rated in a more favorable light (i.e., more realistic expectations) when the task duration was known. For the Long condition, a significant decrease was only noticed for rater 1 $( M = - 0 . 3 1$ , $t = - 5 . 5 8$ , $p < 0 . 0 0 1 $ ) but not rater 2 $M = 0 . 1 0$ , $t = 1 . 5 3$ , $p = 0 . 1 2 8 )$ ), suggesting that rater expectations are more impacted by short durations than long.

Table 1 Means and SDs of response length and scores.   

<html><body><table><tr><td></td><td colspan="2">Short</td><td colspan="2">Long</td><td></td></tr><tr><td>Grade</td><td>M</td><td>SD</td><td>M</td><td>SD</td><td>d</td></tr><tr><td>Length</td><td>72</td><td>32</td><td>168</td><td>113</td><td>1.33</td></tr><tr><td>Rater 1</td><td>2.89</td><td>0.74</td><td>3.28</td><td>0.81</td><td>0.51</td></tr><tr><td>Rater 2</td><td>2.80</td><td>0.73</td><td>3.46</td><td>0.87</td><td>0.84</td></tr><tr><td>Machine</td><td>2.70</td><td>0.92</td><td>3.37</td><td>0.92</td><td>0.73</td></tr></table></body></html>

Note. Length is measured in words.

# 6.2. Research question 2: Test-retest reliability

Fig. 2 shows the test-retest reliability coefficients for both duration conditions, grouped by scoring method. For the average of two human scores (patterns are similar for the two individual raters) reliability coefficients in the Short condition $( r = 0 . 4 8 )$ ) and the Long condition $( r = 0 . 5 1 $ ) are not significantly different $( p = 0 . 6 1$ , using the Z-test for comparing independent correlations). Similarly, for machine scores, reliability coefficients in the Short condition $( r = 0 . 7 0 )$ ) and the Long condition $( r = 0 . 6 6 )$ are not significantly different $( p = 0 . 3 8 )$ ). These results indicate that the reliability of the responses in the Short and Long conditions are similar, no matter which scoring method is used.

# 6.3. Research question 3: Criterion validity

Fig. 3 shows the criterion validity coefficients based on the relationship between the writing task scores from this study and either the participants’ certified DET writing scores or their self-reported IELTS writing scores. In this figure we see that criterion validity coefficients are lower in the Long condition in all cases and therefore there is no need to conduct directional statistical significance tests to reject the hypothesis that the Long condition improves validity coefficients. Nevertheless, we report the non-directional tests of differences in independent correlations. Specifically, for (the average of two) human scores, the correlation with DET scores in the Long condition $( r = 0 . 3 7 )$ is lower than in the Short condition $( r = 0 . 4 3 )$ ), and the non-directional $z = - 1 . 1 7$ $( p = 0 . 2 4 )$ . For machine scores, the correlation with DET scores in the Long condition $( r = 0 . 4 6 )$ is lower than in the Short condition $( r = 0 . 5 8 )$ , $z = - 2 . 6 2 ( p =$ 0.01). Correspondingly, for human scores the correlation with IELTS scores in the Long condition $( r = 0 . 2 0 )$ is lower than in the Short condition $( r = 0 . 3 1 )$ ), $z = - 1 . 2 9$ $( p = 0 . 2 0 )$ ), and for machine scores the correlation with IELTS scores in the Long condition $( r = 0 . 2 7 )$ ) is lower than in the Short condition $( r = 0 . 3 1 )$ ), $z = - 0 . 3 7$ $( p = 0 . 7 1 )$ ).

These results indicate that scores from the Short condition are correlated at least as strongly with external writing scores as scores from the Long condition. This finding is true for both DET writing scores (based on short tasks and AWE) and IELTS writing scores (based on long tasks and human scoring). Furthermore, this pattern was observed regardless of the scoring method, human or machine.

# 6.4. Research question 4: Comparison of human and machine scores

The fourth research question relates to possible differences between human and machine scores. First, Table 2 shows that in each duration condition, the correlation between the two human raters is similar to their correlation with machine scores. The human raters and machine scores are also similarly correlated with response length. These correlations of agreement are in line with previous work (see Section 3). Note, however, that correlations between scores (both human and machine) and response length are lower under the Long condition (0.41–0.47) than the Short condition (0.59–0.63). This finding may reflect the general non-linear relation of length with scores with a decreasing effect of length on scores. Since responses are generally longer under the Long condition, the relation of scores with response length under this condition is muted.

![](img/9dffebfd45a0295dc8e1ac01d31a9c09ed11a00532de40ac296b518e9b9ccea4.jpg)  
Fig. 2. Reliability coefficients (with SE as error bars).

![](img/d98c5fc8b6c2f31874ee9d896ee5c43da9f07b1ff227baf7e43e272cfb5d1278.jpg)  
Fig. 3. Criterion validity coefficients (with SE as error bars).

Table 2 Correlations between response length and scores for each duration condition.   

<html><body><table><tr><td></td><td>Length</td><td>Rater 1</td><td>Rater 2</td><td>Machine</td></tr><tr><td>Length</td><td>:</td><td>0.59</td><td>0.63</td><td>0.62</td></tr><tr><td>Rater 1</td><td>0.41</td><td>-</td><td>0.65</td><td>0.67</td></tr><tr><td>Rater 2</td><td>0.47</td><td>0.65</td><td>-</td><td>0.63</td></tr><tr><td>Machine</td><td>0.43</td><td>0.71</td><td>0.63</td><td>:</td></tr></table></body></html>

Note. Length measured in words. Short condition above diagonal, Long condition below diagonal.

There is also the question of whether there are differences in validity and reliability coefficients between human and machine scores. The DET criterion validity coefficient for machine scores $( r = 0 . 4 8 )$ was higher than for (the average of two) human scores $( r =$ 0.36), and the difference in correlated correlations (Meng et al., 1992) was significant $( z = 6 . 2 9$ , $p < 0 . 0 0 1 )$ . The IELTS criterion validity coefficient for machine scores $( r = 0 . 2 5 )$ ) was also higher than for human scores $( r = 0 . 2 1 $ ), but the difference in correlated correlations was not significant. With respect to reliability coefficients (see also Fig. 2), the test-retest reliability coefficient for machine scores $( r = 0 . 7 1 $ ) was higher than for (the average of two) human scores $( r = 0 . 4 8 )$ , and the difference in correlated correlations was significant $\begin{array} { r } { ( z = 6 . 3 3 } \end{array}$ , $p < 0 . 0 0 1 $ ), using Dunn and Clark’s (1969) $\mathbf { z }$ -test.

A final analysis sought to compare the two scoring methods in terms of possible differences in the magnitude of the prompt effect on scores. In other words, are differences in scores across prompts comparable under the two methods? To answer this question, a random-effects model was estimated for each scoring method, with person, prompt, and rater (only for human scoring). Table 3 shows the SD and the corresponding intraclass correlation (ICC). The ICC reflects the relative effect of a random effect on scores. As the table shows, the ICC of the Prompt effect is similar across methods, accounting for around $1 \%$ of the variance. The table also illustrates the higher reliability of machine scores, as reflected by the magnitude of the Person ICC. In summary, these results are consistent with previous findings that showed machine scores had higher reliability coefficients, and in some cases higher criterion validity coefficients, than human scores (see Attali, 2013 for a review).

# 6.5. Research question 5: Rater cognition

Next, we seek to understand rater behavior, specifically to learn more about the extent to which knowing the task duration would have affected their decisions and what aspects of writing the raters attended to. Focusing on the comments relating to task duration and task expectations, both raters indicated that not knowing that the task duration was five minutes may have subconsciously suppressed their scores:

Table 3 Random effects SDs and ICCs.   

<html><body><table><tr><td></td><td colspan="2">Human</td><td colspan="2"> Machine</td></tr><tr><td>Effect</td><td>SD</td><td> ICC</td><td>SD</td><td>ICC</td></tr><tr><td>Person</td><td>0.823</td><td>0.706</td><td>0.611</td><td>0.533</td></tr><tr><td>Prompt</td><td>0.098</td><td>0.010</td><td>0.073</td><td>0.008</td></tr><tr><td>Rater</td><td></td><td></td><td>0.024</td><td>0.001</td></tr><tr><td> Residual</td><td>- 0.522</td><td>-</td><td>0.566</td><td>-</td></tr></table></body></html>

• Rater 1: I think knowing that was five minutes would have probably helped me give a higher award there. My rationale for thinking I would change it was I didn’t think there was enough evidence. So in only five minutes, if that’s what they came up with, I think that’s pretty good.   
• Rater 2: If I had known some of these responses were produced in the short time frame, that might have helped with the scoring in their favor.

Both raters also further explained how knowing that a writer had a limited amount of time would influence their grading, whereas knowing about longer durations would not necessarily because the short duration tests writers’ abilities to be succinct.

In discussing the characteristics of the most proficient responses (CEFR C1-C2), comments labeled Content or Discourse Coherence accounted for $6 2 \%$ of construct-related comments compared to $3 8 \%$ labeled Grammar or Lexis, though these figures are unsurprising given that one question asked explicitly about content and argumentation. An analysis of these comments also revealed the weight that both raters gave to the content subconstruct, particularly development of ideas, when deliberating whether to award a C2. Explaining this thought process, rater 2 noted that “in my mind, [there are] certain ways of deciding between 6 [C2] and 5 [C1], one of which is the extent of it and the fullness of the development”, and then, when describing a specific response, “the grammar is great, the vocab is great, but I probably didn’t give it a 6, because I felt it wasn’t fully developed enough for 6”. Likewise, rater 1 when justifying a C1 rating gave this hypothetical, which draws attention to the relationship between task expectation and writing quality:

I mean, if somebody gave you that on a master’s program, you’d probably give them it back. Like, it is developed, but it doesn’t really go beyond that, it doesn’t exhibit much in the way of an ability to synthesize ideas of language connected to this topic.

Taken together, the findings in relation to all five research questions suggest that, for adult L2-English writers, increases in task duration can result in increases in performance, especially if task duration is not known to the raters, and that these gains are inde pendent of scoring method. Furthermore, the reliability and validity of scores do not significantly increase with longer task durations. That is to say, with more time, a test taker can produce a more developed writing response, but the scores assigned to this response are not necessarily a superior reflection (more reliable or valid) of the test taker’s ability. Potential reasons for these findings are considered in the following section and discussed in relation to implications for score interpretation and test development. Although obvious, it must be stressed that in an operational test, there are never multiple task durations, and the raters are always aware of the task durations (and thus set their expectations accordingly). Findings must therefore be considered bearing in mind these “artificial” experimental conditions.

# 7. Discussion

One outcome that we hypothesized prior to conducting the study was that there might be criterion validity gains for longer du rations for IELTS writing scores, especially those rated by humans. After all, IELTS scores are based on the average of a 20-minute and a 40-minute writing task, rated by humans. However, to the extent that the DET writing scores reflect much the same construct as IELTS writing, such criterion validity gains would be expected for the DET writing as well. On the other hand, if writing scores based on short tasks do indeed measure a different construct, we would expect the two external measures to diverge, with criterion validity gains for shorter tasks associated with DET writing scores. In fact, what we see in Fig. 2 is that the IELTS criterion validity coefficients are lower, though not significantly so, for the longer condition, regardless of scoring method. These findings prompt us to ask, “To what extent do tasks of different durations actually assess different mental processes or aspects of the writing construct, at least in such a way that can meaningfully impact scores on ELP tests?”

# 7.1. Task authenticity

One possible interpretation of our findings is that 5-minute and 20-minute tasks are in fact much closer on the authenticity spectrum than 20-minute and untimed tasks. Arguments of this nature are not new, with Biola (1982) claiming that essays completed in less than two hours “do not offer students an opportunity to collect facts or exercise sound reasoning” and “may limit the mental activities that writers can experience” (p. 97). From this perspective, both of our task durations would be considered short and inauthentic. In their analysis of the authenticity of IELTS Task 2 prompts, Moore and Morton (1999) suggest as much, finding that prompts such as these elicit responses that are markedly different from authentic university writing despite “[bearing] some resem blance to the predominant genre of university study - the essay” (p. 74). Specifically, these types of independent writing tasks do not require authentic research processes, elicit a limited range of rhetorical functions, and focus on more concrete, rather than abstract, entities. From this perspective, the durations in both conditions in this study equally increase the distance between test task and TLU domain task.

# 7.2. Task expectations

In the semi-structured interviews, raters revealed that knowledge of the task duration might have influenced the scores they gave to the shorter tasks, particularly at the higher end of the scale. These comments were supported by the re-rated responses with task

duration made known. As Hale (1992) points out, score increases are not an issue for norm-referenced tests as long as reliability and validity are maintained, but for criterion-referenced tests (such as the DET which is pegged to the CEFR), score increases must be accounted for as they are theoretically meaningfully different. Klein (1981) puts forward one reasonable explanation:

Grading criteria and standards may shift when readers score answers written under an expanded time limit. They may consciously or unconsciously place more weight on organization as well as expect a higher quality of answer for passing even if they are given instructions not to make such changes in their grading practices. (p. viii)

This hypothesis has empirical support in the literature on rater variability (described in Section 3) which has found raters to weight and interpret aspects of criteria differently. It seems then, based on previous research and the statements of our two experienced raters, that there is an effect of task duration on interpretation of responses. As Hale (1992) recommends,

One must take into account the nature of the scoring procedure… although the scale descriptors do reflect aspects of perfor mance that are nonrelative to a population’s score distribution, the descriptors may still be interpreted and applied relative to the time limits under which the test was administered. (p. 32)

# 7.3. Writing subconstructs

It is sometimes argued that shorter responses do not reflect the same writing subconstructs as longer responses. In particular, aspects of what we call Content and Discourse coherence are often cited as lacking in shorter responses (e.g., Kroll, 1990; Lee et al., 2021). Exploring the relationship of writing argumentation and task duration, Lee et al. (2021) found that argumentation did improve in the 45-minute condition compared to the 30-minute condition. The authors argue that the argumentation subconstruct is especially important for discrimination between the CEFR C1 and C2 levels as strong argumentation indicates a high level of writing competence. In their work with TOEFL raters, Cummings et al. (2002), (2005) observed that the raters attended to argumentation and rhetoric more than aspects of language for high-scoring essays and that the more effective essays were characterized by strong argumentation. In the context of our study, the rater interviews closely align with this previous research.

It is clear from the raters’ comments that at the upper levels the raters prioritized the content and discourse coherence subconstructs. As such, it is imperative to establish clear expectations in relation to the task duration so as to avoid penalizing expert writers due to unrealistic standards of development. In addition, continued monitoring and feedback for raters should be provided with these considerations in mind.

# 8. Conclusions

The study reported here investigated the impact of task duration, focusing on its effect on writing performance, score reliability, and score validity by comparing responses written by adult L2-English test takers under 5-minute and 20-minute conditions. The findings revealed that given more time, holistic scores increased (especially when task duration was unknown to raters) but score reliability and validity did not. These trends held true regardless of whether responses were human-scored or machine-scored. Subsequent interviews with raters indicated that not having knowledge of the duration posed a challenge, especially when distinguishing between higher proficiency levels due to the importance of judging proficiency in relation to realistic expectations.

# 8.1. Limitations and future research

Based on the findings, we suggest that future research continue to explore test perceptions, for example, by showing test-taker responses of different lengths to stakeholders (including raters, admissions officers, and researchers) to see at what point they “feel” there is enough content to be able to satisfactorily assess academic writing proficiency. Investigations manipulating the variables in the current study would also be of interest, including intermediary task durations between 5 and 20 minutes, other task types, and prompts eliciting other rhetorical purposes. We also intend to conduct more fine-grained linguistic analyses of the existing DETduration responses and tasks (e.g., Qian, 2023; Yang et al., 2023). Ideally, to address the limitations of the current study, future research could 1) use a high-stakes test, rather than practice test, to ensure high participant motivation, 2) supplement holistic rubrics with analytic rubrics to better determine which aspects of writing are most affected by task duration, 3) measure short-long duration pairs of responses to supplement the short-short and long-long pairs to capture the impact of duration differences on individual test takers, and 4) analyze writing processes data (e.g., keystroke analysis and stimulated recall) to uncover differences in writer thoughts and behavior. In addition, it should be noted that the findings of this study may not generalize to younger writers, for whom tran scription skills are the predominant constraint on writing performance.

# 8.2. Implications for test development

Although more research is always beneficial, the evidence presented here provides a compelling case for the use of shorter writing tasks, at least in adult L2 assessment contexts. This evidence aligns with recent trends in high-stakes assessment towards shorter test times, for example TOEFL iBT replacing a 30-minute independent writing task with a 10-minute discussion writing task (Davis & Norris, 2023), or Pearson’s PTE Academic shortening the test from three hours to two (Pearson, 2023). If score reliability and validity remain high with shorter tasks, then there is arguably more value in administering multiple shorter tasks (e.g., four 5-min tasks instead of one 20-minute task) in order to maximize the number of opportunities for test takers to write about different topics and for different rhetorical purposes.

It is also by no means a given that a 20-minute (or longer) writing task is appropriate for language learners across the entire proficiency spectrum. In the TLU domain, students may have an entire semester to master the content they must then show evidence of in their writing. In contrast, in a language testing context there is no preparation, and test takers must rely solely on their background knowledge and language skills when confronted with a blank page and no other resources. In such situations, could it be that 20 minutes is in fact too long? On the other hand, to mitigate the potential risks of shorter time conditions, such as time-limit anxiety, test readiness materials may be useful for raising awareness of expectations (Kane, 2020).

From a broader perspective, studies such as this one are a necessary component of test development so as to avoid selecting task durations based purely on intuition or for legacy decisions. The Standards for Educational and Psychological Testing (American Education Research Assocation et al., 2014) recommend that “when speed is not a meaningful part of the target construct, [task durations] should be determined so that examinees will have adequate time to demonstrate the targeted knowledge and skill” (p. 90). More than that, we concur with White (1995) in his assertion that “it is wasteful and intrusive to gather more information than we can well use” (p. 33). Test developers have a duty to go beyond simply ensuring adequate time for power tests and should instead seek to determine the minimum adequate time so as to make the best use of test takers’ time.

# CRediT authorship contribution statement

Yigal Attali: Writing – review & editing, Writing – original draft, Visualization, Methodology, Formal analysis, Data curation, Conceptualization. Ben Naismith: Writing – review & editing, Writing – original draft, Project administration, Methodology, Formal analysis, Data curation, Conceptualization. Geoffrey T. LaFlair: Writing – review & editing, Writing – original draft, Visualization, Methodology, Formal analysis, Conceptualization.

# Declaration of Competing Interest

Ben Naismith, Yigal Attali, and Geoffrey T. LaFlair are employees of Duolingo, Inc. the owners of the Duolingo English Test which is the subject of this article.

# Acknowledgements

We thank the raters for their contribution to the DET-duration dataset and to the reviewers of this paper.

# Appendix A. Writing prompts in the study and comparable examples

Prompt 1: Enjoying work versus making money

• DET: What is more important: to do work that you enjoy, or to make enough money to live comfortably? Explain your reasoning. (DET certified item bank)   
• IELTS: Many people go through life doing work that they hate or have no talent for. Why does this happen? What are the con sequences of this situation? (British Council, 2023)   
• TOEFL iBT: People work because they need money to live. What are some other reasons that people work? Discuss one or more of these reasons. Use specific examples and details to support your answer. (Educational Testing Service, 2020, p. 210)

Prompt 2: Technology and long-distance communication

• DET: Technology has made long-distance communication very easy. In your experience, what are benefits or drawbacks to this? (DET certified item bank)   
• IELTS: Nowadays the way many people interact with each other has changed because of technology. In what ways has technology affected the types of relationships people make? Has this become a positive or negative development? (Cambridge University Press, 2011, p. 54)   
• TOEFL iBT: Do you agree or disagree with the following statement? Although technology has made communication easier, there are still as many misunderstandings among people as there were in the past. Use specific reasons and examples to support your answer. (Educational Testing Service, 2020, p. 213)

Prompt 3: Hard work versus talent

• DET: Do you think that hard work is enough, or does a person also need some talent to achieve difficult goals? Provide reasons to support your opinion. (DET certified item bank)   
• IELTS: It is generally believed that some people are born with certain talents, for instance for sport or music, and others are not. However, it is sometimes claimed that any child can be taught to become a good sports person or musician. Discuss both these views and give your own opinion. (Cambridge University Press, 2009, p. 31)

• TOEFL iBT: Do you agree or disagree with the following statement? Practice and hard work are more important to an athlete’s success than natural ability and talent. Use specific reasons and examples to support your answer. (Educational Testing Service, 2020, p. 212)

# Appendix B. Semi-structured interview questions

1. Why did you rate this response a B2/C1/C2?   
2. What prevented the response from achieving a higher score?   
3. What do you think about the content and argumentation of this response?   
4. The other rater gave this response a B2/C1/C2. Why do you think they might have given this rating?   
5. Is there anything else you would like to say about this response?   
6. Now that you know how much time the test-taker had to complete the task, do you think this would have affected your decision?

# Data Availability

The DET-duration dataset is publicly available at https://osf.io/zy8fb/.

# References

Abdi Tabari, M., & Johnson, M. D. (2023). Exploring new insights into the role of cohesive devices in written academic genres. Assessing Writing, 57, Article 100749. https://doi.org/10.1016/j.asw.2023.100749   
Ackerman, P. L., & Kanfer, R. (2009). Test length and cognitive fatigue: An empirical examination of effects on performance and test-taker reactions. Journal of Experimental Psychology: Applied, 15(2), 163–181. https://doi.org/10.1037/a0015719   
Ackerman, P. L., Kanfer, R., Shapiro, S. W., Newton, S., & Beier, M. E. (2010). Cognitive fatigue during testing: An examination of trait, time-on-task, and strategy influences. Human Performance, 23(5), 381–402. https://doi.org/10.1080/08959285.2010.517720   
American Educational Research Association, American Psychological Association, & National Council on Measurement in Education. (2014). Standards for educational and psychological testing. American Educational Research Association.   
Applebee, A. N., Langer, J. A., Jenkins, L. B., Mullis, I. V. S., & Foertsch, M. A. (1990). Learning to Write in Our Nation’s Schools: Instruction and Achievement in 1988 at Grades 4, 8, and 12 NAEP (AMP-90-3480, NAEP-19-W-02; p. 122). Educational Testing Service.   
Attali, Y. (2012, April). Factor structure of the e-rater automated essay scoring system [Paper presentation]. National Council of Measurement in Education, Vancouver, Canada.   
Attali, Y. (2013). Validity and reliability of automated essay scoring. In M. D. Shermis, & J. Burstein (Eds.), Handbook of automated essay evaluation (pp. 181–198). Routledge/Taylor & Francis Group.   
Attali, Y. (2016). A comparison of newly-trained and experienced raters on a standardized writing assessment. Language Testing, 33(1), 99–115. https://doi.org/ 10.1177/0265532215582283   
Bachman, L. F., & Palmer, A. S. (2010). Language assessment in practice: Developing language assessments and justifying their use in the real world. Oxford University Press.   
Banerjee, J., Franceschina, F., & Smith, A. (2007). Documenting features of written language production typical at different IELTS band score levels. IELTS Research Reports, 7. 〈https://ielts.org/researchers/our-research/research-reports/documenting-features-of-written-language-production-typical-at-different-ielts-bandscore-levels〉.   
Barkaoui, K. (2011). Think-aloud protocols in research on essay rating: An empirical study of their veridicality and reactivity. Language Testing, 28(1), 51–75. https:// doi.org/10.1177/0265532210376379   
Ben-Simon, A., & Bennett, R. E. (2007). Toward More Substantively Meaningful Automated Essay Scoring. The Journal of Technology Learning and Assessment, 6(1). 〈https://ejournals.bc.edu/index.php/jtla/article/view/1631〉.   
Biber, D., Reppen, R., & Staples, S. (2017). Exploring the relationship between TOEFL iBT scores and disciplinary writing performance. TESOL Quarterly, 51(4), 948–960. https://doi.org/10.1002/tesq.359   
Biola, H. R. (1982). Time limits and topic assignments for essay tests. Research in the Teaching of English, 16(1), 97–98. 〈http://www.jstor.org/stable/40170881〉.   
Braddock, R. R., Schoer, L. A., & Lloyd-Jones, R. (1963). Research in written composition. National Council of Teachers of English.   
Breland, H. M., Camp, R., Jones, R. J., Morris, M. M., & Rock, D. A. (1987). Assessing Writing Skill. Research Monograph No. 11. Distributed by ERIC Clearinghouse.   
Breland, H. M., Bridgeman, B., & Fowles, M. E. (1999). Writing assessment in admission to higher education: Review and framework. ETS Research Report Series, 1999 (1), i-37. https://doi.org/10.1002/j.2333-8504.1999.tb01801.x   
Brennan, R., Goa, X., & Colton, D. (1995). Generalizability analyses of WorkKeys listening and writing tests. Educational and Psychological Measurement, 55(2), 157–176. https://doi.org/10.1177/0013164495055002001   
Bridgeman, B. (2020). Relationship between testing time and testing outcomes. In M. J. Margolis, & R. A. Feinberg (Eds.), Integrating Timing Considerations to Improve Testing Practices (pp. 59–72). Routledge. https://doi.org/10.4324/9781351064781-5.   
British Council. (2023). IELTS Practice Academic Writing test 2 - Task 2. In Take IELTS. 〈https://takeielts.britishcouncil.org/take-ielts/prepare/free-ielts-practicetests/writing/academic-2/task-2〉.   
Brown, J. D., & Bailey, K. M. (1984). A categorical instrument for scoring second language writing skills. Language Learning, 34(4), 21–42. https://doi.org/10.1111/ j.1467-1770.1984.tb00350.x   
Burnstein, J., LaFlair, G. T., Kunnan, A. J., & von Davier, A. A. (2022). A theoretical assessment ecosystem for a digital-first assessment—The Duolingo English Test. Duolingo Research Reports (DRR-22-01). https://doi.org/10.46999/KIQF4328   
Burstein, J., Tetreault, J., & Madnani, N. (2013). The e-rater® automated essay scoring system. In M. D. Shermis, & J. Burstein (Eds.), Handbook of automated essay evaluation: Current applications and new directions (pp. 55–67). Routledge/Taylor & Francis Group.   
Cambridge University Press. (2009). Cambridge IELTS 7: Examination Papers from University of Cambridge ESOL Examinations: English for Speakers of Other Languages. Cambridge University Press.   
Cambridge University Press. (2011). Cambridge IELTS 8: Examination Papers from University of Cambridge ESOL Examinations: English for Speakers of Other Languages. Cambridge University Press.   
Cardwell, R., Naismith, B., LaFlair, G. T., & Nydick, S. (2023). Duolingo English Test: Technical Manual. Duolingo Research Reports. https://doi.org/10.46999/ CQNG4625   
Creswell, J. W., & Plano Clark, V. L. (2011). Designing and conducting mixed methods research. SAGE Publications.   
Chen, D., Hebert, M., & Wilson, J. (2022). Examining human and automated ratings of elementary students’ writing quality: A multivariate generalizability theory application. American Educational Research Journal, 59(6), 1122–1156. https://doi.org/10.3102/00028312221106773   
Cho, Y. (2003). Assessing writing: Are we bound by only one method? Assessing Writing, 8(3), 165–191. https://doi.org/10.1016/S1075-2935(03)00018-7   
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates, Publishers.   
Cooper, P. L. (1984). The assessment of writing ability: A review of research. ETS Research Report Series, 1984(1), i-46. https://doi.org/10.1002/j.2330-8516.1984. tb00052.x   
Crawford, L., Helwig, R., & Tindal, G. (2004). Writing performance assessments: How important is extended time? Journal of Learning Disabilities, 37(2), 132–142. https://doi.org/10.1177/00222194040370020401   
Crossley, S. A., Kyle, K., Allen, L. K., Guo, L., & McNamara, D. S. (2014). Linguistic microfeatures to predict L2 writing proficiency: A case study in automated writing evaluation. Journal of Writing Assessment, 7(1). 〈https://escholarship.org/uc/item/06n1v820〉.   
Cumming, A., Kantor, R., Baba, K., Erdosy, U., Eouanzoui, K., & James, M. (2005). Differences in written discourse in independent and integrated prototype tasks for next generation TOEFL. Assessing Writing, 10(1), 5–43. https://doi.org/10.1016/j.asw.2005.02.001   
Cumming, A., Kantor, R., & Powers, D. E. (2002). Decision making while rating ESL/EFL writing tasks: A descriptive framework. The Modern Language Journal, 86(1), 67–96. https://doi.org/10.1111/1540-4781.00137   
Cushing, S. T., & Ren, H. (2022). Comparison of IELTS Academic and Duolingo English Test. IELTS Research Reports, 1. 〈https://ielts.org/researchers/our-research/ research-reports/comparison-of-ielts-academic-and-duolingo-english-test〉.   
Davis, L., & Norris, J. M. (2023). A comparison of two TOEFL® writing tasks. ETS Research Memorandum Series (ETS RM–23-06). 〈https://www.ets.org/Media/ Research/pdf/RM-23-06.pdf〉.   
Dunn, O. J., & Clark, V. (1969). Correlation coefficients measured on the same individuals. Journal of American Statistical Association, 64(325), 366–377. https://doi. org/10.2307/228374   
Deane, P. (2013). On the relation between automated essay scoring and modern views of the writing construct. Assessing Writing, 18(1), 7–24. https://doi.org/ 10.1016/j.asw.2012.10.002   
Eckes, T. (2012). Operational rater types in writing assessment: Linking rater cognition to rater behavior. Language Assessment Quarterly, 9(3), 270–292. https://doi. org/10.1080/15434303.2011.649381   
Educational Testing Service. (2020). The official guide to the TOEFL iBT Test (6th ed.). McGraw Hill.   
Enright, M. K., & Quinlan, T. (2010). Complementing human judgment of essays written by English language learners with e-rater(r) scoring. Language Testing, 27(3), 317–334. https://doi.org/10.1177/0265532210363144   
Fleiss, J. L., Cho Paik, M., & Levin, B. (2003). Statistical Methods for Rates and Proportions (3rd ed.). John Wiley; Sons, Inc.   
Gebril, A. (2009). Score generalizability of writing tasks: Does one test method fit it all? Language Testing, 26(4), 507–531. https://doi.org/10.1177/ 0265532209340188   
Gebril, A. (2010). Bringing reading-to-write and writing-only assessment tasks together: A generalizability analysis. Assessing Writing, 15, 100–117. https://doi.org/ 10.1016/j.asw.2010.05.002   
Goh, C. C. M., & Ang-Aw, H. T. (2018). Teacher-examiners’ explicit and enacted beliefs about proficiency indicators in national oral assessments. In D. Xerri, & P. Vella Briffa (Eds.), Teacher Involvement in High-Stakes Language Testing (pp. 197–216). Springer International Publishing. https://doi.org/10.1007/978-3-319-77177-9_ 11   
Guo, L., Crossley, S. A., & McNamara, D. S. (2013). Predicting human judgments of essay quality in both integrated and independent second language writing samples: A comparison study. Assessing Writing, 18(3), 218–238. https://doi.org/10.1016/j.asw.2013.05.002   
Hale, G. A. (1992). Effects of amount of time allowed on the test of written English. ETS Research Report Series, 1992(1), i-35. https://doi.org/10.1002/j.2333- 8504.1992.tb01458.x   
Hamp-Lyons, L. (1990). Second language writing: Assessment issues. In In. B. Kroll (Ed.), Second language writing (pp. 69–87). Cambridge University Press. https://doi. org/10.1017/CBO9781139524551.009.   
Huang, J. (2012). Using generalizability theory to examine the accuracy and validity of large-scale ESL writing assessment. Assessing Writing, 17(3), 123–139. https:// doi.org/10.1016/j.asw.2011.12.003   
In’nami, Y., & Koizumi, R. (2016). Task and rater effects in L2 speaking and writing: A synthesis of generalizability studies. Language Testing, 33(3). https://doi.org/ 10.1177/0265532215587390   
Jurich, D. P. (2020). A history of test speededness: Tracing the evolution of theory and practice. In M. J. Margolis, & R. A. Feinberg (Eds.), Integrating Timing Considerations to Improve Testing Practices (pp. 1–18). Routledge. https://doi.org/10.4324/9781351064781-1.   
Kane, M. T. (1982). A sampling model for validity. Applied Psychological Measurement, 6(2), 125–160. https://doi.org/10.1177/014662168200600201   
Kane, M. T. (2020). The impact of time limits and timing information on validity. In M. J. Margolis, & R. A. Feinberg (Eds.), Integrating Timing Considerations to Improve Testing Practices (pp. 19–31). Routledge. https://doi.org/10.4324/9781351064781-2.   
Kenworthy, R. (2006). Timed versus at-home assessment tests: Does time affect the quality of second language learners’ written compositions? TESL-Ejemplo, 10(1). 〈http://tesl-ej.org/ej37/a2.html〉.   
Khuder, B., & Harwood, N. (2015). L2 writing in test and non-test situations: Process and product. Journal of Writing Research, 6(3), 233–278. https://doi.org/ 10.17239/jowr-2015.06.03.2   
Klein, S. P. (1981). The effects of time limits, item sequence, and question format on applicant performance on the California Bar Examination (No. 81-7). Committee of Bar Examiners of the State Bar of California; the National Conference of Bar Examiners   
Knoch, U., & Elder, C. (2010). Validity and fairness implications of varying time conditions on a diagnostic test of academic English writing proficiency. System, 38(1), 63–74. https://doi.org/10.1016/j.system.2009.12.006   
Kroll, B. (1990). What does time buy? ESL student performance on home versus class compositions. In In. B. Kroll (Ed.), Second Language Writing (Cambridge Applied Linguistics): Research Insights for the Classroom (pp. 140–154). Cambridge University Press. https://doi.org/10.1017/CBO9781139524551.014.   
Landis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics, 159–174. https://doi.org/10.2307/2529310   
Leavy, P. (Ed.). (2014). The Oxford Handbook of Qualitative Research. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780199811755.001.0001.   
Lee, J., & Son, M. (2023). The effects of varied time constraints on writing performance. 〈https://www.aaal.org/events/aaal-2023-conference—portland-oregon〉.   
Lee, S., Lim, G. S., & Basse, R. (2021). The effect of additional time on the quality of argumentation in L2 writing assessment: A mixed-methods study. Language Assessment Quarterly, 18(3), 253–272. https://doi.org/10.1080/15434303.2021.1872080   
Lee, Y. W., & Kantor, R. W. (2005). Dependability of new writing task scores: Evaluating prototype tasks and alternative rating schemes. TOEFL Monograph Series, 31. Educational Testing Service. https://doi.org/10.1002/j.2333-8504.2005.tb01991.x   
Livingston, S. A. (1987). The effects of time limits on the quality of student-written essays. Paper presented at the Annual Meeting of the American Educational Research Association. New York, NY: Educational Testing Service. 〈http://files.eric.ed.gov/fulltext/ED286936.pdf〉.   
Llosa, L., & Malone, M. E. (2019). Comparability of students’ writing performance on TOEFL iBT and in required university writing courses. Language Testing, 36(2), 235–263. https://doi.org/10.1177/0265532218763456   
Lovett, B. J., Lewandowski, L. J., Berger, C., & Gathje, R. A. (2010). Effects of response mode and time allotment on college students’ writing. Journal of College Reading and Learning, 40(2), 64–79. https://doi.org/10.1080/10790195.2010.10850331   
Margolis, M. J., von Davier, M., & Clauser, B. E. (2020). Timing considerations for performance assessments. In M. J. Margolis, & R. A. Feinberg (Eds.), Integrating Timing Considerations to Improve Testing Practices (pp. 90–103). Routledge. https://doi.org/10.4324/9781351064781.   
Meng, X.-L., Rosenthal, R., & Rubin, D. B. (1992). Comparing correlated correlation coefficients. Psychological Bulletin, 111(1), 172–175. https://doi.org/10.1037/ 0033-2909.111.1.172   
Moore, T., & Morton, J. (1999). Authenticity in the IELTS Academic Module Writing Test: A comparative study of Task 2 items and university assignments. British Council/IDP Australia Research Reports, 2(4). 〈https://www.ielts.org/en-us/for-researchers/research-reports/volume-02-report-4〉.   
Page, E. B. (1966). The imminence of grading essays by computer. Phi Delta Kappan, 48, 238–243.   
Pearson. (2023). Pearson shortens PTE Academic test and launches online version Pearson PTE. Pearson PTE. 〈https://www.pearsonpte.com/news/pearson-shortenspte-academic-test-and-launches-online-version〉.   
Polio, C., Fleck, C., & Leder, N. (1998). If I only had more time:” ESL learners’ changes in linguistic accuracy on essay revisions. Journal of Second Language Writing, 7 (1), 43–68. https://doi.org/10.1016/S1060-3743(98)90005-4   
Polio, C., & Friedman, D. (2016). Thematic analysis. In. Understanding, Evaluating, and Conducting Second Language Writing Research. Routledge. https://doi.org/ 10.4324/9781315747293   
Polio, C., & Glew, M. (1996). ESL writing assessment prompts: How students choose. Journal of Second Language Writing, 5(1), 35–49. https://doi.org/10.1016/S1060- 3743(96)90014-4   
Powers, D. E., & Fowles, M. E. (1996). Effects of applying different time limits to a proposed GRE writing test. Journal of Educational Measurement, 33(4), 433–452. https://doi.org/10.1111/j.1745-3984.1996.tb00500.x   
Qian, L. (2023). Use of lexical features in high-stakes tests: Evidence from the perspectives of complexity, accuracy and fluency. Assessing Writing, 57, Article 100758. https://doi.org/10.1016/j.asw.2023.100758   
Ramineni, C., Trapani, C. S., Williamson, D. M., Davey, T., & Bridgeman, B. (2012). Evaluation of e-rater® for the GRE® issue and argument prompts. ETS Research Report Series, 2012, i-106. https://doi.org/10.1002/j.2333-8504.2012.tb02284.x   
R´ev´esv, A., Michel, M., & Lee, M. (2017). Investigating IELTS Academic Writing Task 2: Relationships between cognitive writing processes, text quality, and working memory. IELTS Research Reports, 3. 〈https://ielts.org/researchers/our-research/research-reports/investigating-ielts-academic-writing-task-2-relationshipsbetween-cognitive-writing-processes-text-quality-and-working-memory〉.   
Schoonen, R. (2005). Generalizability of writing scores: An application of structural equation modeling. Language Testing, 22(1), 1–30. https://doi.org/10.1191/ 0265532205lt295oa   
Shavelson, R. J., Baxter, G. P., & Gao, X. (1993). Sampling variability of performance assessments. Journal of Educational Measurement, 30(3), 215–232. 〈http://www. jstor.org/stable/1435044〉.   
Shin, S.-K. (2011). The effects of time allocation on Korean college students’ performance of drafted and timed essays. English Teaching, 66(1), 163–177. 〈http:// journal.kate.or.kr/wp-content/uploads/2015/01/kate_66_1_8.pdf〉.   
Vaughan, C. (1991). Holistic assessment: What goes on in the rater’s mind? In L. Hamp-Lyons (Ed.), Assessing second language writing in academic contexts (pp. 111–125). Ablex.   
Weigle, S. C. (2002). Assessing writing. Cambridge University Press. https://doi.org/10.1017/CBO9780511732997   
Weigle, S. C. (2010). Validation of automated scores of TOEFL iBT tasks against non-test indicators of writing ability. Language Testing, 27(3), 335–353. https://doi. org/10.1177/0265532210364406   
White, E. M. (1995). An apologia for the timed impromptu essay test. College Composition and Communication, 46(1), 30–45. 〈http://www.jstor.org/stable/358868〉.   
Wu, J., & Erlam, R. (2016). The effect of timing on the quantity and quality of test-takers’ writing. New Zealand Studies in Applied Linguistics, 22(2), 21–34. 〈https:// search.informit.org/doi/abs/10.3316/informit.574713809713517〉.   
Yang, Y., Yap, N. T., & Mohamad Ali, A. (2023). Predicting EFL expository writing quality with measures of lexical richness. Assessing Writing, 57, Article 100762. https://doi.org/10.1016/j.asw.2023.100762   
Yerkes, R. M. (Ed.). (1921), Psychological Examining in the U.S. Army: Memoirs of the National Academy of Sciences, 15. U.S. Government Printing Office.   
Younkin, W. F. (1986). Speededness as a source of test bias for non-native English speakers on the college level academic skills test [PhD thesis]. University of Miami.   
Zhang, Y., & Elder, C. (2014). Investigating native and non-native English-speaking teacher raters’ judgements of oral proficiency in the College English Test-Spoken English Test (CET-SET). Assessment in Education: Principles, Policy Practice, 21(3), 306–325. https://doi.org/10.1080/0969594X.2013.845547

Ben Naismith is a Senior Assessment Scientist at Duolingo where he works on research and development of the Duolingo English Test. Ben has worked in numerou contexts as a teacher, teacher trainer, materials developer, assessment specialist, and researcher. He holds a PhD in Applied Linguistics from the University of Pittsburg

Yigal Attali is a Principal Assessment Scientist for the Duolingo English Test. Some of his interests include research and development of innovative assessment automatic scoring, and automatic item generation.