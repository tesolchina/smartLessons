This year has seen several important changes to the journal: Stefanie Wind and Shulin Yu joined the editorial team as Associate Editors; 16 colleagues joined our newly formed Early Career Editorial Board; and Editor’s Choice picks have been instituted. The book review feature of the journal has been discontinued. Laura Aull’s appointment as Associate Editor responsible for our Tools and Tech section has been extended. The Impact Factor and Citescore rankings of Assessing Writing continue to climb and along with this we continue to see a growth in submissions.

In the past month we released a call for contributions to a new special issue titled: AI and Writing Assessment: Innovations, Validity, and Ethical Considerations. This special issue is being guest-edited by Joshua Wilson and Corey Palermo. See the journal website for information on how to submit your abstract to the guest editors for review and potential inclusion in the special issue. Abstracts can be submitted for consideration up until March 31, 2025. We are grateful for Drs Wilson and Palermo’s proposal, and we are excited by this SI’s potential to help the field of writing assessment navigate the inevitably turbulent road ahead as educators grapple with uses of AI both for writing and for assessment. We are particularly pleased with their focus on ethical considerations related to the use of AI in the assessment of writing.

Volume 62 includes 11 research articles that focus on issues related to AI detection, writing assessment design and validation, feedback, and writing analytics. Authors in this issue are situated across five continents reflecting our commitment to publishing diverse perspectives on the assessment of writing.

Nguyen and Barrot examine how L2 writing teachers distinguish between AI generated and human generated essays. In this exploratory study they report that teachers were able to distinguish well between lower proficiency writers and more advanced and expert writers. They observed that texts produced by lower proficiency writers were not falsely identified as AI generated. Writing by more advanced writers, however, was more likely to be falsely identified as AI generated. The issue of false positive identification of writing as being AI generated is a significant issue. There is an emerging body of research that highlights the fallibility of both human raters and AI detection software in correctly distinguishing between AI and human generated text. This is a problem that, we hope, will be taken up in contributions to our upcoming special issue.

Five papers in this issue report on studies focusing on design and validation of writing assessment instruments.

Avila Reyes, Carrasco, Escribano, Espinosa, Figuera, and Castillo focus on task design through the lenses of fairness, validity, and justice. Their work is driven by a concern that assessments need to measure the construct they intent to measure for all populations completing an assessment. They observe that two students who have the same underlying abilities may perform differently on a writing task given their level of prior access to genre and content knowledge embedded in that task. They examine the inclusion of genre and knowledge activation devices within an assessment’s design as two approaches to minimizing construct irrelevant variance. Based on their findings, they suggest that omitting genre and knowledge activation devices would cause unwanted variance in test-taker performance, an important fairness and validity issue.

Naismith, Attali, and LaFlair examined two timed writing tasks of different durations (5 min vs $2 0 \mathrm { { m i n } }$ ) to determine the impact of task duration on examinee performance, and on the reliability and criterion validity of writing assessment tasks. They report that tasks of shorter duration achieve similar degrees of reliability and criterion validity as do longer duration tasks, and that they similarly enable distinctions between stronger and weaker writing. They observe that there are ethical concerns associated with task duration: Can assessment designers and users justify requiring tasks of longer duration than are needed to make similar inferences from those drawn using shorter duration tasks? This is an important question that needs to be interrogated alongside other ethical questions such as, “Does variation in task duration impact what construct of writing we are able to measure? And if so, can we justify using measures of narrower constructs of writing when we have the capability to measure more robust constructs?”

Zeng, Lui, and Bowen draw attention to this second question in their study that examined the effects of timed $\mathrm { 3 0 m i n } )$ ) versus untimed (2 weeks) writing tasks on voice construction of L2 writers completing argumentative writing tasks. They observe that most research on voice in writing have focused on aspects of voice that are easiest to operationalize. They articulate a robust construct that emphasizes three dimensions of voice: central point articulation, the use of hedges and boosters, and the use of attitude markers. They found that voice was scored significantly higher on untimed writing tasks when compared to timed writing tasks. Their findings indicate that time plays a role in the strength and clarity of argumentative writing, suggesting that the ability to use extended time provides writers with opportunities to refine voice and clarify argument, an issue that speaks to the questions raised by Naismith, Attali and LaFlair’s work published in this volume.

Harsch, Koval, Kanistra, and Delgado-Osorio provide an exemplary study focused on validating a rating scale for use with readinginto-writing tasks. Drawing on Weir’s (2005) socio-cognitive validation framework they follow a systematic approach to rating scale development and validation. They articulate a complex construct for integrated writing that provides the foundation for their rating scale design. Their study punctuates the importance of principled approaches to assessment design and validation—including a clear focus on construct articulation—a theme raised across the five papers on assessment design and validation published in this volume.

Tabari, Wang and Miller examine the role that performance tasks play in the development of L2 writing ability. They investigate how the order of tasks in terms of their cognitive demands (simple to complex, complex to simple, or random) influences the development of linguistic and syntactic complexity in student writers. While they found each pattern produced different effects, they observe that the interplay between task complexity, sequencing, and pacing influences the processes and strategies L2 learners employ in their development as writers. While their study examined textual features, they acknowledge the need for future research to look more directly at writers’ cognitive processes to develop a better understanding of the relationship between task complexity sequencing and textual production.

Two studies in this volume explore the effects of genre on linguistic complexity and its implications for the assessment of writing. Wang and Jiang examined noun phrase complexity in L2 English writing by genre and proficiency level. They identified eight noun phrase complexity features that distinguish between higher and lower proficient writing, though differences in frequency and trajectory within this set of features were identified across genres.

Pun and Li examined linguistic features associated with writing quality of scientific reports, expanding this area of research into new contexts and genres. They found that lexical diversity predicts quality of writing performance, but this is significantly mediated by lexical sophistication. Diversity improved performance when more sophisticated language was being used. Similarly syntactic complexity predicted writing quality, but this too was mediated by lexical sophistication. More complex sentences needed to include more complex words if syntactic complexity were to influence perceptions of writing quality. Their study highlights the importance of lexical sophistication in scientific report writing.

Three papers in this volume explore both how feedback is delivered and how feedback is taken up by students.

Hoomanfard explores feedback use at the graduate level, examining the preferences of master’s and doctoral students with respect to the feedback they receive from their supervisors. Preferences were examined across three dimensions: cognitive, affective, and social. Master’s students preferred comments that were specific while feeling that general comments were less valuable to them. Both groups valued feedback that is clear, with doctoral students reporting a greater set of strategies for seeking clarification on unclear comments. Both groups preferred comments that were encouraging while doctoral students preferred a dialogic approach to feedback. Both groups disliked comments that took control of the text away from the student. Differences in preferences can be attributed to differences in student needs, context, and developmental situation.

Lu, Zhu, Zhu, and Yao examined the relationship between L1 and L2 feedback literacy, feedback engagement, and writing performance. From an ecological perspective, this study highlights the value of developing both L1 and L2 feedback literacies. They found that L1 feedback literacy supports L2 feedback literacy, while L2 feedback literacy supports engagement with feedback. Deeper engagement, they found, leads to more effective use of L2 feedback which enhances student writing performance.

Nguyen, Nguyen, and Phuong explored the use of model texts as a form of feedback. They found that using a model text as a feedback instrument helped students improve content, lexis, and organization of their texts but that it did not help them with improving their grammar. Students only applied half of what they observe in their revisions and subsequent work.

We close this final volume of 2024 by thanking our authors, reviewers, and editorial board members for their contributions to Assessing Writing. Thank you also to our publisher, Marta Baena Jurado and our journal manager, Dipudass Dasan, along with the production team, for their exceptional support throughout the past year.

# References

Weir, C. J. (2005). Language testing and validation: an evidence-based approach. Palgrave Macmillan. https://doi.org/10.1057/9780230514577