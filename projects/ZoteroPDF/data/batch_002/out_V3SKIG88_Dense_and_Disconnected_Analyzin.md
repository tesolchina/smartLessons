# Dense and Disconnected: Analyzing the Sedimented Style of ChatGPT-Generated Text at Scale

Ben Markey1 , David West Brown1 , Michael Laudenbach2 , and Alan Kohler1

# Abstract

ChatGPT and other LLMs are at the forefront of pedagogical considerations in classrooms across the academy. Many studies have spoken to the technology’s capacity to generate one-off texts in a variety of genres. This study complements those by inquiring into its capacity to generate compelling texts at scale. In this study, we quantitatively and qualitatively analyze a small corpus of generated texts in two genres and gauge it against novice and published academic writers along known dimensions of linguistic variation. Theoretically, we position and historicize ChatGPT as a writing technology and consider the ways in which generated text may not be congruent with established trajectories of writing development in higher education. Our study found that generated texts are more informationally dense than authored texts and often read as dialogically closed, “empty,” and “fluffy.” We close with a discussion of potentially explanatory linguistic features, as well as relevant pedagogical implications.

# Keywords

generative AI, large language models, corpus analysis, college writing development, writing technology

# Introduction

In Writing Technologies, Haas (1995) identifies three myths that often inform understandings of literacy in its relation to technology. One myth germane to our efforts here assumes technology “determines itself and its own uses and effects” (p. 35), a myth that entails a concomitant belief in the instrumentality of technology and, consequently, the independence of technology from writing. Despite this myth, it remains to be seen what writing without technology might look like. In his historical survey, Gabrial (2007) describes how writing, its development, and the expansion of literacy have always been tied to new implements, surfaces, machines, and systems of storage and distribution. Nonetheless, new writing technologies routinely inspire anxiety. For example, one can find anxiety over the effect of erasers on students’ preparedness circulating in the early twentieth century: “Think of the moral and intellectual training that comes to a student who writes a manuscript with the knowledge that his [sic] errors will stand out on the page as honestly confessed and openly advertised mistakes” (S. V. G., 1908). Perhaps more surprisingly, one can find instructors articulating almost the exact same reservations more than a century later (Espinoza, 2015). Grammar and style checkers have prompted similar pushback in some quarters, particularly for their presumed effects on developing writers (Fischer & Grusin, 1992; Gerrard, 1989; Pennington, 1993).

Our study analyzes a novel writing technology, ChatGPT 3.5 (hereafter, ChatGPT), by gauging its output against established academic writers and novice student writers. The study is, in part, motivated by the results from a survey at our university, commissioned to explore student perceptions of large language models (LLMs) like ChatGPT within the context of undergraduate writing courses. The survey indicates that students are looking for guidance on the use of LLMs in the classroom. In response to these survey results, we took up this work motivated by a fairly straightforward interest: ChatGPT appears capable of generating one-off texts of unremarkable quality, but how does it write at scale? In other words, when an LLM is prompted to generate not just one instance of a text but many, what does that writing look like in aggregate? And how does it compare to similar texts produced by human writers, both experts and novices?

Though these questions may provoke anxiety, they underscore a need to understand the effects of LLMs on writing and writing development. Moreover, these questions are important in light of the capacity of writing technologies to generate, maintain, and alter forms of social organization (Bazerman, 2013; e.g, Russell, 1991, pp. 102–105). Hence Vee’s (2023) legitimate concern that ChatGPT’s model of writing limits users to the seemingly intuitive, though uncritical, belief that writing is solely a means of transmitting information. Such myopia transforms the constitutive elements of the writing process—the places we write, the people we voice, the identities we form, the difficulties we encounter—into “impediment[s] to be addressed” (Stanton, 2023, p. 184). Therefore, as we rethink what it means to write under the sign of LLMs, as Appleman (2023) suggests we do, writing researchers would be wise to critically inquire into the presuppositions built into and affordances of this technology in order to better grasp the technology as a “bounded, constrained system” (McKee & Porter, 2017, p. 152).

For our purposes, the concerns noted in the paragraph above suggest that writing with ChatGPT may disrupt established writing development trajectories in higher education, and therefore complicate the acquisition of disciplinarity and academic writing proficiency. Motivated by such concerns, we investigate the lexicogrammatical patterns evident in the introduction sections of proposals and IMRD reports produced by four different groups of writers—first-year undergraduate students, graduate students, published experts, and ChatGPT. To measure the similarities and differences among these groups, we apply multidimensional analysis using the method and features developed by Biber (1988) and used in a variety of studies investigating register variation and academic writing (e.g., Friginal & Weigle, 2014; Hardy & Römer, 2013). This research broadly questions whether the recurrent linguistic patterns of generated text instantiate the rhetorical principles, evident in professional academic writing, that we invite students to practice as part of an established writing trajectory (Aull, 2020).

In pursuing this research, we first discuss literature pertinent to both writing technologies and postsecondary writing development. Across this section, theoretical considerations are grounded in historical descriptions of LLM development. Then we recount our methods and findings, before closing with a discussion of pedagogical implications.

# Literature Review

Tools as sedimented objects. Central to our discussion of writing technology is the trope of the tool as material embodiment of historical chains of human activity. A tool is a sedimented object to the extent that it “transforms concrete situated activity into tacit, abbreviated and presuppositional forms” (Prior, 1998, p. 182). In this sense, LLMs are no different than hammers: a sedimented object in which the situated history of its own development—its use cases, ideological underpinnings, etc.—is presupposed in the tool’s design. We exemplify this point across the next few paragraphs.

One of the fundamental assumptions encoded in LLM technology is the use of a contextual window (i.e., a textual range around a target word) to disambiguate polysemous words. This use indexes a theoretical commitment to the idea that individual word meaning is a result of its surrounding text. While the idea dates back to midcentury work in computing (Weaver, 1949) and linguistics (Firth, 1957), the various advances in the operationalization of contextual windows has partially driven LLM evolution across the last decade. Earlier models like word2vec and Global Vectors (GloVe), as “bagof-words” models, did not encode any information related to word order within a moving contextual window. A second generation of models released in the late 2010s (e.g., BERT, GPT) moved windows bidirectionally (not only right to left, but also left to right) and implemented changes to improve performance over larger windows (i.e., longer sequences of words) (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2018).

The point here is that technological developments carry forward (and, in this case, are driven by) underlying assumptions about word meaning and the use of contextual windows to establish it. These assumptions are encoded into the tool such that they are tacitly affirmed whenever users generate texts. While the assumptions may not initially appear as heinous as the prescriptivism encoded into grammar checkers (Curzan, 2014, pp. 66–92), the logic whereby tacit beliefs about language are encoded into technology remains the same.

Additionally, as a sedimented object, ChatGPT is trained on millions of texts produced in concrete situated activities, to use Prior’s words. As training data grow in size, more and more of the concrete situated activities that produce digital texts—think of the millions of participants, purposes, and audiences—are transformed by ChatGPT’s architecture into a tacit and presupposed form that shapes outgoing generated text. The internet affords the scale and availability of text needed for such training sets. As early as the mid-2000s, NLP engineers at Google argued for the value of scale in the name of the “unreasonable effectiveness of data”; although the resulting trillion-word corpus of internet texts was both error-ridden and unbalanced, the engineers believed the size of the corpus “outweighs these drawbacks” insofar as data at that scale “captures even very rare aspects of human behavior” (Halevy et al., 2009, p. 8). While it is unclear exactly what texts comprise ChatGPT’s training data, Bender et al. (2021) point out that the model is only trained on $7 \%$ of English users. This fact pairs with Byrd’s (2023) caution that ChatGPT is not exposed to nonstandard varieties of English. These facts together contest the link between size and insight that seems to motivate the use of larger training sets. By asking what varieties of English are included, Bender et al. (2021) and Byrd (2023) compel further questions: Whose concrete languaged activities get sedimented? Which activities are privileged and which ones marginalized?

Such questions are partially animated by the fact that sedimented objects not only presuppose uses and beliefs, but also further, that they assume a fairly distinct functional system of activity (Holland & Cole, 1995). For example, when a student takes up a writing task, the student participates in an institutionalized network of participants, purposes, and rewards. And when they use ChatGPT, they are bringing to bear on the dynamics of their participation the tacit and assumed ideological commitments encoded into the technology—hence faculty consternation across the academy. For Prior (1998) though, that a tool presupposes both an immediate task and a broader network of functional elements suggests that tools shape and are shaped by the worlds in which they are put to use. As Haas (1995) reminds us, tools are not selfdetermining. Instead, tools and worlds co-evolve.

All of which is just another way of saying that tools make possible particular understandings and constructions of task, participant role, production context, etc. Writing research has largely confirmed the transformational capacity of technological affordances in situated literate activities (Ching, 2018; Clayson, 2018; Haas, 1989; Prior, 1998; Sterponi et al., 2017). As an example of how writing technologies afford participant roles, Sterponi et al. (2017) discuss the ways patients are empowered by paper records. Patients, the researchers observe, exercise control over the identification and organization of pertinent medical records, as well as over the commentary accompanying those documents from previous providers. Elsewhere, research has examined the use of a productivity tool to guide writing work by distributing memory (Bivens & Cook, 2018) and the occasional disruption to writers’ sense of their own text brought on by distraction-free writing environments (Ching, 2018). These cases and others illustrate the insight of Holland and Cole’s (1995) insight: the introduction of new mediational tools in a functional system of activity makes alternate conceptions of task, purpose, context, and participant role possible. Writing technologies, in sum, are both generative and transformative; they reify historical chains of activity and make possible alternate futures for writers, texts, and audiences. With this in mind, research into LLMs in writing contexts, such as the first-year composition class, has the opportunity to link the beliefs and practices sedimented in the tool (for good or ill) to what the tool affords writers.

Student writing development in higher education. When researching ChatGPT as a writing technology, a corpus approach unearths the forms sedimented into the tool by leveraging large data sets to identify lexicogrammatical patterns often hidden from intuition (Aull, 2015). These tacit patterns emerge as visible phenomena in the aggregate. A corpus-based approach in this case, then, establishes a conceptual link between the “tacit, abbreviated, and presuppositional forms” sedimented in ChatGPT and the emergent patterns across its generated text. Despite its methodological insight, corpus analysis is meant to complement, not replace, a “human-in-the-loop approach” (Lang et al., 2023, p. 95), where readers qualitatively analyze the instantiation of tacit patterns in individual texts. Together, quantitative and qualitative perspectives lend themselves to a study of ChatGPT’s tacit affordances, all in an effort to surmise how the introduction of the tool into first-year writing (FYW) classrooms may affect established trajectories of student writing development.

For our purposes, there are two salient lines of inquiry into student writing development in higher education. The first thread of research on academic writing development analyzes generality and stance markers and construes proficiency as the collegial management of alternate perspectives (Aull, 2019, 2020; Aull & Lancaster, 2014; Aull et al., 2017). Relative to professional academics, first-year writers use fewer hedges (e.g., could, seems, slightly) and reformulations (e.g., in other words) and more emphatics (e.g., definitely) and generality markers (e.g., everywhere, always, no one) (Aull & Lancaster, 2014; Aull et al., 2017). As a result, first-year texts often present themselves as confident and generalized and therefore closed to alternative perspectives. In other words, these texts do not open claims to the types of academic conversation integral to knowledge building in the academy (Aull, 2020). As suggested by the developmental trajectory, students tend to use fewer indefinite pronouns (e.g, a, an) as they progress through their postsecondary education (Aull et al., 2017). Moreover, Aull (2019) observed fewer contrastive connectors (e.g., although, however) and boosters (e.g., definitely) in the explanatory writing of upper-level students, suggesting that this developmental trajectory is partially mediated by academic genre. Broadly, then, as students make their way through undergraduate and graduate students, they gradually acquire proficiency with linguistic features that make space for other opinions and ideas.

In the second thread of research, academic writing development is frequently associated with the increased production of informationally dense prose (Biber et al. 2011; Staples et al., 2016). Staples et al. (2016) characterize this trajectory as one that facilitates developing writers’ capability to “follow the informational focus of the discourse and compress detailed information rather than elaborate on the performers of an action” (p. 159). Lexicogrammatically, this means that as students develop, they use more socalled information-carrying words—nouns, nominalizations, participles, gerunds, etc.—which consequently facilitates the compression of information through dense noun phrases (e.g., academic writing development) (Biber et al., 2011; Staples et al., 2016). Hence the term “information density.”

Zooming out from the linguistic features slightly, this second research thread associates academic writing development with the acquisition of written communication skills that facilitate more abstract reasoning. In other words, novice academic writers use clausal features to build arguments across comparative and contrastive relations (Staples et al., 2016). Down the road, for example, as L2 writers develop, they tend to shed more elaborate clausal features and employ postnominal modification features such as preposition and participle phrases (e.g., the writing of our students and the students using nominal features, respectively) (Parkinson & Musgrave, 2014). What this line of research suggests is that students are relying less on relational reasoning and more on abstract reasoning. Such findings give strength to Biber et al.’s (2011) model of L2 development in which academic writing development proceeds through increasingly dense noun phrases embedded in the clause structures that are more typical of novice writing. This developmental trajectory has since been confirmed in L1 settings (Staples et al., 2016). The point here, then, is that development occurs on a few levels. As linguistic features develop and reading experience changes, students’ linguistic patterns inform similarly developing “habits of mind” (Aull & Ross, 2020, p. 22) that dispose students to a specific form of abstract reasoning.

These two threads converge upon the rhetorical qualities of upper-level student writing—specifically compression and civility (Aull, 2020). What we mean is that both lines of inquiry identify an array of linguistic features that “reify the values, social practices, and communicative functions” of various genres and disciplines (Aull, 2020, p. 25). Firstly, the developmental trajectory toward informationally dense prose speaks to compression and is marked by increasingly specialized language that conveys a complex and precise meaning (Staples et al., 2016, p. 136). Secondly, the developmental trajectory toward prose that is open to other perspectives produces civil writers who identify their views in relation to the views of others (Aull & Lancaster, 2014, p. 174). In both cases, the instantiation of select linguistic features produces texts that index disciplinary values and embody salient contextual factors (e.g., audience, participant roles, and site of production).

These two threads of academic writing development are central to this study, and we empirically analyze them by positioning our subcorpora on dimensions of linguistic variation. Initially identified by Biber (1988), these dimensions capture general trends of variation in language use by construing co-correlating lexicogrammatical features as key elements that facilitate communicative purposes (Conrad & Biber, 2001). Put simply, dimensions are groups of features that together realize familiar ways of doing things with words, such as narrating a story, speculating about the future, conveying information, etc. Features are grouped together in a dimension when they positively correlate (e.g., more nouns correlate with more adjectives) or negatively correlate (e.g., more nouns correlate with fewer pronouns). What a multidimensional approach to the study of language allows researchers to see is how these positively and negatively correlating features vary across communicative contexts. For example, prior research has used dimensional variation as a means of understanding the ways in which academic registers and disciplines accomplish their functional purposes in the academy (Biber & Gray, 2016; Biber et al., 2016; Gardner et al., 2015). Most pertinent to our purposes here, Gardner et al. (2015) found that as students made their way through university, their writing became more informationally dense and less overtly persuasive. In other words, developing student writing sheds linguistic features associated with spontaneous language production (i.e., speech) and picks up features that help integrate information and concisely convey meanings. Specifically, the developmental trend toward informationally dense prose observed by Gardner et al. (2015) means that more developed writing instantiates fewer emphatics, amplifiers, and pronouns, and more nouns, prepositions, and attributive adjectives. The use and disuse of this combination of features is typically associated with informationally dense prose because of their capacity to convey information with fewer words.

Given its generative capabilities, students who use ChatGPT run the risk of interfering with this established developmental trajectory. To ascertain the legitimacy of this risk, we project generated and authored texts into dimensions of linguistic variation. As mentioned above, dimensions have been used to study linguistic variation across different communicative contexts, such as register, academic discipline, and stages of writing development; we use dimensions here to study linguistic variation across authorship status and to identify salient lexicogrammatical differences between generated and authored texts. By doing so, we are able to glean how ChatGPT writes in relation to developing and professional academic writers. Results of this study indicate that ChatGPT employs features associated with argumentation and information density in ways that are markedly distinct from human authors. Insight into such differences in turn suggests the tacit forms of activity sedimented into the technology. Our research questions are as follows:

1. In what ways are the lexicogrammatical patterns found in generated texts different from those found in texts authored by developing and professional writers?   
2. What are the qualitative differences between generated and authored texts?

# Methods

An ongoing survey of first-year and intermediate undergraduate writing students at a private midwestern university indicates student interest in LLMs; as well, they want guidance about how to use them in the class. As a response to survey results, and in order to help our students make critical decisions about the use of LLMs, this study analyzes ChatGPT’s generative capabilities by gauging its output against that of established academic writers and novice students. To conduct this inquiry, ChatGPT was prompted to generate hundreds of introductions to two separate assignment prompts from our university’s FYW program.

# Student Survey Description

In Spring 2023, a survey was developed for undergraduate students in foundational and intermediate undergraduate writing courses to elicit student perceptions relevant to college writing and LLMs. Adapted from MacArthur et al.’s (2016) validated measure of writing motivations and beliefs, the survey included six items on a 5-point Likert scale (i.e., “doesn’t describe me at all” $= 1$ , “describes me perfectly” $= 5$ ):

1. I am confident as a writer.   
2. I will use or have used AI-based text generator tools (e.g., ChatGPT, Copy, Jaspar, etc.) to assist in writing.   
3. There are situations in which I will choose not to use AI-based text generators in my writing.   
4. I am interested in learning more about AI-based text generators? (like ChatGPT, Copy.ai, Jasper.ai, Frase, Content AI, etc.).   
5. I believe instructors should allow the use of AI-based text generators for coursework/class assignments.   
6. I believe instructors should provide guidance in their courses to their students on how to use emerging AI-based text generators.

The survey was distributed to instructors throughout 64 sections across a suite of first-year and intermediate undergraduate writing courses. Instructors electing to participate administered the survey to their students in the last 2 weeks of the semester, and student participation was both voluntary and anonymous.

Table 1. Description of Corpus for Analysis.   

<html><body><table><tr><td></td><td>Genre</td><td>Number of texts</td><td>Total words</td><td>Mean text length</td></tr><tr><td>FYW</td><td>IMRD</td><td>100</td><td>17,653</td><td>176.53</td></tr><tr><td></td><td> Proposal</td><td>100</td><td>16,497</td><td>164.97</td></tr><tr><td>Subtotal</td><td></td><td>200</td><td>34,150</td><td>170.75</td></tr><tr><td>MICUSP</td><td>IMRD</td><td>100</td><td>27,418</td><td>274.18</td></tr><tr><td>Subtotal</td><td>Proposal</td><td>47</td><td>10,528</td><td>224.00</td></tr><tr><td> Professional</td><td></td><td>147</td><td>37,946</td><td>258.14</td></tr><tr><td></td><td>IMRD</td><td>100</td><td>66,061</td><td>660.61</td></tr><tr><td>Subtotal</td><td>NP Proposal</td><td> 59</td><td>25,541</td><td>432.90</td></tr><tr><td></td><td></td><td>159</td><td>91,602</td><td>576.11</td></tr><tr><td>ChatGPT</td><td>IMRD</td><td>100</td><td>11,424</td><td>114.24</td></tr><tr><td></td><td> Proposal</td><td>100</td><td>8,818</td><td>88.18</td></tr><tr><td>Subtotal</td><td></td><td>200</td><td>20,242</td><td>101.21</td></tr><tr><td>Total</td><td></td><td>706</td><td>183,940</td><td>260.54</td></tr></table></body></html>

# Corpus Description

Table 1 displays the total number of documents and words for each of our subcorpora, as well as the mean length of documents in each subcorpora. Note that for each document in our corpora, we extracted only the introduction sections, hence the relatively low ratio of documents to total word count.

Student writing. Student writing data were collected during the 2018-2019 academic year to assess the first-year student writing program. The FYW texts analyzed here were written for two separate assignments. The first, the comparative genre analysis (CGA), is a data-driven IMRD-formatted report that tasks students with analyzing and interpreting linguistic patterns in the Michigan Corpus of Upper-level Student Papers (MICUSP). The second assignment is a proposal for change and prompts students to practice a problem-solution-feasibility argument that makes the case for concrete, viable solutions to problems in their communities. Both assignments focus on topics of the students’ choosing. For this project, we randomly sampled 200 total papers from these two major writing assignments.

MICUSP. MICUSP consists of 828 papers from 16 disciplines written by students at the University of Michigan between 2008 and 2009. Analyses of this corpus have proven influential in the understanding of student writing, particularly in comparative studies that reveal the development of academic and genre-specific writing skills (e.g., Aull, 2019; Aull & Lancaster, 2014; Hardy & Römer, 2013). For the purposes of our study, we filtered the corpus to only include papers that we identified as IMRD reports and proposals. Then, from those 147 documents, we extracted the introduction sections.

Professional writing. The professional writing subcorpus uses data from two other corpora: an Elsevier corpus and nonprofit (NP) proposals. The Elsevier corpus was compiled by the authors for a separate research project. It contains published academic articles, which were gathered from Elsevier open access. The entire data set includes more than 40,000 articles from 20 disciplines. We randomly sampled 100 papers that were both divided into contained both four sections and that specified the initial section as an “Introduction.” We refer to this subcorpus as the Professional IMRD corpus.

Our grant proposal corpus contains grants authored by nonprofit groups in the midwest United States. The corpus has been used to identify rhetorical topoi in nonprofit grants distinct from those in research grants (for a history and a detailed description of the corpus, see DeJeu, 2023). For our corpus here, we sampled the initial paragraph of 59 of the 60 nonprofit grants used in DeJeu (2023).

ChatGPT. Our decision to use ChatGPT 3.5 for this research was motivated by an intuitive sense of its accessibility and name recognition. While other LLMs are publicly available, ChatGPT seems the most widely acknowledged. We chose to work with ChatGPT 3.5 in particular because it is free and therefore the likely iteration most students, instructors, and researchers will turn to. Thus, we set out to study the LLM and iteration we believed most likely to be used, not necessarily the most superior one.

The CGA and Proposal assignment prompts were individually provided to ChatGPT, along with instructions to generate an introductory paragraph of roughly 100 words. Initially, ChatGPT summarized assignment directions. After additional directions to write an introduction as if it were completing the assignment, ChatGPT began generating text that read as a response to the assignment prompt. Introductions for both assignment prompts were generated in batches of 10. With each new batch prompt, ChatGPT was instructed to avoid repetition. In total, ChatGPT generated 100 introductions for each assignment.

Admittedly, these prompts are static; there was no iterative engineering of prompts beyond the efforts detailed in the previous paragraph. Our choice to not iteratively engineer our prompts was informed by our belief that most students who turn to ChatGPT to produce academic prose do so under the stress of impending deadlines, and therefore would not engage in prolonged, iterative prompt engineering. Additionally, the identification of output as “good” output or “appropriate” output requires an expertise that many of our students do not yet possess.

Hence, like our choice to study ChatGPT 3.5, our prompts were informed by our intuitive sense— in this case, of what brings students to prompt ChatGPT and use its output in their composition, as well as our sense of what inhibits them from more sophisticated attempts of prompt engineering.

# Description of Methods of Text Analysis

Situated in relation to dimensions of linguistic variation, the subcorpora here are construed as registers. Because registers are identified by corresponding situational factors such as relations among readers and writers, purpose, material constraints, setting, etc. (Biber, 1994; Biber & Conrad, 2019), they vary in capaciousness so long as their respective situational factors remain meaningfully distinct (Conrad & Biber, 2001). These situational factors partially shape and inform patterned linguistic phenomena, which are interpreted as facilitating important communicative functions. Although the research paper and proposal introductions instantiate different genre conventions, these types are analyzed as registers here in light of this study’s focus on pervasive linguistic patterns that serve communicative purposes (Biber & Conrad, 2019). By construing our subcorpora as registers of academic prose, the nonlinguistic factors relevant to the contexts of production of written texts are kept in view. For both authored and generated texts, this theoretical frame affords an interpretative move from linguistic features to salient reflections of situational factors. In other words, by investigating generated text as a register, we can assess the alignment (or, disalignment) between the technology’s affordances and its use in the writing classroom.

The corpora described above were tagged for the 67 lexicogrammatical features described in Biber (1988). Once all features had been aggregated for each text, dimension scores were assigned to texts and subcorpora following methodologies established in previous multidimensional analysis research (Biber, 1988; Conrad & Biber, 2001). In this study, texts are projected onto Dimensions 1 and 4, traditionally interpreted as Involved vs. Informative Production, and Overt Expression of Argumentation. Texts scoring high in Dimension 1 are highly interactive and often face-to-face, while texts scoring low exhibit more so-called information-carrying words with fewer interactional features (e.g., academic journal articles). Meanwhile, texts scoring high in Dimension 4 exhibit features that speak of possibilities, hypotheticals, and necessities (i.e., actions we might take, would take, and should take) that emphasize points as well as directly encourage actions (Conrad & Biber,

2001). These two dimensions were selected because they exhibit the two highest $R ^ { 2 }$ values— $45 \%$ and $2 9 . 5 \%$ , respectively—indicating that these two dimensions account for the highest amount of variance in our subcorpora. For complete lists of salient lexicogrammatical features of Dimensions 1 and 4, see Appendix A.

# Results

In this section, we report survey results and then the results of a quantitative and qualitative textual analysis. While the focus of this research remains with the textual analysis, the survey results provide the needed exigence for the ensuing analysis. Therefore, we wish to briefly touch on them first before reporting on the work they motivate.

# Student Survey Results

Survey participation returned 193 respondents across both first-year and intermediate course sections (see Table 2 below). Bolded values are pertinent to this work, and motivate the following textual analysis.

Student responses showed a distribution of reported attitudes across the scale, but students overwhelmingly reported that they believe there are times when they would choose not to use LLMs to assist in their writing $\scriptstyle { M = 4 . 5 5 }$ , $S D { = } 0 . 7 8 )$ ) and, at the same time, generally disapproved of the idea of instructors allowing the use of LLMs for coursework and class assignment $M { = } 2 . 7 7$ , $S D { = } 1 . 2 0 $ ). Interestingly, this hesitation toward the instructors’ integration of LLMs in their classrooms is paired with a clear interest in learning more about LLMs $\scriptstyle { M = 3 . 5 9 }$ , $S D { = } 1 . 1 9$ ) and an equal eagerness to see instructors “provide guidance” to their students in how to use these emerging technologies $\scriptstyle { M = 3 . 5 9 }$ , $S D { = } 1 . 1 7$ ).

# Quantitative Analysis

The mean scores of our subcorpora in Dimension 1, Involved vs. Informative Production, are visualized in Figure 1. For each projected subcorpora, the first number is the mean score, and the second number is the standard deviation. At “Involved,” the positive end, we see that first-year IMRD and proposal introductions score highest in Dimension 1 (6.59 and 5.19, respectively). Their position at the positive end of Dimension 1 indicates that first-year IMRD and proposal introductions instantiate the highest relative frequencies of features associated with involved texts (e.g., conversation and spontaneous speeches). While the IMRD student subcorpus has the lower standard deviation of the two first-year subcorpora—perhaps on account of the scaffolded nature of the assignment—both standard deviations suggest that FY student writing displays much of the lexical and grammatical variety identified in previous research on linguistic diversity and development in the classroom (e.g., Eckstein & Chang, 2022).

Table 2. Student Survey Responses to Quantitative Questions (Means and Standard Deviations).   

<html><body><table><tr><td>Survey Question</td><td>M</td><td>SD</td></tr><tr><td>I am confident as a writer.</td><td>3.57</td><td>0.86</td></tr><tr><td>I will use or have used Al-based text generator tools (e.g., ChatGPT, Copy, Jaspar, etc.) to assist in writing.</td><td>2.25</td><td>1.28</td></tr><tr><td>There are situations in which I will choose not to use Al- based text generators in my writing..</td><td>4.55</td><td>0.78</td></tr><tr><td>I am interested in learning more about Al-based text generators (like ChatGPT, Copy.ai, Jasper.ai, Frase,e Content Al, etc.)..</td><td>3.59</td><td>1.19</td></tr><tr><td>I believe instructors should allow the use of Al-based text generators for coursework/class assignments.</td><td>2.77</td><td>1.20</td></tr><tr><td>I believe instructors should provide guidance in their courses to their students on how to use emerging Al- based text generators.</td><td>3.59</td><td>1.17</td></tr></table></body></html>

Following the established trajectory of writing development toward more informationally dense, concise prose (Staples et al., 2016), MICUSP IMRD and proposal introductions score somewhat lower in Dimension 1 than those of first-year writing. Their position roughly halfway between first-year student and professional IMRD introductions suggests that upper-level students are gaining proficiency in linguistic devices associated with concision and information density. Upper-level students, nonetheless, instantiate a higher frequency of involved features relative to the professionals in their field. The standard deviations of both upper level IMRD and proposal introductions remain high, perhaps on account of the different disciplinary fields included in MICUSP.

Both types of professional texts are more informationally dense than upper-level student writing, but the professional proposals are more similar in information density to their upper-level analogues than the professional IMRD texts. While perhaps surprising, the professional proposals are made up of nonprofit proposals, not research proposals, and thus employ different argumentative strategies (Dejeu, 2023). Rounding out the authored texts, professional IMRD introductions expectedly score furthest toward the Informative pole.

![](img/87d8fbdb35f349f9ed419f51053e2a7bba927470b568cd53774b7bab34f5acbc.jpg)  
Figure 1. The position of subcopora across Dimension 1, Involved $( + )$ vs Informative $\left( - \right)$ Production. Entry formatting: Name of subcorpus (dimension score, standard deviation).

ChatGPT-generated texts, lastly, are the most informational of the texts analyzed here. With a score of $- 5 . 2$ , generated IMRD introductions score as far from professional IMRD $( - 1 . 5 3 )$ as the latter are from upper-level IMRD introductions (2.67). This data point indicates that the jump in informational density that takes place between upper-level and professional writers similarly occurs between professionally authored and generated IMRD introductions. Moreover, ChatGPT proposal introductions have a mean of $- 9 . 5 2$ , implying that they are nearly twice as informationally dense as generated IMRD introductions and six times as informationally dense as professional IMRD introductions. In addition to their high informational density, ChatGPTgenerated IMRD and proposal introductions have the lowest standard deviations as well, meaning that ChatGPT-generated texts exhibit the least variance around the mean.

One of the benefits of the corpus approach here is that we can capture ChatGPT’s invariance in concordance lines. We provide a few below in Table 3 to give readers a sense of what ChatGPT’s low standard deviation actually reads like. These three groups of concordance lines display degrees of invariance. Concordance lines 9-12 appear nearly identical, whereas in lines 5-8, ChatGPT slots in lexically related items following that. In lines 1-4, ChatGPT shows more syntactic variation, but slotting is still readily apparent (e.g., embark, delve, and traverse).

Importantly, the differences noted in this quantitative analysis occur only along one dimension of linguistic variation. When the subcorpora are projected into Dimension 4, displayed in Figure 2, other relations emerge. Student writing most frequently evidences uses of infinitives (e.g., to study, to read), modal verbs of prediction (e.g., will, would) and necessity (e.g., should, must), and suasive verbs (e.g., recommend, propose). Most noticeably though, the introductions of professional and generated proposals receive similar dimensional scores $( - 0 . 3 7$ and $- 0 . 4 0$ , respectively). This means that professional and generated proposals similarly employ the features noted above. Importantly, Figure 2 means to emphasize that the differences identified in Dimension 1 are only one piece in a composite picture.

# Qualitative Analysis of Text Excerpts

In this section, we provide exemplar sentences from the professionally authored and generated subcorpora in order to gain a qualitative sense of the information density of ChatGPT-generated text. Qualitatively, our first-year and upper-level texts read similarly to those analyzed in previous research (Aull, 2019; Aull & Lancaster, 2014; Staples et al., 2016). With that in mind, we focus exclusively on professional and generated text in this section in order to bring the latter’s qualities most sharply into relief.

<html><body><table><tr><td></td><td>In this</td><td>captivatinge</td><td>academic pursuit, we delve into</td></tr><tr><td>2</td><td>As we traverse this</td><td>cap tivating</td><td>linguistic landscape, our objective is</td></tr><tr><td>3</td><td>we traverse the</td><td>captivatinge</td><td>landscape of language and communicatio</td></tr><tr><td>4</td><td>academic undertaking, we embark on a</td><td>captivatinge</td><td>exploration of language and discourse by</td></tr><tr><td>5</td><td>consumption and advocates for</td><td>comprehensive initiatives.</td><td>that promote conscious consumerism</td></tr><tr><td>6</td><td>advocates for</td><td>comprehensive initiatives.</td><td>that safeguard digital infrastructure</td></tr><tr><td>7</td><td>conservation and advocates for</td><td>comprehensive initiatives.</td><td>that prioritize the protection and</td></tr><tr><td>8</td><td>populations and advocates for</td><td>comprehensive initiatives.</td><td>that prioritize community resilience.</td></tr><tr><td>9</td><td>Ianguage choices, and communication strategies</td><td>evolve</td><td>over time and in comparison to</td></tr><tr><td>10</td><td>Ianguage choices, and communication strategies</td><td>evolve</td><td>over time. By juxtaposing our.</td></tr><tr><td></td><td>Ianguage choices, and communication patterns</td><td>evolve</td><td>over time and in comparison to.</td></tr><tr><td>12</td><td>Ianguage choices, and communication patterns</td><td>evolve</td><td>over time. By juxtaposing our</td></tr></table></body></html>

![](img/9e46eb138b0e857ba2fe30485c779987a188b7d43a68c7790f67e25a1ff90c18.jpg)  
Figure 2. The position of subcopora across Dimension 4. Entry formatting: Name of subcorpus (dimension score, standard deviation).

Professional NP proposals and IMRD introductions are the most informationally dense of our authored texts. A small handful of exemplars closest to the mean scores of the subcorpora are:

1. Pro Proposal: Our goal is to create a safe, inviting, comfortable setting for residents and the public alike in the intake and lobby area of the Ruth Lilly Social Service Center. We are planning a remodeling and renovation project which will improve service delivery, living conditions for the residents and working conditions for the staff.   
2. Pro IMRD: The reduction in global mortality associated with vaccinations is second only to the introduction of safe drinking water [1]. According to the World Health Organisation, childhood vaccinations prevent an estimated 2–3 million deaths per year. Yet despite global increases in childhood vaccine uptake, rates remain sub-optimal $( < 9 5 \% )$ , with vaccine-preventable diseases still posing a public health risk [2].   
3. Pro IMRD: The only specific treatment for snake envenomation is Snake venom antiserum (AVS) developed by Calmette (1894). Snake venom antiserum is of equine origin from the plasma of horse, mules etc that have been hyper-immunized against the most venomous snakes in India. It has been amply documented that intra and inter specific variation in venom composition can affect the neutralization capacity of antivenoms.

These texts demonstrate more phrasal density, containing features such as nominalizations (e.g., The reduction in global mortality), attributive adjectives (e.g., safe drinking water), prenominals (e.g., the intake and lobby area), and postnominal prepositional phrases (e.g., snakes in India). In terms of involved features, we see an occasional use of the to be verb, amplifiers (e.g., amply), intensifiers (e.g, most), and modals of possibility (e.g., can affect). Professional writers, then, demonstrate proficiency with features associated with informational density and more occasionally employ features associated with involved production.

ChatGPT-generated texts, as the following exemplars demonstrate, are nominally dense. Frequent use of prenominal modification, as well as nominalizations, generate sentences brimming with nouns. On account of the nominalizations, the mean word length of our generated subcorpora exceed that of human authored text, as seen in the following exemplars:

4. ChatGPT IMRD: By closely analyzing the language features, narrative structures, and communication strategies employed in these genres, we aim to discern how they reflect the values, goals, and reading purposes of their respective discourse communities, offering valuable insights into the role of language in shaping public understanding and academic inquiry in the realm of social media.

5. ChatGPT IMRD: In this academic expedition, we undertake a comparative study of language and discourse, exploring the writing patterns between two diverse genres: public news reporting and academic writing.   
6. ChatGPT Proposal: By examining the impact of ESD on fostering sustainable practices and showcasing successful ESD implementation models, this document aims to persuade educational policymakers, institutions, and educators to prioritize global citizenship education, fostering a future where individuals understand and actively contribute to global challenges and sustainable development.   
7. ChatGPT Proposal: Transportation is a significant contributor to greenhouse gas emissions and air pollution. This proposal endeavors to confront the issue of unsustainable transportation and advocates for comprehensive green mobility initiatives. By examining the impact of conventional transportation on the environment and showcasing successful green transportation projects, this document aims to persuade governments, transportation companies, and individuals to prioritize sustainable mobility options, fostering a future where transportation is clean, efficient, and environmentally responsible.

Tabling the discussion of nominal density for a moment, these exemplars make apparent ChatGPT’s predilection for recurrent syntactic patterns. Exemplar 7 above is the entire generated paragraph, and we include it here to give a holistic sense of the pragmatic and syntactic inflexibility apparent in our generated texts. All of our ChatGPT-generated exemplars read similarly: the first sentence asserts a broad claim about the topic, the second sentence metadiscursively introduces the text, and the third sentence links textual aims with a thin description of the academic work of the text. Interestingly, the differing dimension scores of generated introductions suggests that ChatGPT has been trained on a certain degree of relevant texts for this genre, as evident by its use of first-person metadiscourse in generated IMRD introductions.

On the subject of nominal density, both the ChatGPT and the professional texts pose a challenge to readers, although perhaps for different reasons. While the professionally authored texts may have more jargon, they nonetheless display a degree of concreteness not evident in ChatGPT generated texts. Even abstract nouns in professional exemplars like childhood vaccine uptake (exemplar 2) are concretized anaphorically by the statistic in the previous sentence. ChatGPT, on the other hand, does not appear to similarly concretize. Readers do not get a concrete sense of what comprehensive green mobility initiatives (exemplar 7) entail; the text goes on to speak of successful green transportation projects and sustainable mobility options, but these are as equally abstract as the initial noun phrase. What this suggests is that high informational density in generated introductions may preclude the generation of text that reasonably grounds abstract concepts in concrete items.

# Discussion

A survey of FYW students at our university found that some students may be cautiously adopting LLMs, but that nearly all report an awareness of the technology’s inappropriateness in some contexts. While the latter may capture students’ sense of “what teachers want to hear,” taken together, these findings suggest that the technology poses a softened threat to academic integrity, at least at our university. In fact, the opposite appears to be the case: students seem genuinely curious about the technology, and report looking to instructors for guidance on how and when to use it. It is to this request for guidance that our work speaks.

This study shows that ChatGPT generates more informationally dense and less involved prose than both novice and professional academic writers. In answer to the first research question (i.e., In what ways are the lexicogrammatical patterns found in generated texts different from those found in texts authored by developing and professional writers?), generated text instantiates more attributive adjectives, nouns, and prepositions—all features associated with information density (Staples et al., 2016)—than authored academic texts. Additionally, the type-token ratio and mean word length is higher in generated texts, which suggests a higher rate of grammatical transformation from other parts of speech into nouns, such as through nominalizations, participles, and gerunds. Moreover, the high Dimension 1 scores suggest that generated texts instantiate fewer hedges and modals of possibility, implying that generated text does not qualify claims or propose diplomatic alternatives on par with professional academic writing (Aull, 2020; Aull & Lancaster, 2014). The resulting texts, then, read as dialogically closed. In sum, generated text is informationally dense, notably more so than professional academic writing. Moreover, this dense information appears to be hermetically packaged and largely closed to outside dialogue.

Our second research question asked after the qualitative differences between generated and authored texts. If generated text seems “empty” or “fluffy,” this sense may be partially informed by its notable information density. In their academic writing textbook, Giltrow et al. (2021) encourage students to concretize high-level concepts with examples in order to “put your reader in closer touch” (p. 79) with abstractions. This is to say that ChatGPT’s proclivity toward nominalizations and post-nominal prepositional modification may impede it from generating pedestrian examples, and in doing so, putting readers in closer touch–to use Giltrow et al.’s words—with abstractions like “ESD implementation models” and “public understanding.”

Alternatively, ChatGPT may struggle to elaborate on abstract signaling nouns (e.g., process, structure, difficulty, and reason), which often depend on more concrete lexical items for a precise meaning (Flowerdew, 2003). In either case, the notable nominal density of generated text seems to partially inform a readerly sense that the text is somehow “empty” or “fluffy.” These findings further substantiate recent suggestions that LLMs “do not make abstract, linguistic generalizations like people do” (Oh & Schuler, 2023, p. 346).

# Pedagogical Implications

A colleague in the English Department at our university shared the following anecdote. She had prompted ChatGPT to generate a CGA of two pieces by the same academic, and her plan was to provide students a copy of the generated text and a rubric, and ask them to assess the generated text as if they were the instructor. She did not tell the students the paper was generated. To her surprise, about half the class spoke negatively of the quality of the paper. Their reasons may sound familiar: the tone was off, it said a lot without saying anything, etc. This instructor reported that these reasons brought most of the students to see legitimate problems with the essay. Four to five students, though, remained enamored with phrasal density—the phrase “communicative strategies” stood out in particular.

We provide this anecdote as an index of our research priorities. ChatGPT appears to generate stylistically dispreferred academic texts, and there is a very real concern that student uptake of such a style may close down opportunities to practice and acquire higher-level rhetorical strategies valued in academic spaces (Aull, 2020). Although LLMs present a dizzying array of research opportunities, we encourage like-minded scholars to prioritize classroom research that balances commitment to student learning with the reality that this technology is here to stay. Our survey of students indicates that students are eager to learn more about LLMs and are equally as eager for guidance from their instructors on how LLMs can be used in their writing. The antipathy that students reported toward allowing “the use of AI-based text generators for coursework/class assignments” may represent the same kind of anxiety that we have, again, seen repeated throughout history (Espinoza, 2015), and whether we are talking about pencil erasers (S. V. G., 1908) or LLMs in college writing classrooms, it is our duty to meet our students’ needs and help them craft a critical sense of the uses and shortcomings of new writing technologies.

As the anecdote is intended to exemplify, LLMs as tools may seduce a small handful of well-intentioned students. For the rest, we are optimistic that they have the capacity to identify the technology’s shortcomings, even without the vocabulary to describe them. To help all our students, then, we need to be ready to talk about why what “sounds good” is in fact dispreferred, perhaps by inviting students to examine the lack of concrete exemplification or meaningful elaboration evident in generated text. This work could engage with, to start, signaling nouns (Flowerdew, 2003), lexical relations and cohesion markers (Halliday & Hasan, 1976), and exemplification (Aull & Lancaster, 2014). Lastly, in this matter, we would be remiss to not direct readers to the ongoing TextGenEd series (Vee et al., 2023) from WAC Clearinghouse, which provides lessons to instructors that encourage students to critically engage with LLMs in the classroom.

Additionally, this research suggests we can serve our students by making ChatGPT’s apparent inflexibility clear for them, especially in matters of syntax. Not only are students who use ChatGPT being shoehorned into a dispreferred style, they are exposing themselves to a narrow window of syntax construction. This generated style appears to vary by slotting related lexical items into rigid syntactic structures—what we among ourselves casually refer to as “mad libbing.” Invariance has its purposes, though, and in this case may help students acquire genre awareness of fairly ossified genres. Taking abstracts as an example, reviewers struggle to differentiate between generated and authored abstracts (Casal & Kessler, 2023), suggesting that generative tools are getting some things right. Answering their students’ call for guidance, instructors might here design a lesson meant to help students identify and practice the conventions of abstracts by generating abstracts at scale, identifying the recurrent features, and prompting students to try their hand at them in an activity.

On the topic of getting some things right, the close Dimension 4 scores of NP and generated proposals suggest another pedagogical use of ChatGPT. Research and NP proposal genres both solicit funds, but NP proposals instantiate distinct forms of argumentation, such as enlarging the problem-to-beaddressed (DeJeu, 2023). Given the similar dimension scores, lessons in proposal writing might be designed to link the explicit forms of argumentation (see Appendix A) evident in generated proposals to the rhetorical features of argumentation identified by DeJeu (2023). These lessons may help students acquire an awareness of the links between a composing context and the various recurrent rhetorical and linguistic features of a genre (Devitt, 2008).

Lastly, as scholars consider the ways inaccessibility is baked into the material and intellectual dimensions of higher education (Dolmage, 2017; Kerschbaum et al., 2017), the nominal style of academic writing has come under needed scrutiny for its capacity to efficiently chunk specific information at the cost of reader comprehension (Aull, 2020). Our research indicates that ChatGPT’s exaggerated nominal style appears even more inaccessible. For the time being, we suggest this fact begs classroom conversations and activities meant to highlight the rhetorical and pragmatic purposes of many interpersonal features of academic text, such as those that signal stance and engagement (Hyland, 2018), as well as cohesion and reformulation. Work with these features may inculcate in students a sense of writing’s interactive capabilities, such that they come to see the importance of conventionally interactive features to a text’s success. This is to say that if we frame successful writing as considerate, we open up for students an examination of the ways that generated text is not.

# Limitations

One set of limitations are linked to our choices in research design. As is often the case in corpus-informed research, one of the predominant limitations of this work is our corpus itself. Simply, the exclusive focus on introductions leaves other potential sections of generated text unexamined. Moreover, there are a number of academic writing tasks we leave unexamined. Beyond the corpus, the choice to project texts against two dimensions of linguistic variation precluded the use of statistical techniques like exploratory factor analysis to identify characteristic patterns of generated text. A second group of study limitations involve ChatGPT and our specific prompts. Other LLMs, and even more advanced versions of ChatGPT, may generate more academically appropriate texts. As well, advances in prompt engineering may eventually indicate that the invariance noted here is more a product of our prompts than the model itself. More empirical research is needed, both in future iterations of ChatGPT and other LLMs, in order to ascertain the degree to which advanced prompt engineering can meaningfully influence lexicogrammatical patterns in output. A third and final cluster of limitations involves our choice in a corpus methodology relative to other, equally insightful means of data collection and analysis. Motivating this research were questions into the potential for interference in established trajectories of writing development that might ensue when we constructively introduce ChatGPT to the writing classroom. We chose to pursue those questions through corpus analysis for reasons provided earlier, and there remain other research approaches and designs that may yield insight into the various affordances and constraints of LLMs.

The promise of the change ChatGPT wields (for better or worse) presents the writing technology as a rich object of inquiry to methodologies common to Composition Studies, Rhetoric, and Linguistics. This paper began with a conversation about technology myths (Haas, 1995), and, as Selfe (1999) reminds us, we should not be lulled into thinking that technology is not the business of instructors, classroom researchers, rhetoricians, linguists, or writers. The ubiquitous specter of LLMs in higher education, as well as the degree to which student writing has emerged as a locus of concern, lays bare the moral dimension of this myth. Text-generative technology may seem a pedagogical problem, but more so it appears to be an opportunity to apply the methods and knowledge this constellation of fields has accrued over the past five decades toward a better understanding of our changing world. As scholars who study writing, technology has always been our business.

# Appendix A

Table A1. Feature Loadings of Dimension 1, Involved Versus Informative Production.   

<html><body><table><tr><td>Feature</td><td>Dimension!</td></tr><tr><td>f_56_verb_private</td><td>0.96</td></tr><tr><td>f_60_that_deletion</td><td>0.91</td></tr><tr><td>f_59_contractions</td><td>0.9</td></tr><tr><td>f_03_present_tense</td><td>0.86</td></tr><tr><td>f_07_second_person_pronouns</td><td>0.86</td></tr><tr><td>f_12_proverb_do</td><td>0.82</td></tr><tr><td>f_67_neg_analytic</td><td>0.78</td></tr><tr><td>f_10_demonstrative_pronoun</td><td>0.76</td></tr><tr><td>f_06_first_person_pronouns</td><td>0.74</td></tr><tr><td>f_49_emphatics</td><td>0.74</td></tr><tr><td>f_09_pronoun_it</td><td>0.71</td></tr><tr><td>f_I9_be_main_verb</td><td>0.71</td></tr><tr><td>f_35_because</td><td>0.66</td></tr><tr><td>f_50_discourse_particles</td><td>0.66</td></tr><tr><td>f_47_hedges</td><td>0.58</td></tr><tr><td>f_48_amplifiers</td><td>0.56</td></tr><tr><td>f_34_sentence_relatives</td><td>0.55</td></tr><tr><td>f_I3_wh_question</td><td>0.52</td></tr><tr><td>f_52_modal_possibility</td><td>0.5</td></tr><tr><td>f_65_clausal_coordination</td><td>0.48</td></tr><tr><td>f_23_wh_clause</td><td>0.47</td></tr><tr><td>f_61_stranded_preposition</td><td>0.43</td></tr><tr><td>f_40_adj_attr</td><td>-0.47</td></tr><tr><td>f_43_type_token</td><td>-0.54</td></tr><tr><td>f_39_prepositions</td><td>-0.54</td></tr><tr><td>f_44_mean_word_length</td><td>0.58</td></tr><tr><td>f_I6_other_nouns</td><td>-0.8</td></tr></table></body></html>

Table A2. Feature loadings of Dimension 4, Overt Forms of Argumentation.   

<html><body><table><tr><td>Feature</td><td>Dimension 4</td></tr><tr><td>f_24_infinitives</td><td>0.76</td></tr><tr><td>f_37_if</td><td>0.47</td></tr><tr><td>f_53_modal_necessity</td><td>0.46</td></tr><tr><td>f_54_modal_predictive</td><td>0.54</td></tr><tr><td>f_57_verb_suasive</td><td>0.49</td></tr><tr><td>f_63_split_auxiliary</td><td>0.44</td></tr></table></body></html>

# Declaration of Conflicting Interests

The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.

# Funding

The authors received no financial support for the research, authorship, and/or publication of this article.

# ORCID iDs

Ben Markey https://orcid.org/0009-0008-9808-2767   
David West Brown https://orcid.org/0000-0001-7745-6354   
Michael Laudenbach https://orcid.org/0000-0003-1691-0562

# References

Appleman, D. (2023). A brave new world requires courage: New directions for literacy research and teaching. Research for the Teaching of English, 58(1), 21–33. https://doi.org/10.58680/rte202332608   
Aull, L. (2015). First-year university writing. Palgrave Macmillian.   
Aull, L. (2019). Linguistic markers of stance and genre in upper-level student writing. Written Communication, 36(2), 267–295. https://doi.org/10.1177/0741088318819472   
Aull, L. (2020). How students write. MLA.   
Aull, L., & Lancaster, Z. (2014). Linguistic markers of stance in early and advanced academic writing: A corpus-based comparison. Written Communication, 31(2), 151–183. https://doi.org/10.1177/0741088314527055   
Aull, L., & Ross, V. (2020). From cow paths to conversation: Rethinking the argumentative essay. Pedagogy: Critical Approaches to Teaching Literature, Language, Composition, and Culture, 20(1), 21–34. https://doi.org/10.1215/15314200- 7878975   
Aull, L. L., Bandarage, D., & Miller, M. R. (2017). Generality in student and expert epistemic stance: A corpus analysis of first-year, upper-level, and published academic writing. Journal of English for Academic Purposes, 26, 29–41. https:// doi.org/10.1016/j.jeap.2017.01.005   
Bazerman, C. (2013). A theory of literate action. The WAC Clearinghouse and Parlor Press. https://doi.org/10.37514/PER-B.2013.4791   
Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? [Conference session]. FAccT ’21 Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. Virtual Event, Canada. https://doi. org/10.1145/3442188.3445922   
Biber, D. (1988). Variation across speech and writing. Cambridge University Press.   
Biber, D. (1994). An analytical framework for register studies. In D. Biber & E. Finegan (Eds.), Sociolinguistic perspectives on register (pp. 31–56). Oxford University Press.   
Biber, D., & Conrad, S. (2019). Register, genre, and style. Cambridge University Press.   
Biber, D., & Gray, B. (2016). Grammatical complexity in academic English. Cambridge University Press.   
Biber, D., Gray, B., & Poonpon, K. (2011). Should we use characteristics of conversations to measure grammatical complexity in L2 writing development? TESOL Quarterly, 45(1), 5–35. https://doi.org/10.5054/tq.2011.244483   
Biber, D., Gray, B., & Staples, S. (2016). Predicting patterns of grammatically complexity across language exam task types and proficiency levels. Applied Linguistics, 37(5), 639–668. https://doi.org/10.1093/applin/amu059   
Bivens, K. M., & Cook, K. C. (2018). Coordinating distributed memory: An environmental engineer’s proposal-writing process using a product calculator. Journal of Business and Technical Communication, 32(3), 285–307. https://doi. org/10.1177/0741088317753348   
Byrd, A. (2023). Truth-telling: Critical inquiries on LLMs and the corpus texts that train them. Composition Studies, 51(1), 135–142. https://compositionstudiesjournal.files.wordpress.com/2023/06/byrd.pdf   
Casal, J. E., & Kessler, M. (2023). Can linguists distinguish between ChatGPT/ AI and human writing?: A study of research ethics and academic publishing. Research Methods in Applied Linguistics, 2(3), 1–12. https://doi.org/10.1016/j. rmal.2023.100068   
Ching, K. L. (2018). Tools matter: Mediated writing activity in alternative digital environments. Written Communication, 35(3), 344–375. https://doi. org/10.1177/0741088318773741   
Clayson, A. (2018). Distributed cognition and embodiment in text planning: A situated study of collaborative writing in the workplace. Written Communication, 35(2), 155–181. https://doi.org/10.1177/0741088317753348   
Conrad, D., & Biber, D. (2001). Variation in English. Routledge.   
Curzan, A. (2014). Fixing English: Prescriptivism and language history. Cambridge University Press.   
DeJeu, E. B. (2023). Topoi of nonprofit proposal writing. Business and Professional Communication Quarterly, 77(1), 1–30. https://doi.org/10.1177/232949062 31182616   
Devitt, A. J. (2008). Writing genres. Southern Illinois University Press.   
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. Proceedings of 2019 NAACL, Human Language Technologies, 1, 4171–4186. https://doi. org/10.18653/v1/N19-1423   
Dolmage, J. T. (2017). Academic ableism: Disability and higher education. University of Michigan Press.   
Eckstein, G., & Chang, R. (2022). How does the language control of L1 and L2 writers develop over time in first-year composition. Written Communication, 39(4), 600–629. https://doi.org/10.1177/07410883221099474   
Espinoza, J. (2015, May 26). Erasers are an ‘instrument of the devil’ which should be banned, says academic. The Telegraph. https://www.telegraph.co.uk/education/ educationnews/11630639/Ban-erasers-from-the-classroom-says-academic.html   
Firth, J. R. (1957). Papers in linguistics, 1934-1951. Oxford University Press.   
Fischer, R., & Grusin, E. K. (1992). Grammar checkers: Programs that may not enhance learning. The Journalism Educator, 47(4), 20–27.   
Flowerdew, J. (2003). Signalling nouns in discourse. English for Specific Purposes, 22(4), 329–346. https://doi.org/10.1016/S0889-4906(02)00017-0   
Friginal, E., & Weigle, S. (2014). Exploring multiple profiles of L2 writing using multi-dimensional analysis. Journal of Second Language Writing, 26, 80–95. https://doi.org/10.1016/j.jslw.2014.09.007   
Gabrial, B. (2007). History of writing technologies. In C. Bazerman (Ed.), Handbook of research on writing (pp. 23–33). Routledge.   
Gardner, S., Biber, D., & Nesi, H. (2015). MDA perspectives on Discipline and Level in the BAWE corpus. In F. Formato & A. Hardie (Eds.), Corpus linguistics: Abstract book (pp. 126–128). UCREL.   
Gerrard, L. (1989). Computers and basic writers: A critical view. In G. E. Hawisher & C. L. Selfe (Eds.), Critical perspectives on computers and composition instruction (pp. 94–108). Teacher’s College Press.   
Giltrow, J., Gooding, R., & Burgoyne, D. (2021). Academic writing (4th ed.). Broadview.   
Haas, C. (1989). How the writing medium shapes the writing process: Effects of word processing on planning. Research in the Teaching of English, 23(2), 181–207. https://www.jstor.org/stable/40171409   
Haas, C. (1995). Writing technology: Studies on the materiality of literacy. Lawrence Erlbaum.   
Halevy, A., Norvig, P., & Pereira, F. (2009). The unreasonable effectiveness of data. IEEE Intelligent Systems, 24(2), 8–12. https://doi.org/10.1109/MIS.2009.36   
Halliday, M. A. K., & Hasan, R. (1976). Cohesion in English. Longman.   
Hardy, J. A., & Römer, U. (2013). Revealing variation in student writing: A multidimensional analysis of the Michigan corpus of upper-student papers (MICUSP). Corpora, 8(2), 183–207. https://doi.org/10.3366/cor.2013.0040   
Holland, D., & Cole, M. (1995). Between discourse and schema: Reformulating a cultural-historical approach to culture and mind. Anthropology & Education Quarterly, 26(4), 475–489. https://www.jstor.org/stable/3195758   
Hyland, K. (2018). Stance and engagement: A model of interaction in academic discourse. Discourse Studies, 7(2), 173–192. https://doi.org/10.1177/1461445605050365   
Kerschbaum, S. L., Eisenman, L. T., & Jones, J. M. (Eds.). (2017). Negotiating disability: Disclosure and higher education. University of Michigan Press.   
Lang, S., Buell, D. A., & Elliot, N. (2023). Computer-assisted corpus analysis: An introduction to concepts, processes, and decisions. IEEE Transactions on Professional Communication, 66(1), 94–113. https://doi.org/10.1109/ TPC.2022.3228026   
MacArthur, C. A., Philippakos, Z. A., & Graham, S. (2016). A multicomponent measure of writing motivation with basic college writers. Learning Disability Quarterly, 39(1), 31–43. https://doi.org/10.1177/0731948715583115   
McKee, H. A., & Porter, J. E. (2017). Professional communication and network interaction: A rhetorical and ethical approach. Routledge.   
Oh, B., & Schuler, W. (2023). Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times? Transactions of the Association for Computational Linguistics, 11, 336–350. https://doi.org/10.1162/ tacl_a_00548   
Parkinson, J., & Musgrave, J. (2014). Development of noun phrase complexity in the writing of English for Academic Purposes students. Journal of English for Academic Purposes, 14, 48–59. https://doi.org/10.1016/j.jeap.2013.12.001   
Pennington, M. C. (1993). Computer-assisted writing on a principled basis: The case against computer-assisted text analysis for non-proficient writers. Language and Education, 7(1), 43–59. https://doi.org/10.1080/09500789309541347   
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. Proceedings from 2018 NAACL: Human Language Technologies, 1, 2227–2237. https://doi.org/18653/v1/ N18-1202   
Prior, P. (1998). Writing/disciplinarity. Routledge.   
Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding with unsupervised learning. Open AI.   
Russell, D. R. (1991). Writing in the academic disciplines: A curricular history. Southern Illinois University Press.   
Selfe, C. L. (1999). Technology and literacy: A story about the perils of not paying attention. College Composition & Communication, 50(3), 411–436.   
Stanton, C. (2023). A dis-facilitated call for more writing studies in the new AI landscape; or, Finding our place among the chatbots. Composition Studies, 51(1), 182–186. https://compositionstudiesjournal.files.wordpress.com/2023/06/stanton.pdf   
Staples, S., Egbert, J., Biber, D., & Gray, B. (2016). Academic writing development at the university level. Written Communication, 33(2), 149–183. https://doi. org/10.1177/0741088316631527   
Sterponi, L., Zucchermaglio, C., Alby, F., & Fatigante, M. (2017). Endangered literacies? Affordances of paper-based literacy in medical practice and its persistence in the transition to digital technology. Written Communication, 34(4), 359–386. https://doi.org/10.1177/0741088317723304 S. V. G. (1908). Do your pupils use erasers? Western Teacher: Devoted to Schoolroom Methods. Practical Aids and Usable Materials for Progressive Teachers, 16(5),   
175–176. https://books.google.com/books?id ${ } = { }$ CKOfn_EPJNgC Vee, A. (2023). Large language models write answers. Composition Studies, 51(1),   
176–181. https://compositionstudiesjournal.files.wordpress.com/2023/06/vee. pdf Vee, A., Laquintano, T., & Schnitzler, C. (Eds.). (2023). TextGenEd: Teaching with text generation technologies. The WAC Clearinghouse. https://doi.org/10.37514/ TWR-J.2023.1.1.02 Weaver, W. (1949). Translation. The Rockefeller Foundation New York.

# Author Biographies

Ben Markey is a PhD Student in Rhetoric at Carnegie Mellon University. His research interests include teaching assistant writing assessment practices in statistics and data science.

David West Brown is Associate Teaching Professor of English and Associate Director of First-Year Writing for Research and Assessment at Carnegie Mellon University. His research examines writing as a social practice, its structures, and its history.

Michael Laudenbach is an Assistant Professor in Humanities and Social Sciences at New Jersey Institute of Technology. His research pursues questions surrounding corpus-based approaches to genre-based writing instruction in engineering, statistics, and data science.

Alan Kohler is a Lecturer in Writing and Communication at Carnegie Mellon University. His research and professional interests include generative AI in education, UX design approaches to writing instruction, higher education and program administration, and teacher training and development.