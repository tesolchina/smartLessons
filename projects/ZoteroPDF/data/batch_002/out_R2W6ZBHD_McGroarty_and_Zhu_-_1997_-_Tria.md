# Triangulation in Classroom Research: A Study of Peer Revision

Mary E. McGroarty Northern Arizona University

Wei Zhu University of Texas, Pan American

This study investigated the effects of training for peer revision in college freshman English composition classes. Four instructors and 169 students participated. Each instructor taught one class in the experimental condition,

We are additionally indebted to an experienced composition instructor, Tim Hacker, who had developed a system of using individual conferences to train students for peer revision before this research, and served as an expert trainer for our study.

Correspondence concerning this article may be addressed to Mary McGroarty, English Department, Box 6032, Northern Arizona University, Flagstaff, Arizona 86011-6032. Internet: Mary.McGroarty@nau.edu

which included training for peer revision via instructor conferences, and one class in the control condition, which employed peer revision without such training. We assessed the effects of training in terms of (a) students’ ability to critique peer writing; (b) quality of student writing; and (c) students’ attitudes toward peer revision and writing in general. We used different measures, data sources, and methods. The combination of measures, data sources, and methods not only allowed triangulation of the finding that training for peer revision improved students’ ability to critique peer writing and their attitudes toward peer revision but also illuminated other aspects of peer revision processes.

Peer revision, in which students work in pairs or small groups to provide feedback on one another’s writing, has become a widely used teaching method in first (L1), second (L2), and foreign language writing instruction. Due to student collaboration’s pedagogical prominence in the composition classroom and its assumed role in advanced literacy development, peer revision has attracted increasing empirical research, often classroom-based. Like other learning phenomena, peer revision is multi-dimensional and “defies a single operational definition” (Webb, 1978); therefore, a deeper understanding of peer revision requires a multidimensional approach combining different research methods, theoretical orientations, and data sources.

Essential to such a multi-dimensional approach is the concept of “triangulation,” which entails “inspection of different kinds of data, different methods, and a variety of research tools” (van Lier, 1988, p. 13) in a single investigation. Denzin (1978) identified different varieties of triangulation: Theoretical triangulation uses different perspectives to analyse the same set of data; data triangulation uses multiple data sources and data sets (different data sets may be obtained through different methods and/or the same method at different times or with different sources, Brannen, 1992); investigator triangulation uses multiple observers, researchers, or evaluators; methodological triangulation uses multiple measures of a given concept (“triangulation of measurement”, Isaac & Michael, 1991) or the integration of different research methods. Recently, Miles and Huberman (1994) have proposed triangulation by data type; Janesick (1994) has argued for interdisciplinary triangulation, which uses different disciplines to inform research processes and to broaden understanding of research methods. Triangulation has an important advantage: It allows corroboration, elaboration, and illumination of the issue in question (Marshall & Rossman, 1995). Studies relying on a single method are more vulnerable to errors linked to that particular method (Patton, 1990). In educational research, triangulation of measurement is particularly crucial because “there are serious risks in making recommendations based on a single criterion which fails to consider the whole educational outcome of an educational process” (Isaac & Michael, 1981, p. 92).

Given the widespread use of peer revision in writing instruction for L1 and L2 writers, it is crucial to integrate methodologies and data sources to examine the complex web of questions related to peer revision and triangulate findings, so that the results can better inform both theory and classroom practice. Although multiple triangulation (the combination of different types of triangulation), is difficult in a single study, use of multiple methods, measures, and data sources is certainly possible.

Research on peer revision has examined various of its aspects, but relatively few studies have integrated different methods, data sources, measures, and theoretical perspectives. Generally speaking, researchers have used quantitative methods to investigate peer revision’s effects on the quality of student writing (Chaudron, 1983; Graner, 1987; Hedgcock & Lefkowitz, 1992; Karegianes, Pascarella & Pflaum, 1980), using scores on student compositions as the single source of data. More recent studies have quantitatively examined the affective advantages of peer feedback (e.g., Zhang, 1995). Qualitative research has predominantly explored student talk and interaction during peer revision (Gere & Abbott, 1985; Lockhart & Ng, 1995; Sommers $\&$ Lawrence, 1992; Villamil & De Guerrero, 1996). These studies generally used taperecordings of student discussions during peer revision as data. Qualitative studies have also examined the influences of peer feedback on students’ revisions (Connor & Asenavage, 1994; Herrington & Cadman, 1991; Mendonca & Johnson, 1994; Nelson & Murphy, 1993) and students’ reactions to peer revision (Carson & Nelson, 1996; Jacobs, 1987; Mangelsdorf, 1992). Unfortunately, utilizing a single method, data source, or measurement precludes internal validation of the findings.

A few studies on peer revision have integrated measurements, data, and methods. For example, in assessing the effects of peer responses on students’ revision, Nystrand and Brandt (1989) employed both data triangulation and triangulation of measurement (Isaac & Michael, 1981). They measured “effects of peer response on revision” in terms of not only student writing quality but also students’ perception of their revision needs and assessments of their revision’s strengths and weaknesses. Analysis of data from these sources revealed significant differences on all measures between students who used peer revision and those who did not. In this case, triangulation allowed corroboration and more confident interpretation of research findings, as well as a more thorough understanding of peer revision’s effects.

In her study of peer revision in 2 ninth-grade classes, Freedman (1992) used methodological and data triangulation by combining quantitative and qualitative approaches and using data from a variety of sources (tape-recordings of student discussions, researcher observations, interviews). This approach enabled her not only to describe different types of talk during peer revision but also to identify the factors influencing peer collaboration. Examining task and social dimensions of an L2 writing group, Nelson and Murphy (1992) used data triangulation within a qualitative methodology, taking data from a number of sources, including videotapes of student discussions, student journals, student interviews with the instructor, student interviews with a third-party interviewer, and student drafts. This data triangulation allowed integration of different perspectives: those of both the participants and the researchers.

Advances in classroom research on peer revision require a full range of triangulations to answer the full range of questions entailed, as is true in most applied settings (Cook, 1985; Dennis, Fetterman, & Sechrest, 1994). Our study attempts to do just that in investigating the effects of training for peer revision in college freshman composition classes. The study was prompted by inconsistent research findings on the effectiveness of peer revision. Because implementation of peer revision takes so many forms, comparing studies without detailed descriptions of the procedures used and the frequency of peer revision can be misleading. Findings from classrooms somewhat comparable to the situation of our study have been decidedly mixed. Nystrand (1986) found that college students who participated regularly in peer revision wrote significantly better than those who did not. However, RothsteinVandergriff and Gilson (1988) found that peer revision in the 2 freshman composition classes they studied frequently focused on the wrong part of the writing process and emphasized sentencelevel grammatical and mechanical errors rather than content. According to Graner (1987), who examined techniques used in post-secondary writing classes, activities such as class discussion and individual revision based on teacher checklists helped students improve their writing just as much as peer revision.

Our study was also motivated by research on the importance of preparing students for cooperative group tasks (McGroarty, 1989; Stevens, Madden, Slavin, & Farnish, 1987). We were especially intrigued by studies of the effects of preparing students for peer revision tasks (Sommers & Lawrence, 1992; Stanley, 1992), notably Stanley’s study, which examined the effects of training students for peer revision in university ESL classrooms. In her study, one group of students (the experimental group) received extensive training; the other (the control group) did not. Stanley audiotaped and analyzed students’ peer revision sessions along with students’ revisions. She assessed the effects of training qualitatively, primarily in terms of quality of student interaction, using tape-recordings of student discussions as the primary data source.

We set out to investigate whether specifically training students in peer revision would enhance its effectiveness in college freshman writing classes. To thoroughly understand the effects of the training and to validate the research findings, we used triangulation, particularly methodological and data triangulation. We achieved methodological triangulation through multiple measurements and a combination of quantitative and qualitative methods, data triangulation through using multiple data sources and data sets. Because “operationalism is better served by multiple measures of a given concept or attribute” (Isaac & Michael, 1981, p. 92), we assessed “effects of training” using multiple criteria (triangulation of measurement): (a) students’ ability to critique peer writing, (b) quality of student writing, and (c) students’ attitudes toward peer revision and writing. We used different methods of data collection, data from different sources, and both quantitative and qualitative analyses.

Our basic research question was: What are the effects of training for peer revision in college freshman composition classes? We posed three specific research questions to guide our investigation:

1. What are the effects of training for peer revision on college students’ ability to comment on peer writing?   
2. What are the effects of training for peer revision on college students’ writing?   
3. What are the effects of training on college students’ attitudes toward peer revision?

Our main purpose here is to report how triangulation enabled us to examine the various aspects of training for peer revision and validate research findings. (Readers desiring more detailed information on various aspects of the study should consult Zhu, 1994, 1995.)

# Method

# Context

The institution and the composition program. The present study took place at a comprehensive, medium-sized, 4-year public university of about 19,000 students, located in a town of about 50,000 people in the southwestern United States. Most students were Anglo $( 8 0 . 6 \% )$ , with $8 . 3 \%$ Hispanic, $5 . 6 \%$ American Indian, and the remaining small percentage from varied ethnic backgrounds. All undergraduate students had to take Freshman Composition, a course designed to assist them in developing critical reading, thinking, and writing skills. In 1993–94, the academic year of the study, freshmen enrolling at the university had an average American College Testing (ACT) English test score of 21.0 (on a scale of 1–36), the same as the U.S. national average for that year. Classroom instruction in English 101 (Freshman Composition, first semester) focused on both reading and writing, with considerable emphasis placed on the writing process. The course required multiple drafts of out-of-class assignments, and peer revision was an integral part of writing instruction. In these respects, the freshman composition population and the general type of instruction were similar to most universities in the U.S. (Atkinson & Ramanathan, 1995, describe a comparable, though larger, composition program.)

# Participants

Participants in this study were 169 students enrolled in 8 sections of English 101 during the Fall 1993 semester and the 4 instructors teaching these classes.

The students. Participating students were 69 men and 100 women. They volunteered to participate because their instructors had also agreed to. Most were average-age (18 years old) freshmen; a few were older. A great majority (144, $8 5 \%$ ) were native speakers of English; of the remaining 25, just under half (11) were self-designated native bilinguals, or native speakers of Spanish, an American Indian language or other languages, who had resided and attended schools in the U.S. for at least some of their school years. According to self-report data, $5 5 \%$ had had some experience with peer revision before English 101.

The instructors. The 4 instructors, 3 men and 1 woman, were all graduate teaching assistants, each teaching two sections of English 101. All had had some previous teaching experience. We selected them primarily for their willingness to participate.

# Design

We used a quasi-experimental, nonrandomized controlgroup pretest-posttest design, set out schematically in Figure 1. We divided the 8 participating sections of English 101 into 2 groups, the experimental group (4 sections) and the control group (4 sections). Each participating instructor taught one class in the experimental group and one in the control group, thus mitigating to some degree any idiosyncratic teacher effect. For each instructor, we randomly designated one class as experimental and the other as control. The experimental group received systematic training for peer revision; the control group did not. We then compared the 2 groups with respect to their ability to critique peer writing, the overall quality of their writing, and their attitudes toward peer revision and writing in general.

Figure 1. Design of Study of Peer Revision Training.   

<html><body><table><tr><td>Instructor</td><td>Experimental Group</td><td>Control Group</td></tr><tr><td>A</td><td>Pr/A/V TC PR/Pt Po/A*</td><td>Pr/A/V RI PR/Pt Po/A*</td></tr><tr><td>B</td><td>Pr/A/V TC PR/Pt Po/A*</td><td>Pr/A/V RI PR/Pt Po/A*</td></tr><tr><td>c</td><td>Pr/A/V TC PR/Pt Po/A*</td><td>Pr/A/V RI PR/Pt Po/A*</td></tr><tr><td>D</td><td>Pr/A/V TC PR/Pt Po/A*</td><td>Pr/A/V RI PR/Pt Po/A*</td></tr><tr><td>Note: Pr = writing pretest</td><td>V = video on revision</td><td>A = attitude questionnaire (pre) TC = training conference</td></tr><tr><td></td><td>RI = regular instruction</td><td></td></tr><tr><td></td><td>PR = peer revision session</td><td>Pt = writing postest</td></tr><tr><td></td><td>Po = portfolio evaluation TC PR/Pt repeated three times for the experimental group;</td><td>A* = attitude questionnaire (post)</td></tr></table></body></html>

# Procedure

Pre-tests. All students signed a consent form allowing researchers access to course-related information. Before the study, they completed an individual writing assignment, used as a pre-test of writing quality. We measured writing quality on a 5- point holistic scale (Appendix).1 In addition, all students completed a questionnaire designed to assess their attitudes toward peer revision. This questionnaire employed a semantic differential (Mueller, 1986; Osgood, Suci & Tannenbaum, 1957) on which students indicated their attitudes toward 4 aspects of writing and writing instruction (writing in general; writing instruction; peer revision; and receiving teacher feedback) along four 7-point scales, each using a different adjective pair, yielding 16 $\left( 4 \times 4 \right)$ items. A sample question, which would represent the 4 different items related to peer revision, is as follows:

What do you think about discussing writing with your classmates?

worthless valuable meaningful meaningless bad _ good pleasant _ unpleasant

The pre-tests were to determine whether the experimental and control groups were similar in terms of their writing abilities and attitudes toward peer revision before the study. Analyses of the writing pre-test and the pre-course questionnaires revealed no significant differences between the experimental and control classes on any dimension. Insofar as possible when using intact classes, the experimental and control groups were indeed comparable.

Post-tests. As a post-test, we scored 2 essay assignments done by each student after peer revision. We used the same scoring system as for the pre-test writing assignments. We also administered a post-questionnaire at the end of the study. The post-questionnaire employed the same semantic differential used in the prequestionnaire on the same 4 aspects of writing and added 3 additional dimensions, training for peer revision, receiving peer feedback,andwatchingavideoonpeerrevision.2Again,weinvestigated each aspect with 4 scales, yielding 28 (7 x 4) items. Also, we included an open-ended question, “Please comment on the use of peer revision groups in this class as it relates to your writing and/or your opinion of this class,” to invite students’ comments on the regular use of peer revision after a semester of composition instruction.

The instructional treatments. Prior to the study, we introduced both groups of students to peer revision by viewing a 28-minute video demonstrating peer critiquing processes (Beginning Writing Groups, Hale, Mallon, and Wyche-Smith, 1991). All classes, after viewing the video, had some discussion of the purpose of peer revision and of the video. At the time, this was the typical preparation for peer revision used at the university. Students in the control group received no further training for peer revision.

Instructor training. The experimental treatment consisted of special teacher/student conferences to model and practice peer revision. We trained the instructors before they used the treatment with students. Most of the instructor training took place in one session, although we provided considerable informal training both before and after the training session itself. All 4 instructors attended the formal training session, about 2 hours. During the training session, we emphasized that the conference was to prepare students for peer revision, not simply for providing feedback on one student’s writing. To illustrate training for these conferences, we analyzed in detail a transcript of a conference in which an instructor was coaching a student for peer revision.3 The instructors and trainers then discussed methods and techniques for group conferences, appropriate length of conferences, classroom procedures, and the teacher’s role during peer revision.

Student training conferences. The instructors then trained students in the experimental group for peer revision through conferences devoted to practicing strategies for effective feedback on peer writing (see Hacker, 1996).4 These conferences took place 3 times during the semester, each prior to student peer revision of a take-home writing assignment. We chose the conference method because it enables teachers to interact with students, to provide more individualized instruction, and to check the comprehensibility of their feedback and instruction (Carnicelli, 1980; Fassler, 1978; Harris, 1987, 1990). The training conferences, from 15 to 25 minutes long, were group conferences, involving one instructor and three students. We chose this format so that (a) training would occur in a situation resembling peer revision in the classroom, which typically involves many students; and (b) teachers could guide the group’s communication to foster the kinds of interaction characteristic of successful peer revision groups. For each conference, one student volunteered writing to be critiqued. The papers volunteered, however, were not drafts on which students were working at the time, but expository papers done for other classes or before the current composition assignment. This ensured that no students received teacher feedback on their first drafts before the actual peer revision sessions where we collected data. Generally speaking, students volunteered expository essays similar to those on which they were working at the time. For each assignment, the instructors canceled one regular class period to schedule the conferences. Instructors also used time outside class for the training conferences.

Procedure for student training conferences. These conferences typically consisted of 2 parts. In the first, the student who had volunteered read the essay out loud while the teacher and the peers followed along on copies of the essay and wrote brief comments. In the second, the instructor and the students together discussed the strengths and weaknesses of the essay and provided suggestions for revision. The instructors often opened the discussion with the question “So, what do you think?” In this part, instructors focused on helping students respond critically to peer writing and to provide specific feedback. They made it clear that when critiquing peer writing, the students should focus on global concerns, such as development of ideas, audience and purpose, and organization. Because a primary goal was to help students generate specific feedback, the instructors often asked students to clarify and/or to specify their comments and suggestions. Where necessary, instructors would model how specific comments could be made. Because negotiation among students was vital to the success of peer revision, the instructors emphasized the importance of interaction both between the writer and peers and among all group members, often specifically underscoring the writer’s active role. Where relevant, instructors provided some direct though concise writing instruction, centered mainly on the more global concerns of writing. They also provided relevant instruction on the purpose of peer revision.

While the experimental group was trained for peer revision, control group students either had regular classes or individual conferences with the instructors in which the instructors and the student discussed essays that the student had already completed but wished to further revise, or other issues pertaining to writing and/or the writing class. No teacher feedback was available on students’ evolving drafts before peer revision.

Student peer revision sessions. Students’ initial drafts for each of the last 3 take-home assignments in both experimental and control classes underwent in-class peer revision. Typically, students worked in groups of 3s, self-selected or randomly assigned by the teacher. The instructors consistently gave their classes the same amount of time, from 45 to 75 minutes depending on the instructors. All classes followed the same procedures. The instructors asked students to bring copies of their drafts for group members and gave them all the same instructions: to provide one another with specific comments and suggestions and to engage in negotiation of meaning. In each group, the writer first read the paper while others followed along with copies of the draft. Next, students responded to the essays in writing, using teacherprovided response sheets (which asked students to consider the writing’s strengths and weaknesses, raise questions about it, and provide suggestions for revision). They then discussed the essays; the discussions were not limited to the written feedback provided on the response sheet. We tape-recorded these group discussions. The instructors did not participate in the discussions; their role was to facilitate by ensuring that the students understood the procedures. Following the peer revision sessions, students in both the experimental and the control classes taught by each instructor received similar amounts of time to revise their writing and hand back their revised drafts for teacher feedback.

Researcher observations and interviews of participating instructors. During the study, we observed all participating classes at least 5 times; our observations revealed that each instructor’s experimental and control classes had received the same instruction, aside from the special peer revision training. Thus, the observations demonstrated fidelity of treatment, requisite in a study exploring treatment effect. We also observed all peer revision sessions. In addition, we observed and tape-recorded at least half of each instructor’s conferences, acting as nonparticipating observers. Further, we talked to all instructors several times to elicit their perceptions of peer revision in their classes. Both of us frequently contacted the director of the freshman composition program, who provided useful information about participating instructors’ reactions to the research project which emerged in the course of his work with them.

Though all participants knew they were part of a study of peer revision, they were not informed of the hypothesized relationships or the expected outcomes. Hence, although Hawthorne effect could cause problems in a study like this, we tried to avoid it by discussing the precise details of the study as little as possible with the participants, by gathering information from unobtrusive measures, and by visiting all classes a similar number of times.

Data. We used data from various sources: (a) students’ written comments on peer writing, (b) students’ initial drafts on which peer feedback was generated, (c) tape-recordings of student peer revision sessions, (d) holistic scores on assignments students had written before the study and essays that they had revised following peer revision, (e) student responses to the pre- and post-test attitude questionnaires, (f) notes of and material from classroom observations, and $( \mathbf { g } )$ notes from informal interviews of the participating instructors and the director of the composition program.

# Analysis and Results

To facilitate presentation, we organize this section around our three research questions.

# Ability to Comment

1. What are the effects of training for peer revision on college students’ ability to comment on peer writing?

To answer this question, we employed data triangulation by using multiple data sources and sets: students’ written comments on peer writing, transcripts of student discussion during peer revision, researchers’ observation notes, and informal interviews of the participating instructors and the composition course director. We also used methodological triangulation, combining quantitative and qualitative methods. To allow one peer revision session for practice, we analyzed students’ written comments collected from the last 2 rounds of peer revision conducted.5

# Peer Feedback: Quantitative Analysis

We hypothesized that the experimental group would provide (a) more feedback on peer writing; (b) more feedback on the global features of peer writing; and (c) more relevant and specific feedback on peer writing. For these analyses, we first counted the number of written comments. We then classified them into 3 categories—global, local, and evaluative—according to their contents. Global feedback addressed such concerns as development of ideas, audience and purpose, and organization of writing. Local feedback addressed such concerns as wording, grammar, and punctuation —a kind of copy-editing approach. Evaluative feedback expressed students’ overall evaluation of the peer writing. Two experienced writing instructors independently coded $11 \%$ $scriptstyle n = 2 0 1$ ) of the entire corpus of students’ written comments; agreement was achieved on $98 \%$ of the comments coded. With satisfactory inter-coder reliability obtained, we then categorized all remaining comments.

In addition, we rated all written comments on a 3-point scale where $3 \mathrm { = }$ comments specific and relevant; $2 =$ comments relevant but general; and ${ \bf 1 } =$ comments irrelevant or inaccurate. We defined relevance on the basis of the drafts on which the feedback was provided. The same 2 readers rated 190 comments to assess reliability of the rating scale and achieved $9 7 \%$ agreement on the classifications. With satisfactory inter-rater reliability obtained, we then rated all comments.

Analyzing students’ written comments on peer writing involved quantifying (counting and ranking) essentially qualitative data. Although researchers question whether or not qualitative data are amenable to quantification, Miles and Huberman (1994) contend that qualitative information can be counted directly or converted into ranks or scales. They offer three reasons for quantifying qualitative information: (a) to see the general drift of the data, (b) to verify a hypothesis, and (c) to test for possible researcher bias and keep the researcher analytically honest. For precisely these reasons we quantified students’written comments.

We separated the 2 rounds of peer revision for analysis, in order to triangulate findings and to maximize the number of complete data sets. The somewhat irregular attendance in composition classes meant that restricting the analysis to those students who had provided complete data for both rounds—that is, those who had completed all pre-treatment measures, been present during either the training conference or the regular instruction, participated in all peer group revising sessions, and subsequently turned in the related essays—would have resulted in an unacceptably low number of cases for the usual statistical techniques. Ultimately, there were 91 complete data sets for Round 1 (Experimental $n { = } 4 5$ , Control $n { = } 4 6$ ) and 87 for Round 2 (Experimental $n { = } 4 4$ , Control $n { = } 4 3$ ). Table 1 presents the descriptive results on the 3 variables on the amount and quality of feedback. As shown, the experimental group had higher means on all 3 variables.

Table 1 Descriptive Statistics on Students’ Written Comments on Peer Writing   

<html><body><table><tr><td></td><td>M</td><td>SD</td></tr><tr><td>Round 1</td><td></td><td></td></tr><tr><td>Amount of Feedback</td><td></td><td></td></tr><tr><td>Exnerimental</td><td>7.37</td><td>2.86</td></tr><tr><td>Control</td><td>4.22</td><td>1.49</td></tr><tr><td>Amount of Feedback on Global Features</td><td></td><td></td></tr><tr><td>Experimental</td><td>5.04</td><td>1.54</td></tr><tr><td>Control</td><td>3.21</td><td>1.28</td></tr><tr><td>Amount of Snecific Feedback.</td><td></td><td></td></tr><tr><td>Exnerimental</td><td>4.12</td><td>2.30</td></tr><tr><td>Control</td><td>1.02</td><td>.80</td></tr><tr><td>Round 2</td><td></td><td></td></tr><tr><td>Amount of Feedback</td><td></td><td></td></tr><tr><td>Experimental</td><td>6.66</td><td>2.96</td></tr><tr><td>Control</td><td>4.65</td><td>2.09</td></tr><tr><td>Amount of Feedback on Global Features</td><td></td><td></td></tr><tr><td>Experimental</td><td>4.53</td><td>1.73</td></tr><tr><td>Control</td><td>2.83</td><td>1.66</td></tr><tr><td>Amount of Specific Feedback.</td><td></td><td></td></tr><tr><td>Exnerimental</td><td>3.17</td><td>2.18</td></tr><tr><td>Control</td><td>1.56</td><td>1.55</td></tr></table></body></html>

Note: For Round 1, experimental $scriptstyle n = 4 5$ ; control $scriptstyle n = 4 6$ For Round 2, experimental $n { = } 4 4$ ; control $scriptstyle n = 4 3$

We used Multivariate Analysis of Variance (MANOVA) to investigate the hypotheses and, when we found a significant multivariate effect, performed univariate ANOVA comparisons to determine which dependent variables were affected. Prior to the MANOVA analyses, we checked multivariate assumptions, following recommendations in Tabachnick and Fidell (1989). Both

MANOVAs, each using data from one round of peer revision, yielded a significant multivariate effect for the treatment, training for peer revision (for Round 1, Pillai’s criterion $= . 5 1$ , df 3,87, $F$ $= 3 0 . 3 1 , p < . 0 5$ ; for Round 2, Wilks’ Lambda $\displaystyle =$ .72, df 3, 83, $F { = } 1 0 . 6 9$ , $p < . 0 5 )$ .6 We then conducted univariate ANOVAs by treatment. We set the significance level for the univariate ANOVAs to determine which dependent variables were affected at .01, to adjust for Type 1 error due to multiple comparisons. The results, displayed in Table 2, indicate that the experimental group significantly exceeded the control group in the amount of feedback provided, the amount of feedback related to global features of writing, and the amount of specific feedback for both rounds. Our hypotheses were confirmed.

Table 2 Univariate Analysis of Variance of Students’ Written Comments on Peer Writing   

<html><body><table><tr><td>Variable</td><td>Source of Variance</td><td>ss</td><td>df</td><td>MS</td><td>F-ratio</td></tr><tr><td>Round 1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AMOF</td><td>Treatment</td><td>226.67</td><td>1</td><td>226.67</td><td>43.92*</td></tr><tr><td>AMOFGL</td><td>Treatment</td><td>76.23</td><td>1</td><td>76.23</td><td>38.12*</td></tr><tr><td>AMOSPE</td><td>Treatment</td><td>218.67</td><td>1</td><td>218.67</td><td>74.39*</td></tr><tr><td>Round 2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AMOF</td><td>Treatment</td><td>87.69</td><td>1</td><td>87.69</td><td>13.28*</td></tr><tr><td>AMOFGL</td><td>Treatment</td><td>62.63</td><td>1</td><td>62.63</td><td>21.86*</td></tr><tr><td>AMOSPE</td><td>Treatment</td><td>56.26</td><td>1</td><td>56.26</td><td>15.72*</td></tr></table></body></html>

Note: $^ { * } p < . 0 1$ AMOF $\mathop { \bf { \bar { \rho } } } =$ Amount of Feedback; AMOFGL $\iota =$ Amount of Feedback on Global Features of Writing; AMOSPE $\stackrel { \cdot } { = }$ Amount of Specific Feedback

Researcher Observations: Qualitative Analysis

The classroom observations did not examine student discourse during peer revision, which we did more systematically through analyzing peer discourse, but provided an overall impression of how the students approached peer revision.

Because several groups were engaged in peer revision during any class period, we could not observe all of them in detail during their class period. To avoid being drawn into the peer group discussions, we made most of the observations informally, sitting at some distance from the group(s) observed; thus, these were nonparticipant observations. Classroom observations indicated that the experimental group was better at peer revision, as shown in their longer discussions and their level of engagement. Generally, the experimental peer groups spent more time and were more involved in the peer revision sessions, staying on task more and paying more attention to the substance of each other’s writing during their discussions. Although the instructors gave students entire class periods for peer revision (45 to 75 minutes), some experimental groups even stayed after class to discuss their papers. In contrast, many control groups performed peer revision perfunctorily, focusing mainly on procedural aspects. Control group students more often aimed simply to complete the task and rushed through the peer revision process: Each writer read the draft; the peers provided only a few comments, often focusing on the more mechanical aspects of writing; as soon as they completed this process, they turned to talking about other things.

The observations also revealed variability in peer revision within experimental and control groups. In both experimental and control classes, there were peer groups that concentrated more on the peer revision task and performed it more seriously. In these groups, students seemed to really trust peers as an audience and provide critical feedback on each other’s writing. Although such groups were more the exception than the norm in the control classes, they could nonetheless be observed in every control class. In one control class, for example, one group of 4 women students stood out in each peer revision session; they stayed on task and talked seriously about each other’s writing, providing each other with substantial feedback.

Teacher Perceptions: Qualitative Analysis

Teacher perceptions invariably favored peer revision in the experimental classes; all instructors said that peer revision in their experimental classes was more effective because students could give each other more substantial feedback. One instructor described peer revision in his classes this way: Students in the control class were cordial and said “Good” to each other, but the experimental groups asked critical questions. Another instructor said that her experimental class was better at peer revision and was more enthusiastic. The other 2 instructors indicated that their experimental classes were more capable of providing critical feedback. One instructor’s comments were particularly interesting: After 2 rounds of peer revision, he was concerned that his control class “was falling apart,” and felt that students’ mode of conducting peer revision was a contributing factor.

All the instructors agreed that students should be trained for peer revision and that the training should take place as early as possible in any class using peer revision. They indicated that the group conferences we employed as a training technique were useful because they (a) provided students with some guidelines regarding peer revision, (b) helped students build up confidence as peer responders, (c) helped students develop skills needed for peer revision, and (d) established rapport among peer group members —all factors contributing to effective peer revision. Two instructors found the group conferences so effective that one used them in a class taught elsewhere and the other planned to use them in future classes.

During the study, the Director of Freshman Composition provided additional information regarding teachers’ perceptions of the training’s effects on the students’ability to do peer revision. He reported that the 2 instructors enrolled in the TA Practicum class, which he taught, had expressed very favorable opinions of the group conferences to train students. In fact, during one class in which TA Practicum students discussed peer revision, one of our participating instructors spontaneously raised the issue of training for peer revision and commented on our training’s positive effects on his students.

# Student Discussions: Qualitative Analysis

Because students’engagement in peer revision tasks and their ability to critique peer writing are reflected in their discussions during peer revision, we sampled and analyzed a discussion by each of 16 peer revision groups, 8 experimental and 8 control. The participants in the experimental groups matched those in the control groupsin numberand,wheneverpossible,gender.Weexaminedthe overall interaction pattern of each discussion by inspecting students’turn-taking behaviors and their efforts to negotiate meaning as they responded to their peers. Because most analysts do not accept “murmurs of assent” (Coulthard, 1985, p. 69) as turns, we excluded back-channels in the form of “O.K.,” “Good,” “I see,” “Oh,” and “Yeah,” as turns in our analysis. In the discourse samples, our unitofanalysiswassubstantive“episodes”.Indefiningepisodes,we followed Goldstein and Conrad (1990) who defined them as subunits of conferences, each episode having “a unique combination of topic and purpose such that a change in either or both signifies a new episode. Episodes could be interrupted by others, continuing at the end of the interruption” (p. 488).

We observed 2 overall interaction patterns: a “readerreporting” pattern characteristic of discussions in the control groups and a “reader-writer sharing” pattern characteristic of discussions in the experimental groups. In the “reader-reporting” pattern, responders dominated the discussion by taking long turns to report their feedback to the writer; there was little negotiation of meaning between the responders and the writer or among group members. The following transcript of a group discussion in which 3 students were collaborating on a comparison/contrast paper exemplifies this pattern. For the assignment, the students were to compare and contrast 2 articles on the same topic. The topic of the paper under discussion was whether records of crime victims should be released.

Student A: Okay, I’ll talk about what I put down. Um, You are with your strength in this paper. Okay, you are very descriptive and you had a lot of good points. And the articles are good attention-catchers. I like that. Um, the weaknesses of the paper, you need to compare and contrast more, cause you know, not, not that like, um, it seems like a personal opinion paper. And um, just like contents of the articles, compare the two. And the question I have is how the two different articles are like and different, like how they compare to each other, like how they are the same and how they are different.

Writer: Okay.

Student A: And like suggestions for revision, compare the two a little more, give some more similarities and differences. That’s about it.

Writer: Okay.

Student B: Um, you had a really strong argument, but you know, for one side. But you had really good examples of the problem. But you didn’t really quite make it clear what your view is, whether the record should be released or not. You know, um, again, what is your view? And the example about the little boy who was sexually abused, you should try to find another example, or another story of a kid whose name was released and that hurt and damaged too.

Student A: A different article, like the other article, and maybe there is something in there.   
Writer: Yeah. I see.   
Student B: Other than that, it’s a good paper.   
Writer: Okay.

This discussion consisted of a total of 5 turns, 3 by student A and 2 by student B. All the turns were taken by the 2 readers; the writer did not take a single turn except back-channeling by uttering “Okay” and “Yes, I see.” The writer barely took part in the discussion; which the readers dominated by taking fairly long turns, all in monologue. The turn-taking system observed here demonstrated features of what Sacks et al. (cited in Coulthard, 1985) called a pre-allocated system, where “there are no interruption pressures, turns tend to be longer and for these reasons consist of a series of linked sentences” (Coulthard, 1985, p. 64). Paralleling this turn-taking system, students did not negotiate; no discussion followed the responders’ comments. Examining the students’ contributions in functional terms sheds additional light on the overall non-interactive nature of this group’s discussion. The readers’ utterances largely served to inform (discussing the strengths and weaknesses of the paper) and to direct (e.g., giving suggestions for revision); the writer’s utterances largely served to acknowledge this feedback. There were no eliciting utterances followed by replies or responses; these students did not negotiate meaning through seeking and providing information and/or clarification of comments and suggestions. This “reader-reporting” pattern is reminiscent of the “authoritative stance” that Lockhart and $\mathbf { N g }$ (1995) identified in ESL peer revision.

In sharp contrast, most discussions sampled from the experimentalgroupfeaturedthe“reader-writersharing”pattern,inwhich both the writer and the readers actively engaged in discussion and negotiation of meaning and feedback. This pattern demonstrated relatively equal distribution of turns among group members, lack of dominant turns, and efforts to negotiate meaning and feedback through seeking and providing information and clarification. The following is an excerpt of a discussion in which 3 students were discussing a paper on second-hand smoke. We have numbered each participant’s turn to aid presentation of the analysis.

(Discussion already started; 7 turns elapsed)

Student A: What you have is really good. But you need so much more.

Writer: Like .

Student A: There is a lot you can put down.

Student B: I don’t know what’s that.

Writer: Well, give me some clues, X (student A’s name). What else can I talk about? (

Student A: That’s what I was trying to figure out. I need to think about it. (

Writer: At least you got some ideas. (To student B) Do you have something to say? (3)

Student B: No, after he is done.

Student A: No, go ahead. I’m trying to, I’m trying to think about . .

(12 turns elapsed while the writer and Student B discussed a sentence)

Student A: You can talk about, write a paragraph on, there is like planes now that are just for smokers, you know. (5

Student B: There are planes?

Student A: There are planes now just for smokers.

Writer: There is?

Student A: There is like (unintelligible). Yeah.

Writer: Oh.

Student A: You can talk about that.

Writer: Like in a positive way or a negative way?

Student A: You can talk about them, well, people were kind of, peo ple were doing, keeping smokers from doing it. It’s like a freedom thing, you know. (9)

Writer: Yeah.

Student A: And then you can talk about, then you can say like it’s their freedom (unintelligible). That’s what it says in the Constitution. In the Constitution . . (10)

Writer: What’s said?

Student A: In the Constitution, it says you have the right to do whatever you want as long as like it doesn’t hurt someone else. If it infringes upon their rights, then (11)

Student B: It’s unconstitutional.

Student A: So, it’s the same way with smoke. You can smoke, you know, if you want, as long as it doesn’t do harm to someone else. (1

Writer: Good.

Student B: Okay, the only other thing, technical errors

Student A: There still needs some more information. [sic]

Student B: (To student A) Okay, while you think about more, I’ just tell her this one technical problem

Writer: Yes, I’d like to hear that.

(eight turns elapsed)

Student A: You can talk about some cases where people have lost custody rights because of smoking. (1

Writer: Custody rights?

Student A: Yeah, this one lady

Student B: You mean child custody?

Student A: Yeah. This one lady took her husband to court, you know, and he lost custody of the kids.

Writer: Because he smoked?

Student A: Because they found traces of nicotine and some other stuff. It was in the kids, it’s like in their system. (1

Writer: It was in the kid’s system?

Student A: Yeah, through second-hand smoke.

Student B: Oh, really?

Student A: Yeah.

Writer: I never heard about that before.

Student A: They found that (unintelligible). I think it was in California. I don’t know. You might want to find out where it was. Okay? (19)

Writer: Okay.

(the discussion continued)

This episode featured relatively equal distribution of turns among group members (39 total, 19 by Student A, 11 by the writer, and 9 by Student B), indicating that all members were participating in the discussion and no one dominated. The participants seemed to employ a turn-by-turn allocation system in which “there are strong pressures from other participants wanting to speak, and the turn is typically only one sentence long” (Coulthard, 1985, p. 64). They negotiated meaning and got feedback through requesting and providing information and clarification. Although readers’ utterances generally fell into the “inform” and “direct” categories, as had those in the control groups, the writer’s did not simply acknowledge peer feedback. What distinguished the “reader-writer sharing” pattern from the “reader-reporting” pattern, at least partially, was the writer’s actively eliciting feedback. In the episode above, the writer used both statements (e.g., “Well, give me some clues,” Writer, turn 2) and questions (e.g., “Like in a positive way or negative way?” Writer, turn 5) to elicit information, feedback, or clarification of feedback. The writer actively played a role instrumental in sustaining the discussion. This “reader-writer sharing” pattern is similar to the “collaborative stance” adopted by some students in ESL peer revision (Lockhart&Ng,1995).

The above episode also illustrated the extended nature of student interactions sampled from the experimental group. Not only were there generally more turns in the discussions, but more turns were devoted to discussing a particular area of writing or a particular comment. In the above episode, 39 turns were devoted to the discussion of Student A’s comment “What you have is really good. But you need so much more.” Although interrupted twice (after the fourth turn by Student A and the seventh turn by the writer respectively), discussion of the same topic resumed and continued for a number of successive turns after each interruption.

Analysis of student discourse during peer revision indicated that students in the experimental group generally engaged in more extended and livelier interactions than those in the control group. It also revealed variability within the experimental and control groups. Although we cannot within the scope of this article discuss this variability in detail, we re-emphasize that the two patterns of interaction described above could be found in both experimental and control groups (for more detail, see Zhu, 1995).

# Effects on Writing

2. What are the effects of training for peer revision on college students’ writing?

Although expecting significant improvement in writing would be ambitious, we nevertheless felt it reasonable to assess peer revision training’s effects on overall writing quality. We examined both the short-term and cumulative effects, using what Isaac and Michael (1981) referred to as triangulation of measurement. We assessed short-term effects by examining the quality of student revisions done following peer revision; here we obtained data triangulation by using student revisions from 2 rounds of peer revision. We collected 89 essays (43 experimental and 46 control) from Round 1, and 86 (44 experimental and 42 control) from Round 2. The essays were by students who had completed all activities and peer revision for the respective round. Two native English-speaking, experienced composition instructors who had M.A.s in English independently scored all the essays, using our holistic scoring criteria (Appendix). Interrater reliability was .86 as measured by Pearson correlation. We assessed the cumulative effects through an end-of-course writing-quality measure, the grade on the students’ writing portfolios. This grade was not designed specifically for this study, but normally assigned by the instructors teaching the courses. Portfolios had to include 4 samples of student writing, 2 in-class assignments (mid-term and final exams) and 2 out-ofclass papers, completed during the semester. Students could choose which out-of-class papers to include in the portfolios, which they handed in at the end of the semester. Most of the papers were written after at least one and as many as 3 peer revision sessions. The instructors gave the portfolios letter grades based on overall quality; the letter grades were then converted to numerical values, with $\mathrm { A } = 4$ , $\mathrm { B } = 3$ , $\mathrm { C } = 2$ and $\mathbf { D } = \mathbf { 1 }$ . We used 136 portfolios, 66 experimental and 70 control.

We hypothesized that the quality of revisions and portfolios would be higher for the experimental group. Because data derived from 3 separate occasions, we performed $_ { 3 \mathrm { ~ t ~ } }$ -tests using students’ holistic scores on their revised essays and their portfolio grades. Table 3 summarizes the results. As it shows, the experimental and control groups did not differ significantly on any measure. However, the experimental group tended to perform better on the portfolio grades measure, suggesting that peer revision training has positive effects on students’ cumulative writing development.

However, in general the 2 different measures of student writing and holistic scores from 2 rounds of peer revision all point to a lack of significant difference in student writing across groups. Our hypothesis that the experimental group would produce better quality writing was rejected.

Table 3 T-tests of Students’ Holistic Scores and Portfolio Grades   

<html><body><table><tr><td></td><td>M</td><td>SD</td><td>t</td></tr><tr><td>Holistic Scores</td><td></td><td></td><td></td></tr><tr><td>Round 1</td><td></td><td></td><td></td></tr><tr><td>Experimental</td><td>3.12</td><td>.87</td><td>.32</td></tr><tr><td>Control</td><td>3.05</td><td>.94</td><td></td></tr><tr><td>Round 2</td><td></td><td></td><td></td></tr><tr><td>Experimental</td><td>3.36</td><td>.73</td><td>.87</td></tr><tr><td>Control</td><td>3.32</td><td>.52</td><td></td></tr><tr><td>Portfolio Grades</td><td></td><td></td><td></td></tr><tr><td>Experimental</td><td>3.33</td><td>.71</td><td>1.46</td></tr><tr><td>Control</td><td>3.14</td><td>.80</td><td></td></tr></table></body></html>

Note: For Holistic scores, Round 1, Experimental $scriptstyle n = 4 3$ ; Control $scriptstyle n = 4 6$ ; $\mathrm { p } { > } . 0 5$ Round 2, Experimental $n { = } 4 4$ ; Control $n { = } 4 2$ ; $\mathrm { p } { > } . 0 5$ For Portfolio Grades, experimental $n { = } 6 6$ ; control $n { = } 7 0$ ; $\mathbf { p > } . 0 5$

# Attitudes to Peer Revision

3. What are the effects of training for peer revision on students’ attitudes towards peer revision?

To answer this question, we combined quantitative and qualitative methods and analyzed data from the post-questionnaire, including the students’ written responses to the open-ended question, which asked them to comment on the effectiveness of peer revision in their classes.

Questionnaire Responses: Quantitative Analysis

For the quantitative analysis, we hypothesized that the experimental group would show more favorable attitudes towards peer revision, training for peer revision, writing, and writing instruction, but that the 2 groups would not differ significantly on attitudes toward teacher feedback and the video on peer revision. One hundred and forty-two students, 70 from the experimental group and 72 from the control group, completed the questionnaire. Table 4 displays descriptive statistics on variables related to the semantic differential. The experimental group had higher means on 5 of the 6 scales.

Table 4 Descriptive Statistics on Students’ Responses to the Post-questionnaire   

<html><body><table><tr><td colspan="3">M</td></tr><tr><td colspan="3">Attitudes toward Peer Revision</td></tr><tr><td>Experimental</td><td>5.77</td><td>1.23</td></tr><tr><td>Control</td><td>4.90</td><td>1.46</td></tr><tr><td colspan="3">Training for Peer Revision</td></tr><tr><td>Experimental</td><td>5.68</td><td>1.05</td></tr><tr><td>Control</td><td>4.84</td><td>1.30</td></tr><tr><td colspan="3">Writing Instruction</td></tr><tr><td>Experimental</td><td>5.70</td><td>1.14</td></tr><tr><td>Control</td><td>5.26</td><td>1.24</td></tr><tr><td colspan="3">Writing</td></tr><tr><td>Experimental</td><td>5.73</td><td>1.12</td></tr><tr><td>Control</td><td>5.22</td><td>1.34</td></tr><tr><td colspan="3">Teacher Feedback</td></tr><tr><td>Experimental</td><td>6.08</td><td>.90</td></tr><tr><td>Control</td><td>5.46</td><td>1.48</td></tr><tr><td colspan="3">Video</td></tr><tr><td>Experimental</td><td>4.21</td><td>1.72</td></tr><tr><td>Control</td><td>4.02</td><td>1.36</td></tr></table></body></html>

Note: Experimental $n { = } 7 0$ ; Control $n { = } 7 2$

We performed a MANOVA to analyze the students’ responses. Prior to the analysis, we checked multivariate assumptions using the methods suggested by Tabachnick and Fidell (1989). The overall multivariate comparison was significant (Pillai’s criterion $= . 1 6 , d f 6 , 1 3 5 , F = 4 . 1 6 , p < . 0 5$ , indicating an identifiable treatment effect on the linear combination of dependent variables.7 We then performed univariate ANOVAs with a significance level of $\dot { p } < . 0 1$ to adjust for Type I error due to multiple comparisons.The results appear in Table 5.

Table 5 Students’ Responses to the Post-questionnaire (Univariate Analysis of Variance)   

<html><body><table><tr><td>Variable</td><td>Source of Variance</td><td>ss</td><td>df</td><td>MS</td><td>F-ratio</td></tr><tr><td>Peer Revision</td><td>Treatment</td><td>27.00</td><td>1</td><td>27.00</td><td>14.77*</td></tr><tr><td>Training</td><td>Treatment</td><td>24.73</td><td>1</td><td>24.73</td><td>17.62*</td></tr><tr><td>Writing Instr.</td><td>Treatment</td><td>6.86</td><td>1</td><td>6.86</td><td>4.85</td></tr><tr><td>Writing</td><td>Treatment</td><td>9.22</td><td>1</td><td>9.22</td><td>6.02</td></tr><tr><td>Teacher Feed.</td><td>Treatment</td><td>13.50</td><td>1</td><td>13.50</td><td>8.93*</td></tr><tr><td>Video</td><td>Treatment</td><td>1.28</td><td>1</td><td>1.28</td><td>.53</td></tr></table></body></html>

Note: $^ { * } p < . 0 1$

As Table 5 shows, the groups differed significantly in students’ attitudes toward peer revision, training for peer revision, and teacher feedback; the experimental groups’ attitudes were more positive in these 3 areas. There were no significant intergroup differences with respect to attitudes toward writing instruction, writing, or the video, although a trend suggested that the experimental group held more positive attitudes toward writing and writing instruction.

# Open-ended Question: Qualitative Analysis

Eighty-two students, 47 from the control group and 35 from the experimental group, responded to the open-ended question “Please comment on the use of peer revision groups in this class as it relates to your writing and/or your opinion of this class” in the post-questionnaire. We adopted an iterative approach to analyze their responses, which we examined repeatedly to identify patterns and categories emerging from them. These responses fell into 3 categories: positive, mixed, and negative. In positive comments, students indicated favorable attitudes toward and/or positive experience with peer revision. For example, one student from the experimental group wrote, “Peer revision groups are an excellent asset toward my writing and revising of papers. I enjoyed this English class a lot!” A student from the control group said, “It helped me with my writing. I think peer groups are very helpful.”

In mixed responses, students acknowledged that peer revision could be or sometimes was helpful, but said that its effectiveness depended on various factors, such as the group members’ attitudes toward peer revision and their honesty about peer writing. One student from the experimental group commented, “It is a good thing if everyone is honest and open about their writing.” A control student wrote, “Peer revision is a good idea, and can be helpful. The only problem is that sometimes your peers don’t help enough, meaning that they are not as involved as they could be, except when their paper is the one under inspection.”

In negative responses, students commented on peer revision’s ineffectiveness. They pointed out that peer revision was not helpful because peers were afraid to criticize, were not serious about the task, or lacked the ability to examine writing critically. For example, one student from the control group complained, “I think that peer revision is basically worthless because other students at the same level are too inclined not to be critical because they don’t feel qualified to do so. In my experience, I received almost no constructive criticism and too much nice things said, which didn’t help my revision.”

To see the general pattern in the data, we did frequency counts of comments in the different categories (i.e., we quantified qualitative data). Not surprisingly, more responses from the experimental group fell into the positive category: 24 $(69 \% )$ of their responses were positive, as compared to 17 $( 3 6 \% )$ of the control group’s. In contrast, almost half (21; $45 \%$ ) of the control group’s responses fell into the mixed category, compared to 8 $( 2 3 \% )$ from the experimental group. Three $( 9 \% )$ of the responses from the experimental group were negative; $9 ( 2 0 \% )$ from the control group were.

Differences between groups emerged with respect to both the number of comments in each category and the substance of the comments. For example, though the experimental group tended to comment on how much peer revision groups helped them with their writing, the control groups tended to focus on peers’ inability to provide critical feedback and on the superiority of teacher feedback. The following comments come from the experimental group:

Peer revision in this class is a very good thing to have. It helps me understand my mistakes and how I can improve how I write.

[Peer revision is] very valuable, without it I probably wouldn’t have done as well. It really helps you formulate ideas on a topic when you can talk about it with someone. I think that the groups helped with my writing. It gave me a chance to see not just my view on certain topics. I think it was helpful because you received comments from someone unfamiliar with the topic and therefore received meaningful suggestions.

Compare the foregoing comments with some from the control group:

Peer revision is a positive side to writing, but your peers do not always give you valuable feedback. Often my writing has changed for the worse when I received comments from my peers. It has positive and negative point of view. It leans more toward positive though.   
Peer revision was Okay, but it did not help me very much when I wanted to revise. Mostly, I revised using my own ideas. Peer writing was OK but I didn’t get the comments that I needed. My instructor was better at telling me what to do to fix my papers.   
I don’t especially like peer revision because it doesn’t seem to help my writing. The instructor helps me a lot more than students.   
Well, in my peer revision group, I did not feel they gave me a lot of feedback on my paper. They always said it was good and to proofread but I always felt I needed more information; especially after having X (instructor’s name) grade it I felt they were no help to me.

We also observed a related difference. Though the experimental group felt it important to write a paper for a wider audience including the instructors and the peers, the control group seemed to believe that the instructors were the only real audience and that therefore their feedback was more important. Consider the following sample comments, first from the experimental group:

[Peer revision] very helpful, helps toward producing a wider audience than just an instructor-based audience paper. It helped a lot.   
I think it [peer revision] is very useful. It gives a couple of more opinions besides the teachers’.   
I think the peer revision groups were very beneficial. I found it helpful to find out what other students felt about my writing.

Now from the control group:

It is helpful but at the same time it doesn’t help because it may not be what the teacher wants.   
I thought peer revision was a good experience to see what other students thought of my paper, but the most revision I did was when the instructor revised it.

Yet another difference: Although some students from the control group reported positive experiences with peer revision, they tended to comment on its general, rather than specific, benefits (e.g., “It’s really good,” “Peer revision is a valuable tool,” “I really liked it”). Students from the experimental group, in contrast, tended to comment on the peer revision’s specific impact on their writing (e.g., it provided a wider audience, exposed them to different writing styles, helped them formulate ideas, and provided specific suggestions for revision). Several positive comments from the control group indicated that, although they found peer revision a pleasant social experience, these students did not necessarily feel that it had helped them improve their writing. One student made this clear: “I enjoyed it. It helped a little too.” For this student, enjoyable and helpful were 2 separate dimensions of peer revision. This finding echoes Nelson and Murphy’s (1992) distinction between the social (group dynamics) and task-related (student engagement in the task) dimensions of peer revision with nonnative speakers of English. In our study, some control students’ comments suggest that although their groups had positive dynamics, rendering peer revision pleasant and enjoyable, they did not become substantively engaged in the peer revision task and therefore failed to give each other useful feedback.

Discussion: Triangulation and Future Research

Triangulation through multiple measures, data sources/sets, and methods enabled us to verify our research findings. In investigating the effects of training for peer revision on students’ ability to critique peer writing, quantitative analysis of students’ written comments from 2 rounds of peer revision and qualitative analysis of student discussions during peer revision, instructor interviews, and researcher observations all supported the finding that training for peer revision improved students’ peer responding skills. Here, results derived from different measures, data sources/sets, and methods corroborated each other. Similarly, analysis of students’ scores on 2 essay assignments and students’ portfolio grades both supported the finding that training for peer revision did not significantly improve student writing, although a trend indicated that the experimental group had better cumulative writing development as assessed in portfolio grades. The results derived from different measures and data sets corroborated each other. Finally, both quantitative analysis of students’ responses to the semantic-differential questionnaire and qualitative analysis of students’ open-ended comments on the effectiveness of peer revision revealed that training for peer revision improved students’ attitudes toward peer revision. Thus, results derived from different data and methods corroborated each other.

Data and methodological triangulation also made it possible to clarify and illuminate our research findings. In this regard, qualitative data and analyses were particularly crucial in describing some of the interactive phenomena producing some of the quantitative results. For example, quantitative analysis of students’ written feedback on peer writing revealed that students trained for peer revision provided significantly more and significantly better comments on each other’s writing. Qualitative analysis of researcher and instructor observations and especially of student discussions during peer revision helped explain the quantitative findings: students trained for peer revision could provide more and better feedback because they participated more actively in peer revision groups, attended to the more global concerns of writing, and engaged in more extended negotiation, applying strategies instructors taught them in the training conferences (e.g., to request and provide feedback through questions and responses). Similarly, quantitative analysis of students’ responses to the semantic-differential questionnaire revealed that the students trained for peer revision demonstrated better attitudes toward it; qualitative analysis of students’ comments on the effectiveness of peer revision helped explain why students in the experimental group found it a more positive experience. Qualitative research also suggested factors influencing student experience with peer revision, among which are students’engagement in the task and their ability to provide critical feedback. Finally, analysis of students’ attitudes provided information concerning an overall evaluation of peer revision from the student rather than instructor or researcher point of view, adding an emic perspective to the study (Davis, 1995).

Linking quantitative and qualitative data and methods thus improves the understanding of the questions examined in this study. While quantitative data and analyses revealed the overall effectiveness of training for peer revision, qualitative data and analyses identified some of the classroom processes that produced these positive outcomes. Further, data and methodological triangulation promoted a more comprehensive grasp of peer revision in writing classes. Incorporation of qualitative data and analyses in particular refined our general interpretation of the effect of peer revision in these classes, revealing the degree of variability within the experimental and control groups—variability that did not show up so clearly in the quantitative results. Despite the training, a few students in the experimental group failed to have meaningful exchanges about one another’s writing; even without the training, a few control group students engaged in genuine negotiation of meaning during revision sessions and some reported favorable experiences with peer revision. This variability indicates that the direct training of students may be only one of the factors affecting implementation and results of peer revision; in order to thoroughly understand peer revision, researchers must systematically identify and investigate other factors such as group composition, participant roles, status of group members, and classroom context (Cohen, 1994; Saunders, 1989; Slavin, 1991; Webb, 1989; Yagelski, 1995).

Use of multiple measures to assess the effects of training for peer revision reveals the limitation, if not the outright danger, of using a single measure such as quality of student writing to assess the effects of peer revision (Graner, 1987; Karegianes et al., 1980) or perhaps to assess the effects of any instructional treatment on student learning. We assessed the effects of training in terms of (a) students’ ability to critique peer writing, (b) quality of student writing, and (c) students’ attitudes toward peer revision. Despite the obvious success of the training in improving students’ peer revision skills and attitudes, the quality of writing by the experimental and control groups showed no significant difference, whether measured by the quality of individual essays or by the cumulative writing portfolio grades. Had we assessed the training’s effects only in terms of student writing quality, we would have had to conclude that it was ineffective; but in fact it improved students’ peer revision abilities and attitudes, both of which may help enhance their writing in the long run. On the other hand, if we had assessed the training’s effects only in terms of students’ attitudes and abilities, we would not know whether the training had a delayed effect on students’ writing or not, though it did improve their ability to interact and negotiate. The measures of quality we used were rather crude holistic scales; in the case of essentially fluent, native-speaker writers at this level, it is difficult to document significant improvement over as short a time span as a 15-week semester, because of writing’s developmental aspects (Rose, 1989; Dyson & Freedman, 1991). Further research is needed to examine the effects of peer revision training on student writing quality.

Triangulation through multiple methods, data sources, and measures additionally contributed to a better understanding of peer revision by relating our findings to those of other studies in other contexts. For example, through analysis of students’ comments on peer writing, our study corroborates Stanley’s (1992) finding that training for peer revision helped students provide more and better feedback, even though she investigated an ESL setting. Our results from analysis of student discussions during peer revision support Sommers and Lawrence’s (1992) finding that preparing students for peer revision promoted more equal participation in peer revision tasks. Our results also show that students trained for peer revision tend to assume the “collaborative stance” during peer interaction, a stance believed to benefit student writing development (Lockhart & $\mathbf { N g }$ , 1995). Our analysis of students’ comments on the effectiveness of peer revision revealed that apathy and inability to critique peer writing are problems that peer revision must address, a finding similar to that of Mangelsdorf’s (1992) work on L2 peer revision.

Many additional questions about peer revision await empirical investigation. More research is needed to examine both the factors affecting implementation of peer revision and its ultimate effect on students’ attitudes and writing performance both at the time and later. It will take a combination of different data sources, research methods, measures, and perspectives to probe these issues further and help to establish whether, and how, specific peer revision training affects classroom processes and outcomes for writing instructors and their students. Research that employs triangulation, preferably multiple triangulation, to explore peer revision’s instructional processes and effects in L1, L2, and foreign language settings promises considerable insight. Triangulation can help open up the “black box” (Long, 1983) of the writing classroom, thus suggesting useful future directions for writing theory and pedagogy.

Revised version accepted 1 October 1996

# Notes

1 We used these criteria because they were similar to those already used by instructors in the participant’s composition program (adopted from Gilles $\&$ Snook, 1990).

2 Whereas the construct of peer revision assessed students’ attitudes toward the act of peer revision, that of peer feedback assessed their perceptions of the effectiveness of peer comments. Inspection of the correlations among all the different constructs however, revealed a very high correlation, .89, between attitude toward peer revision and attitude toward peer feedback, suggesting that these 2 constructs probably measured the same thing. To avoid collinearity, we deleted the construct of peer feedback from further analysis.

3 We analyzed one of Hacker’s (1996) transcripts at the instructor-training session, with particular attention to these features: (a) teacher-student interaction (i.e., how the teacher constantly prompted the student to take the initiative and never dominated the conference); (b) focus on the global features of writing when guiding the student writer; (c) efforts and techniques to help the student make specific comments and suggestions; and (d) teacher modeling and instruction.

4 At the time, Hacker had started to investigate the effects of training for peer revision in his classes. His own experiences with peer revision and student outcomes in composition classes are detailed in Hacker (1996).

5 Although training focused on oral feedback, we decided to include students’ written feedback in our analysis because (a) we expected training, if effective, would influence student peer feedback in general, including written feedback; and (b) it was relatively easy to collect and analyze written feedback from all students.

6 For Round 1, the more conservative Pillai’s criterion was used instead of Wilks’ Lambda because the assumption of homogeneity was not met.

7 Box’s M test was significant at $p < . 0 0 1$ , indicating that the assumption of homogeneity was not met and requiring the use of Pillai’s criterion rather than Wilks’ Lambda.

# References

Atkinson, D., & Ramanathan, V. (1995). Cultures of writing: An ethnographic comparison of L1 and L2 university writing/language programs. TESOL Quarterly, 29, 539–568.

Brannen, J. (Ed.) 1992. Mixing methods: Qualitative and quantitative research. Hants, UK: Aldershot/Brookfield, VT: Avebury.   
Carnicelli, T. (1980). The writing conference: A one-to-one conversation. In T. Donovan & B. McClelland (Eds.), Eight approaches to teaching composition (pp. 101–131). Urbana, IL: National Council of Teachers of English.   
Carson, J., & Nelson, G. (1996). Chinese students’ perceptions of ESL peer response group interaction. Journal of Second Language Writing, 5, 1–19.   
Chaudron, C. (1983). Evaluating writing: Effects of feedback on revision. Paper presented at 17th annual TESOL Convention, Toronto, Ontario.   
Cohen, E. (1994). Restructuring the classroom: Conditions for productive small groups. Review of Educational Research, 64, 1–35.   
Connor, U., & Asenavage, K. (1994). Peer response groups in ESL writing classes: How much impact on revision? Journal of Second Language Writing, 3, 257–276.   
Cook, T.D. (1985). Postpositivist critical multiplism. In R.L. Shotland & M.M. Mark (Eds.), Social science and social policy (pp. 21–62). Beverly Hills, CA: Sage.   
Coulthard, M. (1985). An introduction to discourse analysis. New York: Longman.   
Davis, K. A. (1995). Qualitative theory and methods in applied linguistics research. TESOL Quarterly, 29, 427–453.   
Dennis, M.L., Fetterman, D.M., & Sechrest, L. (1994). Integrating qualitative and quantitative evaluation methods in substance abuse research. Evaluation and Program Planning, 17, 419–427.   
Denzin, N. (1978). Sociological methods: A sourcebook. New York: McGrawHill.   
Dyson, A., & Freedman, S. (1991). Writing. In J. Flood., J. Jensen., D. Lapp., & J. Squire (Eds.), Handbook of research on teaching the English language arts (pp. 754–774). New York: Macmillan.   
Fassler, B. (1978). The red pen revisited: Teaching composition through student conferences. College English, 40, 186–190.   
Freedman, S. (1992). Outside-in and inside-out: Peer response groups in two ninth-grade classes. Research in the Teaching of English, 26, 71–107.   
Gere, A., & Abbott, R. (1985). Talking about writing: The language of writing groups. Research in the Teaching of English, 19, 362–385.   
Gilles, R., & Snook, L. (Eds.) (1990). A student guide to freshman composition, (11th ed.). Tucson: University of Arizona Bellwether Press.   
Goldstein, L., & Conrad, S. (1990). Student input and negotiation of meaning in ESL writing conferences. TESOL Quarterly, 24, 443–460.   
Graner, M. (1987). Revision workshops: An alternative to peer editing groups. English Journal, 76, 40–45.   
Hacker, T. (1996). The effect of teacher conferences on peer response discourse. Teaching English in the Two-Year College, 23, 112–126.   
Hale, C., Mallon, J., & Wyche-Smith, S. (1991). Beginning writing groups. [video]. Available from Wordshop Productions, Inc., 3832 N. 7th St., Tacoma, WA 98406.   
Harris, M. (1987). The ins and outs of conferencing. Writing Instructor, 6, 87–96.   
Harris, M. (1990). Teacher/student talk: The collaborative conference. In S. Hynds & D. Rubin (Eds.), Perspectives on talk & learning (pp. 149–161). Urbana, IL: National Council of Teachers of English.   
Hedgcock, J., & Lefkowitz, N. (1992). Collaborative oral/aural revision in foreign language writing instruction. Journal of Second Language Writing, 1, 255–276.   
Herrington, A., & Cadman, D. (1991). Peer review and revising in an anthropology course: Lessons for learning. College Composition and Communication, 42, 184–199.   
Isaac, S. & Michael, W. (1981). Handbook for research in evaluation (2nd ed.) San Diego, CA: EdITS.   
Jacobs, G. (1987). First experiences with peer feedback on compositions: Student and teacher reaction. System, 15, 325–333.   
Janesick, V. (1994). The dance of qualitative research design: Metaphor, methodolatry, and meaning. In N. Denzin & Y. Lincoln (Eds.), Handbook of qualitative research (pp. 209–219). Thousand Oaks, CA: Sage.   
Karegianes, M., Pascarella, E., & Pflaum, S. (1980). The effects of peer editing on the writing proficiency of low-achieving tenth grade students. The Journal of Educational Research, 73, 203–207.   
Lockhart, C., & Ng, P. (1995). Analyzing talk in ESL peer response groups: Stances, functions, and content. Language Learning, 45, 605–655.   
Long, M.H. (1983). Inside the “black box”: Methodological issues in classroom research on language learning. In H.W. Seliger & M.H. Long (Eds.), Classroom-oriented research in second language acquisition (pp. 3–35). Cambridge, MA: Newbury House.   
Mangelsdorf, K. (1992). Peer reviews in the ESL composition classroom: What do the students think? ELT Journal, 46, 274–284.   
Marshall, C., & Rossman, G. (1995). Designing qualitative research (2nd ed.) Thousand Oaks, CA: Sage.   
McGroarty, M. (1989). The benefits of cooperative learning arrangements in second language instruction. Journal of the National Association for Bilingual Education, 13, 127–143.   
Mendonca, C., & Johnson, K. (1994). Peer review negotiations: Revision activities in ESL writing instruction. TESOL Quarterly, 28, 745–769.   
Miles, M., & Huberman, A. (1994). Qualitative data analysis (2nd ed.) Thousand Oaks, CA: Sage.   
Mueller, D. (1986). Measuring social attitudes: A handbook for researchers and practitioners. New York: Teachers College Press.   
Nelson, G., & Murphy, J. (1992). An L2 writing group: Task and social dimensions. Journal of Second Language Writing, 1, 171–193.   
Nelson, G., & Murphy, J. (1993). Peer response groups: Do L2 writers use peer comments in revising their drafts? TESOL Quarterly, 27, 135–141.   
Nystrand, M. (1986). The structure of written communication. Orlando, FL: Academic Press.   
Nystrand, M., & Brandt, D. (1989). Response to writing as a context for learning to write. In C. Anson (Ed.), Writing and response: Theory, practice and response (pp. 209–230). Urbana, IL: National Council of Teachers of English.   
Osgood, C., Suci, G., & Tannenbaum, P. (1957). The measurement of meaning. Urbana, IL: University of Illinois Press.   
Patton, M. (1990). Qualitative evaluation and research methods (2nd ed.) Newbury Park, CA: Sage.   
Rose, M. (1989). Lives on the boundary. New York: The Free Press.   
Rothstein-Vandergriff, J., & Gilson, J. (1988). Collaboration with basic writers in the composition classroom. Paper presented at the annual meeting of the Conference on College Composition and Communication, St. Louis, MO.   
Saunders, W. (1989). Collaborative writing tasks and peer interaction. The International Journal of Educational Research, 13, 101–112.   
Slavin, R. (1991). Cooperative learning and group contingencies. Journal of Behavioral Education, 1, 105–115.   
Sommers, E., & Lawrence, S. (1992). Women’s ways of talking in teacherdirected and student-directed peer response groups. Linguistics and Education, 4, 1–36.   
Stanley, J. (1992). Coaching student writers to be effective peer evaluators. Journal of Second Language Writing, 1, 217–233.   
Stevens, R., Madden, N., Slavin, R., & Farnish, A. (1987). Cooperative integrated reading and composition: Two field experiments. Reading Research Quarterly, 22, 433–454.   
Tabachnick, B., & Fidell, L. (1989). Using multivariate statistics. New York: Harper Collins.   
van Lier, L. (1988). The classroom and the language learner. New York: Longman.   
Villamil, O. & De Guerrero, M. (1996). Peer revision in the L2 classroom: Social-cognitive activities, mediating strategies, and aspects of social behavior. Journal of Second Language Writing, 5, 51–75.   
Webb, E. (1978). Unconventionality, triangulation, and inference. In N. Denzin (Ed.), Sociological methods: A source book (pp. 322–328). New York: McGraw-Hill.   
Webb, N. (1989). Peer interaction and learning in small groups. The International Journal of Educational Research, 13, 21–39.   
Yagelski, R. (1995). The role of classroom context in the revision strategies of student writers. Research in the Teaching of English, 29, 216–238.   
Zhang, S. (1995). Reexamining the affective advantage of peer feedback in the ESL writing class. Journal of Second Language Writing, 4, 209–222.   
Zhu, W. (1994). Effects of training for peer revision in college freshman composition classes. Unpublished doctoral dissertation, Northern Arizona University, Flagstaff, AZ.   
Zhu, W. (1995). Effects of training for peer response on students’ comments and interaction. Written Communication, 12, 492–528.

# Appendix

# Holistic Scoring Scale

A 5 essay is outstanding in all areas of composition. It is particularly distinguished in content, containing an interesting, original and sometimes even risky thesis. The essay is fully developed and organized in a way that underscores and builds upon the main idea. A 5 essay addresses the assignment thoroughly, incorporates fresh and precise language, while usage and mechanics show few if any deviations from the conventions of standard edited English. What distinguishes a 5 essay is the ease with which mature ideas flow from one to the other in support of a central thesis.

A 4 essay is solid in all areas of composition. The topic is interesting and worthwhile to an educated reader, and the ideas are fully developed and logically organized. A 4 essay addresses the assignment sufficiently. The paper contains few if any deviations from standard usage and incorporates precise and appropriate diction.

A 3 essay is a sound composition that may lack the focus and development of a 5 or 4 essay. It is characterized by a clear thesis given sufficient treatment to validate it; however, a 3 essay often fails to intrigue an educated reader. A 3 essay addresses the assignment, but not in the same way a 5 or 4 paper does. The essay is orderly and contains few deviations from standard English, but may be marked by a rigid or mechanical organization that draws attention away from the ideas themselves.

A2 essay is weak in one or more areas of composition. It is difficult to understand for some reason: it may have a vague, poorly stated, or simplistic thesis; it may lack a cohesive organization that leads the reader from one point to the next; it may fail to develop ideas in a manner satisfactory to an educated reader; it may address the assignment superficially; or it may have significant problems with grammar, spelling, or language usage. Any number of weaknesses may characterize a 2 essay.

The 1 essay is unacceptable in one or more of the areas of composition. It may fail to address the assignment, containing numerous serious grammatical errors, unclear or misleading expressions, illogical organization, or negligible content. In short, it does not succeed in communicating ideas to an educated reader. (Adapted from Gilles & Snook, 1990).