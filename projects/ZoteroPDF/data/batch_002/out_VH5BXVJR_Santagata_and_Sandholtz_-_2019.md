# Preservice Teachers’ Mathematics Teaching Competence: Comparing Performance on Two Measures

Rossella Santagata1 and Judith Haymore Sandholtz1

# Abstract

This exploratory study examines the relationship between preservice teachers’ performance on a teaching performance assessment for licensure in elementary mathematics and a measure of knowledge that, in studies of practicing teachers, was found to predict effective mathematics teaching. A sample of 89 preservice teachers completed the Performance Assessment for California Teachers (PACT) and the classroom video analysis (CVA) instrument. Correlation analyses found overall weak associations between measures for the whole group. For groups of high, medium, and low performers on PACT, average scores on the CVA measure and its subscales varied in the predicted direction. However, individuals within the PACT performing groups had a relatively broad range of scores on the CVA, and several preservice teachers who performed poorly on PACT had average or better than average performance on the CVA. Similarly, several of the PACT high performers scored below the whole group CVA average. In addition to suggesting areas for future research, the findings raise questions about assessing preservice teachers’ readiness to teach mathematics and the use of a single measure to make licensing decisions.

# Keywords

certification/licensure, mathematics teacher education, performance assessment, preservice teacher education, teacher knowledge

# Introduction

A key goal of teacher preparation programs is to prepare candidates to become effective classroom teachers who foster student learning. Licensed teachers should not only manage classroom activities but also promote and evaluate student understanding. However, in the United States, licensing systems have typically focused on preventing the certification of underqualified teachers by determining if candidates meet basic levels of content and professional knowledge (Goldhaber & Hansen, 2010; National Commission on Teaching and America’s Future, 1996). In contrast to many countries, teacher licensing in the United States is a state responsibility rather than a federal responsibility. Consequently, the criteria and testing requirements for teachers to obtain a license vary across states. Some states require that applicants to teacher preparation programs meet a minimum grade point average (GPA) and pass standardized tests that focus on basic skills to be accepted. After completing the programs in these states, candidates then must pass state-mandated tests measuring content and professional knowledge to receive an initial teaching license. In other states, there is no pre-program testing requirement, but candidates must pass licensing tests at the end of their preparation programs. The aim of minimum pass scores on these tests is to prevent underqualified candidates from becoming licensed teachers (Goldhaber & Hansen, 2010). To some degree, the tests also hold teacher preparation programs within a state accountable for the competence of their graduates and allow states to compare graduates from different programs (D’Agostino & Powers, 2009). However, given high pass rates, there is some question about the extent to which the tests identify candidates who are not ready to be licensed classroom teachers.

Teacher licensing exams typically have consisted of multiple choice questions (D’Agostino & Powers, 2009). One key concern about this type of test is that it evaluates lower level subject matter knowledge that is not directly relevant to teaching (R. Mitchell & Barth, 1999). Moreover, multiple choice exams do not involve any direct classroom observations of the teachers being licensed (Goldhaber & Hansen, 2010). Researchers report that teachers’ scores on this type of licensing exam tend not to predict effectiveness in classroom teaching (Berliner, 2005; Darling-Hammond, 2010; DarlingHammond, Wise, & Klein, 1999). In a meta-analysis of 123 studies, D’Agostino and Powers (2009) reported that test scores were “at best modestly related to teaching competence” and concluded that performance in preparation programs was a significantly better predictor of teaching skills (p. 146).

In many states, licensing systems are shifting from assessing teachers’ subject matter knowledge to appraising how candidates apply this knowledge in classroom teaching (AACTE, 2015). But questions persist about whether these assessments adequately measure candidates’ use of subjectspecific pedagogy to promote student learning. In this exploratory study, we examine the relationship of preservice teachers’ performance on two measures of teaching competence in mathematics. The first is a teaching performance assessment for licensure that is based on applied competencies, and the second is a measure of knowledge for teaching mathematics that researchers have found to predict teaching effectiveness. The two measures are more fully described in subsequent sections.

# Theoretical Framework

Our theoretical framework draws from research establishing the role of teacher knowledge in classroom practice and the challenges of assessing effective teaching for licensure.

# Teacher Knowledge

Whereas process–product research attributed effective teaching to discrete, observable teaching acts operating independent of time, place, and subject matter, conceptions of effective teaching shifted over time to reflect the complex contexts and subject-specific demands that teachers encounter in their work (Darling-Hammond & Sykes, 1999; National Board for Professional Teaching Standards [NBPTS], 1999; Shulman, 1986). Researchers, for example, acknowledged core activities of teaching as occurring in real time, involving social and intellectual interactions, and being shaped by students in the classroom; these types of factors increase the complexity of teaching (Leinhardt, 2001). With an aim of promoting student understanding, teachers draw upon their knowledge bases in the course of planning and carrying out instruction. These knowledge bases include not only subject matter knowledge but also pedagogical content knowledge (Shulman, 1987). Pedagogical content knowledge includes conceptions of what it means to teach the subject matter, knowledge of the students’ understandings and misconceptions of a subject, knowledge about curricular materials, and knowledge of subject-specific pedagogical strategies. Considered knowledge of subject matter for teaching, pedagogical content knowledge encompasses “the ways of representing and formulating the subject that make it comprehensible to others” (Shulman, 1986, p. 9).

Researchers have built on Shulman’s work by analyzing the competencies and skills needed to teach mathematics effectively. Some researchers have investigated the various types of knowledge needed to carry out the work of teaching mathematics (Ball, Thames, & Phelps, 2008; Hill, Rowan, & Ball, 2005). They propose that mathematical knowledge for teaching includes three domains of subject matter knowledge and three domains of pedagogical content knowledge. For subject matter knowledge, the domains include common content knowledge, specialized content knowledge, and horizon content knowledge (Ball et al., 2008). Specialized content knowledge refers to “math knowledge and skills unique to teaching” (p. 400), whereas horizon content knowledge is “an awareness of how mathematical topics are related over the span of mathematics included in the curriculum” (p. 403). For pedagogical content knowledge, the domains include knowledge of content and students, knowledge of content and teaching, and knowledge of the curriculum (Ball et al., 2008).

Given that effective teaching centers on instructional decision making, researchers are interested not only in investigating the different facets of teacher knowledge but also in understanding the nature of the knowledge that is most directly connected to teachers’ work in the classroom. The conceptualization of teacher knowledge that served as a framework for the design of the Teacher Education and Development Study in Mathematics (TEDS-M) highlights the differences (Tatto et al., 2008). The TEDS-M team maintained a common distinction between mathematical content knowledge and mathematics pedagogical content knowledge. In defining the latter, they emphasized a distinction that applies to issues related to teacher certification. The team conceived of mathematics pedagogical content knowledge as comprised of three components: mathematical curricular knowledge, knowledge of planning for mathematics teaching and learning (pre-active), and enacted mathematics knowledge for teaching and learning (interactive).

Other researchers have emphasized the distinction between knowledge a teacher may possess and knowledge that becomes relevant in practice (Kersting, Givvin, Thompson, Santagata, & Stigler, 2012), indirectly indicating different levels of usefulness of knowledge. Teachers may possess knowledge but not be able to mobilize that knowledge in the course of teaching. Within this conception, knowledge that teachers can access and apply when interacting with students in the course of teaching may deserve particular attention in the design of both teacher preparation programs and measures of teacher competencies (Ball, 2000; Kersting, Givvin, Sotelo, & Stigler, 2010). Kersting and colleagues, for example, have proposed the notion of usable knowledge as consisting of four facets: (a) consideration of the mathematics content at the core of instruction, (b) attention to student mathematical thinking or understanding, (c) consideration of suggestions for improvement of teaching, and (d) interpretive depth and coherence when making sense of teaching episodes (Kersting et al., 2016).

This notion of usable knowledge shares some aspects with another construct that has received increasing attention in the teacher learning literature: teacher noticing (Sherin, Jacobs, & Philipp, 2011). Teacher noticing refers to teachers’ abilities to attend to and interpret student thinking in the midst of instruction. Sherin and van Es (2009) documented relationships between development of teacher noticing and changes in teachers’ classroom practices, particularly in relation to the extent to which teaching is responsive to student thinking. Blömeke, Gustafsson, and Shavelson (2015) describe three similar situation-specific skills that they argue function as the intermediaries between what teachers know and what they do in their classrooms. These skills are (a) perceiving particular events in an instructional setting, (b) interpreting the perceived activities in the classroom, and (c) decision making, either anticipating a response to students’ activities or proposing alternative instructional strategies.

All three conceptualizations, that is, usable knowledge (Kersting et al., 2006), teacher noticing (Sherin & Van Es, 2009), and situation-specific skills (Blömeke et al., 2015), point to the expectation that teachers who can focus on important elements of instruction, such as content and student thinking; who seek to interpret student disciplinary engagement as evidenced by their written work and oral contributions; and who carefully think about next steps and ways to improve their teaching will thereby enhance student learning. In examining a connection between usable knowledge and student learning, researchers report that higher levels of teacher usable knowledge predict student learning as mediated by quality of instruction (Kersting et al., 2010; Kersting et al., 2012). In addition, teachers’ ability to suggest instructional improvements is directly linked to student learning (Kersting et al., 2010; Kersting et al., 2012).

# Assessment for Teacher Licensing

Addressing the need to foster effective teaching, reports on teacher preparation call for teacher candidates to learn in clinically based experiences that will build their abilities to learn from and improve their teaching practice (DarlingHammond, 2010; Hammerness et al., 2005). Teacher learning is increasingly conceptualized as a continuum that extends across the professional lifespan (Feiman-Nemser 2012). In keeping with these conceptualizations, researchers report that performance-based assessments in teacher education have the potential to improve teacher learning as well as teacher quality (Chung, 2008; Darling-Hammond, Newton, & Wei, 2013; Wei & Pecheone, 2010), and there are calls across the nation for implementing performance-based assessments to provide both formative and summative evidence of teacher effectiveness (Knight et al., 2014). Instead of evaluating teacher knowledge itself, performance-based assessments build on the conceptualizations of knowledge discussed above and focus on how preservice teachers apply their subject-specific knowledge during the act of teaching.

The assessments draw on documents and commentaries that demonstrate how preservice teachers are using their skills in specific classroom contexts (Darling-Hammond & Snyder, 2000).

As performance-based assessments use evidence from classroom practice, researchers suggest that they hold an advantage over other approaches in determining readiness for licensure (Darling-Hammond, 2010; K. J. Mitchell, Robinson, Plake, & Knowles, 2001; Pecheone & Chung, 2006; Porter, Youngs, & Odden, 2001). In contrast to systems based on course completion or multiple choice exams, performancebased assessments are more directly connected to a preservice teacher’s actual teaching. In addition, most performancebased assessments are linked to professional teaching standards that affirm the complex, changing situations that teachers face and that are based on components of effective teaching (Arends, 2006; Darling-Hammond, 2010; NBPTS, 1999; Richardson & Placier, 2001; Sato, 2014). Researchers report links between scores on a performance-based assessment (i.e., the Performance Assessment for California Teachers [PACT]) and student achievement gains during subsequent inservice teaching (Darling-Hammond et al., 2013).

Even as more states adopt performance-based assessments in teacher licensing systems, questions continue. Concerns include, for example, the effects on the teacher education curriculum, potential harm to relationships essential for learning, competing demands, and the human and financial resources required (Arends, 2006; Delandshere & Arens, 2001; Snyder, 2009; Zeichner, 2003). As the high-stakes assessments occur during teacher preparation programs and are based on classroom practice during student teaching, teacher educators are uncertain about the extent to which faculty should provide feedback and support (Lit & Lotan, 2013). Feedback potentially could improve candidates’ performance on the assessment, and lack of feedback could undermine the educational preparation of candidates. An overarching question is the extent to which the performance assessments predict teacher quality in beginning teachers. Some studies report disconnects between preservice teachers’ scores on a teaching performance assessment and their university supervisors’ or cooperating teachers’ judgments about their teaching qualifications (Sandholtz & Shea, 2012; Tellez, 2016). In addition, researchers report that teaching candidates selectively edit and omit teaching artifacts or even fabricate images of teaching to meet the assessment criteria (Meuwissen, Choppin, Cloonan, & Shang-Butler, 2016). A worrisome possibility is that using performance assessments for licensing purposes may increase the difficulty and expense of becoming certified but have no effect on teachers’ practices (Wei & Pecheone, 2010). These concerns underscore the need for more research on teaching performance assessments and their use for making summative judgments about readiness to teach.

This exploratory study addresses that need by investigating the relationship between teacher candidates’ performance on a teaching performance assessment for licensure in elementary mathematics (PACT) and their performance on a different assessment (classroom video analysis [CVA]) that measures usable knowledge for teaching mathematics and, in studies of practicing teachers, was found to predict effective mathematics teaching. In the following sections, we describe the two assessments and the goals of the study.

# PACT

In California, the PACT is aligned with the state’s teaching standards for preservice teachers and is one of several approved teaching performance assessment models for licensure. Based on the PACT, the edTPA is a national performance assessment that can be used across states and is also one of the approved assessments for licensure in California. According to a PACT technical report, the assessment was designed “to measure and promote candidates’ abilities to integrate their knowledge of content, students, and instructional context in making instructional decisions and reflecting on practice” (Pecheone & Chung, 2007, p. 5). The PACT assessment includes teaching artifacts and written commentaries in which candidates describe their teaching context, analyze their classroom work, and explain the rationale for their actions. The preservice teachers plan a curriculum unit that addresses state standards and then teach a three to five lesson sequence from the unit. For the assessment, they submit video clips of their teaching (approximately $1 5 \ \mathrm { m i n }$ long), related teaching artifacts such as lesson plans and student work samples, and written commentaries in which they analyze their plans, instruction, and student learning. In writing the commentaries, the preservice teachers follow analytic prompts that help them consider how student learning is developed through instruction and how analysis of student learning informs their decisions during the act of teaching and upon reflection.

The teaching event involves subject-specific assessments of a candidate’s competency in five areas or categories (see Table 1). For each category, there are two or three questions that guide the evaluation of the preservice teachers’ submitted work in that area. For example, the guiding questions for instruction in elementary mathematics include the following: How does the candidate actively engage students in their own understanding of mathematical concepts and discourse? How does the candidate monitor student learning during instruction and respond to student questions, comments, and needs? All candidates preparing to teach in elementary schools complete a full teaching event in elementary level mathematics and content area tasks in literacy, history-social science, and science. Candidates preparing to teach in secondary schools complete teaching events in the subject areas in which they are seeking to be credentialed.

The teaching events are assessed by trained scorers who use subject-specific rubrics. The rubrics include descriptions of performance on four levels for each guiding question (PACT Consortium, 2009). The rankings are defined as follows: Level 1: not meeting performance standards, Level 2: acceptable level of performance, Level 3: advanced level of performance, and Level 4: outstanding and rare level of performance.

Table 1. Focus of Guiding Questions in PACT Rubrics.   

<html><body><table><tr><td>Focus of guiding questions</td><td>Category</td></tr><tr><td>QI: Establishing a balanced instructional focus Q2: Making content accessible Q3: Designing assessments</td><td>Planning</td></tr><tr><td>Q4: Engaging students in learning Q5: Monitoring student learning during instruction</td><td>Instruction</td></tr><tr><td>Q6: Analyzing student work from an assessment Q7: Using assessment to inform teaching. Q8: Using feedback to promote student learning</td><td>Assessment</td></tr><tr><td>Q9: Monitoring student progress. Q10: Reflecting on learning</td><td>Reflection</td></tr><tr><td>QI I: Understanding language demands. Q12: Supporting academic language</td><td>Academic language</td></tr></table></body></html>

Note. PACT $=$ Performance Assessment for California Teachers.

# CVA

The CVA instrument was developed to measure teachers usable knowledge for teaching mathematics, which is defined as knowledge that teachers access and apply in the classroom (Kersting, 2008; Kersting et al., 2010). The instrument, by assessing teachers’ analysis of classroom events, aims to determine if teachers are able to activate and apply their knowledge in a real teaching situation. The instrument is a video analysis assessment that consists of a series of video clips of mathematics teaching in elementary classrooms.

The idea of using video clips as item prompts for measuring teacher competence that extends beyond more traditional measures is relatively novel but is receiving increasing attention internationally (see special issue of the International Journal of Science and Mathematics Education [Vol. 13, Issue No. 2] and April 2016 special issue of ZDM, Mathematics Education [Vol. 48, Issue No. 1-2]). Researchers are developing and testing various instruments that ask teachers to either comment on video clips or respond to specific questions that require them to identify key elements of instruction in video clips and discuss next steps. Developers of these types of measures argue that video allows the opportunity to assess both action-related and reflective competence.

In the CVA instrument, rather than scripted lessons performed by actors, the 3- to 5-min video clips come from authentic lessons in elementary classrooms. The video clips address key topics areas within a particular domain (e.g., fractions or whole number sense) and feature either teacher assistance during students’ independent work or teacher and student discussions based on student questions or mistakes during whole class instruction. For each clip, contextual information is provided, and subtitles include the verbatim dialogue of the teacher and students (Kersting et al., 2010). In selecting clips, the researchers looked for classroom events that “had potential to elicit rich discussion by teachers, were high-frequency events, and were relatively selfcontained and short” (Kersting et al., 2010, p. 174).

Table 2. CVA Scoring Dimensions.   

<html><body><table><tr><td>Dimension and definition</td><td>Score of 0</td><td>Score of I</td><td>Score of 2</td></tr><tr><td>MC Degree to which respondents considered and commented on the mathematics at hand when making sense of the instructional.</td><td>Comments did not include specific math content</td><td>The mathematics content was discussed in the response, but it was only identifying or describing the observable mathematics in the clip</td><td>An analysis of a specific aspect of the mathematical content was present. A mathematical analysis was usually evidenced by the introduction of some mathematical concept(s) or idea</td></tr><tr><td>ST Extent to which commentaries expressed concern for student mathematical thinking and understanding. This concern usually took the form of either a judgment about student(s) understanding or a comment on opportunities to learn that</td><td>Comments did not address Student thinking</td><td>There was a direct link between the analysis of student thinking or understanding and the mathematics visible in the clip</td><td>not obviously seen in the clip. Comments synthesized, analyzed, or generalized student thinking in mathematical ways or offered mathematics explanation(s) or justification(s) that supported the synthesis/analysis/ generalizations</td></tr><tr><td>students were afforded in the clip. SI Extent to which comments included suggestions to improve instruction or pedagogy by providing a clear alternative to the instruction Shown in the clip.</td><td>Comments provided no suggestions or vague suggestions for improvement</td><td>Comments included a clear suggestion for improvement that addressed the mathematics</td><td>Comments included a clear suggestion that addressed specific mathematics</td></tr><tr><td>DOI Extent to which a comment included interpretations and justified opinions of teaching and learning portrayed in the clip.</td><td>Comments were descriptive accounts of what was in the clip or included a vague or broad interpretation that was not substantiated</td><td>Comments included a substantiated interpretation in the form of a rationale, evidence, or justification for the interpretation.</td><td>Comments offered an analysis that included several interpretations along a common theme.</td></tr></table></body></html>

Note. MC $=$ Mathematics Content; $\mathtt { S T } =$ Student Thinking; ${ \sf S I } =$ Suggestions for Improvement; $\mathsf { D O } \mathsf { I } =$ Depth of Interpretation; CVA $=$ classroom video analysis.

For assessment purposes, teachers review the video clips and then respond in writing to the following prompt: “Discuss how the teacher and the student(s) in the clip interacted around the mathematical content.” The teachers’ responses are scored using rubrics that represent four dimensions of teachers’ work: mathematical content, student thinking, suggestions for improvement, and depth of interpretation. The rubrics include a 3-point scoring scale based on specificity or depth of response (see Table 2). Teachers who score higher on the assessment by producing more insightful analyses are theorized to have more extensive and sophisticated knowledge to apply in their teaching than those who score lower (Kersting et al., 2010). In studies of practicing teachers, researchers found links between teachers’ CVA scores and quality of instruction and, in turn, gains in student learning (Kersting et al., 2012).

# Study Goals

Given that the PACT aims to assess candidates’ readiness to teach elementary mathematics, we explore in this study whether there are positive relationships between teacher candidates’ performance on the PACT and another measure of teaching competence in mathematics (CVA). As described above, the CVA instrument is based on facets of mathematical knowledge that teachers activate during teaching and use to make sense of classroom practice. Similarly, the specific tasks in the PACT are based on candidates’ ability to draw upon subject-specific knowledge and apply it to their classroom teaching. However, the two assessments differ in their format. For the PACT, candidates demonstrate their knowledge through artifacts of their own teaching that they select and analyze over a period of time. The extended time frame allows candidates to reflect, revise, and possibly consult with others. For the CVA, candidates react to video clips of teaching performed by others and complete the assessment independently in one sitting. In addition, in this study, the PACT was a high-stakes assessment for the candidates, but the CVA was a research measure. Candidates received a gift card for completing the CVA, but their performance did not have implications for licensing and certification. Given the differences in format and administrative context for these two measures, it may seem unreasonable, from a measurement perspective, to expect overlap in candidates’ performance on these two assessments. However, as both measures are designed to assess effectiveness in teaching mathematics, it makes sense from a practical point of view to explore the extent of overlap in teacher candidates’ performance, particularly given the reliance on only one assessment for licensing decisions.

Common wisdom suggests that identifying teacher candidate performance at both ends of the continuum is the easiest. In assessments used for teacher licensure, a central aim is to identify candidates who need additional preparation before becoming licensed and taking on solo classroom teaching responsibilities. As performance on the CVA has been found to predict teaching effectiveness in mathematics (Kersting et al., 2010; Kersting et al., 2012), we hypothesize that candidates who are not yet skilled enough to be licensed as fulltime teachers based on PACT scores also would perform at the lower end of the continuum on the CVA. Similarly, we anticipate that high performers on the PACT would score on the higher end of the continuum on the CVA. There are no studies to date that investigate and compare preservice teachers’ performance on these measures.

This exploratory study specifically addresses the following research questions:

Research Question 1: What is the relationship between teacher candidates’ performance on a teaching performance assessment for licensure (PACT) and a measure that predicts teaching effectiveness in mathematics (CVA)? Research Question 2: Does this relationship differ for high, medium, and low performers on the teaching performance assessment?

# Method

# Data Collection

Participants included 89 preservice teachers from two successive cohorts enrolled in a 1-year post-baccalaureate elementary teacher preparation program at a large U.S. university. Participants were predominantly female $( 8 9 \% )$ and White $( 4 7 . 3 \% )$ or Asian American $( 4 0 . 7 \% )$ with an average age of 23.5 years. The majority held bachelor’s degrees in the humanities or social sciences. The minimum undergraduate GPA for program admission was 3.0 (4.0 scale). During a 1-month period near the program’s end, participants completed both the PACT and the CVA.

PACT. As part of program and licensing requirements, the preservice teachers completed the PACT. Using a videotape of an instructional unit, student work samples, and related artifacts for documentation, they analyzed their teaching and their students’ learning by responding to analytic prompts. Preservice teachers worked on the PACT during an 8- to 10-week period. These teaching events were scored by trained scorers using content-specific rubrics organized according to 12 guiding questions within five categories (see Table 1). The scorers of the PACT teaching event must pass a calibration standard set by the PACT Consortium each year. The training sessions for scorers focus on what is used as sources of evidence, how to match evidence to the rubriclevel descriptors, and the distinctions between the four levels that range from “not meeting standards” to “outstanding and rare performance.” Scorers are instructed to assign a score based on a preponderance of evidence at a particular level. In addition to the rubric descriptions, the PACT consortium developed a document to clarify distinctions between levels. The document provides an expanded description for scoring levels for each guiding question and describes differences between adjacent score levels and the related evidence. Validity and reliability studies of the PACT are summarized in a technical report (Pecheone & Chung, 2007).

Candidates received a score of 1 to 4 on each of the 12 guiding questions. Consequently, total possible scores ranged from a low of 12 to a high of 48. To pass the PACT teaching event, preservice teachers must pass all five categories on the rubric (planning, instruction, assessment, reflection, and academic language) and have no more than two failing scores of 1 across categories. To pass a category, preservice teachers must have passing scores of 2 or higher on at least half of the questions within each category. Preservice teachers who fail the teaching event have one opportunity to resubmit.

CVA instrument. The CVA (Kersting, 2008) measured preservice teachers’ usable knowledge for teaching mathematics. The instrument consisted of 10 short video clips $\operatorname { 1 - 3 } \operatorname* { m i n }$ in length) of authentic mathematics teaching in elementary classrooms and covered key ideas within the domains of whole number sense and fractions. Previous studies (Kersting, 2008; Kersting et al., 2010) found an internal item consistency between .89 and .93. In the current study sample, inter-item reliability was somewhat lower but still satisfactory, ranging from .71 to .81 (Cronbach, 1951). After reviewing each clip, the preservice teachers provided written responses to the following prompt: “Discuss how the teacher and the student(s) interact around the mathematical content.” Preservice teachers completed the CVA in a university computer lab during a 1-hr session. Video clips were made accessible through an online portal, and all participants watched clips following the same order. Completion time differed across participants, but the allotted time was sufficient for everyone to finish without rushing. Their responses were saved on a server and later downloaded and scored according to the level of specificity with which they addressed the following dimensions in their comments: mathematics content (CVA-MC), suggestions for improvement (CVA-SI), student thinking (CVA-ST), and depth of interpretation (CVA-DOI). Participants did not have access to scoring rubrics because the instrument is designed to assess teachers’ non-guided interpretations of teaching episodes. Each response received a score of 0, 1, or 2, depending on specificity and/or depth as defined by each rubric (see Table 2). The instrument includes a detailed coding manual with several sample responses with respective scores organized by the four dimensions described above. Additional training material includes video clips and 100 sample responses divided into five practice sessions with master scores for each response. The master scores were compiled from scores by five separate raters. This material was used to train raters for this study. Raters began to score participants’ responses only after they had completed the training and reached reliability with the master scores. Interrater reliability for this study, measured as percent agreement for two independent raters, ranged from $8 3 . 7 \%$ to $8 8 \%$ across four rubrics and three time points. In case of disagreement, a third trained rater reviewed the response to finalize the score.

# Data Analysis

To examine associations between measures, paired-sample correlations were estimated for scores on the PACT and its subscales and the CVA and its subscales. Correlation analyses were also run with clustered standard error to take into account the nested nature of the data (i.e., candidates belonging to two separate cohorts). Findings remained the same. We conducted these correlational analyses to explore relationships among measures that have not been examined previously, but we recognize that the small sample size largely limits the conclusions we can draw from these analyses.

We then categorized preservice teachers into low-, medium-, and high-performing groups based on total PACT scores. We wanted to investigate the assumption that assessments most readily identify those at both ends of the continuum. We identified preservice teachers who scored $1 ~ S D$ or more below the mean (average $= 3 1 . 6 5$ , $S D = 5 . 7 1 4 _ { . }$ ) as low performers (scores $\leq 2 6$ ; $n = 1 2$ ), those who scored $1 \ S D$ or more above the mean as high performers (scores $\geq 3 7$ ; $n =$ 22), and those whose performance fell in between as the medium performers (scores between 27 and 36; $n = 5 5$ ; see Table 3). To compare performance across the two measures for the high-, medium-, and low-performing groups, we calculated the mean score and standard deviation for each group for both the CVA total score and each subscale. Data were normally distributed for all subscales with the exception of CVA-DOI. In the case of this subscale, scores were skewed toward the low end of the curve. We used one-way ANOVA to test for significant group differences in CVA scores on all other subscales and on the total CVA score for the different groups. To further explore and display relationships across groups, we created a grouped scatterplot with PACT scores on the $x$ axis and CVA scores on the $y$ axis. PACT performing groups were identified with different shapes to facilitate a visualization of the distribution of individuals within each group. Finally, we grouped preservice teachers based on their performance on the CVA using the sample mean and 1 SD above or below the mean to identify high, medium, and low performers. We then created a $3 \times 3$ table that displays the number of preservice teachers from each of the PACT performing groups who belong to each of the CVA performing groups. This display allowed us to examine the extent to which levels of low, medium, and high performance on the two measures overlap.

Table 3. Average Scores and SD on PACT-TE by Low-, Medium-, and High-Performing Group.   

<html><body><table><tr><td>PACT-TE performance group and score range</td><td>PACT-TE average score (0-48) and SD</td></tr><tr><td>Low (0-26)</td><td>23.62 (1.94)</td></tr><tr><td>Medium (27-36)</td><td>30.34 (2.69)</td></tr><tr><td>High (37-45)</td><td>39.73 (2.43)</td></tr></table></body></html>

Note. PACT-TE $=$ Performance Assessment for California TeachersTeaching Event.

# Findings and Discussion

We organize the findings according to the research questions. In the first section, we report on the relationship between preservice teachers’ performance on the teaching performance assessment and the CVA measure. We then report on the extent to which these relationships differ for groups of high, medium, and low performers on the teaching performance assessment. In each section, we present the statistical results and discuss the findings.

# Relationships Between Measures

To address the first research question, we examined associations between measures for the total group of participants. Total scores on the two measures were not significantly correlated $( r = . 1 8 1$ , $p = . 0 8 9 _ { , }$ . Moreover, only one of the 20 correlations between the PACT subscale scores and the CVA subscale scores was significant (see Table 4). Scores on the PACT assessment category correlated positively $( r = . 2 3 7 , p$ $= . 0 2 5 )$ with scores on the CVA Mathematical Content subscale. Scores on the PACT assessment category indicate preservice teachers’ ability to (a) analyze student work from an assessment, (b) use assessment to inform teaching, and (c) provide feedback to promote student learning. Preservice teachers’ ability to use assessments along those dimensions was positively related to their ability, when viewing video clips of instructional episodes, to attend to the mathematical content.

Table 4. Correlations Between PACT-TE Categories and CVA Subscales.   

<html><body><table><tr><td>PACT category</td><td>CVA-MC</td><td>CVA-SI</td><td>CVA-ST</td><td>CVA-DOI</td></tr><tr><td>Instruction</td><td>.147</td><td>.183</td><td>.156</td><td>.091</td></tr><tr><td>Assessment</td><td>.237a</td><td>.125</td><td>.103</td><td>.035</td></tr><tr><td>Reflection</td><td>.023</td><td>.143</td><td>.088</td><td>.074</td></tr><tr><td>Academic language</td><td>.047</td><td>.170</td><td>.131</td><td>.102</td></tr><tr><td>Planning</td><td>.024</td><td>.207</td><td>.104</td><td>.080</td></tr></table></body></html>

Note. CVA $=$ classroom video analysis; PACT-TE $=$ Performance Assessment for California Teachers-Teaching Event; CVA-MC $=$ classroom video analysis–mathematics content; CVA-SI $=$ classroom video analysis– suggestions for improvement; CVA-ST $=$ classroom video analysis–student thinking; CVA-DOI $=$ classroom video analysis–depth of interpretation. a Correlation is significant at the .05 level.

These findings suggest that preservice candidates’ teaching competence in the field of mathematics as measured by PACT scores is only weakly related to their usable knowledge for teaching mathematics as captured by the CVA. The two measures overlap on PACT tasks related to assessing student mathematics learning but not on other PACT tasks. Given that both the PACT and the CVA focus on application of knowledge in teaching mathematics, we anticipated that there would be more overlap between CVA subscales and PACT tasks related to planning and instruction.

# Differences for High, Medium, and Low Performers

Our second research question focused on examining whether relationships between PACT and CVA scores vary for different levels of performance on the PACT. As described above, candidates were divided into three groups: high, medium, and low performers. The groups of high and low performers consisted of those preservice teachers who scored 1 SD or more above or below the mean on the PACT. The medium group’s performance was within 1 SD above or below the mean.

Average scores on the CVA measure and its subscales varied in the predicted direction for the three groups of PACT performers. On both the total score scale and each subscale, average CVA scores of high PACT performers were higher than average scores of medium PACT performers, and these scores were higher than low PACT performers (see Table 5). Although differences in average scores for the three groups followed the predicted direction, none of them was statistically significant.

To further examine the relationship for group and individual performance on both measures, we created a scatterplot with PACT scores on the $x$ axis and CVA scores on the y axis. This scatterplot allowed us to see whether the three groups of PACT performers were clearly aligned with low, medium, and high levels of performance on the CVA. Figure 1 shows that individuals within the low-, medium-, or high-performing groups on PACT had a relatively broad range of scores on the CVA. For example, the CVA scores of the PACT low performers ranged from 7 to 33. Four of the 12 low performers scored higher than the whole group CVA average. Similarly, the CVA scores of the PACT high performers ranged from 15 to 46, and nine of the 22 high performers scored below the whole group CVA average. Among the candidates in the medium PACT group, scores on the CVA ranged from 2 to 58 and the standard deviation reached 12.5. Four of these candidates performed higher on the CVA than the top performers in the high PACT group.

In examining individual scores in relationship to performance groupings on both the PACT and CVA (see Table 6), we found that none of the low PACT performers were in the high-performing CVA group, but 10 were in the medium performing CVA group, and only two were in the low-performing CVA group. In the high PACT performing group, only four were high CVA performers, 17 were medium CVA performers, and one was a low CVA performer. These differences in individual performance on the two measures suggest that, if the CVA rather than the PACT had been the licensing exam, a different group of candidates would have failed or excelled. This overall mismatch between performance on PACT and the CVA raises questions about the extent to which these measures similarly capture knowledge and skills essential for effective mathematics teaching.

There are several possible explanations for these discrepancies. While PACT assesses competence based on actual artifacts of teachers’ own practices, the CVA assesses competence as elicited from viewing video clips of other teachers’ classroom teaching. Critical analysis of another teacher’s instruction could be considered somewhat more distant from actual competence that teachers demonstrate in their own teaching. However, as mentioned above, performance on the CVA has been found to be related to student learning in samples of practicing teachers (Kersting et al., 2012). In addition, the ability to analyze teaching performed by others may require extensive understanding of teaching-learning processes and the complexities of teaching. In contrast to selfanalysis, preservice teachers may find it more challenging to analyze teaching when lacking specific contextual information or personal knowledge of the teacher’s intentions, justifications, and reasoning during the act of teaching. Another possibility is that candidates expended more effort on the PACT than the CVA, given that they needed to pass the performance assessment to be licensed. These are potential hypotheses about why some candidates who performed well on the PACT did not score as high on the CVA.

For those candidates who performed poorly on the PACT but did better on the CVA, sometimes even achieving higher scores than those in the high PACT performing group, other potential explanations arise. In contrast to the possibility suggested above, perhaps self-analysis is more challenging for preservice teachers than critical analysis of another teacher’s classroom practice, as displayed on video. As preservice teachers are in the midst of learning how to teach, evaluating their own practice may be a skill that requires more development over time. It also is possible that the format of the PACT assessment obscured actual teaching competence of some candidates. For example, the PACT involves an extensive amount of writing. Candidates with weak writing skills may be disadvantaged in describing their teaching practice. The PACT also is labor and time intensive whereas it only takes approximately an hour to complete the CVA. In the face of competing responsibilities, some candidates may have opted to devote more time and attention to their student teaching than the PACT. Another possibility is that, as their specific PACT scores are not included in transcripts or hiring materials, some candidates may have aimed only to pass the PACT rather than achieve high scores. Perhaps, in some cases, candidates concentrated more attention on the CVA because it involved only a short time period rather than sustained effort over weeks.

Table 5. Average Scores, SD, and Range on CVA by Low-, Medium-, and High-Performing Group.   

<html><body><table><tr><td colspan="2">PACT performing</td><td colspan="6"></td></tr><tr><td>CVA subscale</td><td>group</td><td>n</td><td>M</td><td>SD</td><td>Minimum</td><td></td><td> Maximun</td></tr><tr><td rowspan="4">CVA-MC</td><td>Low</td><td>12</td><td>7.50</td><td>3.680</td><td></td><td></td><td>13</td></tr><tr><td>Medium</td><td>55</td><td>8.44</td><td>4.176</td><td>0</td><td></td><td>17</td></tr><tr><td>High</td><td>22</td><td>10.00</td><td>3.464</td><td></td><td>2</td><td>17</td></tr><tr><td>Total</td><td>89</td><td>8.70</td><td>3.990</td><td>0</td><td></td><td>17</td></tr><tr><td rowspan="4">CVA-SI</td><td>Low</td><td>!2</td><td>2.25</td><td>1.138</td><td></td><td></td><td>4</td></tr><tr><td>Medium</td><td>55</td><td>4.58</td><td>3.980</td><td></td><td>0</td><td>14</td></tr><tr><td>High</td><td>22</td><td>5.05</td><td></td><td>3.457</td><td>0</td><td>14</td></tr><tr><td>Total</td><td>89</td><td>4.38</td><td>3.673</td><td></td><td>0</td><td>14</td></tr><tr><td rowspan="4">CVA-ST</td><td>Low</td><td>!2</td><td>5.92</td><td>3.147</td><td></td><td>0</td><td>10</td></tr><tr><td>Medium</td><td>55</td><td>6.62</td><td>3.504</td><td></td><td>0</td><td>16</td></tr><tr><td>High</td><td>22</td><td>7.77</td><td></td><td>2.793</td><td>2</td><td>13</td></tr><tr><td>Total</td><td>89</td><td>6.81</td><td></td><td>3.316</td><td>0</td><td>16</td></tr><tr><td rowspan="4">CVA-DOI</td><td>Low.</td><td>12</td><td>7.42</td><td>3.728</td><td></td><td>2</td><td>14</td></tr><tr><td>Medium</td><td>55</td><td>8.35</td><td>4.343</td><td></td><td>0</td><td>18</td></tr><tr><td>High</td><td>22</td><td>9.18</td><td>3.500</td><td></td><td>2</td><td>17</td></tr><tr><td>Total</td><td>89</td><td>8.43</td><td></td><td>4.065</td><td>0</td><td>18</td></tr><tr><td rowspan="4">CVA Total</td><td>Low.</td><td>12</td><td>23.08</td><td>7.751</td><td></td><td>7</td><td>33</td></tr><tr><td>Medium</td><td>55</td><td>27.98</td><td>12.506</td><td></td><td>2</td><td>58</td></tr><tr><td>High</td><td>22</td><td>32.00</td><td>8.575</td><td></td><td>15</td><td>46</td></tr><tr><td>Total</td><td>89</td><td>28.31</td><td></td><td>11.323</td><td>2</td><td>58</td></tr></table></body></html>

Note. $\mathsf { C V A } =$ classroom video analysis; PACT $=$ Performance Assessment for California Teachers; CVA-MC $=$ classroom video analysis–mathematics content; $C V A - S I =$ classroom video analysis–suggestions for improvement; CVA-ST $=$ classroom video analysis–student thinking; CVA-DOI $=$ classroom video analysis–depth of interpretation.

Although the average scores for the groups follow the predicted direction, the range in scores for individual candidates as displayed in the scatterplot suggests a need to further investigate whether these measures capture different skills related to teaching competence in mathematics or whether other factors are at play. We are puzzled by the possibility that candidates judged not yet skilled enough to be licensed as full-time teachers, as determined by PACT scores, could perform at the medium level of another measure of teaching competence in mathematics that is comparable with that of their high-performing PACT colleagues. Given that researchers have reported links between scores on both these measures and student learning (Darling-Hammond et al., 2013; Kersting et al., 2010), these mismatches for preservice teachers are even more puzzling. Although it is plausible that PACT and CVA measure different aspects of competencies that are linked to effective mathematics teaching, relying on only one measure for licensing purposes might penalize some candidates.

# Implications

The results of this exploratory study highlight three key issues related to assessing preservice teachers’ teaching competence in mathematics. First, our findings emphasize the complexity of assessing teaching and suggest caution in moving toward the use of a single high-stakes assessment for licensing beginning teachers. Preservice teachers who fail, or excel, on a teaching performance assessment may appear more, or less, competent on other measures. This study found little association between candidates’ scores on a teaching performance assessment and on a measure of knowledge activated in the course of making sense of teaching. Other studies report disconnects between preservice teachers’ scores on a teaching performance assessment and their university supervisors’ or cooperating teachers’ judgments about their teaching assessments may reflect non-teaching skills or choices candidates make about how to represent their teaching practices. For example, candidates may receive low scores due to less than optimal writing skills or inattention to specific requirements. In some cases, candidates adjust, alter, or even falsify teaching artifacts to meet the assessment requirements (Meuwissen et al., 2016). Using multiple methods to assess preservice teachers’ qualifications may offer a more comprehensive assessment of strengths, weaknesses, and readiness to teach.

![](img/34453c92cf68c39c4ca4c7946ed083381c67d5e09f0cfe17e209339d60108848.jpg)  
Figure 1. Individual candidates’ performance on the PACT-TE and CVA by PACT Performing Groups. Note. CVA $=$ classroom video analysis; PACT-TE $=$ Performance Assessment for California Teachers-Teaching Event.

Table 6. Number of Candidates in Each CVA Performing Group by PACT Performing Group.   

<html><body><table><tr><td>PACT groups</td><td>CVA Iow</td><td>CVA medium</td><td>CVA high</td></tr><tr><td>Low</td><td>2</td><td>10</td><td>0</td></tr><tr><td>Medium</td><td>14</td><td>34</td><td>7</td></tr><tr><td>High</td><td></td><td>17</td><td>4</td></tr></table></body></html>

Note. CVA $=$ classroom video analysis; PACT $=$ Performance Assessment for California Teachers.

qualifications (Sandholtz & Shea, 2012; Tellez, 2016). In a study of high and low performers, Sandholtz and Shea (2015) found that the majority of candidates whose supervisors predicted failure ended up passing the performance assessment, and the majority of candidates who did fail had been predicted to pass. In contrast to supervisor predictions, two academic factors, undergraduate GPA and grades in methods courses, were significantly associated with high and low performers’ scores on the PACT (Sandholtz & Shea, 2015). As discussed above, differences between preservice teachers’ observed teaching practices and their performance on high-stakes

Second, this exploratory study supports examining and developing multiple ways in which preservice teacher competencies could be assessed. As mentioned above, recent internationally based research has explored video as a tool to design assessments of teacher competence that extends beyond more traditional measures. Knievel, Lindmeier, and Heinze (2015), for example, developed a measure of elementary preservice teachers’ competence for teaching mathematics that includes video vignettes and related prompts to capture respondents’ ability to react to students’ solution strategies and misconceptions and their ability to deal with particular mathematical representations and explanations. Respondents are asked to react under time pressure as if they were in the midst of a lesson and had to make in-the-moment decisions. Additional items allow unlimited time for responses and include both open-ended written and oral responses to picture (i.e., samples of student work) or video prompts (mathematically rich student statements) that require respondents to react as if they were planning or reflecting on a lesson.

This area of research holds promise for assessments of preservice teachers’ competence for at least two reasons. First, video-based surveys can be designed to be both time and cost effective. Candidates can respond to a series of video clips covering various teaching situations in a relatively short period of time. For example, the measure described above was tested on a sample of 85 German elementary inservice teachers. The measure also included items to assess subject-specific knowledge and took on average 68 min for teachers to complete. Some researchers are also exploring automated scoring and reporting high correlations between computerized and human scoring (Kersting, Sherin, & Stigler, 2014). Consequently, these types of assessments have the potential to reduce both the time required of candidates during their student teaching phase and the time required to score teaching portfolios. Second, these types of video-based measures avoid a problem inherent to most performance assessments. That is, the number and length of written documents that candidates must submit ends up advantaging those candidates with strong literacy skills. Some researchers contend that teaching performance assessments may capture individual reading, writing, and technological skills more than teaching competence (Greenblatt & O’Hara, 2015). Although equally problematic as a single high-stakes assessment for licensure, video-based measures such as these hold promise when combined with other indicators of teaching competence.

Third, this study raises the need for several types of additional studies. The focus of this study was assessment of candidates’ teaching competencies in elementary mathematics. For candidates preparing to teach in elementary schools, the PACT focuses primarily on mathematics and includes supplemental tasks in literacy, science, and social studies. Given that credential programs prepare elementary candidates to teach multiple subject matters, it would be important to conduct similar studies in other content areas. In addition, it would be helpful to conduct a similar study with secondary mathematics candidates who enter teacher preparation with higher levels of subject matter knowledge and focus their studies on mathematics teaching.

In-depth case studies would be valuable for investigating the reasons for differences in performance across measures, particularly for those candidates who perform poorly on licensing assessments in comparison with other types of assessments. Given that research on measures such as the CVA focused primarily on practicing teachers, additional studies that investigate the use of the CVA and other measures with preservice teachers are important for determining what combination of measures may best assess teaching competence for initial licensure. Moreover, the development of alternative measures may contribute to a better understanding of what it means to learn from one’s teaching and how to capture that learning in preservice teachers. As the preparation of teachers is shifting from a focus on bodies of knowledge to depictions of practice, the role of teacher education programs may shift from emphasizing specific visible teaching behaviors to helping prospective teachers to analyze persistent challenges inherent in the work of teaching and to evaluate alternative courses of action (Kennedy, 2016). With this type of shift in the purpose and role of teacher education, alternative assessments that similarly examine one’s ability to resolve problems of practice will be critical.

# Authors’ Note

A version of this article was presented at the 2016 annual meeting of the American Educational Research Association, Washington, DC.

# Declaration of Conflicting Interests

The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.

# Funding

The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This research was supported by the National Science Foundation (REESE program) under Grant DRL-0953038. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the funding agency.

# References

American Association of Colleges for Teacher Education (2015). Participation map. Retrieved from http://edtpa.aacte.org/statepolicy   
Arends, R. I. (2006). Performance assessment in perspective: History, opportunities, and challenges. In S. Castle & B. S. Shaklee (Eds.), Assessing teacher performance: Performancebased assessment in teacher education (pp. 3-22). Lanham, MD: Rowman & Littlefield Education.   
Ball, D. L. (2000). Bridging practice: Intertwining content and pedagogy in teaching and learning to teach. Journal of Teacher Education, 51, 241-247.   
Ball, D. L., Thames, M. H., & Phelps, G. (2008). Content knowledge for teaching. Journal of Teacher Education, 59(5), 389- 407.   
Berliner, D. (2005). The near impossibility of testing for teacher quality. Journal of Teacher Education, 56(3), 205-213.   
Blömeke, S., Gustafsson, J. E., & Shavelson, R. (2015). Beyond dichotomies: Viewing competence as a continuum. Zeitschrift für Psychologie, 223(1), 3-13.   
Chung, R. (2008). Beyond assessments: Performance assessments in teacher education. Teacher Education Quarterly, 35(1), 7-28.   
Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297-334.   
D’Agostino, J. V., & Powers, S. J. (2009). Predicting teacher performance with test scores and grade point average: A metaanalysis. American Educational Research Journal, 46(1), 46-182.   
Darling-Hammond, L. (2010). Evaluating teacher effectiveness: How teacher performance assessments can measure and improve teaching. Washington, DC: Center for American Progress.   
Darling-Hammond, L., Newton, S. P., & Wei, R. C. (2013). Developing and assessing beginning teacher effectiveness: The potential of performance assessments. Educational Assessment, Evaluation, and Accountability, 25(3), 179-204.   
Darling-Hammond, L., & Snyder, J. (2000). Authentic assessment of teaching in context. Teaching and Teacher Education, 16(5- 6), 523-545.   
Darling-Hammond, L., & Sykes, G. (1999). Teaching as the learning profession: Handbook of policy and practice. San Francisco, CA: Jossey-Bass.   
Darling-Hammond, L., Wise, A. E., & Klein, S. P. (1999). A license to teach. San Francisco, CA: Jossey-Bass.   
Delandshere, G., & Arens, S. A. (2001). Representations of teaching and standards-based reform: Are we closing the debate about teacher education? Teaching and Teacher Education, 17, 547-566.   
Feiman-Nemser, S. (2012). Teachers as Learners. Cambridge, MA: Harvard Education Press.   
Goldhaber, D., & Hansen, M. (2010). Race, gender and teacher testing: How informative a tool is teacher licensure testing? American Educational Research Journal, 47(1), 218-251.   
Greenblatt, D., & O’Hara, K. E. (2015). Buyer beware: Lessons learned from edTPA implementation in New York State. Thought and Action: The Journal of the National Education Association, 31, 57-67.   
Hammerness, K., Darling-Hammond, L., Bransford, J., Berliner, D., Cochran-Smith, M., McDonald, M., & Kenneth, Z. (2005). How teachers learn and develop. In L. Darling-Hammond & J. Bransford (Eds.), Preparing teachers for a changing world: What teachers should learn and be able to do (pp. 258-289). San Francisco, CA: Jossey-Bass.   
Hill, H. C., Rowan, B., & Ball, D. (2005). Effects of teachers’ mathematical knowledge for teaching on student achievement. American Research Journal, 42(2), 371-406.   
Kennedy, M. (2016). Parsing the practice of teaching. Journal of Teacher Education, 67(1), 6-17.   
Kersting, N. B. (2008). Using video clips of mathematic classroom instruction to measure teachers’ knowledge of teaching mathematics. Educational and Psychological Measurement, 68, 845-861.   
Kersting, N. B., Givvin, K. B., Sotelo, F. L., & Stigler, J. W. (2010). Teachers’ analysis of classroom video predict student learning of mathematics: Further explorations of a novel measure of teacher knowledge. Journal of Teacher Education, 61(1-2), 172-181.   
Kersting, N. B., Givvin, K. B., Thompson, B., Santagata, R., & Stigler, J. (2012). Measuring usable knowledge: Teachers’ analyses of mathematics classroom videos predict teaching quality and student learning. American Education Research Journal, 49(3), 568-589.   
Kersting, N. B., Sherin, B. L., & Stigler, J. W. (2014). Automated scoring of teachers’ open-ended responses to video prompts: Bringing the classroom-video-analysis assessment to scale. Educational and Psychological Measurement, 74(6), 950-974.   
Kersting, N. B., Sutton, T., Kalinec-Craig, C., Stoehr, K. J., Heshmati, S., Lozano, G., & Stigler, J. W. (2016). Further exploration of the classroom video analysis (CVA) instrument as a measure of usable knowledge for teaching mathematics: Taking a knowledge system perspective. ZDM Mathematics Education, 48(1-2), 97-109.   
Knievel, I., Lindmeier, A. M., & Heinze, A. (2015). Beyond knowledge: Measuring primary teachers’ subject-specific competences in and for teaching mathematics with items based on video vignettes. International Journal of Science and Mathematics Education, 13(2), 309-329.   
Knight, S., Lloyd, G., Arbaugh, F., Gamson, D., McDonald, S., Nolan, J., & Whitney, A. (2014). Performance assessment of teaching: Implications for teacher education. Journal of Teacher Education, 65(5), 372-374.   
Leinhardt, G. (2001). Instructional explanations: A commonplace for teaching and location for contrast. In V. Richardson (Ed.), Fourth handbook of research on teaching (pp. 333-357). Washington, DC: American Educational Research Association.   
Lit, I. W., & Lotan, R. (2013). A balancing act: Dilemmas of implementing a high-stakes performance assessment. The New Educator, 9(1), 54-76.   
Meuwissen, K. W., Choppin, J. M., Cloonan, K. D., & ShangButler, H. (2016, April). Celebrations, confessionals, and creative interpretations: Representing teaching practice in the edTPA as a high-stakes certification exam in New York and Washington States. Paper presented at the Annual Meeting of the American Educational Association, Washington, DC.   
Mitchell, K. J., Robinson, D. Z., Plake, B. S., & Knowles, K. T. (2001). Testing teacher candidates: The role of licensure tests in improving teacher quality. Washington, DC: National Academy Press.   
Mitchell, R., & Barth, P. (1999). Not good enough: A content analysis of teacher licensing examinations. Washington, DC: The Education Trust.   
National Board for Professional Teaching Standards. (1999). What teachers should know and be able to do. Arlington, VA: Author.   
National Commission on Teaching and America’s Future. (1996). What matters most: Teaching for America’s future. New York, NY: Author.   
Pecheone, R. L., & Chung, R. R. (2006). Evidence in teacher education: The Performance Assessment for California Teachers (PACT). Journal of Teacher Education, 57(1), 22-36.   
Pecheone, R. L., & Chung, R. R. (2007). PACT technical report. Stanford: Performance Assessment for California Teachers Consortium.   
Performance Assessment for California Teachers Consortium. (2009). Implementation handbook. Stanford, CA: Author.   
Porter, A., Youngs, P., & Odden, A. (2001). Advances in teacher assessments and their use. In V. Richardson (Ed.), Handbook of research on teaching (4th ed., pp. 259-297). Washington, DC: American Educational Research Association.   
Richardson, V., & Placier, P. (2001). Teacher change. In V. Richardson (Ed.), Fourth handbook of research on teaching (pp. 905-947). Washington, DC: American Educational Research Association.   
Sandholtz, J. H., & Shea, L. M. (2012). Predicting performance: A comparison of university supervisors’ predictions and teacher candidates’ scores on a teaching performance assessment. Journal of Teacher Education, 63(1), 39-50.   
Sandholtz, J. H., & Shea, L. M. (2015). Examining the extremes: High and low performance on a teaching performance assessment for licensure. Teacher Education Quarterly, 42(2), 17-42.   
Sato, M. (2014). What is the underlying conception of teaching of the edTPA? Journal of Teacher Education, 65(5), 421-434.   
Sherin, M. G., Jacobs, V., & Philipp, R. (Eds.). (2011). Mathematics teacher noticing: Seeing through teachers’ eyes. New York, NY: Routledge.   
Sherin, M. G., & Van Es, E. A. (2009). Effects of video club participation on teachers’ professional vision. Journal of Teacher Education, 60(1), 20-37.   
Shulman, L. S. (1986). Those who understand: Knowledge growth in teaching. Educational Researcher, 15(2), 4-14.   
Shulman, L. S. (1987). Knowledge and teaching: Foundations of the new reform. Harvard Educational Review, 57(1), 1-22.   
Snyder, J. (2009). Taking stock of performance assessments in teaching. Issues in Teacher Education, 19(1), 7-11.   
Tatto, M. T., Schwille, J., Senk, S., Ingvarson, L., Peck, R., & Rowley, G. (2008). Teacher Education and Development Study in Mathematics (TEDS-M): Policy, practice, and readiness to teach primary and secondary mathematics. Conceptual framework. East Lansing: Teacher Education and Development International Study Center, College of Education, Michigan State University.   
Tellez, K. (2016, April). An analysis of the structure and assessment of standards for teacher candidates and programs. Paper presented at the Annual Meeting of the American Educational Research Association, Washington, DC.   
Wei, R., & Pecheone, R. (2010). Assessment for learning in preservice teacher education: Performance-based assessments. In M. Kennedy (Ed.), Teacher assessment and the quest for teacher quality: A handbook (pp. 69-132). San Francisco, CA: Jossey-Bass.   
Zeichner, K. M. (2003). The adequacies and inadequacies of three current strategies to recruit, prepare, and retain the best teachers for all students. Teachers College Record, 105, 490-519.

# Author Biographies

Rossella Santagata is an associate professor in the School of Education at the University of California, Irvine. Her research focuses on the design and study of teacher learning experiences that foster reflection and analysis, the use of video and multimedia technology as a tool for teacher learning, and cross-cultural studies of mathematics teaching and learning.

Judith Haymore Sandholtz is a professor in the School of Education at the University of California, Irvine. Her research focuses on teacher professional development, sustainability of professional development outcomes, teacher education, and school– university partnerships.