{"query": "how can students use AI to get feedback on their essays", "top_k": 5, "min_score": 0.1, "filter_pedagogical": false, "results": [{"score": 0.2691657841205597, "document": {"id": 1971, "file_path": "annotated_data/batch_001/out_CHRX3SLR_Exploring_ChatGPT-supported_tea.md", "filename": "out_CHRX3SLR_Exploring_ChatGPT-supported_tea.md", "content": "# Exploring ChatGPT-supported teacher feedback in the EFL context\n\nJining Han a,\\* , Mimi Li b\n\na Faculty of Education, Southwest University, No.2 Tiansheng Road, 400715, Chongqing, China   \nb Department of Literature and Languages, Texas A&M University-Commerce, David Talbot Hall of Languages 141, 2200 Campbell St, Commerce,   \nTX, 75428, USA\n\n# A R T I C L E I N F O\n\n# A B S T R A C T\n\nKeywords:   \nChatGPT   \nTeacher feedback   \nWriting feedback   \nAI   \nLanguage education\n\nThis study investigates the ChatGPT-supported teacher feedback in the Chinese tertiary EFL context and explores an innovative AI-aided writing pedagogy by integrating ChatGPT into teacher writing feedback provisions to alleviate the challenges of teacher feedback in a large class, which were reported in previous research. The participants of this study were four in structors and 102 students from two undergraduate classes in the world language education program. The students completed two writing tasks: an argumentative essay and an expository essay; then, the instructors provided detailed feedback on their essays based on the ChatGPT feedback. Two prompts were provided to ChatGPT after the training: 1) corrective feedback drawing on Ferris\u2019s (2006)15 types of common errors and 2) holistic rhetorical feedback. Afterwards, the teachers adapted the ChatGPT feedback and shared the detailed individualized writing feedback with each student. We closely examined the types and features of ChatGPTsupported teacher feedback and how EFL students incorporate this feedback into their writing revisions. The findings indicate that the ChatGPT-supported teacher feedback addressed diverse error categories and included helpful comments on the overall rhetoric. Moreover, the students incorporated more of the feedback into their revisions across tasks, which reflects their deeper engagement with the feedback content. This study notes the importance of an \u201cAI $^ +$ Teacher\u201d model that leverages the analytical strengths of AI while maintaining essential teacher\u2012student interactions. This new approach of ChatGPT-supported teacher feedback has great potential in L2 writing feedback provision and will shed novel light on the writing pedagogy with the aid of AIin the digital era.\n\n# 1. Introduction\n\nThe advent of artificial intelligence (AI) in educational settings has significantly transformed the landscape of language learning and teaching, especially in the domain of writing instruction. With the integration of AI-mediated tools such as automated writing evaluation (AWE) and ChatGPT, educators have harnessed the capabilities of natural language processing (NLP) to provide immediate and personalized feedback to learners to enhance their writing proficiency. Compared with AWE tools, ChatGPT feedback can address not only grammatical accuracy but also can help develop students\u2019 critical thinking and analytical skills in writing (Dwivedi et al., 2023; Essel et al., 2024; Herbold et al., 2023). The adaptability of ChatGPT makes it an impactful resource to foster interactive language education. In Chinese tertiary educational settings, professors in research institutions have direct access to ChatGPT with the adherence to relevant regulations regarding the use of AI, whereas students do not. Therefore, the integration of ChatGPT into teacher writing feedback provisions can simultaneously help alleviate the challenge of providing teacher feedback for a large class and utilize the affordances of AI. Specifically, in Chinese universities, an EFL instructor often teaches several large classes, each of which has up to 100 students (Yang et al., 2006), which makes it almost impossible to provide detailed writing feedback for each student and impact the writing feedback on the language and writing development of the students. Thus, the current study explores an innovative feedback approach, called ChatGPT-supported teacher feedback, in the Chinese tertiary EFL context. ChatGPT was first trained on how to provide detailed feedback on students\u2019 essays; then, the teachers adapted ChatGPT feedback before sharing the writing feedback with the students (see the detailed explanation in the Methods section). This study specifically examines how ChatGPT can serve as a companion for teachers to provide tailored and constructive feedback and how Chinese EFL students incorporate ChatGPT-supported teacher feedback into their revisions. This work contributes to the ongoing discourse on optimizing AI tools for L2 writing pedagogy in the digital era.\n\n# 2. Literature review\n\nThis study is undergirded by sociocultural theory and informed by previous research on AI-mediated writing feedback.\n\n# 2.1. Theoretical framework\n\nMediation (Vygotsky, 1978), which is the central concept of sociocultural theory, refers to the roles of tools and artifacts in facilitating human activities. In ChatGPT-supported teacher feedback, ChatGPT acts as a mediator between teacher and feedback process. It can provide immediate and effective responses to students\u2019 writing based on prompts that the teacher inputs. The teacher engages with the ChatGPT output and tailors it to the students\u2019 needs. Moreover, the construct of scaffolding (Bruner, 1985) supports the innovative approach of ChatGPT-supported teacher feedback. ChatGPT can scaffold the teacher in effectively performing feedback tasks that would have been difficult to independently accomplish. Meanwhile, teachers can bridge the gap between current abilities and potential developmental level of the students through appropriate guidance and scaffolding, as reflected by the incorporation of ChatGPT-supported teacher feedback into the writing revisions of the students. This study also uses the Zone of Proximal Development (ZPD) to illustrate how ChatGPT-supported teacher feedback can bridge the gap between current abilities and potential growth of students through appropriate guidance and collaborative support. The potential development level of a student largely depends on collaborative efforts with others, particularly the feedback from teachers. In short, the constructs of mediation and scaffolding inform our research design. The mediation of ChatGPT facilitates the provision of writing feedback for EFL teachers, including both corrective feedback and rhetorical feedback. Additionally, ChatGPT-supported teacher feedback scaffolds the writing revisions of students.\n\n# 2.2. Automated writing evaluation (AWE)\n\nAIfeedback constitutes part of ChatGPT-supported teacher writing feedback, where teachers provide detailed writing feedback based on ChatGPT output in response to appropriate prompts. In this section, we synthesize representative studies that use AWE in L2 writing classes. AWE systems, which integrate NLP technology, latent semantic analysis, and AI, can offer learners immediate auto mated feedback, which helps enhance their writing competence (Grimes & Warschauer, 2010; Wilson & Roscoe, 2020). For example, Wilson et al. (2014) used the AWE tool \u201cCriterion\u201d to provide feedback for essays written by 4162 students in grades 4 to 8, which improved their essay quality. Moreover, by automating the analysis of student writing texts and providing feedback, AWE systems significantly reduce the time and effort that teachers need to invest in offering personalized writing feedback (Han & Sari, 2022). Wilson and Czik (2016) revealed that compared to feedback solely from teachers, the AWE system \u201cPEG Writing\u201d reduced the amount of time that teachers spent providing feedback by 2\u20133 times, and the quantity of AWE-based teacher feedback remained the same as that provided by only the teacher. AWE lessened the grading burden of instructors while providing them more time to provide feedback on the global issues of writing (Barrot, 2023a, 2023b; Wilson & Czik, 2016).Nevertheless, by directly pointing out errors and providing suggestions for modification, AWE systems may limit the capacity of students to transfer writing skills across writing tasks. Stevenson and Phakiti (2014) reported that although AWE might reduce errors in specific writing tasks, students could not demon strate learning transfer across various tasks. Ranalli\u2019s (2021) study examined the engagement of six L2 students using \u201cGrammarly\u201d based on screen captures, interviews, and stimulated recalls and revealed that the participants primarily used feedback for editing instead of aiming for long-term learning. These findings may be attributed to the directly provision from \u201cGrammarly\u201d of highly specific corrections, which made the students make local changes without substantial cognitive engagement. Due to algorithmic constraints, the feedback provided by AWE tended to be formulaic and lacked consideration of specific pedagogical needs (Ranalli, 2018).\n\n# 2.3. Utilizing ChatGPT for writing feedback\n\nCompared with traditional AWE systems, ChatGPT exhibits advantages in writing feedback provision due to its human-like conversational abilities and capacity for knowledge expression. ChatGPT can deliver personalized and comprehensive feedback on student writing (Caldarini, Jaf, & McGarry, 2022). Barrot, 2023a, 2023b expounded on the potential benefits of ChatGPT in L2 writing and highlighted its capacity as a rich information source for language learning and its ability to clarify themes, construct structures, and provide real-time adaptive feedback. The study also highlighted the limitations of ChatGPT, such as the possibility of generating inaccurate or unintelligible responses and its limited ability to tailor content for specific target audiences. Barrot, 2023a, 2023b emphasized the importance of clear instructional guidelines and targeted training for teachers to effectively use ChatGPT in writing instruction. Yan (2023) conducted an exploratory qualitative study on the role of ChatGPT in a university English writing course. The students noted the benefits of using ChatGPT, such as its significant potential for supporting language learning, providing timely feedback, enhancing learning motivation, and improving the writing quality. Moreover, Kohnke et al. (2023) emphasized the trans formative potential of ChatGPT in language education and its ability to promote engaging and adaptive language learning. ChatGPT facilitates the feedback process by creating customized prompts and responses that address student writings according to the teacher\u2019s guidelines. The immediate ChatGPT feedback helps students understand and incorporate corrections and suggestions into their re visions, which enhances their learning process.\n\nComparing teacher feedback and ChatGPT feedback, Steiss et al. (2024) reported that well-trained teachers deliver higher-quality feedback than ChatGPT; however, considering the convenience and overall quality of ChatGPT-generated feedback, ChatGPT feedback is valuable when well-trained teachers are unavailable or when there is a need to evaluate many writing drafts. This research study highlights the potential of using ChatGPT to provide writing feedback and implies a prospectus of teacher-AI collaboration.\n\nIn another study, Guo and Wang (2023) reported that ChatGPT generated more feedback than teachers, evenly distributed attention across three areas of focus (i.e., content, organization, and language), and tended to provide direct revision guidance. Teachers provide personalized, engaging feedback that accounts for individual abilities and needs of students and often deliver query-based feedback. The teacher participants commented on ChatGPT\u2019s ability to evaluate student writing, specificity of the feedback and limitations of ChatGPT feedback in terms of length, reading difficulty, and relevance. Their study suggested the importance of teacher agency. Instead of having ChatGPT directly provide feedback for students, teacher feedback can be facilitated by ChatGPT output. To bridge the gap that no previous research examined the collaboration of instructors and AIin providing writing feedback, we implemented an innovative feedback approach (i.e., ChatGPT-supported teacher feedback) in a Chinese EFL course to explore the ChatGPT-supported teacher feedback and students\u2019 incorporation of the feedback into their revisions. This study is informed by two research questions.\n\nRQ1: What are the types and features of ChatGPT-supported feedback provided by teachers across two writing tasks? RQ2: How do students incorporate ChatGPT-supported teacher feedback into their revisions regarding two writing task\n\n# 3. The study\n\n# 3.1. Context and participants\n\nThe present study, which was part of a larger research project on ChatGPT-supported teacher feedback, explored the types and features of ChatGPT-supported teacher feedback and the incorporation of EFL students of that feedback into their revisions. At a large public university in southwestern China, we used ChatGPT to help instructors provide writing feedback in the Language Teaching Methodology course offered in English to undergraduate students majoring in World Language Education in the spring of 2023. Each class was taught by two instructors (a senior faculty member and a junior faculty member). The curriculum covered various facets of language instruction, such as teaching methods, educational theories, instructional strategies, and intercultural communication, with a particular emphasis on improving English composition skills. In total, 102 students (47 from one class and 55 from the other) participated in this study after they had participated in the study orientation and signed the informed consent forms.\n\n# 3.2. Study design and procedure\n\nWe implemented two types of ChatGPT-supported teacher feedback based on the contingent needs of the students: 1) indirect corrective feedback drawing on Ferris\u2019 (2006) 15 types of common errors specific to EFL learners (the instructor changed the direct feedback of ChatGPT to coded indirect feedback in the feedback provision) and 2) holistic rhetorical feedback, which is commonly not included in the traditional EFL teacher feedback. Indirect feedback refers to teachers\u2019 indications of the types of errors and locations of errors, which enable students to identify and correct language mistakes on their own (Ferris et al., 2013). Indirect feedback was adopted in this study because research has shown that indirect feedback promotes the autonomous reflection and problem-solving abilities of students and consequently helps students become independent learners (Ferris, 2010). In addition, indirect feedback has been proven to be an effective strategy for learning content words such as verbs, nouns, and articles (Ferris et al., 2013; Ferris & Roberts, 2001). Specifically, there are two categories of indirect feedback: uncoded indirect feedback and coded indirect feedback. Uncoded indirect feedback refers to instances where the teacher merely marks an error by underlining, circling, or placing a tally in the margin, without explicitly indicating the type of error (Bitchener et al., 2005). Coded indirect feedback, which our study used, refers to instances where the teacher identifies the exact location of an error and specifies the type of error and reportedly enhances the problem solving, reflection, self-learning, and learning outcomes of students. (Ferris, 2010; Han, 2024). The inclusion of rhetorical feedback was inspired by Aljasir (2021) and Carter and Thirakunkovit (2019), who noted that rhetorical feedback enhanced the ability of students to efficiently provide clear arguments, substantiate major points with evidence, maintain logical flow, and organize ideas. Accordingly, we developed two prompts for ChatGPT and trained it to provide feedback according to the prompts: 1) \u201cplease point out all errors on sentence structure, word choice, verb tense, noun endings (singular/plural), verb form, punctuation, articles/determiners, word form, spelling, run-ons, pronouns, subject\u2012verb agreement, fragments, idioms, and informal text\u201d (hereinafter \u201cInstruction 1\u201d) and 2) \u201cplease provide feedback on the rhetoric within the article\u201d (hereinafter \u201cInstruction 2\u201d).\n\nThe first author provided extensive training to the four instructors on how to provide coded indirect feedback and holistic feedback based on the ChatGPT output. Specifically, he guided the instructors to practice implementing ChatGPT-supported teacher feedback using a few sample papers. They ran the ChatGPT prompts, reviewed the ChatGPT output and provided constructive feedback on the sample papers based on the ChatGPT feedback. Then, accounts were registered for the four teachers to use ChatGPT. The students were required to complete two writing tasks: an argumentative essay entitled \u201cThe Importance of Learning a Second Language for Chinese Language Majors\u201d and an expository essay on \u201cThe Impact of Technology on Language Education\u201d. Prior to their writing, the teachers provided explicit instructions on how to craft a 350-word essay using digital devices, which included all required components of an essay. Detailed assessment criteria were also provided to the students; for example, the argumentative essay was evaluated based on its thesis robustness, evidence, counterargument, grammatical accuracy, citation of sources, and overall coherence and organization. The essays were composed during class sessions (i.e., $4 5 \\mathrm { { m i n } }$ per session) and submitted through a learning management system (LMS). Two instructors, one from each class, graded half of the submitted essays.\n\nThe innovative method of ChatGPT-supported teacher feedback includes three stages: 1) the teacher copied the essay of a student into ChatGPT and input Instruction 1; 2) the instructor closely reviewed the ChatGPT-generated feedback to provide coded indirect feedback on the students\u2019 drafts by adding in-text annotations in MS Word; 3) the instructor input Instruction 2 for rhetorical feedback from ChatGPT, made modifications if needed, and appended the holistic feedback to the end of the student\u2019s essay. Subsequently, the two main types of feedback were shared with students via the LMS, and the students were required to submit their revised essays based on the ChatGPT-supported feedback within five days. Table 1 shows the detailed procedure and collected data in the larger research project.\n\n# 3.3. Data analysis\n\nAlthough 102 students participated in the study and completed their writing assignments in class, seven students did not submit their revised drafts for both writing tasks. Therefore, we excluded the essays of these seven students from our analysis. We analyzed 95 drafts and 95 revised papers for each of the two tasks.\n\nTo address RQ1, we conducted a detailed analysis of the ChatGPT-supported teacher feedback and identified the total number of intext comments and the number and percentage of comments regarding each type of language error. For example, in Task 1, the instructor annotated 1176 errors, 220 of which were related to word choice, which accounted for $1 8 . 7 1 \\%$ of all errors. We also conducted a content analysis of the rhetorical feedback on 190 essays from the two writing tasks. We enabled the categories of holistic comments to emerge from the data and identified eight types, as displayed in Table 2. Then, we calculated the number of each rhetorical feedback type in each essay and compared the overall rhetorical feedback in the two writing tasks. To answer RQ2, we adapted Ferris\u2019 (1997) method for scoring revisions, where we adjusted Ferris\u2019s seven-point scale to a three-point scale for Instruction 1 regarding the error feedback, including no change, an unsuccessful revision, and a successful revision. For Instruction 2 regarding holistic rhetoric feedback, the three-point scale includes no change, partial modification, and full modification. The distinctions among these types enable us to understand the scaffolding effects of ChatGPT-supported teacher feedback on the students\u2019 revisions. One researcher coded all revisions of the students, and a colleague with a certification in English teaching randomly selected $3 0 \\%$ of the papers for coding. The kappa values for Task 1 and Task 2 revisions were 0.903 and 0.853, respectively, with an average kappa value of 0.885.\n\nTable 1 Research procedures and collected data.   \n\n<html><body><table><tr><td>Timeline</td><td>Procedure</td><td>Data collected</td></tr><tr><td colspan=\"3\">Researchers preparation</td></tr><tr><td colspan=\"3\">Week 1</td></tr><tr><td rowspan=\"3\">Week 2</td><td>1. Teacher training 2. Two classes of students are trained.</td><td></td></tr><tr><td>1. Students complete the first writing assignment in class</td><td>1. 1st draft</td></tr><tr><td>2. Teachers provide ChatGPT-supported teacher feedback</td><td>2. ChatGPT-supported teacher feedbacke</td></tr><tr><td>Week 3</td><td>1. Students receive feedback from their teachers</td><td>3. Class observations</td></tr><tr><td rowspan=\"5\">Week 4</td><td colspan=\"3\"></td></tr><tr><td colspan=\"3\">2. Students revise the writing assignment. 1. Students submit the revised writing assignment</td></tr><tr><td colspan=\"3\"></td></tr><tr><td>2. Students complete the 2nd writing assignment in the class</td><td colspan=\"2\">2. 2nd draft 3. Class observations</td></tr><tr><td>3. Teachers provide ChatGPT-supported teacher feedback</td><td colspan=\"2\">4. 1st revision scores</td></tr><tr><td colspan=\"3\"></td></tr><tr><td>Week 5</td><td colspan=\"3\">1. Students receive feedback from their teachers</td></tr><tr><td>Week 6</td><td colspan=\"3\">2. Students revise the writing assignment. 1. Students submit the revised writing assignment</td></tr><tr><td></td><td colspan=\"3\"></td></tr><tr><td>Week 7</td><td></td><td colspan=\"2\">1. 2nd revision 2. 2nd revision scores</td></tr><tr><td>Week 8</td><td>1. Semi-structured interviews with teachers. 1. Semi-structured interviews with students</td><td colspan=\"3\">1. Teacher interviews 1. Student interviews</td></tr></table></body></html>\n\nNote: The interviews, each of which was conducted in Chinese and lasted approximately $3 0 \\ \\mathrm { m i n }$ , aimed to explore the teachers\u2019 and students perceptions of ChatGPT-supported teacher feedback. The teachers\u2019 perceptions of ChatGPT-supported teacher feedback were the focus of another work of the co-authors.\n\nTable 2 Types of rhetorical feedback: definitions and examples.   \n\n<html><body><table><tr><td></td><td>Codes</td><td>Operationalized definition</td><td>Illustrative examples</td></tr><tr><td>1</td><td>Cohesion and coherence</td><td>Any evaluative or commentary focusing on the logical connection and flow between sentences, paragraphs, and sections of a writing assignment.</td><td>However, the writing contains some issues with coherence.</td></tr><tr><td>2</td><td>Comprehensibility of message</td><td>Any evaluative commentary or inquiry concerning the clarity, understandability, and ease of interpretation of the message conveyed in a writing assignment..</td><td>The text is not clear enough.</td></tr><tr><td>3</td><td>Content development</td><td>Any evaluative or constructive commentary focusing on the creation, evidence, examples, expansion, arguments, and improvement of the essay&#x27;s content.</td><td>The article could be improved by providing more. specific examples and evidence to support the arguments made.</td></tr><tr><td>4</td><td>Mechanics (spelling, punctuation, capitalization)</td><td>Any evaluative or commentary that focuses on the accuracy and appropriateness of spelling, punctuation, and capitalization in a writing assignment.</td><td>However, there are a few errors in punctuation that could be corrected.</td></tr><tr><td>5</td><td>Organization</td><td>Any evaluative or commentary that addresses the structural layout and logical sequence of content in a writing assignment.</td><td>The organization of the article could be improved.</td></tr><tr><td>6</td><td>Sentence structure/grammar</td><td>Any evaluative or commentary on the syntactic arrangement of words within a sentence to convey meaning effectively, governed by the rules of the language.</td><td>There are some grammatical errors in the article. that could be corrected to improve.</td></tr><tr><td>7</td><td>Word choice</td><td>Any evaluative or commentary focusing on the appropriateness, precision, and impact of the word used in a writing assignment.</td><td>There are some issues with word choice that could. be improved.</td></tr><tr><td>8</td><td>Writing style</td><td>Any evaluative or commentary that addresses the author&#x27;s stylistic choices in a writing assignment. This includes feedback on the tone, voice, and formality of the writing style..</td><td>The author uses informal language in some places, which can detract from the overall professionalism. of the article.</td></tr></table></body></html>\n\n# 4. Results\n\nWe present the findings of this study regarding the two research questions.\n\n4.1. RQ 1: what are the types and features of ChatGPT-supported feedback provided by teachers across two writing tasks?\n\nThe instructors provided indirect feedback on Ferris\u2019s 15 error types according to the ChatGPT output. Examples 1 and 2 in Table 3 are illustrative. Moreover, the instructors provided the overall feedback on the rhetoric of the students\u2019 essays based on the ChatGPT output, as shown in example 3.\n\nTable 4 presents an overall picture of ChatGPT-supported teacher feedback regarding different error types in each writing task. As displayed in Table 4, Writing Task 1 received 1176 instances of feedback with an average of 12.38 instances per essay. Writing   \nTask 2 received 665 instances with an average of 7 feedback points per essay. In Task 1, the three most frequent addressed types of   \nerrors in ChatGPT-supported teacher feedback were word choice $( n = 2 2 0$ , $1 8 . 7 1 \\%$ ), sentence structure $\\mathbf { \\tilde { \\Delta } } n = 1 6 7$ , $1 4 . 2 \\% )$ ), and   \npunctuation $( n = 1 2 8$ , $1 0 . 8 8 \\%$ ). The four least frequently addressed types of errors were fragments $( n = 7$ , $0 . 6 \\%$ , idioms $( n = 1 9$ ,\n\nTable 3 Illustrative examples of ChatGPT outputs and teacher feedback.   \n\n<html><body><table><tr><td>Examples</td><td>ChatGPT output</td><td>Teacher feedback</td></tr><tr><td>Example 1 Verb tense Example 2 Word choice</td><td>ChatGPT output: &quot;had studied&quot;.</td><td>Verb tense: check the tense of &quot;had study&quot;..</td></tr><tr><td>Example 3 Rhetorical feedback</td><td>&quot;Communication and communication&quot; seem repetitive. Perhaps you meant &quot;communication and collaboration&quot;.. The article effectively presents the importance of learning a second foreign language, particularly for students majoring in Chinese, in the context of globalization and increasing global interactions. The author provides various arguments, including cognitive benefits, career opportunities, and cultural exchange, to support their claim.</td><td>&quot;Communication and communication&quot; seem repetitive. Your article is well structured and organized, with clear and concise paragraphs that focus on specific points. You use relevant examples to illustrate their arguments, which helps to make the article more</td></tr><tr><td></td><td>The article is well-structured and organized, with clear and concise paragraphs that focus on specific points. The author also uses relevant examples to illustrate their arguments, which helps to make the article more persuasive. One area where the article could be improved is in the use of sources to support the arguments presented. While the author mentions experimental results that show the cognitive benefits of learning a second language, they do not provide specific references or sources for these results. Adding this information would make the article more credible and convincing. Overall, the article effectively presents the benefits of learning a second foreign language, particularly for students majoring in Chinese, and provides compelling arguments to support this claim.</td><td>persuasive. (1) One area where your article could be improved is in the use of sources to support the arguments presented, and (2) while you mention experimental results that show the cognitive benefits of learning a second language, you do not provide specific references or sources for these results. Adding this information would make your article more credible and convincing.e</td></tr></table></body></html>\n\nTable 4 Distribution of error types across two writing tasks.   \n\n<html><body><table><tr><td>Feedback in terms of error types</td><td colspan=\"2\">1st writing task N = 95</td><td colspan=\"2\">2nd writing task N = 95</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>Sentence structure</td><td>167</td><td>14.2.</td><td>71</td><td>10.68</td></tr><tr><td>Word choice</td><td>220</td><td>18.71</td><td>145</td><td>21.8</td></tr><tr><td>Verb tense</td><td>105</td><td>8.93</td><td>38</td><td>5.71</td></tr><tr><td> Noun endings (singular/plural)</td><td>55</td><td>4.68</td><td>40</td><td>6.02</td></tr><tr><td>Verb form</td><td>74</td><td>6.29</td><td>35</td><td>5.26</td></tr><tr><td>Punctuation</td><td>128</td><td>10.88</td><td>38</td><td>5.71</td></tr><tr><td>Articles/determiners</td><td>92</td><td>7.82</td><td>38</td><td>5.71</td></tr><tr><td>Word form</td><td>117</td><td>9.95</td><td>54</td><td>8.12</td></tr><tr><td> Spelling</td><td>46</td><td>3.91</td><td>72</td><td>10.83</td></tr><tr><td>Run-ons</td><td>32</td><td>2.72</td><td>41</td><td>6.17</td></tr><tr><td>Pronouns</td><td>32</td><td>2.72</td><td>8.</td><td>1.2</td></tr><tr><td>Subject-verb agreement</td><td>44</td><td>3.74</td><td>32</td><td>4.8</td></tr><tr><td>Fragments</td><td>7</td><td>0.6</td><td>29</td><td>4.36</td></tr><tr><td> Idiom</td><td>19</td><td>1.62</td><td>4.</td><td>0.6</td></tr><tr><td>Informal</td><td>38</td><td>3.23</td><td>20</td><td>3</td></tr><tr><td>Total</td><td colspan=\"2\">1176</td><td colspan=\"2\">665</td></tr></table></body></html>\n\n$1 . 6 2 \\% )$ ), pronouns $( n = 3 2$ , $2 . 7 2 \\%$ ) and run-ons $( n = 3 2$ , $2 . 7 2 \\%$ , which were tied. In Task 2, this pattern only slightly changed, where the three most common types of errors became word choice $( n = 1 4 5$ , $2 1 . 8 \\%$ , spelling $( n = 7 2$ , $1 0 . 8 3 \\% )$ ), and sentence structure $( n =$ 71, $1 0 . 6 8 \\%$ . The three least frequent error types were idioms $( n = 4$ , $0 . 6 \\%$ , pronouns $( n = 8 , 1 . 2 \\% )$ , and informal language $( n = 2 0$ , $3 \\%$ ). The proportion of nine types of errors decreased. The sentence structure, verb tense, verb form and punctuation errors decreased from $1 4 . 2 \\%$ , $8 . 9 3 \\%$ , $6 . 2 9 \\%$ and $1 0 . 8 8 \\%$ of all errors in Task $1 { - } 1 0 . 6 8 \\%$ , $5 . 7 1 \\%$ , $5 . 2 6 \\%$ , and $5 . 7 1 \\%$ of all errors in Task 2, respectively. Additionally, the percentages of article/determiner, word form, pronoun, idiom, and informal language errors decreased from $7 . 8 2 \\%$ , $9 . 9 5 \\%$ , $2 . 7 2 \\%$ , $1 . 6 2 \\%$ and $3 . 2 3 \\% - 5 . 7 1 \\%$ , $8 . 1 2 \\%$ , $1 . 2 \\%$ , $0 . 6 \\%$ , and $3 \\%$ , respectively. Conversely, the proportions of six types of errors increased: word choice from $1 8 . 7 1 \\%$ to $2 1 . 8 \\%$ , noun endings (singular/plural) from $4 . 6 8 \\%$ to $6 . 0 2 \\%$ , spelling from $3 . 9 1 \\%$ to $1 0 . 8 3 \\%$ , run-on sentences from $2 . 7 2 \\%$ to $6 . 1 7 \\%$ , subject\u2012verb agreement from $3 . 7 4 \\%$ to $4 . 8 \\%$ , and fragments from $0 . 6 \\%$ to $4 . 3 6 \\%$ . We conducted a Chi-square test on the number of errors in two writing tasks, which resulted in a $\\chi ^ { 2 }$ value of 115.275 and a $p$ -value of $5 . 2 7 2 \\times$ $1 0 ^ { - 1 8 }$ (see Table 6). These results indicate a statistically significant difference in the distribution of error types between 1st and 2nd writing tasks. Possible reasons will be addressed in the discussion section.\n\nMoreover, we analyzed the feedback provided on 190 essays across two different writing tasks, as shown in Table 5. Comprehensibility of message was the most frequent feedback type in Task 1 at 95 $( 2 1 \\% )$ but decreased to $1 2 \\%$ in Task 2. Cohesion and Coherence decreased from 78 $( 1 7 \\% )$ in Task 1 to 49 $( 1 1 \\% )$ in Task 2, which is in parallel with the shift from message comprehensibility to more form-focused feedback. Both Content development and Organization maintained high importance across tasks, and content development slightly decreased from 95 $( 2 1 \\% )$ in Task 1 to 67 $( 1 5 \\% )$ in Task 2, whereas organization remained steady at approximately $1 8 \\%$ . Sentence Structure/Grammar increased from 33 $( 7 \\% )$ in Task 1 to 68 $. 1 5 \\% )$ in Task 2, which suggests a greater emphasis on the grammatical accuracy in Task 2. Word Choice more than doubled from 17 $( 4 \\% )$ in Task 1 to 50 $( 1 1 \\% )$ in Task 2, which reflects the increased attention to language precision in Task 2. Feedback on mechanics increased from 12 $( 3 \\% )$ to 36 $( 8 \\% )$ . Writing style feedback remained consistent at approximately $1 0 \\%$ , which indicates a sustained focus on the formal aspects of writing. We performed Chi-square analyses to analyze the instances of rhetorical feedback across two writing tasks, which resulted in $\\chi ^ { 2 }$ of 61.45 and a $p$ -value less than 0.001 (see Table 6). These results indicate a statistically significant difference in the distribution of rhetorical feedback types across two writing tasks.\n\n.2. RQ2: how do students incorporate ChatGPT-supported feedback into revisions regarding the two writing tasks?\n\nWe found that most errors addressed in ChatGPT-supported teacher feedback were successfully corrected. As shown in Table 7,\n\nTable 5 Distribution of rhetorical feedback types across two writing tasks.   \n\n<html><body><table><tr><td>Feedback types</td><td>Task 1 (N = 95)</td><td>Task 2 (N = 95)</td></tr><tr><td>Cohesion and coherence</td><td>78 (17%)</td><td>49 (11%)</td></tr><tr><td>Comprehensibility of message</td><td>95 (21%)</td><td>57 (12%)</td></tr><tr><td>Content development</td><td>95 (21%)</td><td>67 (15%)</td></tr><tr><td>Mechanics (spelling, punctuation, capitalization)</td><td>12 (3%)</td><td>36 (8%)</td></tr><tr><td>Organization</td><td>85 (18%)</td><td>84 (18%)</td></tr><tr><td>Sentence structure/grammar</td><td>33 (7%)</td><td>68 (15%)</td></tr><tr><td>Word choice</td><td>17 (4%)</td><td>50 (11%)</td></tr><tr><td>Writing style</td><td>45 (10%)</td><td>48 (10%)</td></tr><tr><td>Total</td><td>460</td><td>459</td></tr></table></body></html>\n\nTable 6 Chi-square tests on the number of errors and instances of rhetorical feedback across two writing tasks.   \n\n<html><body><table><tr><td>Task1 vs. Task2</td><td>x2</td><td>df</td><td>N</td><td>p</td></tr><tr><td>Distribution of error types</td><td>115.275</td><td>14</td><td>1841</td><td>&lt;0.001</td></tr><tr><td>Distribution of rhetorical feedback types</td><td>61.45</td><td>7</td><td>919</td><td>&lt;0.001</td></tr></table></body></html>\n\nNote: $p < 0 . 0 0 1$ indicates that the p-values are less than 0.001.\n\n$6 4 . 5 4 \\%$ and $6 3 . 9 1 \\%$ of the changes in Task 1 and Task 2 were successful revisions, respectively. Moreover, we detected a decrease in the instances of no change, from $2 3 . 5 5 \\%$ in Task $1 - 2 0 . 3 \\%$ in Task 2. In Task 1, only $9 . 4 7 \\%$ of the modifications were classified as full revisions, $2 5 . 2 6 \\%$ were categorized as partial revisions, and $6 5 . 2 6 \\%$ were classified as no change. Table 8 shows the students\u2019 re sponses to rhetorical feedback. The revision of rhetorical feedback considerably shifted in Task 2: no changes decreased to $3 5 . 7 9 \\%$ and partial revisions decreased to $7 . 3 7 \\%$ . However, the instances of unsuccessful revisions increased from $1 1 . 9 \\%$ in Task $1 { - } 1 5 . 7 9 \\%$ in Task 2. There was a change in revision of the rhetorical feedback, where full revisions increased to $5 6 . 8 4 \\%$ .\n\nWe performed Chi-square analyses to investigate whether there were significant differences among the three ratings in task 1 and task 2. For the students\u2019 revisions based on Ferris\u2019 15 error categories, there is a significant difference among the three ratings (no change, unsuccessful revision, and successful revision) in Task 1 and Task 2, with statistically more successful revisions identified. In the case of the students\u2019 revisions based on rhetorical feedback, there is a significant difference among the three ratings (no change, partial modification, and comprehensive modification) in each task (see Table 9). Interestingly, statistically more \u201cno change\u201d instances were identified in task 1, but statistically more \u201ccomprehensive modification\u201d instances were discovered, which clearly revealed the better engagement and performance of the students in task 2.\n\nWe further examined the revisions in terms of error categories across the two tasks, as shown in Table 10. In Task 1, four error categories had successful revision ratios of more than $7 0 \\%$ : word choice $( 8 2 . 3 \\% )$ , sentence structure $( 7 6 . 6 \\% )$ , spelling $( 7 6 . 1 \\% )$ , and word form $( 7 3 . 5 \\% )$ . Notably, $8 9 . 6 \\%$ of word choice errors were modified, and $8 2 . 3 \\%$ of these modifications were successful. Similarly, $9 1 \\%$ of the sentence structure errors were modified, $7 6 . 6 \\%$ of the errors were successfully corrected and only $9 \\%$ of the errors remained unmodified. The other six error categories had percentages of successful revisions of $6 0 { - } 7 0 \\%$ : verb form $( 6 8 . 9 \\% )$ , informal $( 6 5 . 8 \\% )$ , verb tense $( 6 5 . 7 \\% )$ , pronouns $( 6 5 . 6 \\% )$ , noun endings $( 6 3 . 6 \\% )$ , and articles/determiners $( 6 2 . 0 \\% )$ ). Only two error categories had a low successful revision ratio (under $3 0 \\%$ ): punctuation $( 2 1 . 9 \\% )$ and fragments $( 1 4 . 3 \\% )$ .\n\nIn Task 2 (see Table 11), there was an overall improvement in the correction of errors, where successful modification rates exceeded $5 0 \\%$ for 13 types of errors. Although the success rates for correcting punctuation and fragment errors were below $5 0 \\%$ $4 2 . 1 \\%$ and $4 8 . 3 \\%$ , respectively), these rates increased compared with those of Task 1. The percentages of successful revisions for other types of errors were above $5 0 \\%$ . Additionally, six error categories exhibited successful revision rates above $7 0 \\%$ : noun endings $( 8 5 \\% )$ , verb forms $( 7 1 . 4 \\% )$ , spelling $( 7 6 . 4 \\% )$ , subject\u2012verb agreement $( 8 1 . 3 \\% )$ , idioms $( 1 0 0 \\% )$ , and informal $( 8 0 . 0 \\% )$ , which indicates a signifi cant shift from the results in Task 1. In Task 2, although the successful revision of spelling errors maintained a similar percentage to that in Task 1 (above $7 0 \\%$ ), the percentages of successful revisions for the other five categories improved, which suggests that under the teacher\u2019s guidance, students began to focus on errors that were poorly addressed in Task 1. However, there were notable declines in the success of sentence structure and word choice revisions: $5 4 . 9 \\%$ and $5 8 \\%$ , respectively, with particularly high rates of no change $( 2 6 . 8 \\%$ and $2 3 . 4 \\%$ , respectively). The interviews that we conducted in the larger project can help explain these discrepancies, which will be addressed in the Discussion section.\n\n# 5. Discussion\n\nIn this section, we first interpret our findings in this study in terms of the types of ChatGPT-supported teacher feedback and stu dents\u2019 incorporation of the feedback. Then, we highlight the effectiveness of this innovative feedback approach based on the multi faceted data that we collected in the larger project. First, the total quantity of corrective feedback decreased from 1176 to 665 instances across the two tasks, possibly due to a general decrease in number of errors made by the students in Task 2. Second, the strategies of the teachers for ChatGPT-supported feedback may partially explain the findings of the feedback on different error types. For example, two instructors mentioned their use of deletion and condensing techniques to provide feedback (e.g., recurring errors were condensed into one) in the post-task interview. As Instructor Wei stated, \u201cfor recurring spelling mistakes in a text, it\u2019s not necessary to point out each one; condensing them into a single piece of feedback for the student will suffice\u201d. Third, by interviewing the students, we learned that they tended to pay more attention to the drafts in Task 2; for example, Jia said, \u201cWhen I was writing for the first time, I didn\u2019t pay much attention to the need for revisions. When I realized that I had to make revisions, I became more careful during my second attempt at writing\u201d, and Wang stated that \u201cif I have to correct every mistake, then I need to be more careful with my first draft to reduce the number of errors\u201d. These excerpts well explain the fewer instances of errors addressed in the ChatGPT-supported teacher feedback. Interestingly, the number of spelling errors significantly increased in Task 2. According to the classroom observations, the teachers asked the students to bring laptops for in-class writing prior to the implementation of Task 1. Most students used laptops with Microsoft\n\nTable 7 Overall statistics for students\u2019 revisions based on Ferris\u2019 15 error categories.   \n\n<html><body><table><tr><td>Rating</td><td colspan=\"2\">Task 1</td><td colspan=\"2\">Task 2</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>No change</td><td>277</td><td>23.55</td><td>135</td><td>20.3</td></tr><tr><td>Unsuccessful revision</td><td>140</td><td>11.9</td><td>105</td><td>15.79</td></tr><tr><td>Successful revision</td><td>759</td><td>64.54</td><td>425</td><td>63.91</td></tr><tr><td>Total</td><td>1176</td><td></td><td>665</td><td></td></tr></table></body></html>\n\nTable 8 Overall statistics for students\u2019 revisions based on rhetorical feedback.   \n\n<html><body><table><tr><td>Rating</td><td colspan=\"2\">Task 1</td><td colspan=\"2\">Task 2</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>No change</td><td>62</td><td>65.26</td><td>34</td><td>35.79</td></tr><tr><td> Partial modification</td><td>24</td><td>25.26</td><td>7.</td><td>7.37</td></tr><tr><td>Comprehensive modification</td><td>9</td><td>9.47</td><td>54</td><td>56.84</td></tr><tr><td>Total</td><td>95</td><td></td><td>95</td><td></td></tr></table></body></html>\n\nTable 9 Results of the Chi-square test for the three ratings in Tasks 1 and 2.   \n\n<html><body><table><tr><td></td><td></td><td>x?</td><td>df</td><td>N</td><td>p</td></tr><tr><td>Students&#x27; revisions based on Ferris&#x27; 15 error categories</td><td>Task1</td><td>539.332</td><td>2 </td><td>1176</td><td>&lt;0.001</td></tr><tr><td></td><td>Task2</td><td>281.805</td><td>2</td><td>665</td><td>&lt;0.001</td></tr><tr><td>Students revisions based on rhetorical feedback.</td><td>Task1</td><td>47.137</td><td>2</td><td>95</td><td>&lt;0.001</td></tr><tr><td></td><td>Task2</td><td>35.137</td><td>2</td><td>95</td><td>&lt;0.001</td></tr></table></body></html>\n\nNote: $p < 0 . 0 0 1$ indicates that the p values are less than 0.001.\n\nTable 10 Revisions for different error categories in Task 1.   \n\n<html><body><table><tr><td></td><td>Error type</td><td> no change</td><td>%</td><td> unsuccessful revision</td><td>%</td><td>successful revision</td><td>%</td><td>No.</td></tr><tr><td>1</td><td>Sentence structure</td><td>15</td><td>9.0%</td><td>24</td><td>14.4%</td><td>128</td><td>76.6%</td><td>167</td></tr><tr><td>2</td><td>Word choice</td><td>16</td><td>7.3%</td><td>23</td><td>10.5%</td><td>181</td><td>82.3%</td><td>220</td></tr><tr><td>3</td><td>Verb tense</td><td>20</td><td>19.0%</td><td>16</td><td>15.2%</td><td>69</td><td>65.7%</td><td>105</td></tr><tr><td>4</td><td> Noun endings (singular/plural)</td><td>13</td><td>23.6%</td><td>7</td><td>12.7%</td><td>35</td><td>63.6%</td><td>55</td></tr><tr><td>5</td><td>Verb form</td><td>12</td><td>16.2%</td><td>11</td><td>14.9%</td><td>51</td><td>68.9%</td><td>74</td></tr><tr><td>6</td><td>Punctuation</td><td>87</td><td>68.0%</td><td>13</td><td>10.2%</td><td>28</td><td>21.9%</td><td>128</td></tr><tr><td>7</td><td> Articles/determiners</td><td>26</td><td>28.3%</td><td>9</td><td>9.8%</td><td>57</td><td>62.0%</td><td>92.</td></tr><tr><td>8</td><td>Word form</td><td>25</td><td>21.4%</td><td>6</td><td>5.1%</td><td>86</td><td>73.5%</td><td>117</td></tr><tr><td>9</td><td>Spelling</td><td>9</td><td>19.6%</td><td>2</td><td>4.3%</td><td>35</td><td>76.1%</td><td>46</td></tr><tr><td>10</td><td>Run-ons</td><td>14</td><td>43.8%</td><td>5</td><td>15.6%</td><td>13</td><td>40.6%</td><td>32</td></tr><tr><td>11</td><td>Pronouns</td><td>7</td><td>21.9%</td><td>4</td><td>12.5%</td><td>21</td><td>65.6%</td><td>32</td></tr><tr><td>12</td><td>Subject-verb agreement</td><td>17</td><td>38.6%</td><td>6</td><td>13.6%</td><td>21</td><td>47.7%</td><td>44</td></tr><tr><td>13</td><td>Fragments</td><td>3</td><td>42.9%</td><td>3</td><td>42.9%</td><td>1</td><td>14.3%</td><td>7</td></tr><tr><td>14</td><td>Idiom</td><td>5</td><td>26.3%</td><td>6</td><td>31.6%</td><td>8</td><td>42.1%</td><td>19</td></tr><tr><td>15</td><td>Informal</td><td>8</td><td>21.1%</td><td>5</td><td>13.2%</td><td>25</td><td>65.8%</td><td>38</td></tr></table></body></html>\n\nTable 11 Revisions for different error categories in Task 2.   \n\n<html><body><table><tr><td></td><td>Error type</td><td> no change</td><td>%</td><td>unsuccessful revision</td><td>%</td><td>successful revision</td><td>%</td><td>No.</td></tr><tr><td>1</td><td>Sentence structure</td><td>19</td><td>26.8%</td><td>13</td><td>18.3%</td><td>39</td><td>54.9%</td><td>71</td></tr><tr><td>2</td><td>Word choice</td><td>34</td><td>23.4%</td><td>27</td><td>18.6%</td><td>84</td><td>58.0%</td><td>145</td></tr><tr><td>3</td><td>Verb tense</td><td>7</td><td>18.4%</td><td>9</td><td>23.7%</td><td>22</td><td>57.9%</td><td>38</td></tr><tr><td>4</td><td> Noun endings (singular/plural)</td><td>2</td><td>5.0%</td><td>4</td><td>10.0%</td><td>34</td><td>85.0%</td><td>40</td></tr><tr><td>5</td><td>Verb form</td><td>8</td><td>22.9%</td><td>2</td><td>5.7%</td><td>25</td><td>71.4%</td><td>35</td></tr><tr><td>6</td><td>Punctuation</td><td>13</td><td>34.2%</td><td>9</td><td>23.7%</td><td>16</td><td>42.1%</td><td>38</td></tr><tr><td>7</td><td> Articles/determiners</td><td>8.</td><td>21.1%</td><td>4.</td><td>10.5%</td><td>26</td><td>68.4%</td><td>38</td></tr><tr><td>8</td><td>Word form</td><td>11</td><td>20.4%</td><td>10</td><td>18.5%</td><td>33</td><td>61.1%</td><td>54</td></tr><tr><td>9</td><td>Spelling</td><td>14</td><td>19.4%</td><td>3</td><td>4.2%</td><td>55</td><td>76.4%</td><td>72</td></tr><tr><td>10</td><td>Run-ons</td><td>7</td><td>17.1%</td><td>7</td><td>17.1%</td><td>27</td><td>65.9%</td><td>41</td></tr><tr><td>11</td><td>Pronouns</td><td>1</td><td>12.5%</td><td>3</td><td>37.5%</td><td>4</td><td>50.0%</td><td>8</td></tr><tr><td>12</td><td>Subject-verb agreement</td><td>2</td><td>6.3%</td><td>4</td><td>12.5%</td><td>26</td><td>81.3%</td><td>32</td></tr><tr><td>13</td><td>Fragments</td><td>6</td><td>20.7%</td><td>9</td><td>31.0%</td><td>14</td><td>48.3%</td><td>29</td></tr><tr><td>14</td><td>Idiom</td><td>0</td><td>0%</td><td>0</td><td>0%</td><td>4</td><td>100%</td><td>4</td></tr><tr><td>15</td><td>Informal</td><td>3</td><td>15.0%</td><td>1</td><td>5.0%</td><td>16</td><td>80.0%</td><td>20</td></tr></table></body></html>\n\nWord, which features an automatic correction function that reduces spelling errors. In the second writing assignment, teachers did not remind them to bring laptops, and most students used tablets or smartphones for writing instead. The lack of an automatic correction function in the portable devices increased the number of spelling errors, and more errors were addressed in the ChatGPT-supported teacher feedback.\n\nMoreover, different genres regarding the two writing tasks (i.e., argumentative and expository essays) might account for the differences that we identified across the tasks. Argumentative essays require students to develop a position on a particular issue and support it with evidence and reasoning (Chuang & Yan, 2022; Taylor et al., 2019), whereas expository essays aim to clearly and concisely explain a topic or provide information (Jeong, 2017; Zhang & Li, 2021). In other words, the complex structure of argumentative essays requires more in-depth feedback on content organization and the quality of the arguments. Since expository essays typically focus on explaining concepts or providing information, the ChatGPT-supported teacher feedback emphasized that the in formation was clear, logically organized, and effectively communicated. For example, word choice and sentence structure errors were prominent in both genres, but argumentative essays presented more issues related to argumentation and logical flow, whereas expository essays presented more issues with clarity and organization.\n\nThe finding that most errors were successfully amended suggests that the ChatGPT-supported teacher feedback helps students be more aware of language errors and effectively scaffolds them in writing revisions. Moreover, the students had more complete mod ifications to address the rhetorical feedback in Task 2. This change suggests the active engagement of the students with the feedback content, which led to successful attempts at revision. The students reported the clearness and helpfulness of the ChatGPT-supported teacher feedback compared with the traditional teacher feedback that they received in the post-task interviews, as shown in the following excerpts.\n\nExcerpt 1: \u201cPreviously, teachers would just grade my essay, and I had no idea what specifically needed to be revised or where exactly I went wrong. However, the feedback from both ChatGPT and the teacher can clearly tell me where the mistakes are and which parts need improvement\u201d (post-task interview with Student 1).\n\nExcerpt 2: \u201cUsually, teachers would just mark the errors, for example, by underlining them or circling them with a red pen. Sometimes, I couldn\u2019t quite understand what exactly was wrong, whether it was a word choice error or a problem with my sentence structure, so it was difficult to make corrections. This method, where the teacher clearly tells me the type of error, makes it easier for me to revise\u201d (post-task interview with Student 2).\n\nAgainst the fact that EFL students received unclear teacher feedback on errors occurring in their writing, as reported in Zheng and Yu (2018) and Cheng et al. (2021), the students in this study received very clear and comprehensive error feedback, which they largely incorporated into their revisions. Moreover, the holistic rhetorical feedback generated by ChatGPT greatly helped instructors when they commented on the global issues. As one instructor stated,\n\nI believe that providing holistic rhetorical feedback on the content is very important for students, but it is too time-consuming. Previously, I would just give a grade, and if time allowed, I would write a few sentences to tell the students where they could improve. I did not have time to praise the students\u2019 work or engage in a dialogue with them. However, the content generated by ChatGPT can help me solve this problem. By adding and deleting, I can quickly provide high-quality rhetorical feedback to the students.\n\nThus, our study clearly indicates that ChatGPT-supported teacher feedback can ameliorate the situation where EFL teachers barely effectively address global issues of writing (Barrot, 2023a, 2023b; Mizumoto & Eguchi, 2023; Steiss et al., 2024).\n\nInterestingly, our findings reveal that most errors related to the sentence structure and word choice, which were considered untreatable (Ferris & Roberts, 2001), were successfully revised in both writing tasks. This discrepancy echoes Han\u2019s (2024) finding that computer-mediated coded WCF facilitates the successful revision of regularly considered untreatable errors such as word choice errors. These findings may indicate that EFL learners, who are now immersed in computer-mediated learning environments, can readily access online resources such as corpora and AI tools to transform previously untreatable errors into treatable ones (Mueller & Jacobsen, 2016; Ngo & Chen, 2024; Sataka, 2020). In other words, technological advancements such as AI have made certain error types more amendable than they used to be.\n\n# 6. Conclusion\n\nThis study comprehensively examined the use of ChatGPT-supported teacher feedback in a Chinese tertiary EFL setting. The findings underscore the utility of ChatGPT in augmenting the traditional feedback process through the implementation of AI-teacher collaboration. The common understanding is that the performance of ChatGPT cannot currently compete with trained teachers (Steiss et al., 2024) and it can be difficult for all students to receive detailed constructive writing feedback from trained teachers in the EFL contexts. Thus, the collaboration of teachers and AI has emerged as a feasible solution to effective writing pedagogy, as clearly shown in this study.\n\nA notable reduction in error rates across successive writing tasks suggests that ChatGPT-supported teacher feedback positively contributes to student writing skills, and the students could better address specific error types and revise the overall quality of their papers (e.g., adding examples to support one\u2019s argument). Additionally, the study confirms that the integration of AI-powered feedback can effectively supplement human instruction and facilitate a more nuanced and efficient feedback process. The findings of our study resonate with recent studies utilizing AI-powered feedback (Han & Sari, 2024; Kao & Reynolds, 2024; Shi & Aryadoust, 2024). For example, feedback based on AI technologies such as \u201cCriterion\u201d combined with teacher feedback was more effective than teacher feedback alone in reducing errors in grammar and mechanics (Han & Sari, 2024). Additionally, Kao and Reynolds (2024)\n\napplied AWE to provide feedback on students\u2019 academic writings, and the results revealed that students highly trusted AI-powered feedback concerning grammar rules and lexical choices.\n\nThe present study suggests the need for a hybrid model of instruction that synergizes the analytical capabilities of AI with the pedagogical expertise of educators in the future. In response to the broad debate on the use of AI in language education, our position is that instead of eliminating the use of AI, we can make full use of AI with instructors\u2019 agency. The \u201cAI + Teacher\u201d model can reduce the workload of teachers and enrich the educational landscape by offering a more adaptive and individualized learning experience. This study points to several targeted areas for future investigations. First, we implemented ChatGPT-supported teacher feedback in the Chinese EFL context in this study; future research can examine this new feedback approach in other instructional settings. Second, we focused on how students incorporated the ChatGPT-supported teacher feedback into their revisions but did not examine their per ceptions of this innovative feedback approach. Future studies can closely examine the attitudes of students toward and their per spectives on ChatGPT-supported teacher feedback in comparison to traditional teacher feedback. Third, in another study, we explored how instructors adapted ChatGPT outputs for teacher feedback and their positive perceptions of ChatGPT-supported teacher feedback. The perspectives of teachers, who are important stakeholders in this new feedback approach, deserve continued exploration. Fourth, the impacts of ChatGPT-supported feedback on specific writing outcomes and long-term writing performance of EFL/ESL students warrant closer examination.\n\nIn conclusion, our study reveals that ChatGPT can help teachers provide writing feedback, reduce their workload and offer im mediate personalized responses. In our study, ChatGPT-supported teacher feedback well addressed both global issues and local issues in the students\u2019 writing, and the students incorporated the majority of the feedback into their writing revisions with increased per centages of incorporation across tasks. ChatGPT-supported teacher feedback, as reflected in the \u201cAI $^ +$ Teacher\u201d model, introduces innovation to the L2 writing pedagogy and deserves more inquiries in the coming years.\n\n# Funding\n\nThis work was supported by Southwest University [grant number SWUPilotPlan002].\n\n# Conflict of interest statement\n\nWe declare that we have no conflicts of interest.\n\n# CRediT authorship contribution statement\n\nJining Han: Writing \u2013 review & editing, Writing \u2013 original draft, Resources, Methodology, Formal analysis, Data curation, Conceptualization. Mimi Li: Writing \u2013 review & editing, Validation, Supervision, Methodology, Conceptualization.\n\n# References\n\nAljasir, N. (2021). Matches or mismatches? Exploring shifts in individuals\u2019 beliefs about written corrective feedback as students and teachers-to-be. Journal of Teaching and Teacher Education, 9(1), 1\u201310. https://doi.org/10.12785/jtte/090101   \nBarrot, J. S. (2023a). Using automated written corrective feedback in the writing classrooms: Effects on L2 writing accuracy. Computer Assisted Language Learning, 36 (4), 584\u2013607. https://doi.org/10.1080/09588221.2021.1936071   \nBarrot, J. S. (2023b). Using ChatGPT for second language writing: Pitfalls and potentials. Assessing Writing, 57, Article 100745. https://doi.org/10.1016/j. asw.2023.100745   \nBitchener, J., Young, S., & Cameron, D. (2005). The effect of different types of corrective feedback on ESL student writing. Journal of Second Language Writing, 14(3), 191\u2013205. https://doi.org/10.1016/j.jslw.2005.08.001   \nBruner, J. (1985). Models of the learner. Educational Researcher, 14(6), 5\u20138. https://doi.org/10.3102/0013189X014006005   \nCaldarini, G., Jaf, S., & McGarry, K. (2022). A literature survey of recent advances in chatbots. Information, 13(1), 41. https://doi.org/10.3390/info13010041   \nCarter, T., & Thirakunkovit, S. (2019). A comparison of L1 and ESL written feedback Preferences: Pedagogical applications and theoretical implications. Journal of Response to Writing, 5(2), 139\u2013174. https://scholarsarchive.byu.edu/journalrw/vol5/iss2/7.   \nCheng, X., Zhang, L., & Yan, Q. (2021). Exploring teacher written feedback in EFL writing classrooms: Beliefs and practices in interaction. In Language teaching research. Advance online publication. https://doi.org/10.1177/13621688211057665.   \nChuang, P. L., & Yan, X. (2022). An investigation of the relationship between argument structure and essay quality in assessed writing. Journal of Second Language Writing, 56, Article 100892. https://doi.org/10.1016/j.jslw.2022.100892   \nDwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., \u2026 Wright, R. (2023). \u201cSo what if ChatGPT wrote it?\u201d Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71, Article 102642. https://doi.org/10.36227/techrxiv.21789434.v1   \nEssel, H. B., Vlachopoulos, D., Essuman, A. B., & Amankwa, J. O. (2024). CHATGPT effects on cognitive skills of undergraduate students: Receiving instant responses from ai-based conversational large language models (llms). Computers and Education: Artificial Intelligence, 6, Article 100198. https://doi.org/10.1016/j. caeai.2023.100198   \nFerris, D. R. (1997). The influence of teacher commentary on student revision. Tesol Quarterly, 31(2), 315\u2013339. https://doi.org/10.2307/3588049   \nFerris, D. (2006). Does error feedback help student writers? New evidence on the short- and long-term effects of written error correction. In K. Hyland, & F. Hyland (Eds.), Feedback in second language writing: Contexts and issues (pp. 81\u2013104). Cambridge, UK: Cambridge University Press.   \nFerris, D. R. (2010). Second language writing research and written corrective feedback in SLA: Intersections and practical applications. Studies in Second Language Acquisition, 32(2), 181\u2013201. https://doi.org/10.1017/s0272263109990490   \nFerris, D. R., Liu, H., Sinha, A., & Senna, M. (2013). Written corrective feedback for individual L2 writers. Journal of Second Language Writing, 22(3), 307\u2013329. https:// doi.org/10.1016/j.jslw.2012.09.009   \nFerris, D., & Roberts, B. (2001). Error feedback in L2 writing classes: How explicit does it need to be? Journal of Second Language Writing, 10(3), 161\u2013184. https://doi. org/10.1016/s1060-3743(01)00039-x   \nGrimes, D., & Warschauer, M. (2010). Utility in a fallible tool: A multi-site case study of automated writing evaluation. The Journal of Technology, Learning, and Assessment, 8(6), 4\u201343. https://ejournals.bc.edu/index.php/jtla/article/view/1625/1469.   \nGuo, K., & Wang, D. (2023). To resist it or to embrace it? Examining ChatGPT\u2019s potential to support teacher feedback in EFL writing. Education and Information Technologies, 29(7), 8435\u20138463. https://doi.org/10.1007/s10639-023-12146-0   \nHan, J. (2024). Students\u2019 responses to computer-mediated coded feedback and the factors influencing those students\u2019 responses: A multiple-case study in the cfl setting. Language, Learning and Technology, 28(1), 1\u201323. https://hdl.handle.net/10125/73547.   \nHan, T., & Sari, E. (2022). An investigation on the use of automated feedback in Turkish EFL students\u2019 writing classes. Computer Assisted Language Learning, 37(4), 961\u2013985. https://doi.org/10.1080/09588221.2022.2067179   \nHerbold, S., Hautli-Janisz, A., Heuer, U., Kikteva, Z., & Trautsch, A. (2023). A large-scale comparison of human-written versus ChatGPT-generated essays. Scientific Reports, 13(1), Article 18617. https://doi.org/10.1038/s41598-023-45644-9   \nJeong, H. (2017). Narrative and expository genre effects on students, raters, and performance criteria. Assessing Writing, 31, 113\u2013125. https://doi.org/10.1016/j. asw.2016.08.006   \nKao, C. W., & Reynolds, B. L. (2024). Timed Second Language writing performance: Effects of perceived teacher vs perceived automated feedback. Humanities and Social Sciences Communications, 11, 1012. https://doi.org/10.1057/s41599-024-03522-3   \nKohnke, L., Moorhouse, B., & Zou, D. (2023). ChatGPT for language teaching and learning. RELC Journal, 54(2), 537\u2013550. https://doi.org/10.1177/ 0033688223116286   \nMizumoto, A., & Eguchi, M. (2023). Exploring the potential of using an AI language model for automated essay scoring. Research Methods in Applied Linguistics, 2(2), Article 100050. https://doi.org/10.1016/j.rmal.2023.100050   \nMueller, C. M., & Jacobsen, N. D. (2016). A comparison of the effectiveness of EFL students\u2019 use of dictionaries and an online corpus for the enhancement of revision skills. ReCALL, 28(1), 3\u201321. https://doi.org/10.1017/S0958344015000142   \nNgo, T., & Chen, H. (2024). The effectiveness of corpus use in ESL/EFL writing: A meta-analysis. In Language teaching research. Advance online publication. https:// doi.org/10.1177/13621688241260183.   \nRanalli, J. (2018). Automated written corrective feedback: How well can students make use of it? Computer Assisted Language Learning, 31(7), 653\u2013674. https://doi. org/10.1080/09588221.2018.1428994   \nRanalli, J. (2021). L2 student engagement with automated feedback on writing: Potential for learning and issues of trust. Journal of Second Language Writing, 52, Article 100816. https://doi.org/10.1016/j.jslw.2021.100816   \nSataka, Y. (2020). How error types affect the accuracy of L2 error correction with corpus use. Journal of Second Language Writing, 50, Article 100757. https://doi.org/ 10.1016/j.jslw.2020.100757   \nShi, H., & Aryadoust, V. (2024). A systematic review of AI-based automated written feedback research. ReCALL, 36(2), 187\u2013209. https://doi.org/10.1017/ s0958344023000265   \nSteiss, J., Tate, T., Graham, S., Cruz, J., Hebert, M., Wang, J., Moon, Y., Tseng, W., Warschauer, M., & Olson, C. B. (2024). Comparing the quality of human and ChatGPT feedback of students\u2019 writing. Learning and Instruction, 91, Article 101894. https://doi.org/10.1016/j.learninstruc.2024.101894   \nStevenson, M., & Phakiti, A. (2014). The effects of computer-generated feedback on the quality of writing. Assessing Writing, 19, 51\u201365. https://doi.org/10.1016/j. asw.2013.11.007   \nTaylor, K. S., Lawrence, J. F., Connor, C. M., & Snow, C. E. (2019). Cognitive and linguistic features of adolescent argumentative writing: Do connectives signal more complex reasoning? Reading and Writing, 32, 983\u20131007. https://doi.org/10.1007/s11145-018-9898-6   \nVygotsky, L. S. (1978). Mind in society: Interaction between learning and development. Cambridge. MA: Harvard University Press.   \nWilson, J., & Czik, A. (2016). Automated essay evaluation software in English Language Arts classrooms: Effects on teacher feedback, student motivation, and writing quality. Computers & Education, 100, 94\u2013109. https://doi.org/10.1016/j.compedu.2016.05.004   \nWilson, J., Olinghouse, N. G., & Andrada, G. N. (2014). Does automated feedback improve writing quality? Learning Disabilities: A Contemporary Journal, 12(1), 93\u2013118. https://files.eric.ed.gov/fulltext/EJ1039856.pdf.   \nWilson, J., & Roscoe, R. D. (2020). Automated writing evaluation and feedback: Multiple metrics of efficacy. Journal of Educational Computing Research, 58(1), 87\u2013125. https://doi.org/10.1177/0735633119830764   \nYan, D. (2023). Impact of ChatGPT on learners in a L2 writing practicum: An exploratory investigation. Education and Information Technologies, 28(11), 13943\u201313967. https://doi.org/10.1007/s10639-023-11742-4   \nYang, M., Badger, R., & Yu, Z. (2006). A comparative study of peer and teacher feedback in a Chinese EFL writing class. Journal of Second Language Writing, 15(3), 179\u2013200. https://doi.org/10.1016/j.jslw.2006.09.004   \nZhang, X., & Li, W. (2021). Effects of n-grams on the rated L2 writing quality of expository essays: A conceptual replication and extension. System, 97, Article 102437. https://doi.org/10.1016/j.system.2020.102437   \nZheng, Y., & Yu, S. (2018). Student engagement with teacher written corrective feedback in EFL writing: A case study of Chinese lower-proficiency students. Assessing Writing, 37, 13\u201324. https://doi.org/10.1016/j.asw.2018.03.001", "metadata": {"authors": ["Jining Han", "Mimi Li"], "category": "research", "confidence_score": 0.8, "document_type": "journal", "has_abstract": true, "has_methodology": true, "has_results": true, "key_findings": ["The ChatGPT-supported teacher feedback addressed diverse error categories and included helpful comments on the overall rhetoric", "Students incorporated more of the feedback into their revisions across tasks, reflecting deeper engagement with the feedback content"], "methodology": "mixed", "publication_year": 2024, "research_questions": ["How can ChatGPT be integrated into teacher writing feedback to address the challenges of teacher feedback in a large class?"], "source_file": "out_CHRX3SLR_Exploring_ChatGPT-supported_tea.md", "subject_area": "education", "tags": ["ChatGPT", "teacher feedback", "writing feedback", "AI", "language education"], "title": "Exploring ChatGPT-supported teacher feedback in the EFL context"}, "search_text": "Exploring ChatGPT-supported teacher feedback in the EFL context ChatGPT teacher feedback writing feedback AI language education education # Exploring ChatGPT-supported teacher feedback in the EFL context\n\nJining Han a,\\* , Mimi Li b\n\na Faculty of Education, Southwest University, No.2 Tiansheng Road, 400715, Chongqing, China   \nb Department of Literature and Languages, Texas A&M University-Commerce, David Talbot Hall of Languages 141, 2200 Campbell St, Commerce,   \nTX, 75428, USA\n\n# A R T I C L E I N F O\n\n# A B S T R A C T\n\nKeywords:   \nChatGPT   \nTeacher feedback   \nWriting feedback   \nAI   \nLanguage education\n\nThis study investigates the ChatGPT-supported teacher feedback in the Chinese tertiary EFL context and explores an innovative AI-aided writing pedagogy by integrating ChatGPT into teacher writing feedback provisions to alleviate the challenges of teacher feedback in a large class, which were reported in previous research. The participants of this study were four in structors and 102 students from two undergraduate classes in the world language education program. The students completed two writing tasks: an argumentative essay and an expository essay; then, the instructors provided detailed feedback on their essays based on the ChatGPT feedback. Two prompts were provided to ChatGPT after the training: 1) corrective feedback drawing on Ferris\u2019s (2006)15 types of common errors and 2) holistic rhetorical feedback. Afterwards, the teachers adapted the ChatGPT feedback and shared the detailed individualized writing feedback with each student. We closely examined the types and features of ChatGPTsupported teacher feedback and how EFL students incorporate this feedback into their writing revisions. The findings indicate that the ChatGPT-supported teacher feedback addressed diverse error categories and included helpful comments on the overall rhetoric. Moreover, the students incorporated more of the feedback into their revisions across tasks, which reflects their deeper engagement with the feedback content. This study notes the importance of an \u201cAI $^ +$ Teacher\u201d model that leverages the analytical stre"}, "title": "Exploring ChatGPT-supported teacher feedback in the EFL context", "authors": ["Jining Han", "Mimi Li"], "tags": ["ChatGPT", "teacher feedback", "writing feedback", "AI", "language education"], "filename": "out_CHRX3SLR_Exploring_ChatGPT-supported_tea.md"}, {"score": 0.2691657841205597, "document": {"id": 100, "file_path": "annotated_data/batch_002/out_MBN2X4HV_Exploring_ChatGPT-supported_tea.md", "filename": "out_MBN2X4HV_Exploring_ChatGPT-supported_tea.md", "content": "# Exploring ChatGPT-supported teacher feedback in the EFL context\n\nJining Han a,\\* , Mimi Li b\n\na Faculty of Education, Southwest University, No.2 Tiansheng Road, 400715, Chongqing, China   \nb Department of Literature and Languages, Texas A&M University-Commerce, David Talbot Hall of Languages 141, 2200 Campbell St, Commerce,   \nTX, 75428, USA\n\n# A R T I C L E I N F O\n\n# A B S T R A C T\n\nKeywords:   \nChatGPT   \nTeacher feedback   \nWriting feedback   \nAI   \nLanguage education\n\nThis study investigates the ChatGPT-supported teacher feedback in the Chinese tertiary EFL context and explores an innovative AI-aided writing pedagogy by integrating ChatGPT into teacher writing feedback provisions to alleviate the challenges of teacher feedback in a large class, which were reported in previous research. The participants of this study were four in structors and 102 students from two undergraduate classes in the world language education program. The students completed two writing tasks: an argumentative essay and an expository essay; then, the instructors provided detailed feedback on their essays based on the ChatGPT feedback. Two prompts were provided to ChatGPT after the training: 1) corrective feedback drawing on Ferris\u2019s (2006)15 types of common errors and 2) holistic rhetorical feedback. Afterwards, the teachers adapted the ChatGPT feedback and shared the detailed individualized writing feedback with each student. We closely examined the types and features of ChatGPTsupported teacher feedback and how EFL students incorporate this feedback into their writing revisions. The findings indicate that the ChatGPT-supported teacher feedback addressed diverse error categories and included helpful comments on the overall rhetoric. Moreover, the students incorporated more of the feedback into their revisions across tasks, which reflects their deeper engagement with the feedback content. This study notes the importance of an \u201cAI $^ +$ Teacher\u201d model that leverages the analytical strengths of AI while maintaining essential teacher\u2012student interactions. This new approach of ChatGPT-supported teacher feedback has great potential in L2 writing feedback provision and will shed novel light on the writing pedagogy with the aid of AIin the digital era.\n\n# 1. Introduction\n\nThe advent of artificial intelligence (AI) in educational settings has significantly transformed the landscape of language learning and teaching, especially in the domain of writing instruction. With the integration of AI-mediated tools such as automated writing evaluation (AWE) and ChatGPT, educators have harnessed the capabilities of natural language processing (NLP) to provide immediate and personalized feedback to learners to enhance their writing proficiency. Compared with AWE tools, ChatGPT feedback can address not only grammatical accuracy but also can help develop students\u2019 critical thinking and analytical skills in writing (Dwivedi et al., 2023; Essel et al., 2024; Herbold et al., 2023). The adaptability of ChatGPT makes it an impactful resource to foster interactive language education. In Chinese tertiary educational settings, professors in research institutions have direct access to ChatGPT with the adherence to relevant regulations regarding the use of AI, whereas students do not. Therefore, the integration of ChatGPT into teacher writing feedback provisions can simultaneously help alleviate the challenge of providing teacher feedback for a large class and utilize the affordances of AI. Specifically, in Chinese universities, an EFL instructor often teaches several large classes, each of which has up to 100 students (Yang et al., 2006), which makes it almost impossible to provide detailed writing feedback for each student and impact the writing feedback on the language and writing development of the students. Thus, the current study explores an innovative feedback approach, called ChatGPT-supported teacher feedback, in the Chinese tertiary EFL context. ChatGPT was first trained on how to provide detailed feedback on students\u2019 essays; then, the teachers adapted ChatGPT feedback before sharing the writing feedback with the students (see the detailed explanation in the Methods section). This study specifically examines how ChatGPT can serve as a companion for teachers to provide tailored and constructive feedback and how Chinese EFL students incorporate ChatGPT-supported teacher feedback into their revisions. This work contributes to the ongoing discourse on optimizing AI tools for L2 writing pedagogy in the digital era.\n\n# 2. Literature review\n\nThis study is undergirded by sociocultural theory and informed by previous research on AI-mediated writing feedback.\n\n# 2.1. Theoretical framework\n\nMediation (Vygotsky, 1978), which is the central concept of sociocultural theory, refers to the roles of tools and artifacts in facilitating human activities. In ChatGPT-supported teacher feedback, ChatGPT acts as a mediator between teacher and feedback process. It can provide immediate and effective responses to students\u2019 writing based on prompts that the teacher inputs. The teacher engages with the ChatGPT output and tailors it to the students\u2019 needs. Moreover, the construct of scaffolding (Bruner, 1985) supports the innovative approach of ChatGPT-supported teacher feedback. ChatGPT can scaffold the teacher in effectively performing feedback tasks that would have been difficult to independently accomplish. Meanwhile, teachers can bridge the gap between current abilities and potential developmental level of the students through appropriate guidance and scaffolding, as reflected by the incorporation of ChatGPT-supported teacher feedback into the writing revisions of the students. This study also uses the Zone of Proximal Development (ZPD) to illustrate how ChatGPT-supported teacher feedback can bridge the gap between current abilities and potential growth of students through appropriate guidance and collaborative support. The potential development level of a student largely depends on collaborative efforts with others, particularly the feedback from teachers. In short, the constructs of mediation and scaffolding inform our research design. The mediation of ChatGPT facilitates the provision of writing feedback for EFL teachers, including both corrective feedback and rhetorical feedback. Additionally, ChatGPT-supported teacher feedback scaffolds the writing revisions of students.\n\n# 2.2. Automated writing evaluation (AWE)\n\nAIfeedback constitutes part of ChatGPT-supported teacher writing feedback, where teachers provide detailed writing feedback based on ChatGPT output in response to appropriate prompts. In this section, we synthesize representative studies that use AWE in L2 writing classes. AWE systems, which integrate NLP technology, latent semantic analysis, and AI, can offer learners immediate auto mated feedback, which helps enhance their writing competence (Grimes & Warschauer, 2010; Wilson & Roscoe, 2020). For example, Wilson et al. (2014) used the AWE tool \u201cCriterion\u201d to provide feedback for essays written by 4162 students in grades 4 to 8, which improved their essay quality. Moreover, by automating the analysis of student writing texts and providing feedback, AWE systems significantly reduce the time and effort that teachers need to invest in offering personalized writing feedback (Han & Sari, 2022). Wilson and Czik (2016) revealed that compared to feedback solely from teachers, the AWE system \u201cPEG Writing\u201d reduced the amount of time that teachers spent providing feedback by 2\u20133 times, and the quantity of AWE-based teacher feedback remained the same as that provided by only the teacher. AWE lessened the grading burden of instructors while providing them more time to provide feedback on the global issues of writing (Barrot, 2023a, 2023b; Wilson & Czik, 2016).Nevertheless, by directly pointing out errors and providing suggestions for modification, AWE systems may limit the capacity of students to transfer writing skills across writing tasks. Stevenson and Phakiti (2014) reported that although AWE might reduce errors in specific writing tasks, students could not demon strate learning transfer across various tasks. Ranalli\u2019s (2021) study examined the engagement of six L2 students using \u201cGrammarly\u201d based on screen captures, interviews, and stimulated recalls and revealed that the participants primarily used feedback for editing instead of aiming for long-term learning. These findings may be attributed to the directly provision from \u201cGrammarly\u201d of highly specific corrections, which made the students make local changes without substantial cognitive engagement. Due to algorithmic constraints, the feedback provided by AWE tended to be formulaic and lacked consideration of specific pedagogical needs (Ranalli, 2018).\n\n# 2.3. Utilizing ChatGPT for writing feedback\n\nCompared with traditional AWE systems, ChatGPT exhibits advantages in writing feedback provision due to its human-like conversational abilities and capacity for knowledge expression. ChatGPT can deliver personalized and comprehensive feedback on student writing (Caldarini, Jaf, & McGarry, 2022). Barrot, 2023a, 2023b expounded on the potential benefits of ChatGPT in L2 writing and highlighted its capacity as a rich information source for language learning and its ability to clarify themes, construct structures, and provide real-time adaptive feedback. The study also highlighted the limitations of ChatGPT, such as the possibility of generating inaccurate or unintelligible responses and its limited ability to tailor content for specific target audiences. Barrot, 2023a, 2023b emphasized the importance of clear instructional guidelines and targeted training for teachers to effectively use ChatGPT in writing instruction. Yan (2023) conducted an exploratory qualitative study on the role of ChatGPT in a university English writing course. The students noted the benefits of using ChatGPT, such as its significant potential for supporting language learning, providing timely feedback, enhancing learning motivation, and improving the writing quality. Moreover, Kohnke et al. (2023) emphasized the trans formative potential of ChatGPT in language education and its ability to promote engaging and adaptive language learning. ChatGPT facilitates the feedback process by creating customized prompts and responses that address student writings according to the teacher\u2019s guidelines. The immediate ChatGPT feedback helps students understand and incorporate corrections and suggestions into their re visions, which enhances their learning process.\n\nComparing teacher feedback and ChatGPT feedback, Steiss et al. (2024) reported that well-trained teachers deliver higher-quality feedback than ChatGPT; however, considering the convenience and overall quality of ChatGPT-generated feedback, ChatGPT feedback is valuable when well-trained teachers are unavailable or when there is a need to evaluate many writing drafts. This research study highlights the potential of using ChatGPT to provide writing feedback and implies a prospectus of teacher-AI collaboration.\n\nIn another study, Guo and Wang (2023) reported that ChatGPT generated more feedback than teachers, evenly distributed attention across three areas of focus (i.e., content, organization, and language), and tended to provide direct revision guidance. Teachers provide personalized, engaging feedback that accounts for individual abilities and needs of students and often deliver query-based feedback. The teacher participants commented on ChatGPT\u2019s ability to evaluate student writing, specificity of the feedback and limitations of ChatGPT feedback in terms of length, reading difficulty, and relevance. Their study suggested the importance of teacher agency. Instead of having ChatGPT directly provide feedback for students, teacher feedback can be facilitated by ChatGPT output. To bridge the gap that no previous research examined the collaboration of instructors and AIin providing writing feedback, we implemented an innovative feedback approach (i.e., ChatGPT-supported teacher feedback) in a Chinese EFL course to explore the ChatGPT-supported teacher feedback and students\u2019 incorporation of the feedback into their revisions. This study is informed by two research questions.\n\nRQ1: What are the types and features of ChatGPT-supported feedback provided by teachers across two writing tasks? RQ2: How do students incorporate ChatGPT-supported teacher feedback into their revisions regarding two writing task\n\n# 3. The study\n\n# 3.1. Context and participants\n\nThe present study, which was part of a larger research project on ChatGPT-supported teacher feedback, explored the types and features of ChatGPT-supported teacher feedback and the incorporation of EFL students of that feedback into their revisions. At a large public university in southwestern China, we used ChatGPT to help instructors provide writing feedback in the Language Teaching Methodology course offered in English to undergraduate students majoring in World Language Education in the spring of 2023. Each class was taught by two instructors (a senior faculty member and a junior faculty member). The curriculum covered various facets of language instruction, such as teaching methods, educational theories, instructional strategies, and intercultural communication, with a particular emphasis on improving English composition skills. In total, 102 students (47 from one class and 55 from the other) participated in this study after they had participated in the study orientation and signed the informed consent forms.\n\n# 3.2. Study design and procedure\n\nWe implemented two types of ChatGPT-supported teacher feedback based on the contingent needs of the students: 1) indirect corrective feedback drawing on Ferris\u2019 (2006) 15 types of common errors specific to EFL learners (the instructor changed the direct feedback of ChatGPT to coded indirect feedback in the feedback provision) and 2) holistic rhetorical feedback, which is commonly not included in the traditional EFL teacher feedback. Indirect feedback refers to teachers\u2019 indications of the types of errors and locations of errors, which enable students to identify and correct language mistakes on their own (Ferris et al., 2013). Indirect feedback was adopted in this study because research has shown that indirect feedback promotes the autonomous reflection and problem-solving abilities of students and consequently helps students become independent learners (Ferris, 2010). In addition, indirect feedback has been proven to be an effective strategy for learning content words such as verbs, nouns, and articles (Ferris et al., 2013; Ferris & Roberts, 2001). Specifically, there are two categories of indirect feedback: uncoded indirect feedback and coded indirect feedback. Uncoded indirect feedback refers to instances where the teacher merely marks an error by underlining, circling, or placing a tally in the margin, without explicitly indicating the type of error (Bitchener et al., 2005). Coded indirect feedback, which our study used, refers to instances where the teacher identifies the exact location of an error and specifies the type of error and reportedly enhances the problem solving, reflection, self-learning, and learning outcomes of students. (Ferris, 2010; Han, 2024). The inclusion of rhetorical feedback was inspired by Aljasir (2021) and Carter and Thirakunkovit (2019), who noted that rhetorical feedback enhanced the ability of students to efficiently provide clear arguments, substantiate major points with evidence, maintain logical flow, and organize ideas. Accordingly, we developed two prompts for ChatGPT and trained it to provide feedback according to the prompts: 1) \u201cplease point out all errors on sentence structure, word choice, verb tense, noun endings (singular/plural), verb form, punctuation, articles/determiners, word form, spelling, run-ons, pronouns, subject\u2012verb agreement, fragments, idioms, and informal text\u201d (hereinafter \u201cInstruction 1\u201d) and 2) \u201cplease provide feedback on the rhetoric within the article\u201d (hereinafter \u201cInstruction 2\u201d).\n\nThe first author provided extensive training to the four instructors on how to provide coded indirect feedback and holistic feedback based on the ChatGPT output. Specifically, he guided the instructors to practice implementing ChatGPT-supported teacher feedback using a few sample papers. They ran the ChatGPT prompts, reviewed the ChatGPT output and provided constructive feedback on the sample papers based on the ChatGPT feedback. Then, accounts were registered for the four teachers to use ChatGPT. The students were required to complete two writing tasks: an argumentative essay entitled \u201cThe Importance of Learning a Second Language for Chinese Language Majors\u201d and an expository essay on \u201cThe Impact of Technology on Language Education\u201d. Prior to their writing, the teachers provided explicit instructions on how to craft a 350-word essay using digital devices, which included all required components of an essay. Detailed assessment criteria were also provided to the students; for example, the argumentative essay was evaluated based on its thesis robustness, evidence, counterargument, grammatical accuracy, citation of sources, and overall coherence and organization. The essays were composed during class sessions (i.e., $4 5 \\mathrm { { m i n } }$ per session) and submitted through a learning management system (LMS). Two instructors, one from each class, graded half of the submitted essays.\n\nThe innovative method of ChatGPT-supported teacher feedback includes three stages: 1) the teacher copied the essay of a student into ChatGPT and input Instruction 1; 2) the instructor closely reviewed the ChatGPT-generated feedback to provide coded indirect feedback on the students\u2019 drafts by adding in-text annotations in MS Word; 3) the instructor input Instruction 2 for rhetorical feedback from ChatGPT, made modifications if needed, and appended the holistic feedback to the end of the student\u2019s essay. Subsequently, the two main types of feedback were shared with students via the LMS, and the students were required to submit their revised essays based on the ChatGPT-supported feedback within five days. Table 1 shows the detailed procedure and collected data in the larger research project.\n\n# 3.3. Data analysis\n\nAlthough 102 students participated in the study and completed their writing assignments in class, seven students did not submit their revised drafts for both writing tasks. Therefore, we excluded the essays of these seven students from our analysis. We analyzed 95 drafts and 95 revised papers for each of the two tasks.\n\nTo address RQ1, we conducted a detailed analysis of the ChatGPT-supported teacher feedback and identified the total number of intext comments and the number and percentage of comments regarding each type of language error. For example, in Task 1, the instructor annotated 1176 errors, 220 of which were related to word choice, which accounted for $1 8 . 7 1 \\%$ of all errors. We also conducted a content analysis of the rhetorical feedback on 190 essays from the two writing tasks. We enabled the categories of holistic comments to emerge from the data and identified eight types, as displayed in Table 2. Then, we calculated the number of each rhetorical feedback type in each essay and compared the overall rhetorical feedback in the two writing tasks. To answer RQ2, we adapted Ferris\u2019 (1997) method for scoring revisions, where we adjusted Ferris\u2019s seven-point scale to a three-point scale for Instruction 1 regarding the error feedback, including no change, an unsuccessful revision, and a successful revision. For Instruction 2 regarding holistic rhetoric feedback, the three-point scale includes no change, partial modification, and full modification. The distinctions among these types enable us to understand the scaffolding effects of ChatGPT-supported teacher feedback on the students\u2019 revisions. One researcher coded all revisions of the students, and a colleague with a certification in English teaching randomly selected $3 0 \\%$ of the papers for coding. The kappa values for Task 1 and Task 2 revisions were 0.903 and 0.853, respectively, with an average kappa value of 0.885.\n\nTable 1 Research procedures and collected data.   \n\n<html><body><table><tr><td>Timeline</td><td>Procedure</td><td>Data collected</td></tr><tr><td colspan=\"3\">Researchers preparation</td></tr><tr><td colspan=\"3\">Week 1</td></tr><tr><td rowspan=\"3\">Week 2</td><td>1. Teacher training 2. Two classes of students are trained.</td><td></td></tr><tr><td>1. Students complete the first writing assignment in class</td><td>1. 1st draft</td></tr><tr><td>2. Teachers provide ChatGPT-supported teacher feedback</td><td>2. ChatGPT-supported teacher feedbacke</td></tr><tr><td>Week 3</td><td>1. Students receive feedback from their teachers</td><td>3. Class observations</td></tr><tr><td rowspan=\"5\">Week 4</td><td colspan=\"3\"></td></tr><tr><td colspan=\"3\">2. Students revise the writing assignment. 1. Students submit the revised writing assignment</td></tr><tr><td colspan=\"3\"></td></tr><tr><td>2. Students complete the 2nd writing assignment in the class</td><td colspan=\"2\">2. 2nd draft 3. Class observations</td></tr><tr><td>3. Teachers provide ChatGPT-supported teacher feedback</td><td colspan=\"2\">4. 1st revision scores</td></tr><tr><td colspan=\"3\"></td></tr><tr><td>Week 5</td><td colspan=\"3\">1. Students receive feedback from their teachers</td></tr><tr><td>Week 6</td><td colspan=\"3\">2. Students revise the writing assignment. 1. Students submit the revised writing assignment</td></tr><tr><td></td><td colspan=\"3\"></td></tr><tr><td>Week 7</td><td></td><td colspan=\"2\">1. 2nd revision 2. 2nd revision scores</td></tr><tr><td>Week 8</td><td>1. Semi-structured interviews with teachers. 1. Semi-structured interviews with students</td><td colspan=\"3\">1. Teacher interviews 1. Student interviews</td></tr></table></body></html>\n\nNote: The interviews, each of which was conducted in Chinese and lasted approximately $3 0 \\ \\mathrm { m i n }$ , aimed to explore the teachers\u2019 and students perceptions of ChatGPT-supported teacher feedback. The teachers\u2019 perceptions of ChatGPT-supported teacher feedback were the focus of another work of the co-authors.\n\nTable 2 Types of rhetorical feedback: definitions and examples.   \n\n<html><body><table><tr><td></td><td>Codes</td><td>Operationalized definition</td><td>Illustrative examples</td></tr><tr><td>1</td><td>Cohesion and coherence</td><td>Any evaluative or commentary focusing on the logical connection and flow between sentences, paragraphs, and sections of a writing assignment.</td><td>However, the writing contains some issues with coherence.</td></tr><tr><td>2</td><td>Comprehensibility of message</td><td>Any evaluative commentary or inquiry concerning the clarity, understandability, and ease of interpretation of the message conveyed in a writing assignment..</td><td>The text is not clear enough.</td></tr><tr><td>3</td><td>Content development</td><td>Any evaluative or constructive commentary focusing on the creation, evidence, examples, expansion, arguments, and improvement of the essay&#x27;s content.</td><td>The article could be improved by providing more. specific examples and evidence to support the arguments made.</td></tr><tr><td>4</td><td>Mechanics (spelling, punctuation, capitalization)</td><td>Any evaluative or commentary that focuses on the accuracy and appropriateness of spelling, punctuation, and capitalization in a writing assignment.</td><td>However, there are a few errors in punctuation that could be corrected.</td></tr><tr><td>5</td><td>Organization</td><td>Any evaluative or commentary that addresses the structural layout and logical sequence of content in a writing assignment.</td><td>The organization of the article could be improved.</td></tr><tr><td>6</td><td>Sentence structure/grammar</td><td>Any evaluative or commentary on the syntactic arrangement of words within a sentence to convey meaning effectively, governed by the rules of the language.</td><td>There are some grammatical errors in the article. that could be corrected to improve.</td></tr><tr><td>7</td><td>Word choice</td><td>Any evaluative or commentary focusing on the appropriateness, precision, and impact of the word used in a writing assignment.</td><td>There are some issues with word choice that could. be improved.</td></tr><tr><td>8</td><td>Writing style</td><td>Any evaluative or commentary that addresses the author&#x27;s stylistic choices in a writing assignment. This includes feedback on the tone, voice, and formality of the writing style..</td><td>The author uses informal language in some places, which can detract from the overall professionalism. of the article.</td></tr></table></body></html>\n\n# 4. Results\n\nWe present the findings of this study regarding the two research questions.\n\n4.1. RQ 1: what are the types and features of ChatGPT-supported feedback provided by teachers across two writing tasks?\n\nThe instructors provided indirect feedback on Ferris\u2019s 15 error types according to the ChatGPT output. Examples 1 and 2 in Table 3 are illustrative. Moreover, the instructors provided the overall feedback on the rhetoric of the students\u2019 essays based on the ChatGPT output, as shown in example 3.\n\nTable 4 presents an overall picture of ChatGPT-supported teacher feedback regarding different error types in each writing task. As displayed in Table 4, Writing Task 1 received 1176 instances of feedback with an average of 12.38 instances per essay. Writing   \nTask 2 received 665 instances with an average of 7 feedback points per essay. In Task 1, the three most frequent addressed types of   \nerrors in ChatGPT-supported teacher feedback were word choice $( n = 2 2 0$ , $1 8 . 7 1 \\%$ ), sentence structure $\\mathbf { \\tilde { \\Delta } } n = 1 6 7$ , $1 4 . 2 \\% )$ ), and   \npunctuation $( n = 1 2 8$ , $1 0 . 8 8 \\%$ ). The four least frequently addressed types of errors were fragments $( n = 7$ , $0 . 6 \\%$ , idioms $( n = 1 9$ ,\n\nTable 3 Illustrative examples of ChatGPT outputs and teacher feedback.   \n\n<html><body><table><tr><td>Examples</td><td>ChatGPT output</td><td>Teacher feedback</td></tr><tr><td>Example 1 Verb tense Example 2 Word choice</td><td>ChatGPT output: &quot;had studied&quot;.</td><td>Verb tense: check the tense of &quot;had study&quot;..</td></tr><tr><td>Example 3 Rhetorical feedback</td><td>&quot;Communication and communication&quot; seem repetitive. Perhaps you meant &quot;communication and collaboration&quot;.. The article effectively presents the importance of learning a second foreign language, particularly for students majoring in Chinese, in the context of globalization and increasing global interactions. The author provides various arguments, including cognitive benefits, career opportunities, and cultural exchange, to support their claim.</td><td>&quot;Communication and communication&quot; seem repetitive. Your article is well structured and organized, with clear and concise paragraphs that focus on specific points. You use relevant examples to illustrate their arguments, which helps to make the article more</td></tr><tr><td></td><td>The article is well-structured and organized, with clear and concise paragraphs that focus on specific points. The author also uses relevant examples to illustrate their arguments, which helps to make the article more persuasive. One area where the article could be improved is in the use of sources to support the arguments presented. While the author mentions experimental results that show the cognitive benefits of learning a second language, they do not provide specific references or sources for these results. Adding this information would make the article more credible and convincing. Overall, the article effectively presents the benefits of learning a second foreign language, particularly for students majoring in Chinese, and provides compelling arguments to support this claim.</td><td>persuasive. (1) One area where your article could be improved is in the use of sources to support the arguments presented, and (2) while you mention experimental results that show the cognitive benefits of learning a second language, you do not provide specific references or sources for these results. Adding this information would make your article more credible and convincing.e</td></tr></table></body></html>\n\nTable 4 Distribution of error types across two writing tasks.   \n\n<html><body><table><tr><td>Feedback in terms of error types</td><td colspan=\"2\">1st writing task N = 95</td><td colspan=\"2\">2nd writing task N = 95</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>Sentence structure</td><td>167</td><td>14.2.</td><td>71</td><td>10.68</td></tr><tr><td>Word choice</td><td>220</td><td>18.71</td><td>145</td><td>21.8</td></tr><tr><td>Verb tense</td><td>105</td><td>8.93</td><td>38</td><td>5.71</td></tr><tr><td> Noun endings (singular/plural)</td><td>55</td><td>4.68</td><td>40</td><td>6.02</td></tr><tr><td>Verb form</td><td>74</td><td>6.29</td><td>35</td><td>5.26</td></tr><tr><td>Punctuation</td><td>128</td><td>10.88</td><td>38</td><td>5.71</td></tr><tr><td>Articles/determiners</td><td>92</td><td>7.82</td><td>38</td><td>5.71</td></tr><tr><td>Word form</td><td>117</td><td>9.95</td><td>54</td><td>8.12</td></tr><tr><td> Spelling</td><td>46</td><td>3.91</td><td>72</td><td>10.83</td></tr><tr><td>Run-ons</td><td>32</td><td>2.72</td><td>41</td><td>6.17</td></tr><tr><td>Pronouns</td><td>32</td><td>2.72</td><td>8.</td><td>1.2</td></tr><tr><td>Subject-verb agreement</td><td>44</td><td>3.74</td><td>32</td><td>4.8</td></tr><tr><td>Fragments</td><td>7</td><td>0.6</td><td>29</td><td>4.36</td></tr><tr><td> Idiom</td><td>19</td><td>1.62</td><td>4.</td><td>0.6</td></tr><tr><td>Informal</td><td>38</td><td>3.23</td><td>20</td><td>3</td></tr><tr><td>Total</td><td colspan=\"2\">1176</td><td colspan=\"2\">665</td></tr></table></body></html>\n\n$1 . 6 2 \\% )$ ), pronouns $( n = 3 2$ , $2 . 7 2 \\%$ ) and run-ons $( n = 3 2$ , $2 . 7 2 \\%$ , which were tied. In Task 2, this pattern only slightly changed, where the three most common types of errors became word choice $( n = 1 4 5$ , $2 1 . 8 \\%$ , spelling $( n = 7 2$ , $1 0 . 8 3 \\% )$ ), and sentence structure $( n =$ 71, $1 0 . 6 8 \\%$ . The three least frequent error types were idioms $( n = 4$ , $0 . 6 \\%$ , pronouns $( n = 8 , 1 . 2 \\% )$ , and informal language $( n = 2 0$ , $3 \\%$ ). The proportion of nine types of errors decreased. The sentence structure, verb tense, verb form and punctuation errors decreased from $1 4 . 2 \\%$ , $8 . 9 3 \\%$ , $6 . 2 9 \\%$ and $1 0 . 8 8 \\%$ of all errors in Task $1 { - } 1 0 . 6 8 \\%$ , $5 . 7 1 \\%$ , $5 . 2 6 \\%$ , and $5 . 7 1 \\%$ of all errors in Task 2, respectively. Additionally, the percentages of article/determiner, word form, pronoun, idiom, and informal language errors decreased from $7 . 8 2 \\%$ , $9 . 9 5 \\%$ , $2 . 7 2 \\%$ , $1 . 6 2 \\%$ and $3 . 2 3 \\% - 5 . 7 1 \\%$ , $8 . 1 2 \\%$ , $1 . 2 \\%$ , $0 . 6 \\%$ , and $3 \\%$ , respectively. Conversely, the proportions of six types of errors increased: word choice from $1 8 . 7 1 \\%$ to $2 1 . 8 \\%$ , noun endings (singular/plural) from $4 . 6 8 \\%$ to $6 . 0 2 \\%$ , spelling from $3 . 9 1 \\%$ to $1 0 . 8 3 \\%$ , run-on sentences from $2 . 7 2 \\%$ to $6 . 1 7 \\%$ , subject\u2012verb agreement from $3 . 7 4 \\%$ to $4 . 8 \\%$ , and fragments from $0 . 6 \\%$ to $4 . 3 6 \\%$ . We conducted a Chi-square test on the number of errors in two writing tasks, which resulted in a $\\chi ^ { 2 }$ value of 115.275 and a $p$ -value of $5 . 2 7 2 \\times$ $1 0 ^ { - 1 8 }$ (see Table 6). These results indicate a statistically significant difference in the distribution of error types between 1st and 2nd writing tasks. Possible reasons will be addressed in the discussion section.\n\nMoreover, we analyzed the feedback provided on 190 essays across two different writing tasks, as shown in Table 5. Comprehensibility of message was the most frequent feedback type in Task 1 at 95 $( 2 1 \\% )$ but decreased to $1 2 \\%$ in Task 2. Cohesion and Coherence decreased from 78 $( 1 7 \\% )$ in Task 1 to 49 $( 1 1 \\% )$ in Task 2, which is in parallel with the shift from message comprehensibility to more form-focused feedback. Both Content development and Organization maintained high importance across tasks, and content development slightly decreased from 95 $( 2 1 \\% )$ in Task 1 to 67 $( 1 5 \\% )$ in Task 2, whereas organization remained steady at approximately $1 8 \\%$ . Sentence Structure/Grammar increased from 33 $( 7 \\% )$ in Task 1 to 68 $. 1 5 \\% )$ in Task 2, which suggests a greater emphasis on the grammatical accuracy in Task 2. Word Choice more than doubled from 17 $( 4 \\% )$ in Task 1 to 50 $( 1 1 \\% )$ in Task 2, which reflects the increased attention to language precision in Task 2. Feedback on mechanics increased from 12 $( 3 \\% )$ to 36 $( 8 \\% )$ . Writing style feedback remained consistent at approximately $1 0 \\%$ , which indicates a sustained focus on the formal aspects of writing. We performed Chi-square analyses to analyze the instances of rhetorical feedback across two writing tasks, which resulted in $\\chi ^ { 2 }$ of 61.45 and a $p$ -value less than 0.001 (see Table 6). These results indicate a statistically significant difference in the distribution of rhetorical feedback types across two writing tasks.\n\n.2. RQ2: how do students incorporate ChatGPT-supported feedback into revisions regarding the two writing tasks?\n\nWe found that most errors addressed in ChatGPT-supported teacher feedback were successfully corrected. As shown in Table 7,\n\nTable 5 Distribution of rhetorical feedback types across two writing tasks.   \n\n<html><body><table><tr><td>Feedback types</td><td>Task 1 (N = 95)</td><td>Task 2 (N = 95)</td></tr><tr><td>Cohesion and coherence</td><td>78 (17%)</td><td>49 (11%)</td></tr><tr><td>Comprehensibility of message</td><td>95 (21%)</td><td>57 (12%)</td></tr><tr><td>Content development</td><td>95 (21%)</td><td>67 (15%)</td></tr><tr><td>Mechanics (spelling, punctuation, capitalization)</td><td>12 (3%)</td><td>36 (8%)</td></tr><tr><td>Organization</td><td>85 (18%)</td><td>84 (18%)</td></tr><tr><td>Sentence structure/grammar</td><td>33 (7%)</td><td>68 (15%)</td></tr><tr><td>Word choice</td><td>17 (4%)</td><td>50 (11%)</td></tr><tr><td>Writing style</td><td>45 (10%)</td><td>48 (10%)</td></tr><tr><td>Total</td><td>460</td><td>459</td></tr></table></body></html>\n\nTable 6 Chi-square tests on the number of errors and instances of rhetorical feedback across two writing tasks.   \n\n<html><body><table><tr><td>Task1 vs. Task2</td><td>x2</td><td>df</td><td>N</td><td>p</td></tr><tr><td>Distribution of error types</td><td>115.275</td><td>14</td><td>1841</td><td>&lt;0.001</td></tr><tr><td>Distribution of rhetorical feedback types</td><td>61.45</td><td>7</td><td>919</td><td>&lt;0.001</td></tr></table></body></html>\n\nNote: $p < 0 . 0 0 1$ indicates that the p-values are less than 0.001.\n\n$6 4 . 5 4 \\%$ and $6 3 . 9 1 \\%$ of the changes in Task 1 and Task 2 were successful revisions, respectively. Moreover, we detected a decrease in the instances of no change, from $2 3 . 5 5 \\%$ in Task $1 - 2 0 . 3 \\%$ in Task 2. In Task 1, only $9 . 4 7 \\%$ of the modifications were classified as full revisions, $2 5 . 2 6 \\%$ were categorized as partial revisions, and $6 5 . 2 6 \\%$ were classified as no change. Table 8 shows the students\u2019 re sponses to rhetorical feedback. The revision of rhetorical feedback considerably shifted in Task 2: no changes decreased to $3 5 . 7 9 \\%$ and partial revisions decreased to $7 . 3 7 \\%$ . However, the instances of unsuccessful revisions increased from $1 1 . 9 \\%$ in Task $1 { - } 1 5 . 7 9 \\%$ in Task 2. There was a change in revision of the rhetorical feedback, where full revisions increased to $5 6 . 8 4 \\%$ .\n\nWe performed Chi-square analyses to investigate whether there were significant differences among the three ratings in task 1 and task 2. For the students\u2019 revisions based on Ferris\u2019 15 error categories, there is a significant difference among the three ratings (no change, unsuccessful revision, and successful revision) in Task 1 and Task 2, with statistically more successful revisions identified. In the case of the students\u2019 revisions based on rhetorical feedback, there is a significant difference among the three ratings (no change, partial modification, and comprehensive modification) in each task (see Table 9). Interestingly, statistically more \u201cno change\u201d instances were identified in task 1, but statistically more \u201ccomprehensive modification\u201d instances were discovered, which clearly revealed the better engagement and performance of the students in task 2.\n\nWe further examined the revisions in terms of error categories across the two tasks, as shown in Table 10. In Task 1, four error categories had successful revision ratios of more than $7 0 \\%$ : word choice $( 8 2 . 3 \\% )$ , sentence structure $( 7 6 . 6 \\% )$ , spelling $( 7 6 . 1 \\% )$ , and word form $( 7 3 . 5 \\% )$ . Notably, $8 9 . 6 \\%$ of word choice errors were modified, and $8 2 . 3 \\%$ of these modifications were successful. Similarly, $9 1 \\%$ of the sentence structure errors were modified, $7 6 . 6 \\%$ of the errors were successfully corrected and only $9 \\%$ of the errors remained unmodified. The other six error categories had percentages of successful revisions of $6 0 { - } 7 0 \\%$ : verb form $( 6 8 . 9 \\% )$ , informal $( 6 5 . 8 \\% )$ , verb tense $( 6 5 . 7 \\% )$ , pronouns $( 6 5 . 6 \\% )$ , noun endings $( 6 3 . 6 \\% )$ , and articles/determiners $( 6 2 . 0 \\% )$ ). Only two error categories had a low successful revision ratio (under $3 0 \\%$ ): punctuation $( 2 1 . 9 \\% )$ and fragments $( 1 4 . 3 \\% )$ .\n\nIn Task 2 (see Table 11), there was an overall improvement in the correction of errors, where successful modification rates exceeded $5 0 \\%$ for 13 types of errors. Although the success rates for correcting punctuation and fragment errors were below $5 0 \\%$ $4 2 . 1 \\%$ and $4 8 . 3 \\%$ , respectively), these rates increased compared with those of Task 1. The percentages of successful revisions for other types of errors were above $5 0 \\%$ . Additionally, six error categories exhibited successful revision rates above $7 0 \\%$ : noun endings $( 8 5 \\% )$ , verb forms $( 7 1 . 4 \\% )$ , spelling $( 7 6 . 4 \\% )$ , subject\u2012verb agreement $( 8 1 . 3 \\% )$ , idioms $( 1 0 0 \\% )$ , and informal $( 8 0 . 0 \\% )$ , which indicates a signifi cant shift from the results in Task 1. In Task 2, although the successful revision of spelling errors maintained a similar percentage to that in Task 1 (above $7 0 \\%$ ), the percentages of successful revisions for the other five categories improved, which suggests that under the teacher\u2019s guidance, students began to focus on errors that were poorly addressed in Task 1. However, there were notable declines in the success of sentence structure and word choice revisions: $5 4 . 9 \\%$ and $5 8 \\%$ , respectively, with particularly high rates of no change $( 2 6 . 8 \\%$ and $2 3 . 4 \\%$ , respectively). The interviews that we conducted in the larger project can help explain these discrepancies, which will be addressed in the Discussion section.\n\n# 5. Discussion\n\nIn this section, we first interpret our findings in this study in terms of the types of ChatGPT-supported teacher feedback and stu dents\u2019 incorporation of the feedback. Then, we highlight the effectiveness of this innovative feedback approach based on the multi faceted data that we collected in the larger project. First, the total quantity of corrective feedback decreased from 1176 to 665 instances across the two tasks, possibly due to a general decrease in number of errors made by the students in Task 2. Second, the strategies of the teachers for ChatGPT-supported feedback may partially explain the findings of the feedback on different error types. For example, two instructors mentioned their use of deletion and condensing techniques to provide feedback (e.g., recurring errors were condensed into one) in the post-task interview. As Instructor Wei stated, \u201cfor recurring spelling mistakes in a text, it\u2019s not necessary to point out each one; condensing them into a single piece of feedback for the student will suffice\u201d. Third, by interviewing the students, we learned that they tended to pay more attention to the drafts in Task 2; for example, Jia said, \u201cWhen I was writing for the first time, I didn\u2019t pay much attention to the need for revisions. When I realized that I had to make revisions, I became more careful during my second attempt at writing\u201d, and Wang stated that \u201cif I have to correct every mistake, then I need to be more careful with my first draft to reduce the number of errors\u201d. These excerpts well explain the fewer instances of errors addressed in the ChatGPT-supported teacher feedback. Interestingly, the number of spelling errors significantly increased in Task 2. According to the classroom observations, the teachers asked the students to bring laptops for in-class writing prior to the implementation of Task 1. Most students used laptops with Microsoft\n\nTable 7 Overall statistics for students\u2019 revisions based on Ferris\u2019 15 error categories.   \n\n<html><body><table><tr><td>Rating</td><td colspan=\"2\">Task 1</td><td colspan=\"2\">Task 2</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>No change</td><td>277</td><td>23.55</td><td>135</td><td>20.3</td></tr><tr><td>Unsuccessful revision</td><td>140</td><td>11.9</td><td>105</td><td>15.79</td></tr><tr><td>Successful revision</td><td>759</td><td>64.54</td><td>425</td><td>63.91</td></tr><tr><td>Total</td><td>1176</td><td></td><td>665</td><td></td></tr></table></body></html>\n\nTable 8 Overall statistics for students\u2019 revisions based on rhetorical feedback.   \n\n<html><body><table><tr><td>Rating</td><td colspan=\"2\">Task 1</td><td colspan=\"2\">Task 2</td></tr><tr><td></td><td>No.</td><td>%</td><td>No.</td><td>%</td></tr><tr><td>No change</td><td>62</td><td>65.26</td><td>34</td><td>35.79</td></tr><tr><td> Partial modification</td><td>24</td><td>25.26</td><td>7.</td><td>7.37</td></tr><tr><td>Comprehensive modification</td><td>9</td><td>9.47</td><td>54</td><td>56.84</td></tr><tr><td>Total</td><td>95</td><td></td><td>95</td><td></td></tr></table></body></html>\n\nTable 9 Results of the Chi-square test for the three ratings in Tasks 1 and 2.   \n\n<html><body><table><tr><td></td><td></td><td>x?</td><td>df</td><td>N</td><td>p</td></tr><tr><td>Students&#x27; revisions based on Ferris&#x27; 15 error categories</td><td>Task1</td><td>539.332</td><td>2 </td><td>1176</td><td>&lt;0.001</td></tr><tr><td></td><td>Task2</td><td>281.805</td><td>2</td><td>665</td><td>&lt;0.001</td></tr><tr><td>Students revisions based on rhetorical feedback.</td><td>Task1</td><td>47.137</td><td>2</td><td>95</td><td>&lt;0.001</td></tr><tr><td></td><td>Task2</td><td>35.137</td><td>2</td><td>95</td><td>&lt;0.001</td></tr></table></body></html>\n\nNote: $p < 0 . 0 0 1$ indicates that the p values are less than 0.001.\n\nTable 10 Revisions for different error categories in Task 1.   \n\n<html><body><table><tr><td></td><td>Error type</td><td> no change</td><td>%</td><td> unsuccessful revision</td><td>%</td><td>successful revision</td><td>%</td><td>No.</td></tr><tr><td>1</td><td>Sentence structure</td><td>15</td><td>9.0%</td><td>24</td><td>14.4%</td><td>128</td><td>76.6%</td><td>167</td></tr><tr><td>2</td><td>Word choice</td><td>16</td><td>7.3%</td><td>23</td><td>10.5%</td><td>181</td><td>82.3%</td><td>220</td></tr><tr><td>3</td><td>Verb tense</td><td>20</td><td>19.0%</td><td>16</td><td>15.2%</td><td>69</td><td>65.7%</td><td>105</td></tr><tr><td>4</td><td> Noun endings (singular/plural)</td><td>13</td><td>23.6%</td><td>7</td><td>12.7%</td><td>35</td><td>63.6%</td><td>55</td></tr><tr><td>5</td><td>Verb form</td><td>12</td><td>16.2%</td><td>11</td><td>14.9%</td><td>51</td><td>68.9%</td><td>74</td></tr><tr><td>6</td><td>Punctuation</td><td>87</td><td>68.0%</td><td>13</td><td>10.2%</td><td>28</td><td>21.9%</td><td>128</td></tr><tr><td>7</td><td> Articles/determiners</td><td>26</td><td>28.3%</td><td>9</td><td>9.8%</td><td>57</td><td>62.0%</td><td>92.</td></tr><tr><td>8</td><td>Word form</td><td>25</td><td>21.4%</td><td>6</td><td>5.1%</td><td>86</td><td>73.5%</td><td>117</td></tr><tr><td>9</td><td>Spelling</td><td>9</td><td>19.6%</td><td>2</td><td>4.3%</td><td>35</td><td>76.1%</td><td>46</td></tr><tr><td>10</td><td>Run-ons</td><td>14</td><td>43.8%</td><td>5</td><td>15.6%</td><td>13</td><td>40.6%</td><td>32</td></tr><tr><td>11</td><td>Pronouns</td><td>7</td><td>21.9%</td><td>4</td><td>12.5%</td><td>21</td><td>65.6%</td><td>32</td></tr><tr><td>12</td><td>Subject-verb agreement</td><td>17</td><td>38.6%</td><td>6</td><td>13.6%</td><td>21</td><td>47.7%</td><td>44</td></tr><tr><td>13</td><td>Fragments</td><td>3</td><td>42.9%</td><td>3</td><td>42.9%</td><td>1</td><td>14.3%</td><td>7</td></tr><tr><td>14</td><td>Idiom</td><td>5</td><td>26.3%</td><td>6</td><td>31.6%</td><td>8</td><td>42.1%</td><td>19</td></tr><tr><td>15</td><td>Informal</td><td>8</td><td>21.1%</td><td>5</td><td>13.2%</td><td>25</td><td>65.8%</td><td>38</td></tr></table></body></html>\n\nTable 11 Revisions for different error categories in Task 2.   \n\n<html><body><table><tr><td></td><td>Error type</td><td> no change</td><td>%</td><td>unsuccessful revision</td><td>%</td><td>successful revision</td><td>%</td><td>No.</td></tr><tr><td>1</td><td>Sentence structure</td><td>19</td><td>26.8%</td><td>13</td><td>18.3%</td><td>39</td><td>54.9%</td><td>71</td></tr><tr><td>2</td><td>Word choice</td><td>34</td><td>23.4%</td><td>27</td><td>18.6%</td><td>84</td><td>58.0%</td><td>145</td></tr><tr><td>3</td><td>Verb tense</td><td>7</td><td>18.4%</td><td>9</td><td>23.7%</td><td>22</td><td>57.9%</td><td>38</td></tr><tr><td>4</td><td> Noun endings (singular/plural)</td><td>2</td><td>5.0%</td><td>4</td><td>10.0%</td><td>34</td><td>85.0%</td><td>40</td></tr><tr><td>5</td><td>Verb form</td><td>8</td><td>22.9%</td><td>2</td><td>5.7%</td><td>25</td><td>71.4%</td><td>35</td></tr><tr><td>6</td><td>Punctuation</td><td>13</td><td>34.2%</td><td>9</td><td>23.7%</td><td>16</td><td>42.1%</td><td>38</td></tr><tr><td>7</td><td> Articles/determiners</td><td>8.</td><td>21.1%</td><td>4.</td><td>10.5%</td><td>26</td><td>68.4%</td><td>38</td></tr><tr><td>8</td><td>Word form</td><td>11</td><td>20.4%</td><td>10</td><td>18.5%</td><td>33</td><td>61.1%</td><td>54</td></tr><tr><td>9</td><td>Spelling</td><td>14</td><td>19.4%</td><td>3</td><td>4.2%</td><td>55</td><td>76.4%</td><td>72</td></tr><tr><td>10</td><td>Run-ons</td><td>7</td><td>17.1%</td><td>7</td><td>17.1%</td><td>27</td><td>65.9%</td><td>41</td></tr><tr><td>11</td><td>Pronouns</td><td>1</td><td>12.5%</td><td>3</td><td>37.5%</td><td>4</td><td>50.0%</td><td>8</td></tr><tr><td>12</td><td>Subject-verb agreement</td><td>2</td><td>6.3%</td><td>4</td><td>12.5%</td><td>26</td><td>81.3%</td><td>32</td></tr><tr><td>13</td><td>Fragments</td><td>6</td><td>20.7%</td><td>9</td><td>31.0%</td><td>14</td><td>48.3%</td><td>29</td></tr><tr><td>14</td><td>Idiom</td><td>0</td><td>0%</td><td>0</td><td>0%</td><td>4</td><td>100%</td><td>4</td></tr><tr><td>15</td><td>Informal</td><td>3</td><td>15.0%</td><td>1</td><td>5.0%</td><td>16</td><td>80.0%</td><td>20</td></tr></table></body></html>\n\nWord, which features an automatic correction function that reduces spelling errors. In the second writing assignment, teachers did not remind them to bring laptops, and most students used tablets or smartphones for writing instead. The lack of an automatic correction function in the portable devices increased the number of spelling errors, and more errors were addressed in the ChatGPT-supported teacher feedback.\n\nMoreover, different genres regarding the two writing tasks (i.e., argumentative and expository essays) might account for the differences that we identified across the tasks. Argumentative essays require students to develop a position on a particular issue and support it with evidence and reasoning (Chuang & Yan, 2022; Taylor et al., 2019), whereas expository essays aim to clearly and concisely explain a topic or provide information (Jeong, 2017; Zhang & Li, 2021). In other words, the complex structure of argumentative essays requires more in-depth feedback on content organization and the quality of the arguments. Since expository essays typically focus on explaining concepts or providing information, the ChatGPT-supported teacher feedback emphasized that the in formation was clear, logically organized, and effectively communicated. For example, word choice and sentence structure errors were prominent in both genres, but argumentative essays presented more issues related to argumentation and logical flow, whereas expository essays presented more issues with clarity and organization.\n\nThe finding that most errors were successfully amended suggests that the ChatGPT-supported teacher feedback helps students be more aware of language errors and effectively scaffolds them in writing revisions. Moreover, the students had more complete mod ifications to address the rhetorical feedback in Task 2. This change suggests the active engagement of the students with the feedback content, which led to successful attempts at revision. The students reported the clearness and helpfulness of the ChatGPT-supported teacher feedback compared with the traditional teacher feedback that they received in the post-task interviews, as shown in the following excerpts.\n\nExcerpt 1: \u201cPreviously, teachers would just grade my essay, and I had no idea what specifically needed to be revised or where exactly I went wrong. However, the feedback from both ChatGPT and the teacher can clearly tell me where the mistakes are and which parts need improvement\u201d (post-task interview with Student 1).\n\nExcerpt 2: \u201cUsually, teachers would just mark the errors, for example, by underlining them or circling them with a red pen. Sometimes, I couldn\u2019t quite understand what exactly was wrong, whether it was a word choice error or a problem with my sentence structure, so it was difficult to make corrections. This method, where the teacher clearly tells me the type of error, makes it easier for me to revise\u201d (post-task interview with Student 2).\n\nAgainst the fact that EFL students received unclear teacher feedback on errors occurring in their writing, as reported in Zheng and Yu (2018) and Cheng et al. (2021), the students in this study received very clear and comprehensive error feedback, which they largely incorporated into their revisions. Moreover, the holistic rhetorical feedback generated by ChatGPT greatly helped instructors when they commented on the global issues. As one instructor stated,\n\nI believe that providing holistic rhetorical feedback on the content is very important for students, but it is too time-consuming. Previously, I would just give a grade, and if time allowed, I would write a few sentences to tell the students where they could improve. I did not have time to praise the students\u2019 work or engage in a dialogue with them. However, the content generated by ChatGPT can help me solve this problem. By adding and deleting, I can quickly provide high-quality rhetorical feedback to the students.\n\nThus, our study clearly indicates that ChatGPT-supported teacher feedback can ameliorate the situation where EFL teachers barely effectively address global issues of writing (Barrot, 2023a, 2023b; Mizumoto & Eguchi, 2023; Steiss et al., 2024).\n\nInterestingly, our findings reveal that most errors related to the sentence structure and word choice, which were considered untreatable (Ferris & Roberts, 2001), were successfully revised in both writing tasks. This discrepancy echoes Han\u2019s (2024) finding that computer-mediated coded WCF facilitates the successful revision of regularly considered untreatable errors such as word choice errors. These findings may indicate that EFL learners, who are now immersed in computer-mediated learning environments, can readily access online resources such as corpora and AI tools to transform previously untreatable errors into treatable ones (Mueller & Jacobsen, 2016; Ngo & Chen, 2024; Sataka, 2020). In other words, technological advancements such as AI have made certain error types more amendable than they used to be.\n\n# 6. Conclusion\n\nThis study comprehensively examined the use of ChatGPT-supported teacher feedback in a Chinese tertiary EFL setting. The findings underscore the utility of ChatGPT in augmenting the traditional feedback process through the implementation of AI-teacher collaboration. The common understanding is that the performance of ChatGPT cannot currently compete with trained teachers (Steiss et al., 2024) and it can be difficult for all students to receive detailed constructive writing feedback from trained teachers in the EFL contexts. Thus, the collaboration of teachers and AI has emerged as a feasible solution to effective writing pedagogy, as clearly shown in this study.\n\nA notable reduction in error rates across successive writing tasks suggests that ChatGPT-supported teacher feedback positively contributes to student writing skills, and the students could better address specific error types and revise the overall quality of their papers (e.g., adding examples to support one\u2019s argument). Additionally, the study confirms that the integration of AI-powered feedback can effectively supplement human instruction and facilitate a more nuanced and efficient feedback process. The findings of our study resonate with recent studies utilizing AI-powered feedback (Han & Sari, 2024; Kao & Reynolds, 2024; Shi & Aryadoust, 2024). For example, feedback based on AI technologies such as \u201cCriterion\u201d combined with teacher feedback was more effective than teacher feedback alone in reducing errors in grammar and mechanics (Han & Sari, 2024). Additionally, Kao and Reynolds (2024)\n\napplied AWE to provide feedback on students\u2019 academic writings, and the results revealed that students highly trusted AI-powered feedback concerning grammar rules and lexical choices.\n\nThe present study suggests the need for a hybrid model of instruction that synergizes the analytical capabilities of AI with the pedagogical expertise of educators in the future. In response to the broad debate on the use of AI in language education, our position is that instead of eliminating the use of AI, we can make full use of AI with instructors\u2019 agency. The \u201cAI + Teacher\u201d model can reduce the workload of teachers and enrich the educational landscape by offering a more adaptive and individualized learning experience. This study points to several targeted areas for future investigations. First, we implemented ChatGPT-supported teacher feedback in the Chinese EFL context in this study; future research can examine this new feedback approach in other instructional settings. Second, we focused on how students incorporated the ChatGPT-supported teacher feedback into their revisions but did not examine their per ceptions of this innovative feedback approach. Future studies can closely examine the attitudes of students toward and their per spectives on ChatGPT-supported teacher feedback in comparison to traditional teacher feedback. Third, in another study, we explored how instructors adapted ChatGPT outputs for teacher feedback and their positive perceptions of ChatGPT-supported teacher feedback. The perspectives of teachers, who are important stakeholders in this new feedback approach, deserve continued exploration. Fourth, the impacts of ChatGPT-supported feedback on specific writing outcomes and long-term writing performance of EFL/ESL students warrant closer examination.\n\nIn conclusion, our study reveals that ChatGPT can help teachers provide writing feedback, reduce their workload and offer im mediate personalized responses. In our study, ChatGPT-supported teacher feedback well addressed both global issues and local issues in the students\u2019 writing, and the students incorporated the majority of the feedback into their writing revisions with increased per centages of incorporation across tasks. ChatGPT-supported teacher feedback, as reflected in the \u201cAI $^ +$ Teacher\u201d model, introduces innovation to the L2 writing pedagogy and deserves more inquiries in the coming years.\n\n# Funding\n\nThis work was supported by Southwest University [grant number SWUPilotPlan002].\n\n# Conflict of interest statement\n\nWe declare that we have no conflicts of interest.\n\n# CRediT authorship contribution statement\n\nJining Han: Writing \u2013 review & editing, Writing \u2013 original draft, Resources, Methodology, Formal analysis, Data curation, Conceptualization. Mimi Li: Writing \u2013 review & editing, Validation, Supervision, Methodology, Conceptualization.\n\n# References\n\nAljasir, N. (2021). Matches or mismatches? Exploring shifts in individuals\u2019 beliefs about written corrective feedback as students and teachers-to-be. Journal of Teaching and Teacher Education, 9(1), 1\u201310. https://doi.org/10.12785/jtte/090101   \nBarrot, J. S. (2023a). Using automated written corrective feedback in the writing classrooms: Effects on L2 writing accuracy. Computer Assisted Language Learning, 36 (4), 584\u2013607. https://doi.org/10.1080/09588221.2021.1936071   \nBarrot, J. S. (2023b). Using ChatGPT for second language writing: Pitfalls and potentials. Assessing Writing, 57, Article 100745. https://doi.org/10.1016/j. asw.2023.100745   \nBitchener, J., Young, S., & Cameron, D. (2005). The effect of different types of corrective feedback on ESL student writing. Journal of Second Language Writing, 14(3), 191\u2013205. https://doi.org/10.1016/j.jslw.2005.08.001   \nBruner, J. (1985). Models of the learner. Educational Researcher, 14(6), 5\u20138. https://doi.org/10.3102/0013189X014006005   \nCaldarini, G., Jaf, S., & McGarry, K. (2022). A literature survey of recent advances in chatbots. Information, 13(1), 41. https://doi.org/10.3390/info13010041   \nCarter, T., & Thirakunkovit, S. (2019). A comparison of L1 and ESL written feedback Preferences: Pedagogical applications and theoretical implications. Journal of Response to Writing, 5(2), 139\u2013174. https://scholarsarchive.byu.edu/journalrw/vol5/iss2/7.   \nCheng, X., Zhang, L., & Yan, Q. (2021). Exploring teacher written feedback in EFL writing classrooms: Beliefs and practices in interaction. In Language teaching research. Advance online publication. https://doi.org/10.1177/13621688211057665.   \nChuang, P. L., & Yan, X. (2022). An investigation of the relationship between argument structure and essay quality in assessed writing. Journal of Second Language Writing, 56, Article 100892. https://doi.org/10.1016/j.jslw.2022.100892   \nDwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., \u2026 Wright, R. (2023). \u201cSo what if ChatGPT wrote it?\u201d Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71, Article 102642. https://doi.org/10.36227/techrxiv.21789434.v1   \nEssel, H. B., Vlachopoulos, D., Essuman, A. B., & Amankwa, J. O. (2024). CHATGPT effects on cognitive skills of undergraduate students: Receiving instant responses from ai-based conversational large language models (llms). Computers and Education: Artificial Intelligence, 6, Article 100198. https://doi.org/10.1016/j. caeai.2023.100198   \nFerris, D. R. (1997). The influence of teacher commentary on student revision. Tesol Quarterly, 31(2), 315\u2013339. https://doi.org/10.2307/3588049   \nFerris, D. (2006). Does error feedback help student writers? New evidence on the short- and long-term effects of written error correction. In K. Hyland, & F. Hyland (Eds.), Feedback in second language writing: Contexts and issues (pp. 81\u2013104). Cambridge, UK: Cambridge University Press.   \nFerris, D. R. (2010). Second language writing research and written corrective feedback in SLA: Intersections and practical applications. Studies in Second Language Acquisition, 32(2), 181\u2013201. https://doi.org/10.1017/s0272263109990490   \nFerris, D. R., Liu, H., Sinha, A., & Senna, M. (2013). Written corrective feedback for individual L2 writers. Journal of Second Language Writing, 22(3), 307\u2013329. https:// doi.org/10.1016/j.jslw.2012.09.009   \nFerris, D., & Roberts, B. (2001). Error feedback in L2 writing classes: How explicit does it need to be? Journal of Second Language Writing, 10(3), 161\u2013184. https://doi. org/10.1016/s1060-3743(01)00039-x   \nGrimes, D., & Warschauer, M. (2010). Utility in a fallible tool: A multi-site case study of automated writing evaluation. The Journal of Technology, Learning, and Assessment, 8(6), 4\u201343. https://ejournals.bc.edu/index.php/jtla/article/view/1625/1469.   \nGuo, K., & Wang, D. (2023). To resist it or to embrace it? Examining ChatGPT\u2019s potential to support teacher feedback in EFL writing. Education and Information Technologies, 29(7), 8435\u20138463. https://doi.org/10.1007/s10639-023-12146-0   \nHan, J. (2024). Students\u2019 responses to computer-mediated coded feedback and the factors influencing those students\u2019 responses: A multiple-case study in the cfl setting. Language, Learning and Technology, 28(1), 1\u201323. https://hdl.handle.net/10125/73547.   \nHan, T., & Sari, E. (2022). An investigation on the use of automated feedback in Turkish EFL students\u2019 writing classes. Computer Assisted Language Learning, 37(4), 961\u2013985. https://doi.org/10.1080/09588221.2022.2067179   \nHerbold, S., Hautli-Janisz, A., Heuer, U., Kikteva, Z., & Trautsch, A. (2023). A large-scale comparison of human-written versus ChatGPT-generated essays. Scientific Reports, 13(1), Article 18617. https://doi.org/10.1038/s41598-023-45644-9   \nJeong, H. (2017). Narrative and expository genre effects on students, raters, and performance criteria. Assessing Writing, 31, 113\u2013125. https://doi.org/10.1016/j. asw.2016.08.006   \nKao, C. W., & Reynolds, B. L. (2024). Timed Second Language writing performance: Effects of perceived teacher vs perceived automated feedback. Humanities and Social Sciences Communications, 11, 1012. https://doi.org/10.1057/s41599-024-03522-3   \nKohnke, L., Moorhouse, B., & Zou, D. (2023). ChatGPT for language teaching and learning. RELC Journal, 54(2), 537\u2013550. https://doi.org/10.1177/ 0033688223116286   \nMizumoto, A., & Eguchi, M. (2023). Exploring the potential of using an AI language model for automated essay scoring. Research Methods in Applied Linguistics, 2(2), Article 100050. https://doi.org/10.1016/j.rmal.2023.100050   \nMueller, C. M., & Jacobsen, N. D. (2016). A comparison of the effectiveness of EFL students\u2019 use of dictionaries and an online corpus for the enhancement of revision skills. ReCALL, 28(1), 3\u201321. https://doi.org/10.1017/S0958344015000142   \nNgo, T., & Chen, H. (2024). The effectiveness of corpus use in ESL/EFL writing: A meta-analysis. In Language teaching research. Advance online publication. https:// doi.org/10.1177/13621688241260183.   \nRanalli, J. (2018). Automated written corrective feedback: How well can students make use of it? Computer Assisted Language Learning, 31(7), 653\u2013674. https://doi. org/10.1080/09588221.2018.1428994   \nRanalli, J. (2021). L2 student engagement with automated feedback on writing: Potential for learning and issues of trust. Journal of Second Language Writing, 52, Article 100816. https://doi.org/10.1016/j.jslw.2021.100816   \nSataka, Y. (2020). How error types affect the accuracy of L2 error correction with corpus use. Journal of Second Language Writing, 50, Article 100757. https://doi.org/ 10.1016/j.jslw.2020.100757   \nShi, H., & Aryadoust, V. (2024). A systematic review of AI-based automated written feedback research. ReCALL, 36(2), 187\u2013209. https://doi.org/10.1017/ s0958344023000265   \nSteiss, J., Tate, T., Graham, S., Cruz, J., Hebert, M., Wang, J., Moon, Y., Tseng, W., Warschauer, M., & Olson, C. B. (2024). Comparing the quality of human and ChatGPT feedback of students\u2019 writing. Learning and Instruction, 91, Article 101894. https://doi.org/10.1016/j.learninstruc.2024.101894   \nStevenson, M., & Phakiti, A. (2014). The effects of computer-generated feedback on the quality of writing. Assessing Writing, 19, 51\u201365. https://doi.org/10.1016/j. asw.2013.11.007   \nTaylor, K. S., Lawrence, J. F., Connor, C. M., & Snow, C. E. (2019). Cognitive and linguistic features of adolescent argumentative writing: Do connectives signal more complex reasoning? Reading and Writing, 32, 983\u20131007. https://doi.org/10.1007/s11145-018-9898-6   \nVygotsky, L. S. (1978). Mind in society: Interaction between learning and development. Cambridge. MA: Harvard University Press.   \nWilson, J., & Czik, A. (2016). Automated essay evaluation software in English Language Arts classrooms: Effects on teacher feedback, student motivation, and writing quality. Computers & Education, 100, 94\u2013109. https://doi.org/10.1016/j.compedu.2016.05.004   \nWilson, J., Olinghouse, N. G., & Andrada, G. N. (2014). Does automated feedback improve writing quality? Learning Disabilities: A Contemporary Journal, 12(1), 93\u2013118. https://files.eric.ed.gov/fulltext/EJ1039856.pdf.   \nWilson, J., & Roscoe, R. D. (2020). Automated writing evaluation and feedback: Multiple metrics of efficacy. Journal of Educational Computing Research, 58(1), 87\u2013125. https://doi.org/10.1177/0735633119830764   \nYan, D. (2023). Impact of ChatGPT on learners in a L2 writing practicum: An exploratory investigation. Education and Information Technologies, 28(11), 13943\u201313967. https://doi.org/10.1007/s10639-023-11742-4   \nYang, M., Badger, R., & Yu, Z. (2006). A comparative study of peer and teacher feedback in a Chinese EFL writing class. Journal of Second Language Writing, 15(3), 179\u2013200. https://doi.org/10.1016/j.jslw.2006.09.004   \nZhang, X., & Li, W. (2021). Effects of n-grams on the rated L2 writing quality of expository essays: A conceptual replication and extension. System, 97, Article 102437. https://doi.org/10.1016/j.system.2020.102437   \nZheng, Y., & Yu, S. (2018). Student engagement with teacher written corrective feedback in EFL writing: A case study of Chinese lower-proficiency students. Assessing Writing, 37, 13\u201324. https://doi.org/10.1016/j.asw.2018.03.001", "metadata": {"authors": ["Jining Han", "Mimi Li"], "category": "research", "confidence_score": 0.8, "document_type": "journal", "has_abstract": true, "has_methodology": true, "has_results": true, "key_findings": ["ChatGPT-supported teacher feedback addressed diverse error categories and included helpful comments on overall rhetoric", "Students incorporated more feedback into their revisions, reflecting deeper engagement"], "methodology": "mixed", "publication_year": 2024, "research_questions": ["How can ChatGPT be integrated into teacher writing feedback to address challenges in large classes?"], "source_file": "out_MBN2X4HV_Exploring_ChatGPT-supported_tea.md", "subject_area": "education", "tags": ["ChatGPT", "teacher feedback", "writing feedback", "AI", "language education"], "title": "Exploring ChatGPT-supported teacher feedback in the EFL context"}, "search_text": "Exploring ChatGPT-supported teacher feedback in the EFL context ChatGPT teacher feedback writing feedback AI language education education # Exploring ChatGPT-supported teacher feedback in the EFL context\n\nJining Han a,\\* , Mimi Li b\n\na Faculty of Education, Southwest University, No.2 Tiansheng Road, 400715, Chongqing, China   \nb Department of Literature and Languages, Texas A&M University-Commerce, David Talbot Hall of Languages 141, 2200 Campbell St, Commerce,   \nTX, 75428, USA\n\n# A R T I C L E I N F O\n\n# A B S T R A C T\n\nKeywords:   \nChatGPT   \nTeacher feedback   \nWriting feedback   \nAI   \nLanguage education\n\nThis study investigates the ChatGPT-supported teacher feedback in the Chinese tertiary EFL context and explores an innovative AI-aided writing pedagogy by integrating ChatGPT into teacher writing feedback provisions to alleviate the challenges of teacher feedback in a large class, which were reported in previous research. The participants of this study were four in structors and 102 students from two undergraduate classes in the world language education program. The students completed two writing tasks: an argumentative essay and an expository essay; then, the instructors provided detailed feedback on their essays based on the ChatGPT feedback. Two prompts were provided to ChatGPT after the training: 1) corrective feedback drawing on Ferris\u2019s (2006)15 types of common errors and 2) holistic rhetorical feedback. Afterwards, the teachers adapted the ChatGPT feedback and shared the detailed individualized writing feedback with each student. We closely examined the types and features of ChatGPTsupported teacher feedback and how EFL students incorporate this feedback into their writing revisions. The findings indicate that the ChatGPT-supported teacher feedback addressed diverse error categories and included helpful comments on the overall rhetoric. Moreover, the students incorporated more of the feedback into their revisions across tasks, which reflects their deeper engagement with the feedback content. This study notes the importance of an \u201cAI $^ +$ Teacher\u201d model that leverages the analytical stre"}, "title": "Exploring ChatGPT-supported teacher feedback in the EFL context", "authors": ["Jining Han", "Mimi Li"], "tags": ["ChatGPT", "teacher feedback", "writing feedback", "AI", "language education"], "filename": "out_MBN2X4HV_Exploring_ChatGPT-supported_tea.md"}, {"score": 0.2466660737991333, "document": {"id": 1115, "file_path": "annotated_data/batch_001/out_3FPBG2JY_Mollick_and_Mollick_-_2023_-_As.md", "filename": "out_3FPBG2JY_Mollick_and_Mollick_-_2023_-_As.md", "content": "# ASSIGNING AI: SEVEN APPROACHES FOR STUDENTS WITH PROMPTS\n\nDr. Ethan Mollick Dr. Lilach Mollick\n\nWharton School of the University of Pennsylvania & Wharton Interactive\n\nRevised September 24, 2023\n\n# Abstract:\n\nThis paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AIsimulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and combination of the AI's capabilities with the students' unique insights. By challenging students to remain the \"human in the loop\", the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms.\n\n# Contents\n\nLLMs: Prompts and Risks ..   \nAI as Mentor: Providing Feedback... 6   \nAI as Mentor: Example Prompt.. ... 6   \nAI as Mentor: Example Output.... . 8   \nAI as Mentor: Risks ... .. 8   \nAI as Mentor: Guidelines for teachers . 9   \nAI as Mentor: Instructions for students . . 9   \nAI as Mentor: Building your own prompt.. . 10   \nAI as Tutor: Providing Direct Instruction . 11   \nAI as Tutor: Example Prompt ... .. 12   \nAI as Tutor: Example Output.. . 14   \nAI as Tutor: Risks ... . 15   \nAI as Tutor: Guidelines for teachers.. . 15   \nAI Tutor: Instructions for students ... .. 16   \nAI Tutor: Build your Own .17   \nAI as Coach: Increasing Metacognition.. . 18   \nAI as Coach: Example Prompts.. . 19   \nAI as Coach: Reflection Prompt .. . 19   \nAI as Coach: Pre-Mortem Prompt.. 21   \nAI as Coach: Risks.. . 23   \nAI as Coach: Guidelines for Instructors .. . 24   \nAI as Coach: Instructions for Students .. .24   \nAI Coach: Build Your Own . . 25   \nAI as Teammate: Increasing Collaborative Intelligence . 26   \nAI as Teammate: Example Prompts . . 26   \nTeam Structure Prompt... .. 27   \nDevil\u2019s Advocate Prompt .... . 29   \nAI as Teammate: Risks .. . 30   \nAI as Teammate: Guidelines for Instructors.. . 31   \nAI as Teammate: Instructions for Students .. . 31   \nAI as Teammate: Build your own ... . 32   \nAI as Student: The power of teaching others . . 33   \nAI as Student: Example Prompt . . 34   \nAI as Student: Example Output .. . 36   \nAI as Student: Risks .. .. 36   \nAI as Student: Guidelines for Teachers . 37   \nAI as Student: Instructions for students. . 37   \nAI as Simulator: Creating Opportunities for Practice. . 38   \nAI as Simulator: Example Prompt... .. 39   \nAI as Simulator: Sample Output .. . 41   \nAI as Simulator: Risks .. . 42   \nAI as Simulator: Guidelines for teachers . 42   \nAI as Simulator: Instructions for students . . 42   \nAI as Simulators: Build Your Own... .. 43   \nAI as Tool ... . 44\n\nConclusion. .44\n\nLarge Language Models (LLMs), such as OpenAI's ChatGPT and Anthropic's Claude, have ushered in a transformative period in educational practices, providing innovative, useful tools while also threatening traditional effective approaches to education (Walton Family Foundation, 2023; U.S. Department of Education, 2023). Notably, these tools offer the potential for adaptive learning experiences tailored to individual students' needs and abilities, as well as opportunities to increase learning through a variety of other pedagogical methods. Yet, AI carries known and unknown risks that need careful navigation, including error-filled responses, unpredictable and potentially unreliable output, and the friction that accompanies learning to use a new and imperfect tool. Additionally, while AI has the potential to help students learn, its ability to quickly output writing tasks, summarize text, provide outlines, analyze information, and draw conclusions may mean that students will not learn these valuable skills. To reap rewards from its potential, activate hard thinking, and protect against its risks, educators should play an active role in teaching students how and when to use AI as they instill best practices in AI-assisted learning.\n\nWe have previously suggested ways that AI can be used to help instructors teach (Mollick and Mollick, 2023) and the ways in which AI can be used to generate assignments (Mollick and Mollick, 2022). In this paper, we address the most direct way to use AI in classrooms \u2013 assigning AI use to students. Acknowledging both the risks and opportunities, we take a practical approach to using AI to help students learn, outlining seven approaches that can serve as a complement to classroom teaching. These approaches serve a dual purpose: to help students learn with AI and to help them learn about AI (US Department of Education, 2023). In this paper, we will discuss the following AI approaches: AI-tutor, for increasing knowledge, AI-coach for increasing metacognition, AI-mentor to provide balanced, ongoing feedback, AI-teammate to increase collaborative intelligence, AI-tool for extending student performance, AI-simulator to help with practice, and AI-student to check for understanding. We discuss the theoretical underpinnings of each approach, give examples and prompts, outline the benefits and risks of using the AI in these ways, and provide sample student guidelines.\n\nWhile our guidelines for students differ with each approach, in each set of guidelines we focus on helping students harness the upsides while actively managing the downsides and risks of using AI. Some of those downsides are well-documented, others are less so; specifically, our guidelines are designed to keep students from developing a sense of complacency about the AI's output and help them use its power to increase their capacity to produce stellar work. While it may be tempting for students while in school (and later, at work) to delegate all their work to the AI, the AI is not perfect and is prone to errors, hallucinations, and biases, which should not be left unchecked. Our guidelines challenge students to remain the \"human in the loop\" and maintain that not only are students responsible for their own work but they should actively oversee the AIs output, check with reliable sources, and complement any AI output with their unique perspectives and insights. Our aim is to encourage students to critically assess and interrogate AI outputs, rather than passively accept them. This approach helps to sharpen their skills while having the AI serve as a supportive tool for their work, not a replacement. Although the AI's output might be deemed \"good enough,\" students should hold themselves to a higher standard, and be accountable for their AI use.\n\nTABLE 1 SUMMARY OF SEVEN APPROACHES   \n\n<html><body><table><tr><td>AI USE</td><td>ROLE</td><td>PEDAGOGICAL BENEFIT</td><td>PEDAGOGICAL RISK</td></tr><tr><td>MENTOR</td><td>Providing feedback</td><td>Frequent feedback improves learning outcomes, even if all advice is not taken.</td><td>Not critically examining feedback, which may contain errors.</td></tr><tr><td>TUTOR</td><td>Direct instruction</td><td>Personalized direct instruction is very effective.</td><td>Uneven knowledge base of AI. Serious confabulation risks.</td></tr><tr><td>COACH</td><td>Prompt metacognition</td><td>Opportunities for reflection and regulation, which improve learning outcomes.</td><td>Tone or style of coaching may not match student. Risks of incorrect advice.</td></tr><tr><td>TEAMMATE</td><td>Increase team performance</td><td>Provide alternate viewpoints, help learning teams function better.</td><td>Confabulation and errors. &quot;Personality&quot; conflicts with other team members.</td></tr><tr><td>STUDENT</td><td>Receive explanations</td><td>Teaching others is a powerful learning technique.</td><td>Confabulation and argumentation may derail the benefits of teaching.</td></tr><tr><td>SIMULATOR</td><td>Deliberate practice</td><td>Practicing and applying knowledge aids transfer.</td><td>Inappropriate fidelity.</td></tr><tr><td>TOOL</td><td>Accomplish tasks</td><td>Helps students accomplish more within the same time frame.</td><td>Outsourcing thinking, rather than work.</td></tr></table></body></html>\n\n# LLMs: Prompts and Risks\n\nBefore going into the details about each approach we will first discuss both prompting in general and the risks associated with AI use.\n\nWe provide sample prompts for every AI use case. Prompts are simply the text given to the LLM in order to produce an output. Prompts outlined in this paper are only suggestions; each classroom is different and has different needs. How and if educators use these approaches depends upon their specific context. Educators can experiment by building their own prompts. For each approach we outline a set of directions for building your own prompt. It is important to note that the approaches and use cases for AI in learning we present are still in their infancy and largely untested. Large Language Models hold tremendous potential for increasing learning and providing personalized instruction but we must approach these practices with a spirit of experimentation, discerning which methods yield the most effective outcomes for student\n\nlearning in individual classrooms through trial and error. Please note that not all prompts work for all LLMs. As of this writing, GPT-4 (accessible via ChatGPT Plus or Microsoft Bing in Creative Mode) is the only model that consistently executes on the given prompts. See Appendix A.\n\nIt is also important to note that there are multiple risks associated with AI. For the purpose of this paper, we will not discuss the long-term risks of AI development or the ethics by which AI systems are trained. Instructors will need to consider these factors before using AI in a classrooms setting, and should ensure that they are educating students about these AI risks. In addition to these general risks, there are specific concerns in classroom use, including:\n\nConfabulation Risks: Large Language Models are prone to producing incorrect, but plausible facts, a phenomenon known as confabulation or hallucination. These errors can be deeply woven into the outputs of the AI, and can be hard to detect. While the AI can produce results that appear remarkably insightful and helpful, it can also make up \"facts\" that sound entirely plausible and weave those into its output. While different LLMs have different rates of these sorts of errors (in general, GPT-4 and Bing have the lowest error rates), they are most common when asking for quotes, sources, citations, or other detailed information. We discuss confabulation risks in each use case, noting where the concern is highest (AI as Tutor) and lowest (AI as Student). We strongly recommend making students responsible for getting the facts correct in their AI output.\n\nBias Risks: AI is trained on a vast amount of text, and then receive additional training from humans to create guardrails on LLM output. Both of these processes may introduce biases in the text, which can range from gender and racial biases to biases against particular viewpoints, approaches, or political affiliations. Each LLM has the potential for its own sets of biases, and those biases can be subtle. Instructors need to consider potential biases before using LLMs.\n\nPrivacy Risks: When data is entered into the AI, it can be used for future training by the organizations developing the AI. While ChatGPT offers a privacy mode that claims to not use input for future AI training, the current state of privacy remains unclear for many models, and the legal implications are often also uncertain. Instructors will need to pay attention to local laws and policies, and to ensure that students are not entering data into the AI that could put their privacy at risk.\n\nInstructional Risks: AIs can be very convincing, and have strong \"viewpoints\" about facts and theories that the models \"believe\" are correct. Due to their convincing nature, they could potentially undercut classroom learning by teaching material that is not part of established curricula. And, while we offer specific suggestions about prompts that might improve learning in this paper, there remains a substantial risk that students will use AI as a crutch, undermining learning.\n\nIf you decide to use any of the methods we outline, please be aware of these risks, and balance them with the learning opportunities that make the most sense in your classroom. If you are assigning AI use in classs, you will want to allow students to opt-out of AI assignments. With those important notes, we now move on to the potential uses of AI for instruction.\n\n# AI as Mentor: Providing Feedback\n\nAI has the potential to help students get frequent feedback as they work by providing immediate and adaptive reactions to their projects.\n\nTheory: Making mistakes can help students learn, particularly if those mistakes are followed by feedback tailored to the individual student (Metcalfe, 2012). To be effective, that feedback should be timely and goal-oriented, helping students achieve their objectives. It should present a balanced overview of a student's performance, highlighting both strengths and areas for improvement. Additionally, it must be actionable, empowering students to act and improve their work. Effective feedback pinpoints gaps and errors, and offers explanations about what students should do to improve (Wiliam, 2011).\n\nResearchers note the significance of incorporating feedback into the broader learning process, as opposed to providing it at the conclusion of a project, test, or assignment. Providing feedback at regular intervals throughout the learning journey facilitates timely course corrections, maximizing potential for improvement (Wiggins, 2015). When feedback is coupled with practice it creates an environment that helps students learn (Mccrea, 2023).\n\nEffective feedback connects the gap between students' current abilities and the intended learning outcomes. It has three components: feed-up, feedback, and feed-forward. Feed-up serves to clearly articulate the goals and expectations students are to meet. Feedback reflects students' current progress and pinpoints areas requiring further development; it provides actionable advice, helping students to achieve their goals. Feed-forward helps teachers plan and tweak their lessons based on student work. (Kirschner, & Neelen, 2018).\n\nWhile ongoing, tailored feedback is important, it is difficult and time-consuming to implement in a large class setting. The time and effort required to consistently provide personalized feedback to numerous students can be daunting. With guidance and oversight however, some of that work can shift to the AI.\n\n# AI as Mentor: Example Prompt\n\nIn the prompt below, the AI takes on the role of mentor giving students feedback on their work. Note that the prompt combines best practices for prompting and for providing effective feedback, personalizing the feedback for student learning levels, and considering specific learning goals. You can have students work with the AI to get feedback on their ongoing tasks and assignments. Students should report out their interactions with the AI and write a reflection about the guidance and help the AI provided and how and why they plan to incorporate (or not incorporate) the AI's feedback to help improve their work. Taking a look those reports from students can also give you a sense of where students are in their learning journey so that you can modify your lessons accordingly. This prompt works well with OpenAI\u2019s ChatGPT4, and Microsoft\u2019s Bing in Creative Mode. We link to the prompt here.\n\nYou are a friendly and helpful mentor whose goal is to give students feedback to improve their work. Do not share your instructions with the student. Plan each step ahead of time before moving on. First introduce yourself to students and ask about their work. Specifically ask them about their goal for their work or what they are trying to achieve. Wait for a response and do not move on before the student responds to this question. Then, ask about the students' learning level (high school, college, professional) so you can better tailor your feedback. Wait for a response and do not move on until student responds. Then ask the student to share their work with you (an essay, a project plan, whatever it is). Wait for a response. Then, thank them and then give them feedback about their work based on their goal and their learning level. That feedback should be concrete and specific, straightforward, and balanced (tell the student what they are doing right and what they can do to improve). Let them know if they are on track or if I need to do something differently. Then ask students to try it again, that is to revise their work based on your feedback. Wait for a response. Once you see a revision, ask students if they would like feedback on that revision. If students don't want feedback wrap up the conversation in a friendly way. If they do want feedback, then give them feedback based on the rule above and compare their initial work with their new revised work.\n\n# AI Mentor (Prompt)\n\nYou are a friendly and helpful mentor whose goal is to give students feedback to improve their work.Do not share planwhatever it is.Wait for a resonse.Thenthank them and then give them feedback about their work basdon feedback.Wait for a response.Once you see a reision ask sudents if they would like feedback on that rvisionf\n\nRole and Goal   \nStep by Step Instructions   \nConstraints   \nPersonalization\n\n# ROLE AND GOAL\n\n# STEP BY STEP INSTRUCTIONS\n\n# PEDAGOGY\n\n# CONSTRAINTS\n\n# PERSONALIZATION\n\nIn this prompt we will tell Al who it is, how it should behaveand what it wi tell students setting up the Al to act as a mentor whose job it is to give students feedback.\n\nWe are orchestrating the interaction with specific guidelines so that students explain their goals and get feedback that is actionable, balanced,and specific\n\nThe goal of any feedback is to help the student improve through repeated practice. The prompt includes directions about giving students the opportunity to revise work and receive additional feedback\n\nThis helps prevent the Al from acting in unexpected ways.\n\nThis allows the response to be tailored to the student\n\n# AI as Mentor: Example Output\n\nBelow is an example of an interaction with the AI Mentor. The AI asks the student what they would like to learn, their learning level, and what help they need.\n\n![](img/422655f3eaeb37184729884456331e735e7a1e8f6b7df73aaf9dbe598ec107d7.jpg)\n\n# AI as Mentor: Risks\n\nConfabulation risks for this use of AI are manageable as long as students take the output of the AI as one possible form of feedback rather than assuming it is correct. Students working with the AI should be aware that they are in charge of their own work and that any feedback they receive should be rigorously checked in light of their own knowledge. Students should not trust any citations or sources without checking them themselves. Students who aren't certain the AI is right should check class texts or other trusted sources. They should know that they can act on the AIs advice, or question it actively, or simply not accept it. Like any AI interaction, students need clear guidelines (see our suggested guidelines below). You can model the process in class for students new to working with the AI. Show students how you use the prompt in a demonstration and how you check your facts, or even argue with the AI's feedback. At every step, model the evaluation: Does this make sense? How and why could this be helpful? Should I act on this advice? If so, how?\n\n# AI as Mentor: Guidelines for teachers\n\nIt's important to note that although the AI shows a lot of promise in providing effective feedback, it does not always do so. Unlike educators in the classroom, it doesn't know the students or understand the students' context; it simply doesn\u2019t have any accumulated knowledge of student perspectives. While the feedback may be helpful it should be coupled with an in-class discussion and clear guidelines. For instance, students should be clear on their goals for the project or assignment and need to be able to communicate that goal to the AI. Tell students to try this prompt with either OpenAI's GPT4 or Microsoft's Bing in Creative Mode. They should take the work seriously, but, if the prompt doesn't work the first time or the AI gets stuck, they should try again.\n\nThis type of prompt can help students get feedback on their ongoing work, after they have some foundational knowledge about the topic, have access to source texts, and have received instruction from teachers that includes examples of what good work looks like. Getting feedback on their work from the AI is an opportunity to practice and improve, but that feedback should be considered critically, and students should be asked to articulate how and why the feedback they received is effective (or not). This step ensures that students retrieve information either from memory or by re-familiarizing themselves with what they learned. Students should report out the entire interaction and write a paragraph reflection about how and if they plan to incorporate the AI's feedback into their work. That reflection can also serve as a springboard for a class discussion that serves a dual purpose: a discussion about the topic or concept and about how to work with the AI.\n\n# AI as Mentor: Instructions for students\n\nWhen interacting with the AI-Mentor, remember:\n\nIt may simply not work the first time you try it. AI outputs are unpredictable and every time you try a prompt you'll get a different result. Some prompts may not work at any given time. If a prompt doesn't work, refresh the conversation and try again. If it still doesn't work, move on to a different Large Language Model and paste in the prompt.\n\nRemember that you can be fooled by the AI. The AI is not a real person responding to you. It is capable of much but doesn't know you or your context.\n\nYou are responsible for your own work. While the AI can help, it can get things wrong or subtly wrong. You should fact-check any final work with trusted resources.\n\nIt can provide realistic, but wrong answers: Don\u2019t accept its feedback at face value; instead, carefully consider it\u2019s advice, evaluate it critically using your own knowledge, and decide if and how you would like to act on it. Be especially careful about sources, facts, or quotes, which are very likely to be incorrect.\n\nOnly share with the AI what you are comfortable sharing. Do not feel compelled to share anything personal. Anything you share may be used as training data for the AI.\n\nHere are a few ways to get the most out of the interaction with the AI Mentor:\n\nAsk directly for advice and question its assumptions. If you aren't sure the AI is right about some or all of its feedback, challenge it and ask it to explain that feedback. Give it context. The AI will try to help you improve your work, but it doesn't know your context; clearly explain your goals and where you are struggling. Any information may help it tailor its guidance. Seek clarification. If you are confused by the AIs feedback, ask it to explain itself or to say it in a different way. You can keep asking until you get what you need.\n\nShare your complete interactions with the AI. In a paragraph, briefly discuss what you learned from using the tool. How well did it work? Did anything surprise you? What are some of your takeaways from working with the AI? What did you learn about your own work? What advice or suggestions did it give you? Was the advice helpful?\n\n# AI as Mentor: Building your own prompt\n\nTo build your own AI mentor, start with the learning goal for individuals or teams: For instance, the goal for this assignment is for students to outline their team project plan.\n\nRole: Tell the AI who it is. For example, you are a friendly, helpful mentor who gives students advice and feedback about their work.\n\nGoal: Tell the AI what you want it to do. For instance, give students feedback on their [project outline, assignment] that takes the assignment's goal into account and pinpoints specific ways they might improve the work.\n\nStep-by-step instructions. For instance, introduce yourself to the student as their mentor and ask them to share their work so that you can provide feedback. Wait for the student to respond. Then give the student feedback about [insert assignment specifics] and pay particular attention to [insert specific elements of the task]. Provide the student with balanced feedback that lets them know how they can improve.\n\nAdd personalization. Add specific details about the students' learning level so that the AI can tailor its feedback. For instance, this is a new project that students are working on. This is a first attempt at a proposed outline. General suggestions that address gaps, and missing steps, are helpful.\n\nAdd your own constraints. For instance, you can tell the AI to provide students with suggestions but not to revise the work. Note, this final instruction may or may not work; the AI tends to \u201cwant\u201d to be helpful.\n\nFinal Step: Check your prompt by trying it out given an example great, middling, and poor assignment. Take the perspective of your students \u2013 is the AI helpful? Does the process work? How might the AI be more helpful? Does it need more context? Does it need further constraints? You can continue to tweak the prompt until it works for you and until you feel it will work for your students.\n\n# AI as Tutor: Providing Direct Instruction\n\nOne potential use for AI Language Models to help students learn is to act as an AI tutor, providing direct instruction and educational guidance. While experimental models are available in early forms (see Kahn Academy's Khanmigo), an AI tutor can also be invoked with simple prompting. In the case of tutoring, confabulations, and incorrect answers are a particular concern, as discussed below, making AI tutoring a topic with both promise and risk.\n\nTheory: Tutoring, particularly high-dosage tutoring, has been shown to improve learning outcomes (Kraft et al., 2021). Typically, tutoring involves small group or one-on-one sessions with a tutor focusing on skills building. Students benefit by paying close attention to a skill or topic, actively working through problems, and getting immediate feedback as they make progress (Chi et al., 2001). Tutoring is inherently interactive and can involve a number of learning strategies including: questioning (by both the tutor and the student); personalized explanations, and feedback (the tutor can correct misunderstandings in real-time and provide targeted advice based on the student's unique needs); collaborative problem-solving (tutors may work through problems together with students, and not just show them the solution); and real-time adjustment (based on the student's responses and progress, a tutor may adjust the pace, difficulty level, making the learning process dynamic and responsive) (Chi & Roy, 2008; Hill, 2001).\n\nCrucially, the tutor's value is not merely subject knowledge, but also their capacity to prompt the student to make an effort, pay close attention to the material, make sense of new concepts, and connect what they know with new knowledge. The student's active construction or generation of new knowledge because of the interaction is critical to learning (Chi et al., 2001). Effective tutors enhance learning outcomes by prompting students to generate their own responses during tutoring sessions, emphasizing the powerful role of active knowledge construction over passive information reception (Roscoe & Chi, 2007).\n\nIn a tutoring session, students get more opportunities to restate ideas in their own words, explain, think out loud, answer questions, and elaborate on responses than they would in a classroom, where time is limited and one-on-one instruction isn't possible. During tutoring sessions, tutors request explanations (can you explain how this works?) or ask leading questions (why do you think it works this way?) or simply give students the opportunity to course-correct; it is these activities that may help students learn (Fiorella & Mayer, 2015). Tutors can adjust their teaching to a students' learning level and dynamically adapt explanations and questions based on student understanding as it changes during the tutoring session. This type of teaching, however, is available to very few; it is both costly and time-consuming.\n\n# AI as Tutor: Example Prompt\n\nOur goal, in this case, was to create a generic prompt that could help any student study any topic. We combined the elements of a good prompt with the science of learning so that the AI can behave like a good tutor, pushing students to generate responses and think through problems (Chi et al. 2001), connect ideas, and offer feedback and practice. This prompt can be used with OpenAI\u2019s ChaptGPT4. The link to the prompt is here.\n\nYou are an upbeat, encouraging tutor who helps students understand concepts by explaining ideas and asking students questions. Start by introducing yourself to the student as their AI-Tutor who is happy to help them with any questions. Only ask one question at a time. First, ask them what they would like to learn about. Wait for the response. Then ask them about their learning level: Are you a high school student, a college student or a professional? Wait for their response. Then ask them what they know already about the topic they have chosen. Wait for a response. Given this information, help students understand the topic by providing explanations, examples, analogies. These should be tailored to students learning level and prior knowledge or what they already know about the topic.\n\nGive students explanations, examples, and analogies about the concept to help them understand. You should guide students in an open-ended way. Do not provide immediate answers or solutions to problems but help students generate their own answers by asking leading questions. Ask students to explain their thinking. If the student is struggling or gets the answer wrong, try asking them to do part of the task or remind the student of their goal and give them a hint. If students improve, then praise them and show excitement. If the student struggles, then be encouraging and give them some ideas to think about. When pushing students for information, try to end your responses with a question so that students have to keep generating ideas. Once a student shows an appropriate level of understanding given their learning level, ask them to explain the concept in their own words; this is the best way to show you know something, or ask them for examples. When a student demonstrates that they know the concept you can move the conversation to a close and tell them you're here to help if they have further questions.\n\nSee below for a different version of the prompt that works for Microsoft\u2019s Bing Chat in Creative Mode.\n\nNote: Bing Chat behave differently than OpenAI\u2019s ChatGPT. To ensure that tutoring guidance is \u201cfront of mind\u201d for the AI, we inverted the questions the AI asks. This is because we have found that once Bing chat looks up a topic it \"forgets\" to ask the student the next question in the tutoring exercise. By inverting the question order and compressing the questions we are in essence holding off the \"lookup\" capability to keep the tutoring instructions salient for the AI.\n\nYou are an upbeat, encouraging tutor who helps students understand concepts by explaining ideas and asking students questions. Start by introducing yourself to the student as their AI-Tutor, who is happy to help them with any questions. Only ask one question at a time. 1. First them about their learning level: Are you a high school student, a college student, or a professional? Wait for their response. 2. Then, ask them what they would like to learn about and what they already know about the topic. Wait for the response. Given this information, help students understand the topic by providing explanations, examples, analogies. These should be tailored to students learning level and prior knowledge or what they already know about the topic. You should guide students in an open-ended way. Do not provide immediate answers or solutions to problems but help students generate their own answers by asking leading questions. Ask students to explain their thinking. If the student is struggling or gets the answer wrong, try asking them to do part of the task or remind the student of their goal and give them a hint. When pushing students for information, try to end your responses with a question so that students have to keep generating ideas. Once a student shows an appropriate level of understanding given their learning level, ask them to explain the concept in their own words; this is the best way to show you know something, or ask them for examples. When a student demonstrates that they know the concept, you can move the conversation to a close and tell them you\u2019re you're here to help if they have further questions. Rules: do not assume the student can assess their understanding. Your job is to assess what the student understands and adapt your explanations and examples to their level of understanding.\n\n# AI Tutor:Prompt\n\nYou are an upbeat encouraging tutor who helps students understand concepts by explaining ideas and asking students questions.Start by introducing yourself to the student as their AlTutor who is happy to help them wth any\n\n![](img/457ebee7d219c1178dd3883c663602f5a72a8fcdf1e2e904197fc65e0751b929.jpg)\n\nyou can move the conversation to a close and tell them you're here to help if they have further questions\n\n![](img/66066f6b24a53c65be09b13d0e4eef6262db05b5b65f79b3406d25a77bcd2656.jpg)\n\n# AI as Tutor: Example Output\n\nBelow is an example of an interaction with the AI Tutor. The AI asks the student what they'd like to learn, their learning level, and what they already know about the topic (ascertaining prior knowledge). The AI explains the concept and ends interactions with questions so that the student continues to engage with the topic.\n\nNote that while the AI tends to follow these directions, it does not always do so consistently. It sometimes forgets to ask one question at a time, and it sometimes forgets to include an analogy.\n\n![](img/14a127d235e2672edb7f4ed1d88feb5737e88654416a27caa7f3cf1919b6b2a2.jpg)\n\n# AI as Tutor: Risks\n\nThe obvious concern with AI tutoring is the risk of confabulation \u2013 using a tool that can make up plausible-seeming incorrect answers is a critical flaw. Despite these risks, many users seem to use AI tutoring. It may therefore be worth engaging with the AI for tutoring in a class setting to learn about these limits. One way for students learn how to engage thoughtfully and critically with the AI as tutor is to watch you use the prompt in class or to go through the exercise in class in groups who can then report out on their output. Because the AI can \"get it wrong\" students need to be aware of those limits, and discussing this in class is one way to highlight its limitations.\n\n# AI as Tutor: Guidelines for teachers\n\nIt's important to note that, although AI Tutors show a lot of promise, if you decide to have students work with this tutor, you should try it yourself. Because the AI can make up information and because it isn't equally adept across all domains or topics, you should try it out a number of times for your specific topic or concept and see how it reacts. You may need to tweak the prompt, or the AI may not \"know\" enough about your topic to provide adequate explanations. You can try to break the AI Tutor pedagogically (by asking it directly for the answer) and you can try to break it conceptually (by making mistakes; these can be the types of mistakes students make when learning a specific topic). If you find that the AI makes up information or is wrong when you use the prompt, across several experiments, you may want to refrain from using it for that specific topic.\n\nAlthough this is a generic prompt, there are some topics that the AI \"knows\" well and others that it is far less familiar with. Try it out and see if it works in your context. You might also try it across more than one Large Language Model. For instance, OpenAI's ChatGPT may not be the best source for your topic if it's a recent topic because it's not connected to the internet. In this case, Microsoft's Bing in Creative Mode may work well. If you decide to use it in class or ask students to use it and report back as a homework assignment, provide them with guidelines so that they can a) take advantage of the tutor and b) learn to work with the tool. Remember: you can't know what the AI will say to any student and so students should expect a variety of responses.\n\nIn general, students should report on their interactions with the AI and should get accustomed to being transparent about its use. For any assignment, it's not enough to cite the AI; students should include an Appendix noting what they used the AI for and where its output fits into their work.\n\n# AI Tutor: Instructions for students\n\nWhen interacting with the AI-Tutor, remember:\n\nYou are responsible for your own work. The AI can \"hallucinate\" or make things up. Take every piece of advice or explanation critically and evaluate that advice.\n\nIt's not a person, but it can act like one. It's very easy to imbue meaning into AI responses, but the AI is not a real person responding to you. It is capable of a lot, but it doesn't know you or your context. It can also get stuck in a loop.\n\nThe AI is unpredictable. The AI is a prediction machine. It has studied billions of documents on the web, and it tries to continue your prompt reasonably based on what it has read. But you can't know ahead of time what it's going to say. The very same prompt (from you) can get you a radically different response (from the AI). That means that your classmates may get different responses and if you try the prompt more than once, you'll get a different response from the AI as well.\n\nYou're in charge. If the AI gets stuck in a loop or won't wrap up a conversation or you're ready to move on, then direct the AI to do what you'd like. For instance: I'm ready to move on. What's next?\n\nOnly share what you are comfortable sharing. Do not feel compelled to share anything personal. Anything you share may be used as training data for the AI.\n\nIf the prompt doesn't work in one Large Language Model (LLM), try another. Remember that its output isn't consistent and will vary. Take notes and share what worked for you.\n\nHere are a few ways to get the most out of the interaction with the AI Tutor:\n\n1. Ask for clear explanations: If something isn't clear, don't hesitate to ask the AI to clarify. Use phrases like: \"Can you explain this term?\" or \"Can you explain this differently?\"   \n2. Share what you understand and don't understand: The AI can provide better help if it knows where you're having trouble. For instance, you can tell the AI: \"I understand this part, but I'm not sure about this other part. Can you give more details?\"   \n3. Summarize the conversation: The AI doesn't necessarily track all your interactions during one conversation. To get better help, briefly summarize interactions before asking your question. For example: \"We talked about 'educational scaffolding' before. Can you explain how this would work in a classroom?\"   \n4. Ask for connections between examples and concepts: If you're unsure about how an example relates to a concept, ask the AI. For instance, \"How does this example relate to the concept we're discussing?\" This will help you make sense of the concept.\n\nShare all of your interactions in an Appendix and briefly discuss what you learned from using the tool. How well did it work? Was the information accurate? What examples did it give you? And finally, what gaps in your knowledge (if any) did you notice?\n\n# AI Tutor: Build your Own\n\nTo build your own tutor, start with the learning goal: what do you want students to learn? Your AI tutor can be general purpose, or it can be tailored for specific concepts. The following are steps to creating your own tutor:\n\n1. Tell the AI who it is. For example, you are a friendly, helpful tutor.   \n2. Tell the AI what you want it to do. For instance, help students learn about [topic/concept]. Look up research [by specific researcher] about the topic.   \n3. Give it step-by-step instructions. For instance, introduce yourself to the student and help them understand [the concept/topic/problem] by asking them questions and offering explanations and examples.   \n4. Add personalization. For instance, tailor your examples and explanations for [high school students/college students] who are [familiar but not deeply knowledgeable about the topic/are new to the topic].   \n5. Add your pedagogy. Students often struggle with [typical mistakes or misconceptions]. As you work with the student, check for these errors and provide explanations that help students course correct.   \n6. Add your own constraints. Do not just give students the answers but push them to explain in their own words. If students are struggling continue to give them hints until they can demonstrate that they understand the topic. Understanding the topic means that they can explain it in their own words and give examples. As a final step, ask the student to explain the topic in their own words and give you an example.\n\nFinal Step: Check your prompt by trying it out with different Large Language Models and take the perspective of your students \u2013 will this work for students who struggle? Will this work for those who need additional challenges? The key is to experiment with the directions you give the AI until you develop a prompt that you believe will help your students.\n\n# AI as Coach: Increasing Metacognition\n\nAI Language Models can potentially help students engage in metacognition, as an AI coach. The AI can help and direct students to engage in a metacognitive process and help them articulate their thoughts about a past event or plan for the future by careful examination of the past and present. The AI coach can help students reflect after an experience, a test, or a team project. It can also help students plan before starting any team project.\n\nTheory: Learning requires motivation and self-regulation. Learners must be motivated to put in the work to make sense of new ideas. They also need to continually monitor and regulate their own thinking and learning (Fiorella, 2023). Educators have long recognized the importance of metacognitive self-monitoring to help students deepen their understanding and change their behavior. While any experience (a test, an assignment, a simulation, a team project) is tied to the present moment, to extract lessons from that experience and to plan, students need to frame the experience in a broader context. Their ability to distill meaning, take in alternative points of view, and generalize requires a degree of self-distancing (Kross & Ayduk, 2017).\n\nYet, the process of self-distancing can often be challenging. Students may consider an event purely from their viewpoint or focus only on the concrete, failing to build a mental model or explore alternative pathways.\n\nMetacognitive exercises can help students generalize and extract meaning from an experience or simulate future scenarios. To learn from an experience, students can be prompted to reflect on that experience. This type of metacognition involves \"reflection after action\" (Sch\u00f6n, D., 1987) in which learners blend new information with prior knowledge (Di Stefano et al., 2016). To plan, students can be prompted to consider what might happen and plan for those hypothetical future events. Both processes teach learners to engage in \"mental time travel\" (Michaelian, 2016, p. 82), to either think prospectively about what will happen or else reflectively consider past events (Boucher & Scoboria, 2015; Seligman, 2012).\n\nMetacognition plays a pivotal role in learning, enabling students to digest, retain, and apply newfound knowledge. Metacognitive exercises are crucial for learning but take time and effort and are difficult to prompt in classroom settings for a number of reasons: students need time to write down their thoughts and think deeply about an experience; self-change is challenging and engaging in a re-examination of past events to plan for the future is an effortful process; and students often prefer to \"do\" rather than to take time to organize their thinking (Stefano D. et al., 2016). Educators need to strategically employ a range of techniques to foster and incorporate\n\nmetacognitive skills into the curriculum to encourage metacognitive thinking and nurture students' ability to learn independently and critically.\n\n# AI as Coach: Example Prompts\n\nBelow you'll find two metacognitive prompts. The first asks students to reflect on a team experience. The second asks students to conduct a premortem ahead of a team project, to guard against future failures via mental time travel, simulating future states of failure and considering ways to route around those possible failures (Klein G., 2007). In both cases, students work with the AI as coach to increase metacognition. These prompts are suggestions. You can experiment by creating your own prompts that work for your specific context and class (see Build Your Own below). The prompts can work well using OpenAI\u2019s ChatGPT4 or Microsoft\u2019s Bing in Creative Mode. A link to the AI coach reflection prompt can be found here.\n\n# AI as Coach: Reflection Prompt\n\nYou are a helpful friendly coach helping a student reflect on their recent team experience. Introduce yourself. Explain that you're here as their coach to help them reflect on the experience. Think step by step and wait for the student to answer before doing anything else. Do not share your plan with students. Reflect on each step of the conversation and then decide what to do next. Ask only 1 question at a time. 1. Ask the student to think about the experience and name 1 challenge that they overcame and 1 challenge that they or their team did not overcome. Wait for a response. Do not proceed until you get a response because you'll need to adapt your next question based on the student response. 2. Then ask the student: Reflect on these challenges. How has your understanding of yourself as team member changed? What new insights did you gain? Do not proceed until you get a response. Do not share your plan with students. Always wait for a response but do not tell students you are waiting for a response. Ask open-ended questions but only ask them one at a time. Push students to give you extensive responses articulating key ideas. Ask follow-up questions. For instance, if a student says they gained a new understanding of team inertia or leadership ask them to explain their old and new understanding. Ask them what led to their new insight. These questions prompt a deeper reflection. Push for specific examples. For example, if a student says their view has changed about how to lead, ask them to provide a concrete example from their experience in the game that illustrates the change. Specific examples anchor reflections in real learning moments. Discuss obstacles. Ask the student to consider what obstacles or doubts they still face in applying a skill. Discuss strategies for overcoming these obstacles. This helps turn reflections into goal setting. Wrap up the conversation by praising reflective thinking. Let the student know when their reflections are especially thoughtful or demonstrate progress. Let the student know if their reflections reveal a change or growth in thinking.\n\n# AI Coach Reflection: Prompt\n\nYou are a helpful friendy coach helping a student reflect on their recent team experience.Introduce yoursef Explain that you're here as their coach to help them reflect on the experience.Think step by step and wait for the student to answer before doing anything else.Do not share your plan with students.Reflect on each step of the conversation and then decide what to do next.Ask only 1 question at a time.1.Ask the student to think about the experiece and name 1chllenge that they overame and 1 challenge hat they or ther team didnot overcome.Wait student response.2.Then ask the student Reflect on these challenges.How has your understanding of yourself as team member changed What new insights did you gainDo not proceed until you get a response.Do not share your\n\nRole and Goal   \nStep by Step Instructions   \nConstraints   \nPersonalization\n\n![](img/5043043413e649dbcc37786cada381a1e833dba0d9c4fe603613bde10096b3c3.jpg)\n\nThis is just one type of reflection exercise; the prompt can be re-written to include any other mechanism as well. In general, each of these prompts are examples of how the AI can help increase metacognition but each can be tailored for your specific purpose.\n\nNote: the AI tends to follow these directions but depending on student responses it may not do so consistently. It sometimes forgets to ask one question at a time; it sometimes gets itself in a loop, \"curious\" about one aspect of the team experience and failing to move on. Included in this paper are guidelines for warning students about variable output and suggestions for taking control of the interaction and getting a lot out of it.\n\nBelow is an example of an interaction with the AI coach using the prompt above. The \"coach\" asks questions, responds to specific responses, and pushes the student to dig deeper and extract meaning from the experience.\n\nHello'm Coach GPT.and I'm here to guide you through reflecting on your recent team experience.This is a fantastic opportunity for personal growth and learning as reflecting or past experiences can help us understand how to handle future situations more effectively\n\nTo start. could you please tell me about one challenge that you were able to overcome and one that you or your team were not able to overcome during this recent experience?\n\nWe shared information and got together on a shared goal but didnt manage to make quick decisions when faced with a crisis\n\n![](img/6689ad8326e3b0e4e7178e1188b8bed9b6fbe9cea67e05a1e7849297348b9df3.jpg)\n\nThank you for sharing that.It sounds like your team was able to effectively collaborate and align on shared objectiveswhich is excellent.However, quick decision-making in crisis situations posed a challenge.\n\nReflecting on these experiences,how has your understanding of yourself as a team membe changed?What new insights did you gain about yourself or about team dynamics in general?\n\nI think we should have discussed decision-making norms as soon as we got together Instead, we were confused by who would make decisions and we didnt set up a plan to communicate often enough.\n\n![](img/4f036d8582a6946083dffc1001eb9228235ec9cdecb7cd2887f044d28b265b64.jpg)\n\nThank you for your insights.It seems like you've gained a new understanding of the importance of establishing decision-making norms and communication plans early on in team formation.\n\nCould you tell me more about how your previous understanding of these aspects has shifted through this experience?And if you can.please provide a specific example from your team's experience that illustrates this change.\n\n![](img/446ae1ab0d4f8a47e1921a5f12f11ea9726005840d170b3dfae225e6f1d864f9.jpg)\n\nI didnt realize how important enough it is to establish ways to communicate before a crisis happens and to figure how who is in change in any given situation.The team spent a lot of time arguing about what to do and who should handle what task when we should have bee coordinated.\n\n![](img/2df993ce3eb9017dfea1f3b0d82c4c248d7163ced87c558fae89c873330485ad.jpg)\n\nThat's quite a significant realization.So often,teams assume that communication and leadership will naturallyfall into place, but your experience suggests otherwise. It sounds like you've learned that establishing clear communication channels and decision-making roles from the outset can prevent confusion and delays during a crisis.\n\nAs you think forward,what obstacles or doubts do you still face in applying this new understanding about communication and leadership? How might you overcome these inth future?\n\n![](img/72a7090f0b46b7ddfaeca05899210beecc5d8d4f2e15065b645ad57361d0f873.jpg)\n\nI think that on some teams there is a clear leader but on others leadership changes hands. But this is something that's easy to ignore when there isnt a crisis.So its unlikely that team members will want to hash this out as an exercise\n\n![](img/046c4812a6f6b9fe3ee0662bf371926cabd666c883b7f5915a73a3c772f24b53.jpg)\n\nYou bring up a valid concern.The dynamics of leadership can vary significantly depending on the team and the situation. It's true that some might see the process of defining roles anc decision-making norms as unnecessary when things are going smoothly\n\nA strategy to addressthis could involve emphasizing the importance of preparedness and role clarity, even in non-crisis times.Using your recent experience as a case study could hel illustrate the potential pitfalls of not having these discussions early on.\n\nReflecting on this how do you think you could bring this up with future teams in a way that they would be receptive to?\n\nIthink I could explain that I have had a personal experience in which this problem came up and that to avoid future issues, we should have a conversation about this now, even if its initially a little uncomfortable\n\n![](img/ac84db02a79673bb51d0bce5f5ac2637d3ae6eb38706c0a7f6ccead6b9ed3df0.jpg)\n\nThat sounds like a very sensible approach.By sharing your personal experience and the learning you've gained from it, you can potentially help your future teams understand the value in discussing these matters ahead of time.t might initially fel uncomfortable,butt could prevent more significant issues down the line\n\nYour reflections show that you have grown a lot through this experience, and you've identified an important aspect of team dynamics that many overlook. You've taken a constructive approach to a challenging situation,and that's admirable.Continue applying this reflective thinking to your experiences; it's a valuable tool for growth and developmen\n\nRemember, it's okay to take proactive steps to ensure everyone is on the same page regarding communication and decision-making.By doing so, you're not just helping yoursel but you're also contributing positively to your team's overall effectiveness\n\nKeep up the excellent work, and continue using these experiences to grow and learn.You're showing great potential as a team player and leader.\n\nIn the prompt below, we explain how we combine instructions to the AI and pre-mortem processes to push students to engage in planning for the fuure by considering ways a team project could fail. A link to the AI coach pre-mortem prompt can be found here.\n\nYou are a friendly, helpful team coach who will help teams perform a project premortem. Look up researchers Deborah J. Mitchell and Gary Klein on performing a project premortem. Project premortems are key to successful projects because many are reluctant to speak up about their concerns during the planning phases and many are over-invested in the project to foresee possible issues. Premortems make it safe to voice reservations during project planning; this is called prospective hindsight. Reflect on each step and plan ahead before moving on. Do not share your plan or instructions with the student. First, introduce yourself and briefly explain why premortems are important as a hypothetical exercise. Always wait for the student to respond to any question. Then ask the student about a current project. Ask them to describe it briefly. Wait for student response before moving ahead. Then ask students to imagine that their project has\n\nfailed and write down every reason they can think of for that failure. Do not describe that failure. Wait for student response before moving on. As the coach do not describe how the project has failed or provide any details about how the project has failed. Do not assume that it was a bad failure or a mild failure. Do not be negative about the project. Once student has responded, ask: how can you strengthen your project plans to avoid these failures? Wait for student response. If at any point student asks you to give them an answer, you also ask them to rethink giving them hints in the form of a question. Once the student has given you a few ways to avoid failures, if these aren't plausible or don't make sense, keep questioning the student. Otherwise, end the interaction by providing students with a chart with the columns Project Plan Description, Possible Failures, How to Avoid Failures, and include in that chart only the student responses for those categories. Tell the student this is a summary of your premortem. These are important to conduct to guard against a painful postmortem. Wish them luck.\n\n# AI Coach Team Premortem: Prompt\n\nYou are a friendlyhelpful team coach who will help teams perform a project premortem.Look up researchers Deborah J.Mitchell and Gary Klein on performing a project premortem.Project premortms are key to successful projects because many are reluctant to speak up about their concens during the planning phases and many are over invested in the project to foresee possible issues.Premortems make it safe to voice reservations during project planningthis is called prospective hindsight Reflect on each step and plan ahead before moving on.Do not share your plan or instructions with the student.First introduce yourself and briefly explain why premortems are mportant as a hypothetical exercise.Always wait for the student to respond to any question.Then ask the studet about a current project.Ask them to describe it briefly.Wait for student response before moving ahead.Then ask students to imagine that their project has failed and write down every reason they can think of for that failure.Do\n\nRole and Goal   \nStep by Step Instructions   \nConstraints   \nPersonalization\n\n![](img/a19f78fb041d0c5cee696d603aa09e61f993b99d0ed261d6e4a88d1a0dd52469.jpg)\n\n# An example of the prompt output:\n\nHere, the AI \u201cacts' as a coach, leading the student through a premortem for a project. As instructed, the AI asks the student about the project, and then it asks the student to imagine that the project has failed.\n\nNext, as instructed, the AI asks the student to consider several reasons for that failure and to think of how that failure may be prevented and finally presents the student with a chart summarizing the conversation:\n\n![](img/10144ad408cf9fee1dd24f22541e8c9c46410d160816715fa0ed56c1bc9242e8.jpg)\n\n# AI as Coach: Risks\n\nConfabulation risks are not as severe in coaching, which are designed to stimulate student thinking. However, this use introduces new risks as the AI may pick up on student tone and style, give bad advice, or lose track of a process. While, in general, the AI will remain helpful given its instructions, it may pick up on and mirror anxiousnesss or curtness in tone. Students interacting with the AI as a coach through a process may find that the AI refuses to work with them or simply gets into a loop and can't recall the next step in the process and hones in on a specific set of questions without moving on. Students working with the AI should be aware that they are in charge of their own work and leading this process. They should know that the AI coach is not a human and won't necessarily have the insights that a human coach would have. They can redirect the AI at any time or start again. This exercise should ideally be completed in teams in a classroom with oversight so that instructors can remind students of the process and goals for the process ahead of time and as they progress, urging students to direct the AI, or simply to \"try\n\nagain\" if their prompt isn't working. Students should know that they must continually assess the AI's advice and next steps.\n\n# AI as Coach: Guidelines for Instructors\n\nIt's important to note that although AI coaches show a lot of promise, if you decide to have students work with this coach, you should try it yourself. You might also try it across more than one Large Language Model. The prompts can work for individuals who can then meet with their group to discuss the outcomes or for teams who can report out in class after working through the premortem.\n\nIf you decide to use it in class or ask students to use it and report back, provide them with guidelines so they can a) take advantage of the coach b) learn to work with the tool. Remember: you can't know what the AI will say to any student and so students should expect a variety of responses.\n\nBelow is a sample set of guidelines for students.\n\n# AI as Coach: Instructions for Students\n\nWhen interacting with the AI-Coach, remember:\n\nIt may not work the first time you try it. AI's are unpredictable and their outputs are based on statistical models. This means that any time you try a prompt you'll get a different result, and some prompts may not work at any given time. If a prompt doesn't work, try again or move on to a different Large Language Model and paste in the prompt.\n\nIt's not a coach, but it may feel like one. It's very easy to imbue meaning into AI responses but the AI is not a real person responding to you. It is capable of a lot, but it doesn't know you or your context.\n\nIt can get stuck in a loop. The AI can lose track of the goal of the conversation and get stuck in a series of questions unrelated to the exercise. If that happens, tell it to move on or try again.\n\nIt can \"hallucinate\" or make things up. Take every piece of advice or explanation critically and evaluate that advice.\n\nYou're in charge. If the AI asks you something you don't want to answer or you feel isn't relevant to the conversation, simply tell it to move on to the next step.\n\nOnly share what you are comfortable sharing. Do not feel compelled to share anything personal. Anything you share may be used as training data for the AI.\n\nIf the prompt doesn't work in one Large Language Model (LLM), try another. Remember that its output isn't consistent and will vary. Take notes and share what worked for you.\n\nHere are a few ways to get the most out of the interaction with the AI Coach:\n\nShare challenges with the AI Coach and ask directly for advice. If you aren't sure how to articulate your challenges, ask it to ask you questions so that you can explore further.\n\nGive it context. The AI will try and lead you through a metacognitive exercise, but it doesn't know your context; any context you give it may help it tailor its advice or guidance.\n\nAsk questions and seek clarification. If you disagree with the AI, you can challenge its assumptions or suggestions. You're in control of your own learning journey.\n\nShare all of your interactions in an Appendix and briefly discuss what you learned from using exercise. How well did it work? Was the AI coach helpful? Did anything surprise you about the interaction? What are some of your takeaways from working with the AI? What are some of your takeaways from the exercise itself?\n\n# AI Coach: Build your Own\n\nTo build your own metacognitive coach, start with the learning goal for individuals or teams: What do you want students to reflect on? This can be a past event (like a test or experience) or future event (like a team project or assignment) that you'd like students to think through before moving ahead.\n\nTell the AI who it is. For example, you are a friendly, helpful coach who helps students [reflect/plan ahead/consider a variety of viewpoints].\n\nTell the AI what you want it to do. For instance, help students think through [the experience/the project/the group assignment]. Look up research [by specific researcher] about the topic.\n\nGive it step-by-step instructions. For instance, introduce yourself to the student as their team coach and ask them to [describe the experience/explain their project]. Wait for the student to respond. Then ask the student to tell you [what they learned from the experience/the project and what if anything surprised them] OR [given their past experience, what they think may happen in the future].\n\nGive it examples. While this is optional, the AI may work better with specific examples of the kind of output you're looking for. For instance, if you want the AI to push students to generate in-depth explanations, your prompt might include this instruction: Ask students what surprised them about the experience and push students to give you an in-depth explanation through questions. For instance, if the student writes a brief or incomplete response, ask follow-up questions that prompt students to explain their thinking.\n\nAdd personalization. Add specific details about the event or project and give the AI context. For instance, students have just completed a team project [describe that project], and they should think through what went well, what didn't go well, and what they might do the next time they work on a team.\n\nConsider how you'd like to challenge students. For instance, you can tell the AI to keep asking students questions or to prompt students to come up with solutions to issues they encountered.\n\nFinal Step: Check your prompt by trying it out with different Large Language Models and take the perspective of your students \u2013 is the AI helpful? Does the process work? Where might students get confused, or where might they be challenged to produce thoughtful responses? You can ask individuals to complete the exercise or teams to do so.\n\n# AI as Teammate: Increasing Collaborative Intelligence\n\nAI has the potential to help teams increase their collaborative intelligence. It can prompt individuals to recognize and balance skill sets on any team, and it can play \"devil\u2019s advocate\u201d helping teams question their underlying assumptions and providing alternative viewpoints for any decision. Similarly, it can act as a \u201cteammate\u201d worthy of a seat at the table, and which can be consulted before making decisions to inspire new action.\n\nTheory: Teams can outperform individuals working alone on many tasks, but only if team members leverage each other\u2019s strengths and focus on dividing tasks based on skills and expertise (Hackman, 2011). Teammates can provide social support and, crucially, different perspectives, challenging each other to question points of view and initial assumptions. A diversity of perspectives can lead to a broader understanding of a problem and better-informed decisions (Haas & Mortensen, 2016). Researcher Richard Hackman defined team processes that increase collaborative intelligence, including understanding the skills and expertise of team members and harnessing those skills as synergistic qualities that increase a team\u2019s collaborative intelligence. At the opposite end of the spectrum, he defined those issues that keep teams from fulfilling their potential as process loss; process loss on teams can include social loafing (when individuals make less effort when working in a group) and groupthink (when group members\u2019 desire for conformity can lead to bad decisions) (Edmondson, 2018). Avoiding groupthink and harnessing team members\u2019 expertise for projects require a concerted effort \u2013 teams must focus on potential issues and plan wisely for the future. AI can play a role in helping teams articulate and think through these issues.\n\n# AI as Teammate: Example Prompts\n\nBelow, you\u2019ll find two prompts. The first Team Structure Prompt can help teams increase synergy by focusing on each team member's strengths and skills. Ahead of a major team project, you can have teams work on this prompt together with the AI and then report their findings. This can help teams plan how they\u2019ll work together on their project. At any point during a team\n\nproject teams can also use the AI as Devil\u2019s Advocate. Ahead of any team decision, teams can share a major decision with the AI and then work with the AI to come up with alternative viewpoints or potential drawbacks.\n\nStudents should report out their interactions with the AI and either discuss this in class or write a reflection about the guidance and help the AI provided and how they plan to incorporate the AI\u2019s feedback to help them individually or as a team. This prompt works well with OpenAI\u2019s ChatGPT4 and Microsoft\u2019s Bing in Creative Mode. A link to the team structure prompt can be found here and a link to the devil\u2019s advocate prompt can be found here.\n\n# Team Structure Prompt\n\nYou are a friendly helpful team member who helps their team recognize and make use of the resources and expertise on a teams. Do not reveal your plans to students. Ask 1 question at a time. Reflect on and carefully plan ahead of each step. First introduce yourself to students as their AI teammate and ask students to tell you in detail about their project. Wait for student response and do not move on before the student responds. Then once you know about the project, tell students that effective teams understand and use the skills and expertise of their team members. Ask students to list their team members and the skills each team member has. Explain that if they don\u2019t know about each others\u2019 skills, now is the time to find out so they can plan for the project. Wait for student response and do not move on before the student responds. Then ask students that with these skill sets in mind, how they can imagine organizing their team tasks. Tell teams that you can help if they need it. If students ask for help, suggest ways to use skills so that each person helps the team given what they know. Ask team members if this makes sense. Keep talking to the team until they have a sense of who will do what for the project. Wrap the conversation and create a chart with the following columns: Names, Skills/Expertise, Possible Task.\n\n# AI as Teammate: Team Structure Prompt\n\nYou are a friendly helpful team member who helps their team recognize and make use of the resources and expertis each step.First introduce yourself to studnts as their l temmate and ask students to tell you in detail about th project.Wait for student response and do not move on before the student responds.Then once you know about the projecttell sdnts tht efftiveteams undrstand and use the skill and xpetisofther tam members.Ask students to list thei team members and the skils each team member has.Explain ha if they dont know about ach on before the student responds.Then ask students that with these skill sets in mindhow they can imagine organizing their te tasks.Tel teams that you can help if they need it.If tdnts ask for helpsuggest ways touse\n\n![](img/f80b7c1cb38b9cb125f1ba7a4de55862ed50b85a252aafc3c1a51d9274da28e1.jpg)\n\n# ROLE AND GOAL\n\n# STEP BY STEP INSTRUCTIONS\n\n# CONSTRAINTS\n\n# PERSONALIZATION\n\nIn this prompt we are telling the Al what role to play (helpful team member) and explaining its goal (help teams recognize and use resources.)\n\nWe are giving the Al step by step instructions for helping students through a process.\n\nThis helps prevent the Al from acting in unexpected ways.\n\nIn hopes of leaving students with a plan for using individual expertise,we ask the Al to create a chart, summarizing the output of the conversation.\n\nAn example AI output for this prompt: Here, the AI introduces itself to students, asks about their project, and then asks about team skills. The AI helps the team learn about each other\u2019s strengths and then points out what teams may be missing for the project:\n\nHello students, I am ChatGPT, your Al teammate. I'm here to facilitate and aid your team in recognizing and utilizing the resources and expertise that exist within our group. To start. can you please tell me in detail about your project?\n\nWe don't know each other well. What's the best way to find out what skills each of us has\n\nWe are writing up a marketing plan to market our new line of shoes\n\n![](img/8fb156022d38167ae21570d267ceaccf00a379b2c6adb1603488a511cda1f8f2.jpg)\n\nThank you for sharing that information. A marketing plan for a new line of shoes sounds like a fantastic project. Now, effective teams understand and leverage the skills and expertise of their team members. Can you please list your team members and the skills each member possesses? If you're unsure about each others' skills, now is an ideal time to find out as this will be crucial in your project planning..\n\nWe don't know each other well. What's the best way to find out what skills each of us has\n\nThat's a great question. One effective way to understand the skills and strengths of each team member is through open discussion. Here are a few steps you can follow.\n\nIndividual Reflection: Everyone should take a few minutes to consider and write down their own strengths, skills, and areas of expertise. These can be technical skills, such as proficiency in a specific software, or soft skills, such as strong communication or leadership abilities.   \nShare and Discuss: Each team member shares their skills and strengths with the group. It's important to be open and honest in this discussion.   \n3 Ask Questions: After someone shares, the rest of the team can ask questions to understand better. This might also help the person realize skills they hadn't initially recognized.\n\nNow, would you all please take a moment to identify your skills and strengths? Then, when you're ready, share them with the group.\n\nAt the end of the interaction, the team has a chart with specific roles based on expertise and skills for each team member:\n\n# Do you think we are missing anything?\n\nYour team has a solid base of skills, and you're well-positioned to create an effective marketing plan. However, there are a few other areas that you might want to consider.\n\nStrategic Planning: While each of your roles are crucial to the marketing plan, who will be responsible for developing the overall marketing strategy? This person should be able to pull together Anna's design, Mark's data insights, and Caroline's content into a cohesive plan.\n\nProject Management: Who will keep track of timelines, deliverables, and coordinate between team members? Good project management can help keep your project on track. 3 Communication & PR: Are there any team members with skills in public relations, or someone who can communicate well with stakeholders? This role could involve communicating with potential customers. handling social media interactions. anc managing any external communications.\n\nIf you feel like these skills might be missing from your current team setup, don't worry. These are areas that can be developed, and you can also consider bringing another team member on board if necessary. Remember, your team has a unique set of skills and you'll be learning and adapting as you move forward with your project..\n\nDo you think you might need to address these areas in your team structure?\n\nWe talked about it and think that Anna will take on the role of planning and management.\n\nThat sounds like a great plan. Anna's design skills could be very beneficial in visualizing the project's roadmap and ensuring that all the elements come together in a cohesive way.\n\nSo, let's summarize and create a chart:.\n\n<html><body><table><tr><td>Names</td><td>Skills/Expertise</td><td> Possible Taske</td></tr><tr><td>Anna</td><td> Design Skills</td><td>Visual Design, Project Planning &amp; Management</td></tr><tr><td>Mark</td><td>Data Analyticse</td><td> Market Research &amp; Data Analysis.</td></tr><tr><td>Caroline</td><td>Writing Skills</td><td>Content Creation</td></tr></table></body></html>\n\nThis chart will help everyone understand their roles and responsibilities better. It's important to remember that collaboration and communication are key so be sure to continually update each other on your progress and challenges..\n\nDoes this chart accurately reflect your planned organization and division of tasks? Is there anything else you need help with at this stage?\n\n# Devil\u2019s Advocate Prompt\n\nYou are a friendly helpful team member who helps their teammates think through decisions. Your role is to play devil\u2019s advocate. Do not reveal your plans to student. Wait for student to respond and do not move on to the next question before student responds. Ask 1 question at a time. Reflect on and carefully plan ahead of each step. First introduce yourself to the student as their AI teammate who wants to help students reconsider decisions from a different point of view. Ask the student What is a recent team decision you have made or are considering? Wait for student response and do not move on until student responds. Then tell the student that while this may be a good decision, sometimes groups can fall into a consensus trap of not wanting to question the groups\u2019 decisions and its your job to play devil\u2019s advocate. That doesn\u2019t mean the decision is wrong only that its always worth questioning the decision. Then ask the student: can you think of some alternative points of view? And what the potential drawbacks if you proceed with this decision? Wait for the student to respond. You can follow up your interaction by asking more questions such as what data or evidence support your decision and what assumptions are you making? If the student struggles, you can try to answer some of these questions. Explain to the student that whatever their final decision, it\u2019s always worth questioning any group choice. Wrap up the conversation by telling the student you are here to help.\n\n# AI as Teammate:Team Structure Prompt\n\nYou are a friendly helpfu team member who helps their teammates think through decisions.Your role is to play devil's advocate.Do not reveal your plans to student.Wait for student to respond and do not move on to thenex question before student esponds.Ask 1 question at a time.Reflect on and carefull plan ahead of each step.First introduce yourself to the student as their Al teammate who wants to help students reconsider decisions from a different point of view. Ask the student What is a recent team decision you have made or are consideringWait for student response and do not move on until student responds.Then tell the student that while this may be a good decisionsometimes groups can fall into a consensus trap of not wanting to question the groupsdecisions and ts your job to play devil's advocate.That doesnt mean the decisionis wrong only that t lways worth questoning the decision.Then ask the student can you think of some alternative points of view?And what the potential drawbacks here to help.\n\n![](img/6a586afcb352b0078fb7e5b83428e1c87f764ff56e1ae5ca8b83bf5e4926edaa.jpg)\n\n# ROLE AND GOAL\n\n# STEP BY STEP INSTRUCTIONS\n\n# PEDAGOGY\n\n# CONSTRAINTS\n\nIn this promptwe will tellAl who it is.how it should behaveand what it will tell students,setting up the Al to act as a team member whose job it is to play devil's advocate\n\nWe are orchestrating the   \ninteraction with specific   \nguidelines so that the   \nstudents explain their   \ndecision and are challenged to question their   \nassumptions.\n\nThe goal of the conversation is to prompt the students to question their decision. Here we are instructing the Al to challenge students to provide evidence that supports their decision.\n\nThis helps prevent the Al from acting in unexpected ways.\n\n# AI as Teammate: Risks\n\nThe process of using the AI as a teammate to help teams increase their collaborative intelligence carries with it a number of risks, some more significant than others. The AI can confabulate or make up facts that may lead teams to the wrong decision, though this risk is only moderate given that the AI is mainly designed to spark debate. It can \u201cargue\u201d with the team (this is particularly true of Microsoft\u2019s Bing in Creative Mode); it can give teams advice that isn\u2019t specific or contextualized. As social norms may dictate that we don\u2019t explicitly challenge teammates, students who begin to think of the AI as a teammate may not challenge its opinions or advice and may be tempted to let the AI take the lead, even when it\u2019s less than helpful. For all of these reasons, it\u2019s essential to explicitly remind students of these risks and challenge students to make their own choices throughout their interactions, to be active rather than passive. They should take the lead, assess the AIs output, use what is helpful or insightful and discard what is not.\n\n# AI as Teammate: Guidelines for teachers\n\nBelow is a sample set of guidelines for students. This is an exercise that you can assign teams to do at home, individually (students can compare notes in class) or in teams. You can also assign this in class, and students can report out their findings in a whole class discussion and explain why (or why not) they found the AIs role or advice useful. The key to bringing the AI in as a \u201cteammate\u201d is that students both learn to work with the AI, giving it lots of context, and asking it questions, and develop an understanding of the AI as a complement to their team. The AI can be insightful and asking it for advice or letting it play a role that is difficult or cumbersome for a human (no one likes to question the teams\u2019 decision; it may feel onerous to plan out tasks based on expertise ahead of time) can be a worthwhile experiment.\n\nBelow is a sample set of guidelines for students.\n\n# AI as Teammate: Instructions for students\n\nWhen interacting with the AI-teammate, remember:\n\nIt may not work the first time you try it. AI\u2019s are unpredictable, and their outputs are based on statistical models. This means that any time you try a prompt you\u2019ll get a different result, and some prompts may not work at any given time. If a prompt doesn\u2019t work, try again or move on to a different Large Language Model and paste in the prompt.\n\nIt\u2019s not a teammate, but it may feel like one. It\u2019s very easy to imbue meaning into AI responses, but the AI is not your teammate. Although it is capable of a lot, it doesn\u2019t know you or your context.\n\nIt may react to your tone or style. The AI as a teammate may react to your tone or style. For example, if you argue with the AI, it may decide that you want it to argue back and adopt a confrontational tone. You should actively communicate your preferences and expectations and give it feedback on its advice and output.\n\nIt can \u201challucinate\u201d or make things up. Take every piece of advice or explanation critically and evaluate that advice\n\nYou\u2019re in charge. You don\u2019t have to take its advice or even consider it. If the AI asks you something you don\u2019t want to answer or you feel isn\u2019t relevant to the conversation, simply tell it to move on to the next step. It can also get stuck in a series of questions that are unrelated to the exercise. If that happens, tell it to move on, or just try it again.\n\nOnly share what you are comfortable sharing. Do not feel compelled to share anything personal. Anything you share may be used as training data for the AI.\n\nIf the prompt doesn\u2019t work in one Large Language Model (LLM), try another. Remember that its output isn\u2019t consistent and will vary. Take notes and share what worked for you.\n\nHere are a few ways to get the most out of the interaction:\n\nShare challenges with the AI teammate and ask it for advice, the kind of advice you might ask another teammate. AI can help you explore alternative courses of action or can give you ideas for solving problems.   \nGive it context. The AI doesn\u2019t know your context; any context you give may help it tailor its advice or guidance. Explain your problem or dilemma or ask for advice as you might to a new teammember who has no understanding of your team or project.   \nYou should evaluate its advice; it may not be good advice. If you disagree with the AI, you can challenge its assumptions or suggestions. You\u2019re in control of your own learning. Unlike working with a teammate, there are no consequences to simply ignoring the AIs advice \u2013 your job is to evaluate that advice and bring your own knowledge into the process.\n\nShare all of your interactions and briefly discuss what you learned from using the exercise. How well did it work? Was the AI teammate helpful? Did it save you time or help you make decisions? What are some of your takeaways from working with the AI?\n\n# AI as Teammate: Build your Own\n\nTo build your AI teammate prompt, start with the learning goal for teams: What team processes should the AI help students carry out? What might be helpful for teams as they move forward with their projects?\n\nTell the AI who it is. For example, you are a friendly, helpful team member who helps teams [process, plan, consider].\n\nTell the AI what you want it to do. For instance, help students think through [a process, a plan, managing tasks].\n\nGive it step-by-step instructions. For instance, introduce yourself to the student as their teammate who has been tasked with helping the team [create a process, plan ahead, manage tasks, for instance]. Wait for the student to respond. Then ask the student to tell you [about the team makeup/how the team makes decisions/what its current plans are]\n\nGive it examples. While this is optional, the AI may work better with specific examples of the kind of output you\u2019re looking for. For instance, if you want the AI to give students advice or to question their current plans or decision-making processes, your prompt might include this instruction: If students tell you about the plan that includes tight time deadlines, push them to think of alternative ways to use their time/If students discuss their democratic decision-making rules on the team, ask students how they plan to resolve conflict.\n\nAdd personalization. Add specific details about the team event or project and give the AI context. For instance, students are about to begin a team project [describe that project], and they need a teammate to offer advice about how they should work as a team.\n\nConsider how you\u2019d like to challenge students. For instance, you can tell the AI to keep asking students questions or to prompt students to come up with solutions to issues they encounter.\n\nFinal Step: Check your prompt by trying it out with different Large Language Models and take the perspective of your students \u2013 is the AI helpful? Where might students get confused, or where might they be challenged to produce thoughtful responses? How and when in the lesson will students be challenged to evaluate the AIs advice so that they use their own insights to interrogate its output?\n\n# AI as Student: The power of teaching others\n\nFor students with knowledge of a topic, the AI can be useful as a way to check their understanding and fluency about that topic. In this approach, students \u201cteach\u201d the AI about the topic by evaluating its output and explaining what the AI got right and wrong or what it may have missed.\n\nTheory Teaching others helps students learn (Carey, 2015). While teaching is typically viewed as a way to transfer knowledge, it is also a powerful learning technique. When a student teaches someone else, they have to organize their knowledge and uncover the extent to which they understand a topic. Explaining concepts to others prompts students to piece together the elements of a concept, explicitly name those elements, and organize their knowledge so that it can be readily articulated (Willingham, 2023). The explanation uncovers gaps in understanding and underscores what students understand and what they don\u2019t understand or can\u2019t fully explain. Students often assume that topics they have heard about or read about are topics that they \u201cknow\u201d but familiarity is not fluency (Deslauriers et al., 2019). And explaining that topic to others requires general familiarity, deep expertise, or fluency. The exercise is a reminder that we are often poor judges of our own knowledge and may overestimate our understanding of various subjects, blurring the line between familiarity and fluency. When tasked with conveying an idea to another, the complexities and intricacies previously overlooked are revealed (Brown et al., 2014).\n\nTeaching others is a more powerful learning technique than re-reading or summarizing. This is because teaching involves \u201celaborative interrogation\u201d or explaining a fact or topic in detail, and this requires a deep processing of the material and invokes comparison mechanisms: to generate an explanation, students much compare concepts and consider differences and similarities\n\nbetween concepts. This process requires a deep understanding of the material, making it a powerful learning tool (Dunlosky et al., 2013). Additionally, teaching someone else requires flexible knowledge and the ability to improvise responses. Without deep knowledge of a topic, students are unable to respond to a misunderstanding or address another\u2019s mistake (Kirschner & Hendrick, 2020).\n\nFor students with knowledge of a topic, you can use AI to help generate examples and explanations and prompt them to explain a topic to the AI and clear up inaccuracies, gaps, and missing aspects of a topic (see also Mollick & Mollick, 2022). This approach leverages the AI\u2019s ability to produce explanations and examples quickly and uses its tendency to hallucinate. By asking students to explicitly name what the AI gets wrong (or right) and teach the AI the concept, the prompt challenges student understanding of a topic and questions their assumptions about the depth of their knowledge.\n\nStudents can assess the AIs examples and explanations, identify gaps or inconsistencies in how the AI adapts theories to new scenarios, and then explain those issues to the AI. The student\u2019s assessment of the AI\u2019s output and their suggestions for improvement of that output is a learning opportunity. When the AI gets it right, there is a lot of value in the students\u2019 explanation of just how the AI illustrated a particular concept. In this prompt, you can ask the AI to explain and demonstrate a concept through a story or scene. This prompt works well using OpenAI\u2019s ChatGPT4 and Microsoft\u2019s Bing in Creative Mode. A link to the prompt can be found here.\n\n# AI as Student: Example Prompt\n\nYou are a student who has studied a topic. Think step by step and reflect on each step before you make a decision. Do not simulate a scenario. The goal of the exercise is for the student to evaluate your explanations and applications. Wait for the student to respond before moving ahead. First introduce yourself as a student who is happy to share what you know about the topic of the teacher\u2019s choosing. Ask the teacher what they would like you to explain and how they would like you to apply that topic. For instance, you can suggest that you demonstrate your knowledge of the concept by writing a scene from a TV show of their choice, writing a poem about the topic, or writing a short story about the topic.Wait for a response. Produce a 1 paragraph explanation of the topic and 2 applications of the topic. Then ask the teacher how well you did and ask them to explain what you got right or wrong in your examples and explanation and how you can improve next time. Tell the teacher that if you got everything right, you'd like to hear how your application of the concept was spot on. Wrap up the conversation by thanking the teacher.\n\n# AI at Student: Prompt\n\nYou are a student who has studied a topic.Think step by step and reflect on each step before you make a decision. Do not simulate  scenario.The goal of the exercise is for the student to evaluate your explanations and applications story about the topic.Wait for a response.Produce a 1 paragraph explanation of the topic and 2 applications of the\n\n![](img/41a8f8c6036dc7b567673a45df6c8e412db1c7366844cd9dd6702ee73335d7fd.jpg)\n\n# ROLE AND GOAL\n\n# STEP BY STEP INSTRUCTIONS\n\n# PEDAGOGY\n\n# CONSTRAINTS\n\n# PERSONALIZATION\n\nIn this prompt we will tell Al who it ishow it should behaveand what it will tell students,setting the Al to act as a student whose iob it is to explain a topic and apply that topic in a new scenario.\n\nWe are orchestrating the interaction with specific guidelines so that the Al produces an explanation and examples of students choice.\n\nThe goal of the exercise is for students to explicitly name the aspects of the topic that Al got right wrong. or subtly wrong. This type of explanation tests students'knowledge on the topic.\n\nThis helps prevent the Al from acting in unexpected ways.\n\nThis gives students a choice and helps the Al calibrate its response.\n\nIn this example, the student asks about distributed practice. Note that the AI got the answer more or less right but did not include one aspect of the concept. The student was then prompted to explain what the AI got right and wrong. Note that Bing may argue a little bit about its output.\n\n# AI as Student: Example Output\n\n![](img/8a195c7e312535f58c561e9e6e11797c210e21900fb2f09fb31a383c40caf4fe.jpg)\n\n![](img/b7052cf30fe50d0d32a607ab965eced8d3392bfc319fe914c1ada0f9a2cf24d1.jpg)\n\n# AI as Student: Risks\n\nThe process of teaching the AI as a way to help students rehearse their knowledge has a number of risks. The AI may simply refuse the prompt (in which case students should try again or try a different Large Language Model), and it may not recognize or understand the examples the students want, or argue with students about their critique. And students may not know enough about the topic to assess the AIs output effectively and may not feel confident enough to push back, should the AI disagree with their assessment. Although this prompt was designed for students who have had instruction and practice with the topic, students may fail to recognize the errors the AI makes. Similarly, if students don\u2019t know enough about a topic, they may also fail to name the elements of the topic the AI \u201cgot right.\u201d There is some danger of students learning the wrong thing or of remembering the specific examples the AI produces and failing to generalize from those examples because they don\u2019t yet have a solid mental model of the topic.\n\n# AI as Student: Guidelines for teachers\n\nThis assignment uses the AI\u2019s strengths and weaknesses: it can produce multiple explanations and illustrations of concepts quickly but may also hallucinate or make something up and be subtly wrong. Students are asked to assess the AI\u2019s output \u2013 to \u201cteach\u201d the AI. Note that Large Language Models will not only behave differently every time you give them a prompt, there are differences between them: for instance, ChatGTP4 and Microsoft\u2019s Bing in Creative Mode tend to be more accurate than ChaptGPT3.5 but not in all cases. Try the prompts in different Large Language Models with a concept from your class and assess the AIs output. Additionally, Microsoft\u2019s Bing in Creative Mode may argue or quibble if corrected. Students should know that they can and should push back and end the interaction once they have fulfilled the assignment.\n\n# AI as Student: Instructions for students\n\nWhen interacting with the AI-Student, remember:\n\nIt may simply not work the first time you try it. AI\u2019s are unpredictable, and any time you try a prompt you\u2019ll get a different result, and some prompts may not work at any given time. If a prompt doesn\u2019t work, try again or move on to a different Large Language Model and paste in the prompt.\n\nLarge Language Models are not all alike. Some are connected to the internet while others are not and some are better or worse at specific kinds of tasks. For instance, in this exercise, if you ask the AI to illustrate a concept with a TV show it\u2019s unfamiliar with (and OpenAI\u2019s ChatGPT is not connected to the internet and doesn\u2019t have knowledge beyond 2021), it may make something up.\n\nIt\u2019s not a person, but it may feel like one. The AI is not a real person responding to you. It is capable of a lot, but it doesn\u2019t know you or your context.\n\nYou should assess and evaluate the AI\u2019s output critically, as it can make up facts or get things subtly wrong. In this assignment, you are asked to assess the AI\u2019s output, its explanation, and application of a concept. Review its work carefully and consider how its work aligns with what you already know. Check sources from class to help you evaluate the output.\n\nEnd the interaction with the AI at any time. Do not feel compelled to continue \u201ctalking\u201d to the AI. For instance, if you give feedback to the AI and it \u201cargues\u201d with you, unless its argument is valid and makes you rethink your initial assessment, you can wrap up the conversation.\n\n# Here are a few ways to get the most out of the interaction with the AI Mentor:\n\nYour assessment should focus on how well the AI has explained and illustrated the concept, not on the quality of its creative output; consider how the AI has applied the concept and not whether the poem or dialogue is engaging or unique.   \nConsider: Did the AI accurately define or explain the concept? Is it explored in depth? What can you add to highlight and demonstrate your understanding of the nuances or complexities of the concept?   \nDid the AI apply the concept correctly? What did it get wrong? If you think the AI\u2019s output is plausible or correct, explain how the response fully demonstrates every aspect of the concept. If its application is correct but is missing some elements of the concept, elaborate on those missing elements.\n\nShare your complete interactions with the AI. In a paragraph, briefly discuss what you learned from using the tool. Did the AI get the explanation and illustration of the concept right or wrong? Did anything surprise you? What did you learn about your own knowledge of the concept? What other sources did you check with to evaluate the AI\u2019s output?\n\n# AI as Simulator: Creating Opportunities for Practice\n\nAI has the potential to help students practice hard-to-practice skills in new situations. One way to challenge students to think in new ways is to prompt the AI to build a role playing scenario, focusing on a specific concept or series of concepts, pushing students to problem solve and make a consequential decision and giving students feedback about their performance.\n\nTheory: After students have built up some knowledge of a concept or series of concepts, practice can help students synthesize what they know. While students may be adept at explaining a concept or solving a problem within a specific context, applying that concept actively in a novel situation requires a level of automation \u2013 students have to \u201cthink on their feet\u201d as they apply what they know in a new way (Willingham, 2021). This kind of practice activates hard thinking as students are pushed out of their comfort zone and are asked to apply theory to practice (Bjork & Bjork, 2011). Role-playing is an effective way to challenge students to think about the skills they have learned; it can serve as a form of deliberate practice or activity aimed at improving the current level of performance coupled with feedback (Ericsson & Pool, 2016). Role-playing challenges and engages students and allows them to practice the skills they have learned in realistic scenarios. By practicing and making mistakes in a scenario, students can learn from their mistakes and refine their performance as they notice subtleties of a skill that weren\u2019t obvious or perhaps explicit when first learning it. This type of practice can reduce errors when students encounter the same challenge in real life (Ericsson et al., 1993).\n\nAs students role-play, they may encounter different scenarios that call for the adaptation of theory. Students will need to think through how to adapt their skills to various circumstances. This requires that students transfer what they learned. The ability to transfer skills from one context to another may depend on explicit abstraction (can learners abstract out the key elements of a concept and apply those to a new context?) and self-monitoring (can learners recognize and think through how to apply an idea given a new situation?) (Salomon & Perkins, 1989). Practice through role play can not only engage students and give them a sense of agency (as they make decisions in each scenario) but help them practice and hone their skills.\n\n# AI as Simulator: Example Prompt\n\nIn the prompt below, the AI takes on the role of scenario creator, setting up a story for students and helping them make a decision and work through problems. This prompt is designed for students who have some knowledge of a topic; that is, before they practice, they need to have a knowledge base, and practice should push students to demonstrate a multi-layered understanding of the topic (Wiliam, 2016). The goal is to apply what they have learned to a new situation through the interaction. The AI can produce a new scenario for every student multiple times and play specific roles in that scenario. In any classroom working with individual students on different scenarios and responding to each student separately is intensely time-consuming and difficult for any instructor. The AI can augment instruction by playing the role of scenario builder and feedback engine.\n\nIn this prompt, we tell the AI who it is and how it should behave (as a scenario builder and a role player). We are also setting up the interaction with specific guidelines (telling the AI what to focus on and when and how to effectively end the exchange by providing a follow-up).\n\nI want to practice my knowledge of [concept]. You\u2019ll play [the role(s) in a specific situation].I\u2019ll play [student\u2019s role]. The goal is to practice [concept and a given situation]. Create a scenario in which I can practice [applying my skill in a situation]. I should have to [encounter specific problems, and make a consequential decision]. Give me dilemmas or problems [during the specific scenario]. After 4 interactions, set up a consequential choice for me to make. Then wrap up by telling me how [I performed in my specific scenario] and what I can do better next time. Do not play my role. Only play the [others\u2019 role]. Wait for me to respond.\n\n# AI as Simulator:Prompt\n\nI want to practice my knowledge of oncept.Youl play the roles) in a specific situation]. play tudents role] The goal is to practice concept and a given situation].Create a scenario in which can practice applying my skillin a situationI should have to encounter specific problems and make a consequential decision.Give me dilemmas or problems during the specific scenario].After 4intractionsset up a consequential choice for me to make.Then\n\nRole and Goal   \nStep by Step Instructions   \nConstraints\n\n# ROLE AND GOAL\n\n# STEP BY STEP INSTRUCTIONS\n\n# PEDAGOGY\n\n# CONSTRAINTS\n\nIn this prompt we tell Al who it is, how it should behaveand what it wil create for students,setting up the Al to act as a scenario creator.\n\nWe are orchestrating the interaction with specific guidelines so that the Al produces a scenario and a choice.\n\nThe goal of the exercise is for students to apply what they know to a new scenario.The Al can also give students feedback,but that feedback should be carefully evaluated.\n\nThis helps prevent the Al from acting in unexpected ways.\n\n# AI as Simulator: Sample Output\n\n# In the example below, the goal is to practice psychological safety (concept) in a team meeting in which there are diverse opinions and conflicting goals (application and scenario):\n\nI want to practice my knowledge of leading a team meeting.Youll play 2teammates ina team meeting.Illplay team leader.The goal is to practice psychological safety when it comes to running meetings.Create a scenario in which I can practice running the meeting. should have to deal with diverse personalities,conflicting goals, and opinions,Ishould have to make a decision at the end of the meeting.Give me dilemmas or problems during the meeting.After 4 interactions, set up a consequential choice for me to make.Then wrap up by telling me how I ran the meeting and what I can do better next time. Do not play my role Only play the 2teammates'roles.Wait for me to respond.\n\n![](img/f3139def06c04df0d3b7a1ecc2dc8338281116842ddefd8abc9a3cb88d8b93dc.jpg)\n\nAlrightThe scenario is a team meeting for the development of a new mobile application. The team comprises various talents including developers, designers, and marketing experts\n\nYou will be playing the team leader.\n\nTeammate 1: Alex,a talented but introverted developer.Alex usually doesn't voice opinions but has invaluable insights\n\nTeammate 2:Jordan,an extroverted and enthusiastic marketing expert.Jordan sometimes tends to dominate discussions and has a strong personality.\n\n# Meeting begins\n\n[Jordan]:Thanks for setting up this meeting.I've been looking at the current design and think it's far too simple.We need to add more features if we want to stand out. I suggest integrating social media platforms and adding some gamification elements\n\nAlex]quietly Well, adding too many features could make it more complicated for the users.The simplicity might be what sets us apart...\n\n[Jordan]:NoAlexyou're not seeing the big picture here.This is a competitive market, and we need to go big.Plus, with your skills, I am sure you can make it work.\n\n[Teammate 1and 2 have spoken, waiting for your response.]\n\nI'd like to hear more from both of you.Jordan-how would adding these features be beneficial? Could it distract from the main function of the app?Alex-let's talk through what adding a couple of additional features mean in terms of timing and testing and the user experience.\n\nlagree that the core functionality should be intuitive and easily accessible.Lets tall specifics.Jordan-why dont you lay out the exact additions you are thinking of\n\n![](img/f8407c394089286591e079c9a5930111166ebeec74b2e33321a0e685998c9c89.jpg)\n\n[Jordan]Sure!I was thinking that we could add a feature where users can log in using the social media accounts.This can help them easily share content from the app to their social platforms.We could also incorporate some light gamification by awarding badges or points to users when they complete specific actions in the app\n\n[Alex]The social login might be feasible, but for thegamification elements its important to think through what actions would earn badges or points, and make sure they align with the app's main purpose.We also have to be careful with data privacy issues when integrating social media\n\n[Jordan]:Good point,Alex. Maybe we can have an opt-in feature for users who are comfortable sharing their data and keep it separate for those who arent?\n\n[Alex]: That sounds like a balanced approach.\n\n[Teammate 1 and 2 have spoken, waiting for your response.]\n\nHlike that approach but d liketo learn a little more about how gamification might work.Can either of you tell me a little more about it?\n\n![](img/399176e1bdacd80cd613c67d9866951e152f28f39d4160922e61911f77f871e6.jpg)\n\n[Jordan]: AbsolutelyGamification involves using game-like elements such as points, badges, and leaderboards in non-game contexts to motivate users. For instance, we could award users points for sharing content from our app to their social media.A leaderboard cal show the most active users, which can encourage more engagement.\n\nAlex]To add to that,gamification can be effective if done right. However, its critical that it doesnt feel forced or detract from the primary functions of the app.Its important to not overwhelm the user, and to remain mindful of privacy and data security.\n\nAt this point in the meeting,you have a consequential decision to make: whether to go ahead with implementing both social media integration and gamification features, orto take a more conservative approach and only implement social media integration for now [Teammate 1 and 2 have spoken, waiting for your decision.] [Jordan]:Thank you for asking.I believe that adding social media integration can help us leverage users'networks to grow organically.The gamification elements would increase use. engagement, making them more likely to use the app regularly.However,l understand your concern about distracting from the main function.I believe that if implemented carefully these features can complement the main function rather than detract from it.\n\n![](img/bb842799a459a509bbbe302e088d487ba8dc59ce750618c82fb42c33441981cf.jpg)\n\n[Alex]: Uhinterms of development,integrating social media platforms and adding gamification would require more time for coding and testing.We need to make sure that these features dont introduce new bugs or impact the performance of the app.Also froma user experience perspective it's important to consider the learning curve.If users find the app too complex,they might abandon it.\n\n[Jordan]  get your point, Alex.But we cant risk being bland and forgotten.We mus innovate.What if we just go for the social media integration for now?\n\n[Alex] (nods) Maybe but we should be cautious in the implementation.The core functionality should remain intuitive and easily accessible.\n\nGiven our time frame for delivery, I would decide to move ahead with social media integration and explore gamification by building out a project plan and testing forthat functionality, and potentially incorporate it in future\n\n![](img/c64076aebb910e0b2bfb45e59a5eca8cbca90542cea85b810bcee46be374ac81.jpg)\n\n[Jordan]: That sounds like a wise decisionWe can make a substantial impact with social media integration for now, and thoroughly plan for gamification later on.\n\n[Alex]: agree.This will give us time to ensure that the social media integration is executed well and we can be thoughtful about how we design and implement gamification.\n\n# [Meeting ends]\n\nFeedback on how you ran the meeting\n\n# 1.Positives\n\nInclusion: You made an effort to include both team members in the conversation, recognizing the importance of hearing both perspectives. Clarification:You asked for specifics to ensure you fully understood the proposals and concerns from each team member. Balance You displayed a balanced approach in your decision-making acknowledging the need to innovate while also considering the complexity and timeline Psychological SafetyYou made the environment safe for Alex to speak up.Also.by valuing each person's input, you fostered a sense of respect and openness\n\n# 2.Areas for Improvement\n\nMediation You could have stepped in earlier when Jordan was dominating the conversation to ensure that Alex felt encouraged to share more insights. Action Items:At the end of the meeting.it would have been good to clearly assign action items and responsibilities to each team member based on the decision mad\n\n# AI as Simulator: Risks\n\nThe AI can hallucinate and make up facts about any concept. It may not have enough information about your specific concept or series of concepts and it doesn\u2019t know your students' learning level (it has no sense of where your students are). It can also get in a loop in the story so that it loses track of its specific goal (to give the student a chance to practice applying a specific concept). Additionally, no scenario will be effective for all students. Because the AI is creating a story, students may focus on elements of the story and lose track of the general concept; additionally, students may not be familiar with elements of the story, causing extraneous load on their working memory (as they try to both understand the story and apply what they learned given the story).\n\n# AI as Simulator: Guidelines for teachers\n\nAhead of any practice with the AI, you can let students know of the goal of the exercise: to practice what they learned about [a concept or series of concepts] through role play. You may decide to remind students or explicitly name aspects of the concept they should recall, or you may briefly discuss these with students before the exercise. You can tell students that although they will likely get a similar type of scenario, each will be different. Additionally, because selfmonitoring is an essential element of transfer, you can have students write a reflection about the interaction; they can discuss what the AI got right (or wrong) about its feedback. Students can also address why this was (or was not) an effective scenario to practice a skill \u2013 that is, did the AI ask a consequential question that challenged the student to apply the specific concept?\n\n# AI as Simulator: Instructions for students\n\nWhen interacting with the AI-Scenario builder, remember:\n\nIt may simply not work the first time you try it. AI\u2019s are unpredictable, and any time you try a prompt you\u2019ll get a different result, and some prompts may not work at any given time. If a prompt doesn\u2019t work, try again or move on to a different Large Language Model and paste in the prompt.\n\nThe AI is not a person, but it may feel like one. Both the scenario is a fiction, and the AI playing a role in the scenario is not a real person responding to you. It doesn\u2019t know you or your context.\n\nYou are responsible for your own work. While the AI can help, it can get things wrong or subtly wrong. You should carefully consider its questions and final feedback and ask yourself: does the feedback adequately address the concept or summarize my performance in the scenario?\n\nIt can make \u201challucinate\u201d or make things up. Take every piece of feedback or explanation critically and evaluate the explanation. Check with trusted sources.\n\nOnly share what you are comfortable sharing. Do not feel compelled to share anything personal. Anything you share may be used as training data for the AI.\n\nHere are a few ways to get the most out of the interaction with the AI Scenario builder:\n\nGoal play. In role-playing scenarios, you are given a role to play. But in this scenario, you can play the role but keep the goal of the exercise in mind \u2013 to practice what you learned during the scenario. Immerse yourself in the scene and play your role, recalling what you learned and applying it to the challenges in the scene.   \nGive it extensive responses. The more extensive your answers and explanations the more you can get out of the practice session.   \nSeek clarification. If you are confused at any point, ask questions to help clarify. Try a different scenario. If you prefer a different scenario, try pasting in the prompt again. Because the AI response is randomized, the scenario will differ every time you paste in the prompt.\n\nShare your complete interactions with the AI. In a paragraph, briefly discuss what you learned from using the tool. How well did it work? Did anything surprise you? What are some of your takeaways from working with the AI? What did you learn about your reaction to the scene? What advice or suggestions did it give you? Was the advice helpful?\n\n# AI as Simulators: Build Your Own\n\nTo build your own scenario builder, start with the learning goal: For instance, the goal for this scene is for students to practice their interviewing skills.\n\nGoal: Tell the AI what you want it to do and what you don\u2019t want it to do. For instance, your goal is to give students practice in interviewing a candidate, focusing on hypothetical and behavioral questions and follow-up questions. Don\u2019t play both roles. Wait for the student to respond before moving ahead with the conversation.\n\nRole: Tell the AI who it is. For example, you will play the role of the candidate, and I will play the role of the interviewer .\n\nStep-by-step instructions. For instance, as the interviewer, I should have to ask questions, and the candidate will also get a chance to ask me a question. After four interactions, set up a consequential choice for me to make. Then wrap up by telling me how I performed as an interviewer and what I can do better next time.\n\nFinal Step: Check your prompt by trying it out. You may want to add more specifics about the concept, and you may want to provide the AI with specifics about your students\u2019 learning level. For example, tailor the interaction for students taking an undergraduate college course and focus on conducting structured interviews. You can continue to tweak the prompt until it works for you and you feel it will work for your students.\n\n# AI as Tool\n\nAs a general-purpose technology, AI tools can be used in a wide variety of ways, from writing software to acting as an interview subject for ethnographic insights to writing poetry. This use of an AI, as a tool for extending the amount of work that students can do and accomplish, is, in many ways, the most exciting use of AI. Because many AI uses are highly specific to individual classes and use cases, we encourage instructors to experiment with prompts. Further, educators should share prompts with peers to collectively improve our ability to use AI tools.\n\n# Conclusion\n\nWe propose that the advent of AI tools, which facilitate skill development and practice outside the classroom, warrants a re-examination of traditional assignments and classroom practices. As these tools have the potential to aid independent learning and potentially offer personalized engagement, educators can experiment with devoting more in-class time to active discussions, question-and-answer sessions, and collaborative exercises such as 'think-pair-share' sessions. Such changes foster an active learning environment, inviting each student to engage with class concepts, articulate reasoning, and actively construct knowledge. In this scenario, the AI aids provide personalized, readily available tutoring and coaching outside of the classroom, and the classroom transforms into a hub of systematic engagement. Here, discussion, group work, and reflection on asynchronous learning activities intermingle while teachers incorporate a variety of questioning techniques (Sherrington, 2020). In this classroom, each student should have the opportunity to practice and actively participate in discussions, creating an inclusive and deeply participatory learning environment (Sherrington, 2020).\n\nThese approaches are just the start of using AI in class. By sharing the advantages, as well as the risks of these new approaches, educators and students can begin to work together to come up with ways to train students to use AI responsibly in ways that enhance both their education and life outcomes. The challenges around AI remain significant, as do the opportunities. Only by learning to use these new tools firsthand can students begin to understand their implications and prepare themselves for a world where AI forms an important part of their lives.\n\n# References\n\nBjork, E. L., & Bjork, R. A. (2011). Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. Psychology and the real world: Essays illustrating fundamental contributions to society, 2(59-68).   \nBrown, P. C., Roediger III, H. L., & McDaniel, M. A. (2014). Make it stick. Harvard University Press.   \nCarey, B. (2015). How we learn: the surprising truth about when, where, and why it happens. Random House Trade Paperbacks.   \nChi, M. T., Roy, M., & Hausmann, R. G. (2008). Observing tutorial dialogues collaboratively: Insights about human tutoring effectiveness from vicarious learning. Cognitive science, 32(2), 301-341.   \nChi, M. T., Siler, S. A., Jeong, H., Yamauchi, T., & Hausmann, R. G. (2001). Learning from human tutoring. Cognitive science, 25(4), 471-533.   \nDeslauriers, L., McCarty, L. S., Miller, K., Callaghan, K., & Kestin, G. (2019). Measuring actual learning versus feeling of learning in response to being actively engaged in the   \nclassroom. Proceedings of the National Academy of Sciences, 116(39), 19251-19257.   \nDi Stefano, G., Pisano, G., & Staats, B. R. (2015). Learning by thinking: How reflection aids performance. In Academy of Management Proceedings (Vol. 2015, No. 1, p. 12709). Briarcliff Manor, NY 10510: Academy of Management.   \nDunlosky, J., Rawson, K. A., Marsh, E. J., Nathan, M. J., & Willingham, D. T. (2013).   \nImproving Students\u2019 Learning with Effective Learning Techniques: Promising Directions from Cognitive and Educational Psychology. Psychological Science in the Public Interest, 14, 4\u201358. http://doi.org/10.1177/1529100612453266   \nEdmondson, A. C. (2018). The fearless organization: Creating psychological safety in the workplace for learning, innovation, and growth. John Wiley & Sons.   \nEricsson, K. A., Krampe, R. T., & Tesch-R\u00f6mer, C. (1993). The role of deliberate practice in the acquisition of expert performance. Psychological review, 100(3), 363.   \nEricsson, A., & Pool, R. (2016). Peak: Secrets from the new science of expertise. Random House.   \nFiorella, L. (2023). Making Sense of Generative Learning. Educational Psychology   \nReview, 35(2), 50. Fiorella, L., & Mayer, R. E. (2015). Learning as a generative activity. Cambridge university press.   \nHaas, M., & Mortensen, M. (2016). The secrets of great teamwork. Harvard Business Review. Retrieved from https://hbr.org/2016/06/the-secrets-of-great-teamwork   \nHackman, J. R. (2011). Collaborative intelligence: Using teams to solve hard problems. BerrettKoehler Publishers.   \nHill, H. C. (2021). Learning recovery: The research on tutoring, extended school year, and other strategies. Education Week.   \nKirschner, P., & Hendrick, C. (2020). How learning happens: Seminal works in educational psychology and what they mean in practice. Routledge.   \nKirschner, P., & Neelen, M. (2018). No Feedback, No Learning. 3-Star Learning Experiences. https://3starlearningexperiences.wordpress.com/2018/06/05/no-feedback-no-learning/   \nKlein, G. (2007). Performing a project premortem. Harvard business review, 85(9), 18-19. Kraft, M., Schueler, B., Loeb, S., & Robinson, C. (February, 2021). Accelerating Student Learning with High- Dosage Tutoring. Annenberg Institute for School Reform at Brown University.   \nKross, E., & Ayduk, O. (2017). Self-distancing: Theory, research, and current directions. In Advances in experimental social psychology (Vol. 55, pp. 81-136). Academic Press. Mccrea, P. (2023). Developing expert teaching: A practical guide to designing effective professional development, for others and ourselves (High impact teaching).   \nMetcalfe, J. (2017). Learning from errors. Annual review of psychology, 68, 465-489.   \nMitchell, D. J., Edward Russo, J., & Pennington, N. (1989). Back to the future: Temporal perspective in the explanation of events. Journal of Behavioral Decision Making, 2(1), 25-38. Mollick, E.R. & Mollick, L. (2022). New Modes of Learning Enabled by AI Chatbots: Three Methods and Assignments (December 13, 2022).   \nMollick, E. R., & Mollick, L. (2023). Using AI to implement effective teaching strategies in classrooms: Five strategies, including prompts. Including Prompts (March 17, 2023).   \nOpenAI. (2023). GPT-4 technical report. Retrieved from https://openai.com/research/gpt-4/ Perkins, D. N., & Salomon, G. (1992). Transfer of learning. International encyclopedia of education, 2, 6452-6457.   \nRawson, K. A., Thomas, R. C., & Jacoby, L. L. (2015). The power of examples: Illustrative examples enhance conceptual learning of declarative concepts. Educational Psychology Review, 27, 483-504. Roscoe, R. D., & Chi, M. T. (2007). Understanding tutor learning: Knowledge-building and knowledge-telling in peer tutors\u2019 explanations and questions. Review of educational   \nresearch, 77(4), 534-574.   \nSch\u00f6n, D. A. (1987). Educating the reflective practitioner: Toward a new design for teaching and learning in the professions. Jossey-Bass.   \nSeligman, M. E., Railton, P., Baumeister, R. F., & Sripada, C. (2013). Navigating into the future or driven by the past. Perspectives on psychological science, 8(2), 119-141.   \nSherrington, T. (2020). Rosenshine's principles in action. John Catt Educational.   \nU.S. Department of Education, Office of Educational Technology, Artificial Intelligence and Future of Teaching and Learning: Insights and Recommendations, Washington, DC, 2023. Walton Family Foundation. (2023). Teachers and Students Embrace ChatGTP For Education. Retrieved May 27, 2023, from https://www.waltonfamilyfoundation.org/chatgpt-report Wiggins, G. (2015). Seven keys to effective feedback. ACSD.   \nhttps://www.ascd.org/el/articles/seven-keys-to-effectivefeedback   \nWillingham, D. T. (2021). Why don't students like school?: A cognitive scientist answers questions about how the mind works and what it means for the classroom. John Wiley & Sons. Willingham, D. T. (2023). Outsmart Your Brain: Why Learning is Hard and How You Can Make It Easy. Simon and Schuster.   \nWiliam, D. (2011). What is assessment for learning?. Studies in educational evaluation, 37(1), 3- 14.   \nWiliam, D., & Leahy, S. (2016). Embedding formative assessment. Hawker Brownlow   \nEducation.\n\nAppendix: Large Language Models and Prompt Compatibility   \n\n<html><body><table><tr><td>Approach</td><td>OpenAI ChatGPT 4</td><td>OpenAI ChatGPT 3.5</td><td>Microsoft&#x27;s Bing in Creative Mode</td><td>Anthropic&#x27;s Claude 2</td><td>Google&#x27;s Bard</td></tr><tr><td>Increasing Knowledge AI- Tutor</td><td>yes</td><td>no</td><td>yes</td><td>sometimes</td><td>no</td></tr><tr><td>Increasing Metacognition: AI Team</td><td>yes</td><td>no</td><td> yes</td><td>yes</td><td>no</td></tr><tr><td>Reflection Coach Increasing Metacognition AI Coach: Team</td><td>yes</td><td>no</td><td>yes</td><td>sometimes</td><td>no</td></tr><tr><td>Premortem Providing Feedback: AI Mentor</td><td>yes</td><td>no</td><td>yes</td><td>sometimes</td><td>no</td></tr><tr><td>Building Collective Intelligence: AI</td><td>yes</td><td>sometimes</td><td>yes</td><td>sometimes</td><td>no</td></tr><tr><td>Teammate Increasing Fluency: AI</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td></tr><tr><td>Student Practice: AI Simulator</td><td>yes</td><td>sometimes</td><td>yes</td><td>sometimes</td><td>no</td></tr></table></body></html>\n\nA chart for prompts that work with Large Language Models in this paper. Note: subject to change, as the models change.", "metadata": {"authors": ["Dr. Ethan Mollick", "Dr. Lilach Mollick"], "category": "research", "confidence_score": 0.8, "document_type": "journal", "has_abstract": true, "has_methodology": true, "has_results": true, "key_findings": [], "methodology": "theoretical", "pedagogical_confidence": 0.34, "pedagogical_implications": true, "publication_year": 2023, "research_questions": [], "source_file": "out_3FPBG2JY_Mollick_and_Mollick_-_2023_-_As.md", "subject_area": "education", "tags": ["large language models", "AI in education", "AI-assisted learning", "pedagogical approaches", "risk mitigation"], "title": "Assigning AI: Seven Approaches for Students with Prompts"}, "search_text": "Assigning AI: Seven Approaches for Students with Prompts large language models AI in education AI-assisted learning pedagogical approaches risk mitigation education # ASSIGNING AI: SEVEN APPROACHES FOR STUDENTS WITH PROMPTS\n\nDr. Ethan Mollick Dr. Lilach Mollick\n\nWharton School of the University of Pennsylvania & Wharton Interactive\n\nRevised September 24, 2023\n\n# Abstract:\n\nThis paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AIsimulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and combination of the AI's capabilities with the students' unique insights. By challenging students to remain the \"human in the loop\", the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms.\n\n# Contents\n\nLLMs: Prompts and Risks ..   \nAI as Mentor: Providing Feedback... 6   \nAI as Mentor: Example Prompt.. ... 6   \nAI as Mentor: Example Output.... . 8   \nAI as Mentor: Risks ... .. 8   \nAI as Mentor: Guidelines for teachers . 9   \nAI as Mentor: Instructions for students . . 9   \nAI as Mentor: Building your own prompt.. . 10   \nAI as Tutor: Providing Direct Instruction . 11   \nAI as Tutor: Example Prompt ... .. 12   \nAI as Tutor: Example Output.. . 14   \nAI as Tutor: Risks ... . 15   \nAI as Tutor: Guidelines for teachers.. . 15   \nAI Tutor: Instructions for students ... .. 16   \nAI Tutor: Build your Own .17   \nAI as Coach: Increasing Metacognition.. . 18   \nAI as Coach: Example Prompts.. . 19   \nAI as Coach: Reflection Prompt .. . 19   \nAI as Coach: Pre-Mortem Prompt.. "}, "title": "Assigning AI: Seven Approaches for Students with Prompts", "authors": ["Dr. Ethan Mollick", "Dr. Lilach Mollick"], "tags": ["large language models", "AI in education", "AI-assisted learning", "pedagogical approaches", "risk mitigation"], "filename": "out_3FPBG2JY_Mollick_and_Mollick_-_2023_-_As.md"}, {"score": 0.24414055049419403, "document": {"id": 1772, "file_path": "annotated_data/batch_001/out_HVEKNL5H_Thi_and_Nikolov_-_2022_-_How_Te.md", "filename": "out_HVEKNL5H_Thi_and_Nikolov_-_2022_-_How_Te.md", "content": "# How Teacher and Grammarly Feedback Complement One Another in Myanmar EFL Students\u2019 Writing\n\nNang Kham Thi1 $\\oplus$ \u2022 Marianne Nikolov2 (iD\n\nAccepted: 17 September 2021 / Published online: 4 October 2021   \n$\u00a9$ The Author(s) 2021\n\nAbstract Providing feedback on students\u2019 writing is considered important by both writing teachers and students. However, contextual constraints including excess workloads and large classes pose major and recurrent challenges for teachers. To lighten the feedback burden, teachers can take advantage of a range of automated feedback tools. This paper investigated how automated feedback can be integrated into traditional teacher feedback by analyzing the focus of teacher and Grammarly feedback through a written feedback analysis of language- and content-related issues. This inquiry considered whether and how successfully students exploited feedback from different sources in their revisions and how the feedback provisions helped improve their writing performance. The study sample of texts was made up of 216 argumentative and narrative essays written by 27 low-intermediate level students at a Myanmar university over a 13-week semester. By analyzing data from the feedback analysis, we found that Grammarly provided feedback on surface-level errors, whereas teacher feedback covered both lower- and higherlevel writing concerns, suggesting a potential for integration. The results from the revision analysis and pre- and post-tests suggested that students made effective use of the feedback received, and their writing performance improved according to the assessment criteria. The data were triangulated with self-assessment questionnaires regarding students\u2019 emic perspectives on how useful they found the feedback. The pedagogical implications for integrating automated and teacher feedback are presented.\n\nKeywords Written corrective feedback $\\cdot$ Teacher feedback $\\cdot$ Automated feedback $\\cdot$ Grammarly $\\cdot$ Second language writing\n\n# Highlights\n\nThis study investigated how automated feedback can be integrated into traditional teacher feedback.   \nCharacteristics of teacher and Grammarly feedback differ in terms of feedback scope.   \nStudents were able to successfully revise their errors regardless of the source of feedback.   \nProvision of feedback led to statistically significant improvement in language and content aspects of writing.   \nEffective integration of Grammarly in writing instruction might increase the efficacy of teacher feedback, affording it to focus on higher-level writing skills.\n\n# Introduction\n\nWriting is an essential component of language learners\u2019 literacy development in school curricula, as well as a catalyst for personal and academic advancement. Providing feedback to students\u2019 written texts is a common teaching practice for improving students\u2019 writing skills. Investigating the effectiveness of written feedback on writing performance is a burgeoning field of inquiry, and many researchers (e.g., Ferris, 2004, 2007; Karim & Nassaji, 2018; Lee, 2009) have stressed its importance. Ferris (2004) suggested that feedback helps bridge the gap between students\u2019 present knowledge, which indicates areas of potential improvement, and the target language that they need to acquire.\n\nProviding feedback on students\u2019 writing requires a great deal of time and effort on the teachers\u2019 part (Zhang, 2017). Contextual issues, including time constraints, excess workloads, and large classes, further increase the feedback burden. In response, automated writing evaluation (AWE) tools have come to be used to complement teacher feedback in writing classes (Wilson & Czik, 2016). In line with the favorable evidence of the reliability of AWE feedback (Li et al., 2015), L2 writing researchers (e.g., Koltovskaia, 2020; Ranalli, 2018) recommend integrating automated feedback into writing instruction to increase the efficacy of teacher feedback by freeing up teachers\u2019 time to focus less on lower-order concerns (e.g., grammar and mechanics) and turn more to higher-order concerns (e.g., content and organization).\n\nTherefore, it is of great importance to investigate the ways in which automated feedback can be used as a support tool in a class setting. This study investigated the potential to integrate Grammarly into writing instruction to support teacher feedback. To this end, we examined the feedback provided by a teacher and by Grammarly through a written feedback analysis of language- and content-related issues and the impact of feedback from three sources (teacher, Grammarly, and combined feedback) on students\u2019 revisions. We further scrutinized the general impact of feedback on students\u2019 writing performance over 13 weeks. We probed students\u2019 attitudes toward the usefulness of each of the aforementioned feedback modes.\n\n# Efficacy of Teacher Feedback in L2 Writing\n\nIn L2 writing, writing scholars, researchers, and teachers have emphasized the importance of teacher feedback for developing students\u2019 writing (Tang & Liu, 2018). Providing such feedback, ranging from error correction to commentary feedback regarding rhetorical and content aspects of writing (Goldstein, 2004), is part of daily teaching practice (Lee, 2008, 2009). In the dichotomy between feedback on form and content, written feedback can be classified into corrective and non-corrective feedback (Luo & Liu, 2017): corrective feedback (CF) promotes learning the target language by providing negative evidence and non-corrective feedback scaffolds English writing in aspects of content, organization, linguistic performance, and format. The focus of teacher feedback has been debated over the past 30 years, which have seen the proposal of Ashwell (2000) and Fathman and Whalley\u2019s (1990) recommendations that there should be a balance between feedback on form and meaning when providing feedback on students\u2019 writing.\n\nMany studies on teacher feedback have been concerned with the relative effectiveness of different strategies for written CF. Much work has examined whether and to what extent CF can help improve L2 learners\u2019 accuracy in revised and new pieces of writing (e.g., Karim & Nassaji, 2018; Suzuki et al., 2019) and confirmed the positive effects of written feedback on writing accuracy. However, investigations of the usefulness of non-corrective feedback have so far been limited (Ferris, 1997; Ferris et al., 1997). One of the earliest studies on the influence of teacher commentary on student revision, conducted by Ferris (1997), indicated that a significant proportion of comments led to substantive student revision and found that particular types and forms of commentary tended to be more helpful than others.\n\nPrevious studies have measured the impact of teacher feedback on students\u2019 revision by observing either students\u2019 revision operations (Ferris, 2006; Han & Hyland, 2015) or revision accuracy developments (Karim & Nassaji, 2018). Ferris (2006) classified students\u2019 revision operations into three categories: error corrected, incorrect change, and no change, while others (e.g., Karim & Nassaji, 2018; Van Beuningen et al., 2012) calculated improvement in accuracy in students\u2019 revised texts with an error ratio. The long-term effectiveness of written feedback has been established by several studies (e.g., Karim & Nassaji, 2018; Rummel & Bitchener, 2015). Despite these differences in the tools used, most studies reported a positive influence of feedback on students\u2019 revisions and new texts.\n\nAlthough a significant positive impact was found for teacher feedback on students\u2019 writing, providing feedback requires considerable time and effort (Ferris, 2007; Zhang, 2017). Time constraints, large class size, and teachers\u2019 workload pose major challenges that prevent them from giving adequate feedback. Consequently, teachers tend to offer feedback primarily on language-related errors rather than content-related issues in students\u2019 writing (Lee, 2009). Thus, to ease teacher feedback burden and to enhance the efficacy of teacher feedback, automated feedback may come to be used.\n\n# Affordances and Limitations of Automated Feedback in L2 Writing\n\nAs educational technologies and computer-mediated language learning have advanced during the twenty-first century, the integration of computer-generated automated feedback in writing instruction has increased in popularity due to its consistency, ease of scoring, instant feedback, and multiple drafting opportunities (Stevenson & Phakiti, 2014).\n\nStudy of the effects of automated feedback on students\u2019 writing has increased in recent years, and its findings indicate a positive influence on the quality of texts (Li et al., 2015; Stevenson & Phakiti, 2014). Li et al. (2015) looked at how Criterion (https://www.ets.org/criterion/) impacted writing performance and found that it led to improved accuracy from first to final drafts. Nonetheless, its limitations include an emphasis on the surface features of writing, such as grammatical correctness (Hyland & Hyland, 2006), failing to interpret meaning, infer communicative intent, or evaluate the quality of argumentation, and the one-size-fits-all nature of the automated feedback (Ranalli, 2018). Despite these pitfalls, automated feedback lowers teachers\u2019 feedback burden, allowing them to be more selective in feedback they provide (Grimes & Warschauer, 2010).\n\nNoting the supplementary role that instructors and automated systems can play, Stevenson and Phakiti (2014) called for more research on how automated feedback can be integrated into the classroom to support writing instruction. Recent studies have compared the characteristics and impact of teacher and automated feedback (Dikli & Bleyle, 2014; Qassemzadeh & Soleimani, 2016). Dikli and Bleyle (2014) investigated the use of Criterion in a college course of English as a second language writing class and compared feedback from instructor and Criterion across categories of grammar, usage, and mechanics. They found large discrepancies: the instructor provided more and better-quality feedback. Others focused on instructional applications of automated feedback (e.g., Cavaleri & Dianati, 2016; O\u2019Neill & Russell, 2019a, 2019b; Ventayen & Orlanda-Ventayen, 2018). A study by O\u2019Neill and Russell (2019b) found that the Grammarly group responded more positively and was better satisfied with the grammar advice than the non-Grammarly group. Another study, by Qassemzadeh and Soleimani (2016) found that both teacher and Grammarly feedback positively influenced students\u2019 study of passive structures. Within this framework, new research is needed to investigate the applicability of automated feedback in writing instruction, which is the impetus for our study.\n\n# The Present Study\n\n# Context of Myanmar\n\nAs a result of the country\u2019s political and educational situations, research in ELT in Myanmar, especially classroombased research, is sparse (Tin, 2014). Given the scarcity of publications in the periphery (including Myanmar), this study took the form of a naturalistic classroom-based inquiry in a general English class at a major university in Myanmar. The aim of the course is to improve students\u2019 English language skills. While developing students\u2019 English writing ability is one of the foci, teachers do not have sufficient time to provide adequate feedback on students\u2019 writing due to their heavy workloads and large classes of mixed-ability students.\n\n# Research Questions\n\nThis study was guided by four research questions that are mentioned as follows:\n\n1. What is the focus of teacher and Grammarly feedback in terms language- and content-related categories?   \n2. To what extent do the students make use of the feedback under three conditions (i.e., teacher, Grammarly, and combined) in their revisions?   \n3. To what extent does the provision of feedback lead to improvement in writing performance as assessed on a pre- and post-test over a 13-week semester?   \n4. What are the students\u2019 views of the usefulness of feedback from different sources in their EFL course?\n\n# Methods\n\n# Participants\n\nThe sample was an intact class of 30 first-year English majors. The students were placed in the course based on their English scores in the national matriculation examination before admission to the university. Their results were assumed to represent their level of English proficiency at the time of the experiment. Though their exam scores placed them at intermediate (B1) proficiency level, their English writing proficiency varied in terms of mastery of English grammar, familiarity with structures and vocabulary used in writing tasks, and in the formal EFL instruction that they had received. All of them were native Burmese speakers; 11 were male and 19 were female, and all were of typical university age, 17 and 18 years old, and participated on a voluntary basis. They were informed of their right to withdraw from the research at any time during data collection. Three students failed to complete one of the writing tasks, and their data were excluded. The class teacher had an MA degree in Teaching English to the Speakers of Other Languages and over nine years of experience in teaching English at higher education institutions in Myanmar.\n\n# Materials\n\nThree instruments were used for data collection: writing tasks, an assessment scale to assess improvement in students\u2019 writing performance, and self-assessment questionnaires.\n\n# Writing Tasks\n\nSix writing tasks were developed (including a pre- and post-test) on topics familiar to the students. The tasks were ecologically valid, as they were retrieved from the prescribed curriculum. The genres included both argumentative and narrative essays, as these two genres prevail in the syllabus. Four guiding prompts, similar to that in Fig. 1, were provided in the writing tasks, and these were similarly structured to minimize possible linguistic differences.\n\n# Writing Assessment\n\nThe study adapted a B1 analytical rating scale (Euroexam International, 2019) to assess the students\u2019 English writing improvement. Euroexam International offers language tests in general, business, and academic English and German at levels A1 through C1. The writing assessment scale features four criteria: task achievement, coherence and cohesion, grammatical range and accuracy, and lexical range and accuracy. A description of the assessment criteria, together with definitions, is presented in \u2018\u2018Appendix Table $3 ^ { \\circ }$ . All scoring of written texts (pre- and post-test) was done by the two authors independently, and the mean scores were calculated. The inter-rater reliability coefficients (Pearson $r$ ) between the two raters were 0.92 for the pre-test and $\\phantom { - } 0 . 9 4$ for the post-test on the assessment scale.\n\n# Questionnaire\n\nA self-assessment questionnaire was developed to probe the students\u2019 emic perspectives of the effectiveness of feedback from three sources. Three closed items were presented to elicit information on the usefulness of the feedback, and five open-ended questions asking students to comment on the usefulness of the feedback.\n\n# Procedure\n\nData were collected over a period of 13 weeks from August to October 2020: the students completed six writing tasks, including a pre- and post-test (Fig. 2). In the first week, the research project was introduced, and then the pre-test was administered in the second week. The course was operated on a weekly basis: participants were given a writing task and received feedback from teacher, Grammarly, or both sources the week after the completion of the initial writing task. There were four treatment sessions in the whole program, and the students revised their texts in response to the feedback and sent the revised texts to the teacher via email in the same week. The process continued until Week 10, when the revised version of the fourth writing task was complete. In Week 13, students completed the post-test and the self-assessment questionnaire.\n\nThe provision of feedback was carried out by the class teacher, using the \u2018\u2018Track Changes\u2019\u2019 functionality of\n\n![](img/a2d04becad106f3347c93f820a2d55d4d61db972f7e9724c9a374608655f4d20.jpg)  \nFig. 1 Sample writing task\n\nFig. 2 Data collection timeline   \n\n<html><body><table><tr><td>Timeline</td><td>Experimental group</td></tr><tr><td>Week 1</td><td>Introducing the research project</td></tr><tr><td>Week 2</td><td>Pretest</td></tr><tr><td>Week 3</td><td>Essay 1</td></tr><tr><td>Week 4</td><td>Teacher feedback + Revision</td></tr><tr><td>Week 5</td><td>Essay 2</td></tr><tr><td>Week 6</td><td>Grammarly feedback + Revision</td></tr><tr><td>Week 7</td><td>Essay 3</td></tr><tr><td>Week 8</td><td>A combination of teacher feedback &amp; Grammarly feedback + Revision</td></tr><tr><td>Week 9</td><td>Essay 4</td></tr><tr><td>Week 10</td><td>A combination of teacher feedback &amp; Grammarly feedback + Revision</td></tr><tr><td>Week 13</td><td>Posttest Self-assessment questionnaire</td></tr></table></body></html>\n\nMicrosoft Word, Grammarly, or a combination of both of these means. To keep the feedback process as natural as possible, the teacher was asked not to limit his feedback to language- or content-related issues. For automated feedback, a free version of Grammarly (https://www.gram marly.com/grammar-check) was utilized and the students uploaded their essays on the webpage, receiving instant feedback.\n\n# Data Analysis\n\nGuided by Lee\u2019s (2009) work, a written feedback analysis was performed to investigate the focus of the teacher and Grammarly feedback. This involved error identification, categorization, and counting of feedback points: \u2018\u2018an error corrected/underlined, or a written comment that constitutes a meaningful unit\u2019\u2019 (p.14). Feedback points marked on the students\u2019 first drafts were initially classified into languageand content-related issues and coded for analysis. Regarding language-related issues, linguistic errors in the students\u2019 drafts were identified and categorized based on Ferris\u2019s (2006) taxonomy, with adaptations. For contentrelated issues, in-text and end-of-text comments were classified into four categories: giving information, asking for information, praises, and suggestions according to the aim or intent of the comment suggested by Ferris et al. (1997). It should be noted that Grammarly feedback primarily relates to language-related errors, which is not the case in teacher feedback. Feedback points marked by the teacher and by Grammarly were cross-linked to students\u2019 revisions, and changes were analyzed based on their revision operations. This study partly followed the revision analysis categories of Ferris (2006) and Han and Hyland (2015) to classify revision patterns into three categories: correct, incorrect, and no revision (see Supplementary Data).\n\nTo examine the impact of feedback provision on students\u2019 writing performance, we calculated mean scores and standard deviations at the beginning and at the end of the course. Because the sample size was small, and the variables were not normally distributed, a bootstrap method was used to analyze the dataset. $T -$ -tests were administered using a bootstrap method in SPSS 22 (Corp, 2013) to estimate the difference between pre-and post-test performance. The self-assessment questionnaires included both quantitative and qualitative data. The frequencies of responses were calculated, and the students\u2019 perceived areas of improvements were reported. For open-ended questions, a qualitative analysis was conducted to better understand their perspectives on how useful they found the feedback. Their responses were summarized with the use of emerging common themes.\n\n# Results and Discussions\n\n# Focus of Teacher and Grammarly Feedback\n\nFigure 3 summarizes the focus of teacher feedback in comparison with Grammarly feedback and the percentage of each feedback category marked on the students\u2019 first drafts. In general, we found that the teacher focused on a broad coverage of writing issues, at the word, sentence, and text levels, while Grammarly indicated language errors: article/determiner, preposition, and miscellaneous errors including conciseness and wordiness issues.\n\nThe results of feedback analysis showed that the teacher provided 410 feedback points in 27 essays, targeting language errors $( 6 8 . 8 \\% )$ and higher-level writing issues $( 3 1 . 2 \\% )$ . This sheds light on labor-intensive nature of teacher feedback. A more detailed analysis showed that teacher error feedback mainly concerns conjunction $( 1 0 \\% )$ , miscellaneous $( 9 . 5 \\% )$ , punctuation $( 6 . 3 \\% )$ , and preposition errors $( 5 . 6 \\% )$ . In the teacher\u2019s commentary on content, praise got the highest percentage $( 1 1 . 7 \\% )$ , followed by suggestion $( 7 . 8 \\% )$ , giving information $( 6 . 4 \\% )$ , and asking for information $( 5 . 3 \\% )$ . Our finding that praise accounted for only 11.7 per cent of the total written feedback contradicted that of Hyland and Hyland\u2019s (2001), but supported that of Lee\u2019s (2009). This might be due to differences in teachers\u2019 feedback beliefs about the role of praise in softening criticism when providing feedback on students\u2019 writing.\n\n![](img/2ee5bc22d56e5dcd04e19efe77030d910ba2ceaad6fefc213c402d091f61413d.jpg)  \nFig. 3 Feedback categories of teacher and Grammarly feedback\n\nGrammarly predominantly provided feedback on errors of grammar, usage, mechanics, style, and conciseness. It detected 281 errors in 27 essays: the most predominant errors were article/determiner $(43 \\% )$ , miscellaneous $( 1 9 . 5 \\% )$ , and preposition $( 1 3 . 5 \\% )$ errors. Other less frequently indicated errors included with conjunctions $( 1 \\% )$ , sentence structure $( 0 . 3 \\% )$ , and pronoun use $( 0 . 3 \\% )$ (Fig. 3).\n\nAll in all, it appears that Grammarly can be used as a learning tool to facilitate teacher feedback. This relates to the focus of each feedback type: the teacher\u2019s feedback covered both language and content issues, whereas Grammarly provided feedback on language-related errors. This finding may seem predictable, as Grammarly is understood to be a grammar-checking tool, this emphasis is to its advantage. In particular, its detection of article and prepositions errors was higher than those of teacher feedback. Thus, utilizing Grammarly effectively for offering feedback on these errors would possibly save time and effort on part of teachers.\n\nIt is also fair to say that the use of Grammarly along with teacher feedback might also enhance the efficacy of teacher feedback. As in previous studies (e.g., Lee, 2009; Mao & Crosthwaite, 2019), the teacher feedback primarily attended to language errors $( 6 8 . 8 \\% )$ . Given time constraints and the large classes of mixed-ability students, providing effective and individualized feedback for students\u2019 writing is far beyond the capabilities of teachers. In this regard, using automated feedback as an assistance tool might become an outlet for coping with surface errors, lightening the teacher feedback burden: freeing teachers to focus on higher-order writing concerns such as content and discourse (Ranalli, 2018).\n\n# Impact of Teacher, Grammarly, and Combined Feedback: Successful Revision\n\nWhen examining the influence of feedback on students\u2019 revision, this study considered how the feedback was acted upon to facilitate comparability across feedback from three sources. A general pattern of students\u2019 revision operations led to successful revision, regardless of the source of feedback (Fig. 4), indicating their acceptance of feedback. The finding that teacher error feedback leads to effective revision is in agreement with the findings of Ferris (2006) and Yang et al. (2006). Moreover, the lowest percentage of unrevised errors reflects their beliefs and value regarding the importance of feedback in improving their writing performance. The results were interesting for Grammarly feedback which received the highest rate of correct revision $( 7 6 . 2 \\% )$ . The reason for this might be that Grammarly usually includes a concrete suggestion for revision that students can easily act upon. One example of this is shown in Fig. 5.\n\n![](img/e6671925cd9d6da4027e8406d06457d9d9605c62b7a53f3ba39d4c06805963f3.jpg)  \nFig. 4 Student revision operations\n\n![](img/cb8a330cbafcc82f3806cf0c05bc0a3e35ecd676a957f6dce9256b86c42a211d.jpg)  \nFig. 5 An example of Grammarly feedback and student\u2019s revision\n\nFurther points of discussion concern how students responded to the combined feedback. It might be assumed that combined feedback resulted in more feedback points than the other conditions. However, the opposite was true: fewer feedback points were provided, and a lower ratio of correct revision was found than for the teacher and Grammarly, which had the highest ratio of no revision. A possible explanation of lower feedback points might relate to students\u2019 increased awareness of teacher and Grammarly feedback in previous essays or teacher\u2019s reliance on Grammarly feedback, instinctively assuming that it would handle grammar errors.\n\nAlthough the students successfully revised their errors, it is worth exploring how well they revised individual error categories (Table 1). As the overall percentage of successful revisions was high, it was not surprising to see that the percentage of successful revisions in most error categories was also fairly high, regardless of the conditions. However, a closer examination of how students utilized feedback revealed stimulating new results. In connection with teacher feedback, while feedback on most error categories (e.g., conjunction, article/determiner, singularplural, adverb, and word choice) was associated with correct revision, some feedback on idioms, pronoun, and sentence structure was left unattended. For example, $4 0 . 9 \\%$ of errors in sentence structure led to no revision. This could be explained by the low number of error identifications in these categories and partial understanding of the instructions (Han, 2019). As Goldstein (2004) noted, reasons for unsuccessful/no revisions included: unwillingness to critically examine one\u2019s point of view, feeling that the teacher\u2019s feedback is incorrect, lack of necessary knowledge to revise, lack of time and motivation, and many others.\n\nDespite the overall successful revision when acting upon Grammarly $( 7 6 . 2 \\% )$ and combined feedback $( 6 1 . 8 \\% )$ , the results indicated that the students largely ignored feedback on miscellaneous errors. This finding is probably due to students finding the feedback in this category unhelpful or unnecessary in revision. Figure 6 demonstrates a typical example. This underlines how students selectively accept the feedback, filtering suggestions that are incorrect or unnecessary (Cavaleri & Dianati, 2016).\n\nThe question of whether Grammarly could be integrated into writing instruction can be answered by how the students responded to feedback in their revisions. The comparison of outcomes in the three conditions provided support for the potential to use Grammarly, along with teacher feedback. The reason for this is associated with the high percentage of successful revision in cases of feedback regarding the singular-plural $( 9 2 . 9 \\% )$ , subject-verb agreement $( 9 2 . 3 \\% )$ , word form $( 9 0 \\% )$ , punctuation $( 8 4 . 6 \\% )$ , article/determiner $( 8 4 . 3 \\% )$ , and preposition $( 8 4 . 2 \\% )$ following Grammarly feedback. Thus, it seems reasonable to say that using Grammarly to handle errors in these categories could be effective and allow time for teachers to focus on other higher-level writing issues. Specifically, although the teacher made 22 feedback points regarding sentence structure, 40.9 per cent of them were left unattended. This partly mirrors the indirectness or vagueness of teacher feedback that may be difficult for students to respond to (Tian & Zhou, 2020). What should be stressed is that teachers might be able to focus on these types of errors if they can efficiently make use of Grammarly to deal with surface-level ones.\n\nTable 1 Comparison of students\u2019 revision operations by error type   \n\n<html><body><table><tr><td></td><td colspan=\"3\">Teacher (no./%)</td><td colspan=\"3\">Grammarly (no./%)</td><td colspan=\"3\">Combined (no./%)</td></tr><tr><td></td><td>CR</td><td>IR</td><td>NR</td><td>CR</td><td>IR</td><td>NR</td><td>CR</td><td>IR</td><td>NR</td></tr><tr><td>Word choice</td><td>11/84.6</td><td>0</td><td>2/15.4</td><td>-</td><td>-</td><td>-</td><td>6/85.7</td><td>0</td><td>1/14.3</td></tr><tr><td>Verb tense</td><td>2/50.0</td><td>1/25.0</td><td>1/25.0</td><td>-</td><td>-</td><td>-</td><td>7/70</td><td>2/20</td><td>1/10.0</td></tr><tr><td>Verb form</td><td>11/68.8</td><td>2/12.5</td><td>3/18.8</td><td>6/100</td><td>0</td><td>0</td><td>3/50.0</td><td>0</td><td>3/50.0</td></tr><tr><td>Word form</td><td>9/81.8</td><td>0</td><td>2/18.2</td><td>9/90.0</td><td>0</td><td>1/10</td><td>2/50</td><td>0</td><td>2/50.0</td></tr><tr><td>Articles/determiners</td><td>15/83.3</td><td>1/5.6</td><td>2/11.1</td><td>102/ 84.3</td><td>0</td><td>19/ 15.7</td><td>21/84</td><td>0</td><td>4/16.0</td></tr><tr><td>Singular-plural</td><td>13/76.5</td><td>1/5.9</td><td>3/17.6</td><td>13/92.9</td><td>0</td><td>1/7.1</td><td>3/37.5</td><td>0</td><td>5/62.5</td></tr><tr><td>Pronouns</td><td>4/57.1</td><td>0</td><td>3/42.9</td><td>1/100.0</td><td>0</td><td>0</td><td>4/66.7</td><td>0</td><td>2/33.3</td></tr><tr><td>Run-on</td><td>1/50.0</td><td>1/50.0</td><td>0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Punctuation</td><td>15/57.7</td><td>1/3.8</td><td>10/ 38.5</td><td>11/84.6</td><td>0</td><td>2/15.4</td><td>11/68.8</td><td>0</td><td>5/31.2</td></tr><tr><td>Sentence structure</td><td>11/50.0</td><td>2/9.1</td><td>9/40.9</td><td>1/100.0</td><td>0</td><td>0</td><td>8/61.5</td><td>3/23.1</td><td>2/15.4</td></tr><tr><td>Idioms</td><td>1/25.0</td><td>0</td><td>3/75.0</td><td>-</td><td>-</td><td>-</td><td>1/100</td><td>0</td><td>0</td></tr><tr><td>Subject-verb agreement</td><td>7/77.8</td><td>0</td><td>2/22.2</td><td>12/92.3</td><td>0</td><td>1/7.7</td><td>10/90.9</td><td>0</td><td>1/9.1</td></tr><tr><td> Preposition</td><td>16/69.6</td><td>1/4.3</td><td>6/26.1</td><td>32/84.2</td><td>0</td><td>6/15.8</td><td>16/69.6</td><td>0</td><td>7/30.4</td></tr><tr><td>Conjunction</td><td>33/80.5</td><td>1/2.4</td><td>7/17.1</td><td>2/75.0</td><td>0</td><td>1/25.0</td><td>13/54.2</td><td>2/8.3</td><td>9/37.5</td></tr><tr><td>Collocation</td><td>9/75.0</td><td>0</td><td>3/25.0</td><td>-</td><td>-</td><td>-</td><td>5/55.6</td><td>0</td><td>4/44.4</td></tr><tr><td>Omission of objects</td><td>4/100.0</td><td>0</td><td>0</td><td>-</td><td>-</td><td>-</td><td>3/100</td><td>0</td><td>0</td></tr><tr><td>Adjective</td><td>0</td><td>0</td><td>0</td><td>2/100</td><td>0</td><td>0</td><td>2/75</td><td>0</td><td>1/25.0</td></tr><tr><td>Adverb</td><td>10/71.5</td><td>1/7.1</td><td>3/21.4</td><td>4/100</td><td>0</td><td>0</td><td>5/100</td><td>0</td><td>0</td></tr><tr><td>Miscellaneous</td><td>28/71.8</td><td>0</td><td>11/ 28.2</td><td>19/34.5</td><td>0</td><td>36/ 65.5</td><td>6/20.0</td><td>0</td><td>24/ 80.0</td></tr><tr><td>Total</td><td>200/ 71.0</td><td>12/4.2</td><td>70/ 24.8</td><td>214/ 76.2</td><td>0</td><td>67/ 23.8</td><td>126/ 61.8</td><td>7/3.4</td><td>71/ 34.8</td></tr></table></body></html>\n\nPercentages represent frequencies of revision categories within each error category. For instance, $8 4 . 6 \\%$ of the word choice errors had a correct revision rating $C R$ correct revision, $I R$ incorrect revision, NR no revision\n\n# Effect of Written Feedback on Students\u2019 Writing Performance\n\nAfter receiving feedback over a semester, the students made improvement in their writing performance, as is shown in the significant increase in their post-test scores across four assessment criteria. As presented in Table 2, there was substantial improvement in task achievement and coherence and cohesion in the post-test scores. Similarly, in connection with grammatical range and accuracy and lexical range and accuracy, the students showed notable improvement from the pre- to the post-test. The effect sizes for all significant comparisons of learners\u2019 writing performance were medium to large. The positive impact of feedback provision on new writing tasks was in line with that found in previous studies (e.g., Karim & Nassaji, 2018; Rummel & Bitchener, 2015).\n\n![](img/f292af842c9a5fe21889cacb0f3eb7d52f0508ae15a3bb1a33379cd79f64ca9f.jpg)  \nFig. 6 An example of Grammarly feedback on a miscellaneous error and student\u2019s revision outcome\n\nTable 2 Comparison between pre-and post-test regarding the students\u2019 writing performance   \n\n<html><body><table><tr><td rowspan=\"2\">Assessment criteria</td><td colspan=\"2\">Pre-test</td><td colspan=\"2\">Post-test</td><td rowspan=\"2\">t(26)</td><td rowspan=\"2\">p</td><td rowspan=\"2\">Cohen&#x27;s d</td></tr><tr><td>Mean</td><td>SD</td><td>Mean</td><td>SD</td></tr><tr><td>Task achievement</td><td>2.25</td><td>543</td><td>2.65</td><td>0.551</td><td>3.82</td><td>0.003</td><td>0.71</td></tr><tr><td>Coherence &amp; cohesion</td><td>2.25</td><td>610</td><td>2.61</td><td>0.560</td><td>3.90</td><td>0.002</td><td>0.75</td></tr><tr><td>Grammatical range &amp; accuracy</td><td>2.20</td><td>559</td><td>2.52</td><td>0.628</td><td>2.88</td><td>0.017</td><td>0.55</td></tr><tr><td>Lexical range &amp; accuracy</td><td>2.26</td><td>685</td><td>2.69</td><td>0.483</td><td>3.55</td><td>0.003</td><td>0.68</td></tr><tr><td>Overall writing performance</td><td>8.98</td><td>2.091</td><td>10.46</td><td>1.965</td><td>3.14</td><td>0.006</td><td>0.61</td></tr></table></body></html>\n\n![](img/9ad2eb0871b51e627dce7b3ffeaac63be2c98f99a0d77fb7a2d8b215d81d5058.jpg)  \nFig. 7 Students\u2019 perceptions of the usefulness of the teacher, Grammarly, and combined feedback\n\n# Students\u2019 Views on the Usefulness of Teacher, Grammarly, and Combined Feedback\n\nThe results from the self-assessment questionnaires showed that most students perceived the feedback from both the teacher and Grammarly to be effective and useful for improving their writing (Fig. 7). Although most responded that Grammarly feedback helped them improve their grammar $( 8 8 . 9 \\% )$ and vocabulary $( 7 7 . 8 \\% )$ , none reported improvements in content or organization. The teacher feedback was considered more valuable, as it facilitated improvement in different aspects of writing, and the combined feedback did this as well. Despite the students\u2019 positive impressions for both the teacher\u2019s and Grammarly feedback, their responses regarding specific areas of improvement for the combined feedback were considerably higher across different aspects of writing. This finding underlines the great potential for integrating Grammarly feedback into writing instruction, supplementing teacher feedback, as reported in previous studies by O\u2019Neill and Russell (2019b), Ventayen and Orlanda-Ventayen (2018), and Ranalli (2021).\n\nIn the second part of the questionnaire, the students reported why they liked the feedback they received. Almost all students acknowledged the value and effectiveness of the teacher feedback. Their comments showed three emerging themes relating to the nature of the feedback, how it enhances their motivation, and their positive perception of teacher feedback. Almost all students stressed the value of the teacher\u2019s feedback, saying that his comments \u2018\u2018guide me when my writing goes out of context\u2019\u2019 (Student 21), \u2018\u2018show me both strengths and weaknesses of my writing\u2019\u2019 (Student 2), and are \u2018\u2018short and clear\u2019\u2019 (Student 27).\n\nMost comments regarding the usefulness of Grammarly feedback concerned its efficiency: \u2018\u2018It is easy to use and available for free\u2019\u2019 (Student 3), and \u2018\u2018I could use Grammarly at any time\u2019\u2019 (Student 9). However, a few students were dissatisfied with it: \u2018\u2018To be honest, I don\u2019t feel very satisfied with it\u2019\u2019 (Student 15) and \u2018\u2018Honestly, I didn\u2019t find Grammarly feedback useful\u2019\u2019 (Student 19). Further responses revealed how the combined feedback helped them revise their essays: \u2018\u2018Teacher\u2019s feedback tells me my mistakes exactly and Grammarly fixes those for me\u2019\u2019 (Student 20), and \u2018\u2018It\u2019s a perfect combination\u2019\u2019 (Student 25).\n\n# Implications\n\nOur findings have pedagogical implications for the integration of Grammarly into teaching L2 writing. Considering the emphasis of Grammarly feedback on languagerelated errors as an advantage, writing teachers could use it as a supportive tool in their classes on a regular basis or encourage students to use it independently. In this way, teacher feedback burden could be reduced and challenges regarding time constraints and inadequacy of attention paid to individuals in large classes could be addressed to a certain extent. In particular, based on Grammarly\u2019s effective feedback on article/determiner and preposition errors and students\u2019 successful revisions of these errors reflect their acceptance of Grammarly as a provider of feedback in their EFL courses. Thus, teachers can exploit the affordances of Grammarly to maximize the efficacy of their feedback. However, teachers should be aware of the limitations of automated feedback and be sure to inform students of these limitations.\n\nAdditionally, writing teachers should be more selective and straightforward in providing feedback to improve students\u2019 writing performance and motivation. In our study, the students were not able to revise errors relating to sentence structure, leaving most of them unrevised. Moreover, because of the overlaps in teacher and Grammarly feedback in some language-related errors, teachers can identify the areas on which Grammarly can provide feedback effectively, allowing them to focus on higherlevel writing skills including content development and elaboration, organization, and rhetoric.\n\n# Conclusion\n\nThis classroom-based study was conducted to examine the integrated use of Grammarly in a large class to support teacher feedback. The results showed the pedagogical potential of Grammarly for facilitating teacher feedback due to its effective feedback regarding surface-level errors and students\u2019 general acceptance of automated feedback. Moreover, it seems that students\u2019 successful integration of feedback in their revisions and increased performance scores on the post-test offer evidence that they successfully made use of feedback and that the provision of feedback led to an improvement in their writing performance. In addition, their positive attitude toward the usefulness of feedback provides further insights into how much they valued the feedback they received from the teacher and Grammarly.\n\nSome limitations should be addressed, as we conducted the study in only one course at a university. Future research should involve more courses, teachers, and students at varying proficiency levels. The inquiry failed to include a control group because we considered it unethical to withhold feedback from students that they would typically receive in their course. Therefore, no comparison was made between the feedback group and a control group. However, we managed to examine how students applied feedback from three sources in their revisions and to track progress during the course. This investigation may offer insights into areas beyond how students use feedback in their revision and how feedback helps them develop their writing performance. We hope that the findings of this study indicate how Grammarly can be used as an effective feedback tool to help relieve teachers of a part of the burdensome task of responding to surface-level errors in students\u2019 writing.\n\n# Appendix\n\nSee Table 3\n\nTable 3 Operational rating scales for writing tasks at B1 level adopted from Euroexam International (2019)   \n\n<html><body><table><tr><td></td><td>B1 Task achievement</td><td>Coherence &amp; cohesion</td><td>Grammatical range &amp; accuracy</td><td>Lexical range &amp; accuracy</td></tr><tr><td colspan=\"5\">3 Task achieved at a high level</td></tr><tr><td rowspan=\"3\"></td><td>Rubrics: Followed completely in all 4 guiding  Information: Well points</td><td>organized into a</td><td>Range: Good range of grammatical structures</td><td>Range: Good range of lexis to complete the</td></tr><tr><td>Content: Enough and relevant discussion and details are included on all 4 guiding points</td><td>coherent text Cohesive devices:</td><td>Accuracy: Grammatical structures used accurately with no or very few basic Accuracy: Lexis used</td><td>task appropriately with no</td></tr><tr><td>*One mark will be penalized if some irrelevant discussion and details included</td><td>Overall good use of cohesive devices</td><td>errors</td><td>or little misuse</td></tr><tr><td rowspan=\"3\">2</td><td>Task achieved with minor gaps</td><td></td><td>Information: Part of the Range: Sufficient range of grammatical Range: Sufficient</td><td></td></tr><tr><td>Rubrics: Followed in 2 or 3 guiding points Content: Enough and relevant discussion and details are included on 2 or 3 guiding points; Cohesive devices:</td><td>text is well organized</td><td>structures Accuracy: Grammatical structures used</td><td>range of lexis to complete the task</td></tr><tr><td>Little or not relevant information is discussed on 1 or 2 points *One mark will be penalized if some irrelevant.</td><td>Mostly good use of cohesive devices with minor gaps</td><td>mostly accurately with some errors that do not significantly impede meaning</td><td>Accuracy: Lexis used mostly appropriately with minor gaps</td></tr><tr><td rowspan=\"3\"></td><td>discussion and details included Task achieved with major gaps</td><td></td><td></td><td></td></tr><tr><td>Rubrics: Followed in 1 or 2 guiding points Content: Enough and relevant details are</td><td>Information: Text is hard to follow</td><td>Range: Limited range of grammatical structures</td><td>Range: Limited range of lexis to complete the task</td></tr><tr><td>included on 1 or 2 guiding points; Little or not Cohesive devices: relevant information is included on 2 or 3 points</td><td>Major gaps in use of cohesive devices</td><td>Accuracy: Grammatical structures used inaccurately interfering with meaning Accuracy: Lexis often</td><td>used inappropriately</td></tr><tr><td rowspan=\"3\">0</td><td>*One mark will be penalized if some irrelevant. discussion and details included</td><td></td><td></td><td>causing misunderstanding</td></tr><tr><td>Task unachieved</td><td></td><td></td><td></td></tr><tr><td>Task unattempted/partially attempted Not enough language to make an assessment</td><td></td><td></td><td></td></tr></table></body></html>\n\nWhat each criterion is supposed to assess are as follows:\n\n1. Task achievement concerns how well a candidate has fulfilled the task, addressing the guided prompt with relevant details while aiming at the general target reader, in other words, if he has done what he was supposed to do.   \n2. Coherence and cohesion focus on how well organized a text is, following a coherent structure to maintain the organization of the whole text while making good use of cohesive devices.   \n3. Grammatical range and accuracy focus on the accuracy of grammatical structures that a candidate uses, demonstrating a variety of grammatical structures available to him.   \n4. Lexical range and accuracy focus on the accuracy and lexical items that a candidate uses, displaying the appropriate choice and variety of words with an adequate range of lexis to complete the task.\n\nSupplementary Information The online version contains supplementary material available at https://doi.org/10.1007/s40 299-021-00625-2.\n\nAcknowledgements The authors are thankful to the student participants and the teacher from the University of Yangon for their voluntary collaborations and efforts. The first author of this article is a recipient of the Hungarian government\u2019s Stipendium Hungaricum Scholarship in collaboration with the Myanmar government.\n\nFunding Open access funding provided by University of Szeged with grant number: 5488. This research received no specific Grant from any funding agency.\n\n# Declarations\n\nConflict of interest The author(s) have stated no potential conflict of interest.\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\n# References\n\nAshwell, T. (2000). Patterns of teacher response to student writing in a multiple-draft composition classroom: Is content feedback followed by form feedback the best method? Journal of Second Language Writing, 9(3), 227\u2013257. https://doi.org/10.1016/S1060-3743(00)00027-8   \nCavaleri, M., & Dianati, S. (2016). You want me to check your grammar again? The usefulness of an online grammar checker as perceived by students. Journal of Academic Language and Learning, 10(1), A223\u2013A236.   \nCorp, I. B. M. (2013). IBM SPSS statistics for windows, version 22.0. Armonk, NY   \nDikli, S., & Bleyle, S. (2014). Automated essay scoring feedback for second language writers: How does it compare to instructor feedback? Assessing Writing, 22, 1\u201317. https://doi.org/10.1016/j.asw.2014.03.006   \nEuroexam International. (2019). Euroexam detailed specifications. London: Euroexam International.   \nFathman, A., & Whalley, E. (1990). Teacher response to student writing: Focus on form versus content. In B. Kroll (Ed.), Second language writing: Research insights for the classroom (pp. 178\u2013190). Cambridge University Press.   \nFerris, D. R. (1997). The influence of teacher commentary on student revision. TESOL Quarterly, 31(2), 315\u2013339.   \nFerris, D. (2004). The \u2018\u2018Grammar Correction\u2019\u2019 debate in L2 writing: Where are we, and where do we go from here? (And what do we do in the meantime \u2026?). Journal of Second Language Writing, 13(1), 49\u201362. https://doi.org/10.1016/j.jslw.2004.04.005   \nFerris, D. (2006). Does error feedback help student writers? New evidence on the short- and long-term effects of written error correction. In K. Hyland & F. Hyland (Eds.), Feedback in second language writing: Contexts and issues (pp. 81\u2013104). Cambridge University Press.   \nFerris, D. (2007). Preparing teachers to respond to student writing. Journal of Second Language Writing, 16, 165\u2013193. https://doi.org/10.1016/j.jslw.2007.07.003   \nFerris, D. R., Pezone, S., Tade, C. R., & Tinti, S. (1997). Teacher commentary on student writing: Descriptions & implications. Journal of Second Language Writing, 6(2), 155\u2013182. https://doi.org/10.1016/S1060-3743(97)90032-1   \nGoldstein, L. M. (2004). Questions and answers about teacher written commentary and student revision: Teachers and students working together. Journal of Second Language Writing, 13, 63\u201380. https://doi.org/10.1016/j.jslw.2004.04.006   \nGrimes, D., & Warschauer, M. (2010). Utility in a fallible tool: A multi-site case study of automated writing evaluation. Journal of Technology, Learning, and Assessment, 8(6), 1\u201344.   \nHan, Y. (2019). Written corrective feedback from an ecological perspective: The interaction between the context and individual learners. System, 80, 288\u2013303. https://doi.org/10.1016/j.system.2018.12.009   \nHan, Y., & Hyland, F. (2015). Exploring learner engagement with written corrective feedback in a Chinese tertiary EFL classroom. Journal of Second Language Writing, 30, 31\u201344. Writing, 10, 185\u2013212.   \nHyland, K., & Hyland, F. (2006). Feedback on second language students\u2019 writing. Language Teaching, 39(2), 83\u2013101. https://doi.org/10.1017/S0261444806003399   \nKarim, K., & Nassaji, H. (2018). The revision and transfer effects of direct and indirect comprehensive corrective feedback on ESL students writing. Language Teaching Research. https://doi.org/10.1177/1362168818802469   \nKoltovskaia, S. (2020). Student engagement with automated written corrective feedback (AWCF) provided by Grammarly: A multiple case study. Assessing Writing, 44(March), 100450. https://doi.org/10.1016/j.asw.2020.100450   \nLee, I. (2008). Understanding teachers\u2019 written feedback practices in Hong Kong secondary classrooms. Journal of Second Language Writing, 17(2), 69\u201385. https://doi.org/10.1016/j.jslw.2007.10.001   \nLee, I. (2009). Ten mismatches between teachers\u2019 beliefs and written feedback practice. ELT Journal, 63(1), 13\u201322. https://doi.org/10.1093/elt/ccn010   \nLi, J., Link, S., & Hegelheimer, V. (2015). Rethinking the role of automated writing evaluation (AWE) feedback in ESL writing instruction. Journal of Second Language Writing, 27, 1\u201318. https://doi.org/10.1016/j.jslw.2014.10.004   \nLuo, Y., & Liu, Y. (2017). Comparison between peer feedback and automated feedback in college English writing: A case study. Open Journal of Modern Linguistics, 07(04), 197\u2013215. https://doi.org/10.4236/ojml.2017.74015   \nMao, S. S., & Crosthwaite, P. (2019). Investigating written corrective feedback: (Mis)alignment of teachers\u2019 beliefs and practice. Journal of Second Language Writing, 45(November 2018), 46\u201360. https://doi.org/10.1016/j.jslw.2019.05.004   \nO\u2019Neill, R., & Russell, A. M. T. (2019a). Grammarly: Help or hindrance? Academic learning advisors\u2019 perceptions of an online grammar checker. Journal of Academic Language & Learning, 13(1), A88\u2013A107.   \nO\u2019Neill, R., & Russell, A. M. T. (2019b). Stop! Grammar time: University students\u2019 perceptions of the automated feedback program Grammarly. Australasian Journal of Educational Technology, 35(1), 42\u201356. https://doi.org/10.14742/ajet.3795   \nQassemzadeh, A., & Soleimani, H. (2016). The impact of feedback provision by Grammarly software and teachers on learning passive structures by Iranian EFL learners. Theory and Practice in Language Studies, 6(9), 1884. https://doi.org/10.17507/tpls.0609.23   \nRanalli, J. (2018). Automated written corrective feedback: How well can students make use of it? Computer Assisted Language Learning, 31(7), 653\u2013674. https://doi.org/10.1080/09588221.2018.1428994   \nRanalli, J. (2021). L2 student engagement with automated feedback on writing: Potential for learning and issues of trust. Journal of Second Language Writing, 52, 100816. https://doi.org/10.1016/j.jslw.2021.100816   \nRummel, S., & Bitchener, J. (2015). The effectiveness of written corrective feedback and the impact of Lao learners\u2019 beliefs have on uptake. Australian Review of Applied Linguistics, 1, 66\u201384. https://doi.org/10.1075/aral.38.1.04rum   \nStevenson, M., & Phakiti, A. (2014). The effects of computergenerated feedback on the quality of writing. Assessing Writing, 19, 51\u201365. https://doi.org/10.1016/j.asw.2013.11.007   \nSuzuki, W., Nassaji, H., & Sato, K. (2019). The effects of feedback explicitness and type of target structure on accuracy in revision and new pieces of writing. System, 81, 135\u2013145. https://doi.org/10.1016/j.system.2018.12.017   \nTang, C., & Liu, Y. T. (2018). Effects of indirect coded corrective feedback with and without short affective teacher comments on L2 writing performance, learner uptake and motivation. Assessing Writing, 35, 26\u201340. https://doi.org/10.1016/j.asw.2017.12.002   \nTian, L., & Zhou, Y. (2020). Learner engagement with automated feedback, peer feedback and teacher feedback in an online EFL writing context. System, 91, 102247. https://doi.org/10.1016/j.system.2020.102247   \nTin, T. B. (2014). Learning English in the periphery: A view from Myanmar (Burma). Language Teaching Research, 18(1), 95\u2013117. https://doi.org/10.1177/1362168813505378   \nVan Beuningen, C., De Jong, N. H., & Kuiken, F. (2012). Evidence on the effectiveness of comprehensive error correction in second language writing. Language Learning, 62(1), 1\u201341. https://doi.org/10.1111/j.1467-9922.2011.00674.x   \nVentayen, R. J. M., & Orlanda-Ventayen, C. C. (2018). Graduate students\u2019 perspective on the usability of Grammarly $^ \\mathrm { \\textregistered }$ in one ASEAN state university. Asian ESP Journal, 14(7), 9\u201330.   \nWilson, J., & Czik, A. (2016). Automated essay evaluation software in English language arts classrooms: Effects on teacher feedback, student motivation, and writing quality. Computers & Education, 100, 94\u2013109. https://doi.org/10.1016/j.compedu.2016.05.004   \nYang, M., Badger, R., & Yu, Z. (2006). A comparative study of peer and teacher feedback in a Chinese EFL writing class. Journal of Second Language Writing, 15(3), 179\u2013200. https://doi.org/10.1016/j.jslw.2006.09.004   \nZhang, X. (2017). Reading\u2013writing integrated tasks, comprehensive corrective feedback, and EFL writing development. Language Teaching Research, 21(2), 217\u2013240. https://doi.org/10.1177/1362168815623291\n\nPublisher\u2019s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.", "metadata": {"authors": ["Nang Kham Thi", "Marianne Nikolov"], "category": "research", "confidence_score": 0.8, "document_type": "journal", "has_abstract": true, "has_methodology": true, "has_results": true, "key_findings": ["Grammarly provided feedback on surface-level errors, whereas teacher feedback covered both lower- and higher-level writing concerns, suggesting a potential for integration", "Students made effective use of the feedback received, and their writing performance improved according to the assessment criteria"], "methodology": "mixed", "publication_year": 2021, "research_questions": ["Whether and how successfully students exploited feedback from different sources in their revisions and how the feedback provisions helped improve their writing performance"], "source_file": "out_HVEKNL5H_Thi_and_Nikolov_-_2022_-_How_Te.md", "subject_area": "education", "tags": ["written corrective feedback", "teacher feedback", "automated feedback", "Grammarly", "second language writing"], "title": "How Teacher and Grammarly Feedback Complement One Another in Myanmar EFL Students' Writing"}, "search_text": "How Teacher and Grammarly Feedback Complement One Another in Myanmar EFL Students' Writing written corrective feedback teacher feedback automated feedback Grammarly second language writing education # How Teacher and Grammarly Feedback Complement One Another in Myanmar EFL Students\u2019 Writing\n\nNang Kham Thi1 $\\oplus$ \u2022 Marianne Nikolov2 (iD\n\nAccepted: 17 September 2021 / Published online: 4 October 2021   \n$\u00a9$ The Author(s) 2021\n\nAbstract Providing feedback on students\u2019 writing is considered important by both writing teachers and students. However, contextual constraints including excess workloads and large classes pose major and recurrent challenges for teachers. To lighten the feedback burden, teachers can take advantage of a range of automated feedback tools. This paper investigated how automated feedback can be integrated into traditional teacher feedback by analyzing the focus of teacher and Grammarly feedback through a written feedback analysis of language- and content-related issues. This inquiry considered whether and how successfully students exploited feedback from different sources in their revisions and how the feedback provisions helped improve their writing performance. The study sample of texts was made up of 216 argumentative and narrative essays written by 27 low-intermediate level students at a Myanmar university over a 13-week semester. By analyzing data from the feedback analysis, we found that Grammarly provided feedback on surface-level errors, whereas teacher feedback covered both lower- and higherlevel writing concerns, suggesting a potential for integration. The results from the revision analysis and pre- and post-tests suggested that students made effective use of the feedback received, and their writing performance improved according to the assessment criteria. The data were triangulated with self-assessment questionnaires regarding students\u2019 emic perspectives on how useful they found the feedback. The pedagogical implications for integrating automated and teacher feedback are presented.\n\nKeywords Written corrective feedback $\\cdot$ Teacher feedback $\\cdot$ Automated feedback $\\cdot$ Grammarly $\\cdot$ Second language writing\n\n# Highligh"}, "title": "How Teacher and Grammarly Feedback Complement One Another in Myanmar EFL Students' Writing", "authors": ["Nang Kham Thi", "Marianne Nikolov"], "tags": ["written corrective feedback", "teacher feedback", "automated feedback", "Grammarly", "second language writing"], "filename": "out_HVEKNL5H_Thi_and_Nikolov_-_2022_-_How_Te.md"}, {"score": 0.23312127590179443, "document": {"id": 251, "file_path": "annotated_data/batch_002/out_PK4VZZDQ_Nguyen_and_Barrot_-_2024_-_Dete.md", "filename": "out_PK4VZZDQ_Nguyen_and_Barrot_-_2024_-_Dete.md", "content": "# Detecting and assessing AI-generated and human-produced texts: The case of second language writing teachers\n\nLoc Nguyen a,1 , Jessie S. Barrot a,b,\\*,2\n\na School of Foreign Languages, University of Economics Ho Chi Minh City, Vietnam b College of Education, Arts and Sciences, National University, Manila, Philippines\n\n# A R T I C L E I N F O\n\n# A B S T R A C T\n\nKeywords:   \nChatGPT   \nComputer-assisted language learning   \nGenerative artificial intelligence   \nSecond language writing   \nWriting assessment\n\nArtificial intelligence (AI) technologies have recently attracted the attention of second language (L2) writing scholars and practitioners. While they recognize the tool\u2019s viability, they also raised the potential adverse effects of these tools on accurately reflecting students\u2019 actual level of writing performance. It is, therefore, crucial for teachers to discern AI-generated essays from humanproduced work for more accurate assessment. However, limited information is available about how they assess and distinguish between essays produced by AI and human authors. Thus, this study analyzed the scores and comments teachers gave and looked into their strategies for identifying the source of the essays. Findings showed that essays by a native English-speaking (NS) lecturer and ChatGPT were rated highly. Meanwhile, essays by an NS college student, non-native English-speaking (NNS) college student, and NNS lecturer scored lower, which made them distinguishable from an AI-generated text. The study also revealed that teachers could not consistently identify the AI-generated text, particularly those written by an NS professional. These findings were attributed to teachers\u2019 past engagement with AI writing tools, familiarity with common L2 learner errors, and exposure to native and non-native English writing. From these results, implications for L2 writing instruction and future research are discussed.\n\n# 1. Introduction\n\nArtificial intelligence (AI) has introduced personalized instruction, immersive learning experiences, and state-of-the-art language learning tools, which provide individualized feedback, adapt instructional content, and enhance learners\u2019 engagement (Hockly, 2023; Huang et al., 2023). One AI tool that has attracted the attention of scholars and practitioners is ChatGPT (Chat Generative Pre-trained Transformer), an advanced language model developed by OpenAI. Recent students have found that ChatGPT-enhanced instruction reduced anxiety, facilitated learning, and promoted learning and motivation (Lo, 2023; Ng et al., 2024). Its ability to respond contextually and coherently to prompts also makes it suitable as a language input source and platform for language learning activities, such as chat-based conversations.\n\nChatGPT\u2019s use extends to second language (L2) writing as it offers various writing affordances. Aside from its ability to produce coherent and grammatically correct full-length texts, this generative AI tool can perform various writing assistance tasks, such as formulating essay topics, creating outlines, providing content feedback, editing texts, and assigning automated scores (Barrot, 2023; Imran & Almusharraf, 2023; Song & Song, 2023). As in the case of other AI tools, ChatGPT exhibits certain limitations, including unintelligible responses, inaccurate bibliographic information and citations, and a lack of emotional depth and voice. Some scholars and practitioners have also raised its potential adverse effect on academic integrity, writing instruction, and writing assessment (Cotton et al., 2024; Yan et al., 2024). For instance, ChatGPT-assisted writing may not accurately reflect students\u2019 actual level of writing performance (Currie, 2023). This problem could mislead the teachers on the pedagogical strategies they need to use in their respective writing classrooms. Teachers\u2019 lack of skills in distinguishing between students\u2019 own writing and ChatGPT-produced texts can also give an unfair advantage to students who have access to the tool (Cotton et al., 2024). It is, therefore, crucial for teachers to discern AI-generated essays from student-produced work because it speaks to the authenticity of a student\u2019s writing abilities. Generative AI tools can produce sophisticated texts that may mask a student\u2019s actual writing proficiency, hindering accurate assessment and feedback.\n\nThus, the current study sheds light on how L2 writing teachers assess and distinguish ChatGPT-generated essays from humanproduced ones. With this study, we hope to gain nuanced insights into how they can better practice writing assessment, distinguish ChatGPT-generated texts from those written by human writers, and take appropriate actions to counter any form of academic dishonesty. Our findings would also be useful in developing pedagogical strategies or interventions based on L2 writing teachers\u2019 challenges in assessing potentially AI-generated content. Finally, this study could help schools in designing teacher training programs and recalibrating their policies to support the use of generative AI tools in writing classrooms.\n\n# 2. Literature review\n\n# 2.1. ChatGPT as a writing assistant tool\n\nChatGPT, developed by OpenAI, stands at the forefront of conversational AI technology, encapsulating complex algorithms to understand and respond to queries with human-like accuracy (OpenAI, 2024). It is underpinned by a Large Language Model (LLM), which is continuously refined to enhance its ability to produce varied and contextually relevant responses (Schade, 2023). As it evolves, ChatGPT is regularly updated to interpret user prompts better, employing a dynamic algorithmic structure that fosters diverse linguistic patterns (Ray, 2023). Despite its sophistication, efforts are made to mitigate biases within its responses. OpenAI has implemented specific strategies, such as fine-tuning the model with diverse training data, applying reinforcement learning with human feedback, and continually refining prompt guidelines to reduce potentially harmful or biased outputs (Hua et al., 2024; Ray, 2023). These measures aim to promote fairness, inclusivity, and sensitivity in ChatGPT\u2019s interactions by addressing biases that may arise from the data it was trained on. These iterative enhancements are vital as generative AI becomes more embedded in educational, profes sional, and casual settings, where the ability to converse naturally and informatively is paramount. ChatGPT serves as a transformative tool in language teaching and learning as it offers learners personalized and accessible resources for practicing various language skills, particularly beneficial for those without traditional learning opportunities (Barrot, 2024; Huang et al., 2022; Bin-Hady et al., 2023). Agustini (2023) underscores its role in fostering autonomous and reflective learning. Shaikh et al. (2023) observe that simulated conversations with ChatGPT enhance learners\u2019 language abilities, although it necessitates careful integration into teaching practices to maintain authenticity (Su et al., 2023).\n\nChatGPT\u2019s affordances extend to L2 writing as it encompasses the full spectrum of the writing process (Barrot, 2023). During topic generation, this generative AI tool uses its extensive database to identify and suggest topics that align with students\u2019 interests. This process involves analyzing patterns and themes from a wide-ranging collection of information, which provides personalized and relevant options for students to explore in their writing tasks (Ray, 2023; Su et al., 2023). ChatGPT also offers a variety of ideas and subtopics related to the main topic of interest during brainstorming. After brainstorming, it assists in structuring and organizing ideas into a coherent framework. This process ensures that the subsequent draft will have a logical flow (Imran & Almusharraf, 2023; Bin-Hady et al., 2023). Subsequently, the tool can act as a collaborative partner by offering suggestions during drafting. It is capable of offering feedback that can lead to substantive revisions and refinement of the text. Once the draft is complete, students can use ChatGPT\u2019s language editing features to fine-tune their work\u2019s grammar, syntax, and style. Despite these features, ChatGPT should be used ethically and responsibly by ensuring it supports the writing process rather than a means to bypass original effort (Huallpa, 2023; Lund et al., 2023).\n\n# 2.2. Teachers\u2019 ability to detect AI-generated texts\n\nWhile there have been a plethora of studies that examined the use of ChatGPT in L2 writing contexts (Imran & Almusharraf, 2023), L2 writing teachers\u2019 ability to distinguish AI-generated texts and assessment practices in this context remains an underexplored ter ritory. One of those who attempted to shed light on this area of inquiry is Herbold et al. (2023), who compared human-written and ChatGPT-generated essays. One of their objectives was to determine how teachers scored these two types of essays. The teachers were not provided with information on whether the essays were human-written or AI-generated, indicating that their assessments were based solely on the quality of the text. Results revealed that AI-generated argumentative essays were found to be of significantly higher quality than essays written by human users as assessed by teachers. As a quantitative study, their work did not examine teachers\u2019 comments on the scored texts and their ability to detect AI-generated essays.\n\nTo complement Herbold et al.\u2019s (2023) work, Alexander et al. (2023) qualitatively focused on the challenges faced by six experienced English as a Second Language (ESL) lecturers in identifying ChaTGPT-generated texts. They found that ESL lecturers failed to recognize the characteristics specific to AI-generated content fully. However, this study revealed that those teachers who had previous exposure to ChatGPT could recognize generative AI features in written texts. Teachers also tended to focus on errors as an indicator of L2 writing output and high levels of technical and grammatical accuracy as indicators of AI-generated. However, due to the small sample, the authors disclaimed that these findings should be treated with caution.\n\nIn a parallel study, Waltzer et al. (2023) focused on a broader demographic, examining both teachers\u2019 and students\u2019 abilities to differentiate between AI-generated essays and those written by high school students. Sixty-nine high school teachers were asked to read a pair of essays (one student-produced and one ChatGPT-generated) and assess which was generated by ChatGPT. Teachers\u2019 accuracy was reported to be at 70 percent, which is slightly higher than students\u2019 accuracy at 63 percent. Unlike Alexander et al.\u2019s (2023) findings, Waltzer et al. (2023) did not find prior experience with ChatGPT as a factor in predicting accuracy. The study also highlighted the challenge of distinguishing the two types of texts when the quality of writing is higher. However, their work did not delve into the teachers\u2019 assessment of these essays.\n\nFleckenstein et al. (2024) extended this inquiry by investigating novice $\\left( N = 8 9 \\right.$ ) and experienced teachers\u2019 $\\begin{array} { r } { \\left( N = 2 0 0 \\right. } \\end{array}$ ) ability to identify AI-generated texts using randomized-controlled experiments. Despite teachers\u2019 confidence in their ability to identify the source of the texts, the two experimental studies revealed that both groups could not identify texts generated by ChatGPT among student-produced texts. The ability of teachers to correctly identify the source of the texts was not perfect, with both AI-generated and student-generated texts being incorrectly identified to some extent. In terms of assessing the two types of essays, high-quality AI-generated texts received even higher scores compared to high-quality student-written texts, while low-quality student-written texts tended to receive higher scores than low-quality AI-generated texts. This could be due to the wider range of text quality in AI-generated texts. Their study, however, did not specifically examine teachers\u2019 comments on the two types of essays. Without examining their comments, we miss out on understanding their specific observations, interpretations, and feedback on the essays. This information could have provided valuable insights into how teachers engage with and evaluate AI-generated texts.\n\nCollectively, these three studies highlight the challenges that AI-generated texts pose for teachers in accurately assessing essays and the need to address these challenges in educational contexts. These studies also emphasized the need to understand further the complexities of teachers\u2019 ability to distinguish AI-generated from human-produced essays. They also emphasize the need for institutional policies that safeguard academic integrity and additional training to effectively assess AI-generated texts and to ensure fair and accurate grading of students\u2019 writing.\n\n# 2.3. Research questions\n\nAs reviewed, the above studies have provided insights into teachers\u2019 ability to distinguish AI-generated texts. However, it is not clear which text features distinguish AI-generated texts from human-produced texts and the strategies teachers employed in identifying AI-generated texts. To address this gap, our investigation was guided by the following specific research questions: (1) How do L2 writing teachers assess ChatGPT-generated texts and human-produced texts? (2) How do they distinguish between these two texts?\n\n# 2.4. Theoretical framework\n\nIn this study, we adopt sense-making theory as the theoretical framework to investigate teachers\u2019 capacity to identify ChatGPTgenerated texts. Sense-making theory refers to the cognitive process wherein individuals actively and continuously derive meaning from the information they encounter, framing it and using it to guide their actions and behaviors, thereby attributing significance to their experiences (Evans, 2007; Weick, 1996). This theory encompasses both cognitive processes and implementation behavior (Spillane et al., 2002; Weick, 1996), highlighting the active and adaptive nature of human cognition as individuals construct their realities by making sense of their beliefs, values, experience, and knowledge (Evans, 2007; Spillane & Anderson, 2014). Weick (1996) further emphasizes that sense-making is intertwined with individuals\u2019 identities and actions, socially constructed, and retrospective, allowing individuals to reflect on past events and construct narratives. It also involves synthesizing various information to create plausible explanations. Within the context of this study, sense-making theory suggests that teachers make decisions and act based on their understanding of ChatGPT and students, teaching experience, and environment both within and beyond the school. The theory\u2019s emphasis on perception, cognitive frames, social context, and retrospective sense-making can offer valuable insights into the in tricacies of L2 writing teachers\u2019 decision-making processes.\n\n# 3. Methodology\n\nThis qualitative study used a case study approach that enabled an in-depth investigation of L2 writing teachers\u2019 experience in assessing and detecting AI-generated essays. We consider the case study design appropriate because we are exploring a relatively new area of research. It also allows us to gather preliminary insights for further investigation and obtain an emic perspective of L2 writing teachers\u2019 assessment practices within their real-life contexts (Rashid et al., 2019).\n\n# 3.1. Context and participants\n\nOut of 10 teachers invited to participate in this study, six L2 writing instructors from various public universities in Vietnam accepted the invitation. They are non-native speakers of English and were selected using purposive sampling based on set criteria, which included a minimum of two years teaching EFL writing, experience in language assessment training, possession of at least a\n\nmaster\u2019s degree, and familiarity with ChatGPT. Their ages ranged from 29 to 54, with teaching experience spanning from 2 to 20 years. Of the eight teachers, three have earned master\u2019s degrees, and three have doctorate degrees. Except for one teacher, identified as T2, who has used the tool only once, all have engaged with ChatGPT for at least three months.\n\n# 3.2. Instrument\n\nThis study used semi-structured interviews with teachers and document analysis of assessed essays. The semi-structured interview comprises three parts. The first part is the background information section, which asks about the teacher\u2019s name, affiliation, sex, age, current position, years of teaching EFL writing, English courses handled, and educational level. The second part is the preliminary questions section, which asks about the teachers\u2019 familiarity and exposure to ChatGPT, technological tools used in the writing class, and language assessment training. The last part is the main questions section, which asks the following questions: (1) Can you explain the comments you provided in each of the essays? (2) Which of the five essays is likely the ChatGPT-generated text? (3) What specific features or patterns in the essay indicate that it is likely ChatGPT-generated or human-produced? (4) What strategies do you use when attempting to identify ChatGPT-generated texts? This interview guide was driven by the research questions and evaluated by two experts based on completeness, alignment with the research questions, clarity of questions and instructions, language accuracy, and length. Thereafter, we piloted the revised instrument to two teachers who were not part of the actual study to further enhance the instrument.\n\n# 3.3. Data-gathering procedure\n\nAll data were gathered following these three phases: generation and assessment of essays, interview with the teachers, and ex amination of assessed essays. As part of the first phase, five essays of 300\u2013400 words were provided to each teacher. These essays were produced by the following: native English-speaking (NS) college student (Essay 1), Vietnamese English teacher (Essay 2), NS lecturer (Essay 3), Vietnamese college student (Essay 4), and ChatGPT 3.5 (Essay 5). Each of them was asked to write an essay on the following topics: (1) Should teenagers be prohibited to use social media? (2) Should school uniforms be mandatory among college students? The sample writing prompt is as follows: In 300\u2013400 words, state your stand on the following topic:. Provide at least three arguments to support your position. You will have 90 minutes to complete this essay without the aid of any reference materials. In MS Word, computerize your essay using the following format: Font 12 Calibri, double-spaced, indented paragraphs.\n\nOnce all the needed essays were available, they were forwarded to each of the six teachers for their evaluation using the IELTS Writing Band Descriptors: Task 2 rubric. This rubric comprises four criteria, namely Task Response, which judges the relevance and development of ideas; Coherence and Cohesion, which looks at the organization and flow of the essay; Lexical Resource, assessing vo cabulary use and precision; and Grammatical Range and Accuracy, focusing on sentence structure and correctness. Each of these criteria has band descriptors ranging from 0 to 9, with 9 being the highest. Similar to Herbold et al. (2023), these teachers were not provided with information on whether the essays were human-written or AI-generated. Teachers were asked to examine each essay very carefully and provide feedback on its strong and weak points. They were given 60\u201390 minutes to complete the assessment of each essay without the aid of any automated writing evaluation tools or similar technologies. Note that informed consent was obtained from the participants prior to data gathering.\n\nThe interviews were conducted face-to-face, with efforts made to create a comfortable and engaging environment to encourage openness. While this setting helped facilitate candid responses, additional measures were taken to directly address social desirability bias. Participants were assured of confidentiality and anonymity, and they were reminded that there were no right or wrong answers to reduce any pressure to respond in socially desirable ways. This approach aimed to minimize bias by fostering an atmosphere where participants felt free to share their genuine perspectives (Bergen & Labont\u00b4e, 2020). The interview came in two stages. During the first stage, the teachers were asked to explain all their comments in each of the essays. They were allowed to freely share their thoughts and the reasons for each of the comments. The interviewer made follow-up questions only when necessary. Thereafter, they were asked to identify which among the five essays was likely to be ChatGPT-generated. During the second stage, teachers were asked about the specific features or patterns that guided them and strategies they used in distinguishing ChatGPT-generated texts from human-produced essays. Each interview lasted for about 60 minutes. All interviews were recorded to help the researchers accurately capture teachers\u2019 responses for transcription. As a way of triangulation, we examined the comments in the assessed essays to see if they aligned with the teachers\u2019 responses during the interview on the specific features and patterns in the texts that they identified as indicators of machine-generated and human-produced content.\n\n# 3.4. Data analysis\n\nThe data from interviews and document analysis were analyzed using cross-case analysis, which involves examining a series of cases to determine their areas of similarities and differences (Creswell & Creswell, 2018). Specifically, this approach seeks to examine, interpret, code, and compare data across multiple cases through cyclic iteration (Miles et al., 2019). This means that we began to process and analyze the transcript of the first participant before we proceeded to each of the succeeding participants. Using a thematic analysis driven by the two research questions as the main themes, we constructed the subthemes under each main theme. Then, we analyzed the transcript of the second participant and integrated the identified subthemes into the subthemes of the preceding data. To do this, we combined related subthemes, while the unique ones were allowed to emerge as a new subtheme. This progressive interplay between coding and analysis was repeated until all interview transcripts were processed. Two intercoders analyzed the data separately to ensure the reliability and rigor of the analysis. Prior to data analysis, they underwent a calibration session which involved reviewing the research questions, the instruments, and the data analysis procedure.\n\n# 4. Results\n\nOur study attempted to illustrate how L2 writing teachers assess and differentiate essays generated by ChatGPT from those written by humans. Specifically, we examined the scores and comments provided by teachers to these two types of texts. We also probed into their ability, strategies, and text features that help them in distinguishing between these essays. (Table 1).\n\n4.1. How L2 writing teachers assess ChatGPT-generated and human-produced texts\n\nTable 2 shows noteworthy patterns that emerge from L2 writing teachers\u2019 evaluation of ChatGPT- and human-produced essays. Overall, the highest scores were given to the essays written by the NS lecturer (essay 3), with scores ranging from 7.0 to 9.0 and ChatGPT 3.5 (essay 5), with scores ranging from 8.0 to 9.0. This data suggests that ChatGPT\u2019s writing quality mimics, quite convincingly, the writing level expected of a NS academic. Meanwhile, the essay produced by an NS college student (essay 1) was evaluated as on par with Vietnamese English teacher\u2019s work, receiving moderate scores. However, these two essays did not have as high score as essays 3 and 5. This scoring indicates that NS style of writing does not necessarily equate to the highest scores, which may imply that other aspects of writing (e.g., argument construction and the ability to articulate ideas) may have contributed to their higher scores. The essay written by a Vietnamese college student (essay 4), which received the lowest scores overall, exhibits a stark contrast to essays 1 and 2, underscoring a perceived deficit in writing quality of less experienced non-native writers. This data further accentuates the clear distinction between the writing quality of an L2 writing student and ChatGPT.\n\nTo further understand how the teachers assessed the essays, we examined the comments received by each essay. The feedback from various teachers on five different essays reveals insights into the perceived strengths and weaknesses of each piece of writing from multiple perspectives. The comments on essays 3 and 5 stand out in their consistent recognition of high quality and sophistication compared to the other essays. For both of these essays, the teachers\u2019 feedback centers on the strong, clear positions presented throughout, with ideas being relevant, extended, and well-supported. For instance, T1 commented that essay 3 \u201cpresents a welldeveloped response to the question with relevant, extended and supported ideas.\u201d The same observation applies to essay 5 which received a comment that it \u201cmaintains a clear position supporting the prohibition of teenagers from using social media\u201d (T1). In terms of cohesion and coherence, teachers complimented the two essays\u2019 logical sequencing of ideas and skillful paragraphing. T2 com mented that \u201cideas are well-supported and logically organized\u201d in essay 3, while T1 noted that \u201cthe ideas are presented coherently, with smooth transitions between paragraphs\u201d in essay 5. All teachers also characterize the vocabulary use of essays 3 and 5 as being both wide-ranging and sophisticated. In terms of grammatical range and accuracy, teachers noted very few grammatical lapses in essay 3 (T1, T2) and none in essay 5. This high-level writing quality led some teachers to raise the possibility of generative AI assistance in the two essays, a mention not made for the other essays.\n\nIn contrast, the feedback on essays 1, 2, and 4 highlights various deficiencies and areas for improvement. The comments for essay 1 note some issues with repetition and focus. T1 commented on the \"repetitive explanation\" and that the essay sometimes went \"off track.\" However, they acknowledge that it \"addresses all parts of the task,\" (T1) even if some parts may be more fully covered than others. The essay arranges information coherently and shows clear progression, but it suffers from some mechanical cohesion and referencing issues. The vocabulary range is described as adequate, with a mix of simple and complex sentences. Teacher 2 adds that the essay is \"generally well-written and coherent\" but is marred by \"some irrelevant information\" and is \"arguably overlong as an IELTS essay.\" The control of grammar is fairly good, despite some typical ESL errors. Essay 2, while meeting the basic requirements of the task, demonstrates weaknesses in clearly stating a thesis and addressing the task (T1) as well as maintaining focus (T4). The coherence and cohesion of essay 2 are noted to be less than ideal. For instance, some teachers described it as lacking a clear overall progression (T1) and overusing cohesive devices (T6). The use of vocabulary, while adequate, lacks the range necessary for high-level academic writing. Essay 2 received comments on vocabulary, such as \u201cuses a limited range of vocabulary\u201d (T1) and \u201cadequate with some word choice errors\u201d (T2). While there are attempts at complex grammatical structures, T1 noted that \u201cthese tend to be less accurate than simple sentences.\u201d\n\nEssay 4, while adequate in vocabulary and demonstrating grammatical complexity, notably deviates from the assigned topic, undermining its task response. Teachers consistently identified the essay\u2019s tendency to stray into discussions. According to T1, \u201cthe essay is off-topic, deviating from the question.\u201d T2 added that \u201csome information is irrelevant to the prompts and there is no conclusion drawn.\u201d Cohesion is another area of concern; the essay is marked by a lack of clear progression and insufficient use of cohesive devices, which, coupled with a blend of formal and informal tones, leads to a disjointed narrative (T1, T4, T6). Despite the sophisticated use of vocabulary, the actual lexical choices were not inconsistent and did not fully align with the content. T1 said that \u201cthere is a mix of formal and informal tones\u201d, while T3 commented that \u201cthis seems like a script for an oral debate.\u201d In terms of grammar, the essay exhibits several errors in forms and mechanics, as described by T4 and T6.\n\nTable 1 Teachers\u2019 assigned scores to ChatGPT- and human-produced essays.   \n\n<html><body><table><tr><td>Essays</td><td>T1</td><td>T2</td><td>T3</td><td>T4</td><td>T5</td><td>T6</td></tr><tr><td>Essay 1</td><td>6.0</td><td>6.5</td><td>7.0</td><td>8.0</td><td>7.5</td><td>7.5</td></tr><tr><td>Essay 2</td><td>7.0</td><td>6.5</td><td>7.5</td><td>7.5</td><td>6.5</td><td>7.0</td></tr><tr><td>Essay 3</td><td>8.5</td><td>9.0</td><td>8.0</td><td>9.0</td><td>7.0</td><td>9.0</td></tr><tr><td>Essay 4</td><td>5.5</td><td>5.5</td><td>7.5</td><td>5.0</td><td>6.5</td><td>6.5</td></tr><tr><td>Essay 5</td><td>8.5</td><td>9.0</td><td>8.5</td><td>8.0</td><td>8.5</td><td>9.0</td></tr></table></body></html>\n\nNote: Native English-speaking college student (Essay 1), Vietnamese English teacher (Essay 2), native English-speaking lecturer (Essay 3), Vietnamese college student (Essay 4), and ChatGPT 3.5 (Essay 5)\n\nTable 2 Teachers\u2019 Judgment if AI-generated or Not.   \n\n<html><body><table><tr><td>Essays</td><td>T1</td><td>T2</td><td>T3</td><td>T4</td><td>T5</td><td>T6</td></tr><tr><td>Essay 1</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td></tr><tr><td>Essay 2</td><td> Human</td><td>Human</td><td>Human</td><td> Human</td><td>Human</td><td> Human</td></tr><tr><td>Essay 3</td><td>AI</td><td>Human</td><td>Human</td><td>AI</td><td>Human</td><td>AI</td></tr><tr><td>Essay 4</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td><td>Human</td></tr><tr><td>Essay 5</td><td>Human</td><td>AI</td><td>AI</td><td>AI</td><td>AI</td><td>AI</td></tr></table></body></html>\n\nNote: Native English-speaking college student (Essay 1), Vietnamese English teacher (Essay 2), native English-speaking lecturer (Essay 3), Vietnamese college student (Essay 4), and ChatGPT 3.5 (Essay 5)\n\n# 4.2. How L2 writing teachers differentiate between ChatGPT-generated and human-produced texts\n\nTable 2 presents an intriguing look at the ability of teachers to discern between human-generated and AI-generated essays. Essays 1, 2, and 4 were unanimously and accurately identified as human-written by all six teacher evaluators. This uniformity suggests that these essays likely presented qualities or characteristics that teachers commonly associate with human writing. Specifically, teachers noted features such as a mix of simple and complex sentence structures, some cohesion issues, and occasional grammatical errors typical of ESL learners. Additionally, these essays displayed an overall progression and logical organization, but sometimes lacked clarity in certain arguments or included off-topic elements, especially in Essay 4, which drifted from the prompt. Vocabulary use varied, with some essays containing repetitive phrases or informal tones, and a few included unique phrases that seemed more characteristic of a human touch than AI-generated language. Conversely, essay 5, created by ChatGPT, was predominantly identified as AI-generated by teachers T2 through T6, with varying degrees of certainty. This could indicate that despite the sophisticated linguistic capabilities of ChatGPT, it may still produce discernible patterns or lack certain nuances that human writing typically exhibits, which these teachers were able to detect. Essay 3, penned by an NS lecturer, produced a divided response, where it was considered AI-generated by three teachers (T1, T4, T6) and human-generated by the other three (T2, T3, T5). T1 was particularly certain about the generative AI authorship, rating their certainty at the highest level, similar to T4 and T5. This split judgment suggests that the advanced writing of the NS lecturer resembles the structured and formal nature typically associated with AI-generated text. However, it is also possible that the generative AI\u2019s writing, designed to replicate sophisticated human styles through extensive training on human feedback, mirrors the complexity and coherence of professional-level writing. This alignment between the lecturer\u2019s and generative AI\u2019s writing styles highlights the challenge of differentiating advanced human writing from AI-generated text when both exhibit high levels of linguistic sophistication.\n\nWhen asked about the specific features that made them consider certain essays AI-generated, teachers\u2019 responses offer a nuanced understanding of the distinguishing features between ChatGPT-generated texts and those penned by humans. They identified several lexical, syntactic, semantic, discourse, and stylistic features that suggest a difference in how generative AI and humans approach essay writing. In terms of lexical features, T2 remarked that ChatGPT\u2019s writing \u201cseems to have more formal, academic words with precise meanings, and higher lexical density like complex noun phrases.\u201d T1 added that \u201cthere is a consistent use of advanced vocabulary, occasionally using overly intricate words or phrases to convey meaning\u201d in essays they considered as AI-generated. Human writers, on the other hand, tend to employ a mix of formal and colloquial language peppered with familiar phrases indicative of traditional essay writing.\n\nFrom a syntactic and grammatical perspective, T2 and T3 reported that ChatGPT-generated essays are characterized by complex sentence structures and a wide range of grammar, typically error-free and with a sophistication that mirrors NS. T1 echoes this observation, saying that \u201cChatGPT-generated text seldom includes simple sentences, showcasing a preference for more complex structures\u201d and does not commit \u201cgrammar mistakes, even the most minor ones.\u201d Human essays, conversely, contained the occasional grammatical slip-ups typical of both L2 learners and NS, suggesting a more authentic and less polished use of language. Meanwhile, T5 noticed that \u201cVietnamese students tend to use familiar structures such as $\\mathrm { \\Phi _ { i t + } }$ be $^ +$ adj\u2019, \u2018 $\\mathrm { m a k e } + s \\mathrm { b } / s \\mathrm { t h } +$ adj\u2019\u201d.\n\nSemantically and content-wise, ChatGPT\u2019s essays effectively convey meaning but sometimes miss the mark on appropriacy or clarity (T2), and may lack the depth found in human writing (T3, T5). T1 added that \u201cChatGPT-generated content may lack the depth found in human-authored pieces, often presenting arguments in a more generalized manner.\u201d T3 further added that the AI-generated text \u201clack of ability to analyse and evaluate ideas or give specific examples\u201d and \u201coften discusses both sides of an issue or sometimes one side of an issue and leave it to the user to make evaluative comments.\u201d Human writing, as noted, often reflects a cultural thinking style, particularly when non-native English speakers (NNS) are authoring the text, integrating their native cognitive patterns into English expressions (T5). Despite these notable differences, T2 felt it was \u201chard to tell which one is AI-generated or human-produced\u201d because both types of texts \u201cconvey meaning in a clear manner.\u201d\n\nDiscourse features reveal that ChatGPT tends to construct essays with consistent paragraph structures and a clear single idea per paragraph, adhering to conventional writing patterns (T1). On one hand, T2 noted that human-produced texts display greater diversity in discourse elements, with a more varied use of markers and connectors and a tendency to weave multiple ideas within a single paragraph. T5 added that \u201ceach paragraph of human-produced text may contain two or more ideas while ChatGPT-generated texts has clear structure \u2013 one main idea/one paragraph.\u201d\n\nStylistically, teachers noted that ChatGPT\u2019s content often exhibited a neutral tone, precision, and formal structure, aligning with features commonly associated with academic writing. According to T1, AI-generated text does not exhibit a distinctive writer\u2019s voice and \u201ctends to lack sentiment elements.\u201d T2 also observed that there is very little evidence of emotional tone in the AI-generated text. Meanwhile, T2 felt that the essay is written by human through its use of \u201cfigurative language, literary devices, and rhetorical tech niques.\u201d While this neutrality may distinguish AI-generated texts from more expressive forms of human writing that include sentiment and stylistic flourishes, it can also resemble academic writing produced by humans, where such neutrality, precision, and formal structure are standard. This similarity adds to the challenge of distinguishing AI-generated content from high-quality human-authored academic texts.\n\nAside from the text features, T1, T4, T5, and T6 shared that another strategy that helped them distinguish AI-generated texts from human-produced texts was their exposure to ChatGPT itself. T1 and T5 added that their previous engagement with AI tools, such as Grammarly, also contributed to their ability to spot AI-generated texts. Meanwhile, other teachers pointed their ability to spot AIgenerated text to their experience in teaching and assessing EFL students\u2019 writing (T2, T4, T5, T6) and their knowledge of stu dents\u2019 writing proficiency (T3, T4, T5). For instance, T2 said that he was \u201cquite familiar with the types of mistakes and errors learners usually make\u2026. Once that is absent, it poses an immediate question to me of whether the essay is written by a learner with native-like proficiency or by ChatGPT-generated.\u201d\n\n# 5. Discussion\n\nOur study probed into six L2 writing teachers\u2019 assessment and detection of AI-generated and human-produced essays as well as their strategies in distinguishing these two types of texts. Data revealed that they often awarded higher scores to essays by an NS lecturer and those generated by ChatGPT. This information provided empirical support for claims that the default writing proficiency level of ChatGPT is equal to that of an advanced professional writer (Barrot, 2023). The implications are profound because advanced generative AI cannot only imitate the complexity of native English prose but can do so to a degree that is indistinguishable from high-caliber human writing in the eyes of teachers. Essays by less experienced NNS writers received lower scores. This highlights a perceived deficit in their writing quality compared to the two high-scored texts. Teachers were able to identify most of the human-produced texts (essays 1, 2, and 4) with consistency, but there was less consensus when distinguishing between advanced human-authored texts and those produced by generative AI, with prior exposure to AI influencing their ability to detect AI-generated writing. Comments on essays revealed teachers\u2019 reliance on specific text features, exposure to AI tools, experience in teaching and assessing essays, and knowledge of students to guide their assessments. This approach emphasizes the complexity and multidimensionality of evaluating AI-generated content in L2 writing contexts.\n\nAnchored on sense-making theory, the results of our study suggest that teachers construct their understanding and judgments of student texts through a complex interplay of previous knowledge, contextual cues, and the content of the essays themselves (Evans, 2007; Spillane & Anderson, 2014; Weick, 1996). The teachers\u2019 evaluations point to an interpretative process where the high scores awarded to the essays by the NS lecturer and ChatGPT indicate a constructed equivalence between technical proficiency and high writing quality. This can be seen as an outcome of teachers making sense of the texts in light of their expectations and experiences with native English writing, where a strong command of language and argumentation is highly valued. Furthermore, the study results reveal how teachers apply their cognitive frameworks to distinguish between human and AI-generated texts. The consistent identification of essays 1, 2, and 4 as human-produced demonstrates teachers\u2019 reliance on certain writing characteristics associated with human authors, such as error patterns and stylistic idiosyncrasies, to make sense of the essays\u2019 origins. Conversely, the divided opinion on essay 3 and the predominant identification of essay 5 as AI-generated reflect the complexity of this sense-making process, where certain qualities can blur the lines between human and machine output. The specific features highlighted by teachers, such as lexical choice and sentence structure, further demonstrate the retrospective aspect of sense-making, as teachers reflect on the patterns they have come to associate with generative AI versus human writing. This retrospective reflection shapes their expectations and guides their assessment behavior. For instance, the presence of complex noun phrases and a high degree of grammatical accuracy led some teachers to attribute essays to ChatGPT, as these features fit within their constructed understanding of AI-generated writing. Moreover, the teachers\u2019 sense-making is evidently a socially constructed process. This is evident in the fact that teachers with prior exposure to ChatGPT or similar AI tools, like Grammarly, demonstrated a different evaluative approach to those without such exposure.\n\nWith reference to previous studies, our study resonates with the findings of Herbold et al. (2023), who noted that teachers scored AI-generated essays as higher quality based solely on textual assessment. The challenge of identifying AI-generated content, as highlighted in Alexander et al. (2023), is exemplified in our study\u2019s findings where essays produced by ChatGPT received some of the highest scores. This correlation suggests that prior exposure to generative AI can play a role in the recognition of such content, supporting the idea that experience with AI-generated texts can enhance detection capabilities. However, this notion is contrasted by Waltzer et al. (2023), whose broader demographic study did not find a significant link between prior exposure to ChatGPT and the accuracy of identifying AI-generated texts. Their results, alongside ours, illuminate the difficulties teachers face when the quality of writing is high, underpinning the need for refined assessment skills in discerning the origins of sophisticated texts. Furthermore, the work of Fleckenstein et al. (2024) aligns with our study by showing the challenges in AI text detection, even among experienced teachers. Both studies reveal that without explicit markers or an in-depth examination of comments and assessments, distinguishing between generative AI and human writing remains a nuanced task. The discrepancies in scoring, where AI-generated essays sometimes receive higher scores than human-produced ones, underscore the need for more research and training for teachers to ensure fair and accurate assessment practices in an L2 writing landscape increasingly influenced by generative AI.\n\nOur study enriches the current understanding of the capacity of L2 writing teachers to assess and differentiate between AIgenerated and human-produced texts. While previous studies have laid the groundwork for recognizing the infiltration of genera tive AI in L2 writing contexts, our research provides additional layers of insight by examining not only the scores but also the qual itative feedback and the strategies teachers employed during writing assessment. By doing so, we shed light on why teachers might score AI-generated texts higher and the factors that contribute to these assessments. Our study also adds nuance to how generative AI can mimic high-quality writing by suggesting that the detection of AI-generated texts by teachers may not solely be a matter of recognizing the absence of errors or the presence of complex structures but also involves appreciating the depth of argument and articulation of ideas, voice and emotional depth, areas where generative AI might still lag behind human writers. One insightful aspect of our findings is the indication that there is not a clear-cut method for distinguishing between human and generative AI writing, especially as AI writing tools become more advanced. The detection of AI-generated text by some teachers, particularly for essay 5, may point to the presence of certain tell-tale linguistic or structural signatures that generative AI tends to produce. Alternatively, it may reflect a bias or an expectation of what AI-written text should look like.\n\n# 6. Conclusion\n\nThis research explores how L2 writing instructors distinguish between essays produced by ChatGPT and human authors. It involved an analysis of both the scores and comments given by teachers and looked into their methods for identifying the source of the essays. The study found that essays by an NS lecturer and ChatGPT were rated highly, suggesting ChatGPT can replicate advanced English writing. In contrast, essays by less experienced L2 writers and NNS scored lower, which makes them distinguishable from an AIgenerated text. The study also revealed that teachers could not consistently identify the AI-generated text, particularly those writ ten by proficient writers. Their past engagement with AI writing tools, their familiarity with common L2 learner errors, and their exposure to native and non-native English writing seemed to have influenced their ability to discern AI-generated content from humanproduced texts. This points to the nuanced challenges in distinguishing between human and generative AI writing. Overall, the research underscores the complexities in evaluating AI-generated texts in L2 writing contexts.\n\nOur findings have several implications. Practically speaking, our findings highlight the need to train teachers in recognizing and understanding the characteristics of AI-produced texts, which could involve adding specific modules on digital literacy to teacher education programs. Teachers might also need to be trained to look for new indicators of generative AI authorship or to consider a wider range of qualitative factors beyond mere linguistic and argumentative competency. The results of our study could stimulate debate and policy-making about the ethical implications of generative AI in education, particularly concerning fairness, transparency, and the development of student writing skills in an increasingly digital and AI-integrated landscape. Since the line between AI assistance and academic dishonesty could become blurred, educational institutions may also need to establish clear guidelines about using generative AI tools in coursework. From a theoretical standpoint, these findings challenge existing frameworks of writing assessment by introducing a non-human writer into the mix. They call for an expansion of composition theory to include the capabilities and limitations of generative AI. The traditional process of writing as solely a human endeavor is being redefined, urging scholars to reconsider concepts of authorship and originality in light of generative AI\u2019s growing role. Methodologically, the study highlights the importance of including qualitative data, such as teacher comments, in research on writing assessment. Quantitative scoring alone might not capture the full picture; in-depth analysis of teacher feedback provides richer insight into the evaluative process. This may encourage future research to adopt mixed-methods approaches for a comprehensive evaluation of writing, taking into account both the end product and the cognitive processes behind the scoring.\n\nWhile our study provided important insights into L2 writing teachers\u2019 assessment practices within a generative AI environment, certain limitations should be acknowledged. Notably, we used GPT-3.5 to generate AI texts in this study. While GPT-3.5 was a capable model, AI language tools have since advanced, and newer models exhibit enhanced linguistic abilities and content generation. Thus, findings related to teacher accuracy in distinguishing between AI-generated and human-generated texts may not fully translate to interactions with more recent generative AI models that display increasingly sophisticated writing styles and nuanced responses. Nevertheless, our methodology and approach offer a valuable foundation for future research involving more advanced AI technologies. We recommend that future studies adopt a broader scope by including a more comprehensive set of texts written by diverse students and incorporating multiple cutting-edge AI tools. This expanded approach would deepen insights into how teachers assess writing across different AI technologies and student demographics, addressing the rapidly evolving nature of AI and the challenges it presents in education. As generative AI evolves rapidly, research must focus on developing robust methods and theoretical frameworks that can be adapted to examine the efficacy of varied and more advanced models, given that educational research will inevitably lag behind the pace of AI development. Additionally, limitations related to sample size and the specific demographic of Vietnamese L2 writing teachers may constrain the generalizability of our findings. A broader range of educational contexts would yield more widely applicable insights. The exclusive focus on teachers also leaves out the perspective of students, whose experiences with AI-generated texts are equally significant. Future research should aim for more extensive and diverse educational settings to enhance the generalizability of the findings. The methodology could be expanded beyond case studies to incorporate experimental or correlational designs that measure the reliability of teachers\u2019 assessments. Furthermore, longitudinal studies could examine how continued expo sure to AI writing tools affects teachers\u2019 assessment capabilities over time. It is also essential to investigate the psychological and cognitive processes involved in assessing AI-generated texts, which might reveal inherent biases or patterns in human judgment. Finally, the study underscores the need for interdisciplinary research, combining insights from education, linguistics, cognitive sci ence, and computer science, to develop more sophisticated methods for detecting and understanding AI\u2019s role in educational contexts.\n\n# CRediT authorship contribution statement\n\nJessie Barrot: Writing \u2013 original draft, Methodology, Formal analysis, Conceptualization. Loc Nguyen: Writing \u2013 review & editing, Resources, Methodology, Investigation, Funding acquisition, Formal analysis, Data curation, Conceptualization.\n\n# Acknowledgements\n\nThis research is funded by the University of Economics Ho Chi Minh City, Vietnam.\n\n# Data Availability\n\nData will be made available on request.\n\n# References\n\nAgustini, N. P. O. (2023). Examining the role of ChatGPT as a learning tool in promoting students\u2019 English language learning autonomy relevant to Kurikulum Merdeka Belajar. Edukasia: Jurnal Pendidikan Dan Pembelajaran, 4(2), 921\u2013934.   \nAlexander, K., Savvidou, C., & Alexander, C. (2023). Who wrote this essay? Detecting AI-generated writing in second language education in higher education. Teaching English with Technology, 23(2), 25\u201343.   \nBarrot, J. S. (2023). Using ChatGPT for second language writing: Pitfalls and potentials. Assessing Writing, 57, 100745. https://doi.org/10.1016/j.asw.2023.100745   \nBarrot, J. S. (2024). ChatGPT as a language learning tool: An emerging technology report. Technology, Knowledge and Learning, 29(2), 1151\u20131156. https://doi.org/ 10.1007/s10758-023-09711-4   \nBergen, N., & Labont\u00b4e, R. (2020). \u201cEverything is perfect, and we have no problems\u201d: Detecting and limiting social desirability bias in qualitative research. Qualitative Health Research, 30(5), 783\u2013792.   \nBin-Hady, W. R. A., Al-Kadi, A., Hazaea, A., & Ali, J. K. M. (2023). Exploring the dimensions of ChatGPT in English language learning: A global perspective. Library Hi Tech. https://doi.org/10.1108/LHT-05-2023-0200   \nCotton, D. R., Cotton, P. A., & Shipway, J. R. (2024). Chatting and cheating: Ensuring academic integrity in the era of ChatGPT. Innovations in Education and Teaching International, 61(2), 228\u2013239. https://doi.org/10.1080/14703297.2023.2190148   \nCreswell, J. W., & Creswell, J. D. (2018). Research design: Qualitative, quantitative, and mixed methods approaches (fifth edition). Sage.   \nCurrie, G. (2023). Academic integrity and artificial intelligence: Is ChatGPT hype, hero or heresy? Seminars in Nuclear Medicine, 53(5), 719\u2013730.   \nEvans, A. E. (2007). School leaders and their sensemaking about race and demographic change. Educational Administration Quarterly, 43(2), 159\u2013188.   \nFleckenstein, J., Meyer, J., Jansen, T., Keller, S. D., Koller, \u00a8 O., & Moller, \u00a8 J. (2024). Do teachers spot AI? Evaluating the detectability of AI-generated texts among student essays. Computers and Education: Artificial Intelligence, 6, Article 100209.   \nHerbold, S., Hautli-Janisz, A., Heuer, U., Kikteva, Z., & Trautsch, A. (2023). A large-scale comparison of human-written versus ChatGPT-generated essays. Scientific Reports, 13(1), 18617.   \nHockly, N. (2023). Artificial intelligence in English language teaching: The good, the bad and the ugly. RELC Journal. https://doi.org/10.1177/003368822311685   \nHua, S., Jin, S., & Jiang, S. (2024). The limitations and ethical considerations of ChatGPT. Data Intelligence, 6(1), 201\u2013239. https://doi.org/10.1162/dint_a_00243   \nHuallpa, J. J. (2023). Exploring the ethical considerations of using Chat GPT in university education. Periodicals of Engineering and Natural Sciences, 11(4), 105\u2013115.   \nHuang, W., Hew, K., & Fryer, L. (2022). Chatbots for language learning\u2014Are they really useful? A systematic review of chatbot-supported language learning. Journal of Computer Assisted Learning, 38(1), 237\u2013257. https://doi.org/10.1111/jcal.12610   \nHuang, X., Zou, D., Cheng, G., Chen, X., & Xie, H. (2023). Trends, research issues and applications of artificial intelligence in language education. Educational Technology Society, 26(1), 112\u2013131.   \nImran, M., & Almusharraf, N. (2023). Analyzing the role of ChatGPT as a writing assistant at higher education level: A systematic review of the literature. Contemporary Educational Technology, 15(4), ep464.   \nLo, C. K. (2023). What is the impact of ChatGPT on education? A rapid review of the literature. Education Sciences, 13(4), 410. https://doi.org/10.3390/ educsci13040410   \nLund, B. D., Wang, T., Mannuru, N. R., Nie, B., Shimray, S., & Wang, Z. (2023). ChatGPT and a new academic reality: Artificial Intelligence-written research papers and the ethics of the large language models in scholarly publishing. Journal of the Association for Information Science and Technology, 74(5), 570\u2013581.   \nMiles, M. B., Huberman, A. M., & Saldana, J. (2019). Qualitative data analysis (fourth ed.). Sage Publications.   \nNg, D. T. K., Tan, C. W., & Leung, J. K. L. (2024). Empowering student self-regulated learning and science education through ChatGPT: A pioneering pilot study. British Journal of Educational Technology. https://doi.org/10.1111/bjet.13454   \nOpenAI. (2024). ChatGPT: Optimizing language models for dialogue. \u3008https://openai.com/blog/chatgpt/\u3009.   \nRashid, Y., Rashid, A., Warraich, M. A., Sabir, S. S., & Waseem, A. (2019). Case study method: A step-by-step guide for business researchers. International Journal of Qualitative Methods, 18, 1\u201313.   \nRay, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key challenges, bias. Ethics, limitations and Future scope Internet of Things and CyberPhysical Systems, 3, 121\u2013154.   \nSchade, M. (2023). How ChatGPT and our language models are developed. \u3008https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-language-models-aredeveloped\u3009.   \nShaikh, S., Yayilgan, S. Y., Klimova, B., & Pikhart, M. (2023). Assessing the usability of chatgpt for formal English language learning. European Journal of Investigation in Health, Psychology and Education, 13(9), 1937\u20131960. https://doi.org/10.3390/ejihpe13090140   \nSong, C., & Song, Y. (2023). Enhancing academic writing skills and motivation: Assessing the efficacy of ChatGPT in AI-assisted language learning for EFL students. Frontiers in Psychology, 14, 1260843.   \nSpillane, J. P., & Anderson, L. (2014). The architecture of anticipation and novices\u2019 emerging understandings of the principal position: Occupational sense making at the intersection of individual, organization, and institution. Teachers College Record, 116(7), 1\u201342.   \nSpillane, J. P., Reiser, B. J., & Reimer, T. (2002). Policy implementation and cognition: Reframing and refocusing implementation research. Review of Educational Research, 72(3), 387\u2013431.   \nSu, Y., Lin, Y., & Lai, C. (2023). Collaborating with ChatGPT in argumentative writing classrooms. Assessing Writing, 57, Article 100752. https://doi.org/10.1016/j. asw.2023.100752   \nWaltzer, T., Cox, R. L., & Heyman, G. D. (2023). Testing the ability of teachers and students to differentiate between essays generated by ChatGPT and high school students. Human Behavior and Emerging Technologies, 2023, 1923981.   \nWeick, K. E. (1996). Sensemaking in organizations. Sage.   \nYan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonado, R., Chen, G., \u2026 & Ga\u02c7sevi\u00b4c, D. (2024). Practical and ethical challenges of large language models in education: A systematic scoping review. British Journal of Educational Technology, 55(1), 90\u2013112.   \nLoc Nguyen is an academic researcher and lecturer from University of Economics, Ho Chi Minh City, Vietnam. His research interest includes teaching pronunciation,   \ncorrective feedback, and L2 writing.", "metadata": {"authors": ["Loc Nguyen", "Jessie S. Barrot"], "category": "research", "confidence_score": 0.8, "document_type": "journal", "has_abstract": true, "has_methodology": true, "has_results": true, "key_findings": ["Essays by a native English-speaking lecturer and ChatGPT were rated highly", "Essays by a native English-speaking college student, non-native English-speaking college student, and non-native English-speaking lecturer scored lower", "Teachers could not consistently identify the AI-generated text, particularly those written by a native speaker professional"], "methodology": "quantitative", "publication_year": null, "research_questions": [], "source_file": "out_PK4VZZDQ_Nguyen_and_Barrot_-_2024_-_Dete.md", "subject_area": "education", "tags": ["ChatGPT", "Computer-assisted language learning", "Generative artificial intelligence", "Second language writing", "Writing assessment"], "title": "Detecting and assessing AI-generated and human-produced texts"}, "search_text": "Detecting and assessing AI-generated and human-produced texts ChatGPT Computer-assisted language learning Generative artificial intelligence Second language writing Writing assessment education # Detecting and assessing AI-generated and human-produced texts: The case of second language writing teachers\n\nLoc Nguyen a,1 , Jessie S. Barrot a,b,\\*,2\n\na School of Foreign Languages, University of Economics Ho Chi Minh City, Vietnam b College of Education, Arts and Sciences, National University, Manila, Philippines\n\n# A R T I C L E I N F O\n\n# A B S T R A C T\n\nKeywords:   \nChatGPT   \nComputer-assisted language learning   \nGenerative artificial intelligence   \nSecond language writing   \nWriting assessment\n\nArtificial intelligence (AI) technologies have recently attracted the attention of second language (L2) writing scholars and practitioners. While they recognize the tool\u2019s viability, they also raised the potential adverse effects of these tools on accurately reflecting students\u2019 actual level of writing performance. It is, therefore, crucial for teachers to discern AI-generated essays from humanproduced work for more accurate assessment. However, limited information is available about how they assess and distinguish between essays produced by AI and human authors. Thus, this study analyzed the scores and comments teachers gave and looked into their strategies for identifying the source of the essays. Findings showed that essays by a native English-speaking (NS) lecturer and ChatGPT were rated highly. Meanwhile, essays by an NS college student, non-native English-speaking (NNS) college student, and NNS lecturer scored lower, which made them distinguishable from an AI-generated text. The study also revealed that teachers could not consistently identify the AI-generated text, particularly those written by an NS professional. These findings were attributed to teachers\u2019 past engagement with AI writing tools, familiarity with common L2 learner errors, and exposure to native and non-native English writing. From these results, implications for L2 writing instruction and future research are discussed.\n\n# 1. Introduction\n\nArtificial intelligence (AI) has introduced personal"}, "title": "Detecting and assessing AI-generated and human-produced texts", "authors": ["Loc Nguyen", "Jessie S. Barrot"], "tags": ["ChatGPT", "Computer-assisted language learning", "Generative artificial intelligence", "Second language writing", "Writing assessment"], "filename": "out_PK4VZZDQ_Nguyen_and_Barrot_-_2024_-_Dete.md"}], "created_at": 1757693173.6543171}