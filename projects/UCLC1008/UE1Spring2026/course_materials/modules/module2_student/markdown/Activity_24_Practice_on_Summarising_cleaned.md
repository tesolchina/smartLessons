# Module 2: Paraphrasing, Summarising & Synthesising Skills
**UCLC1008 UE1 (2025-26, S1)**  
© 2025 Language Centre-HKBU

## Activity 2.4: Practice on Summarising
To practice what you have learned, apply the recommended steps to complete the following task related to the article by Andrejevic and Selwyn (2020):

1. Focus on the sections provided below, carefully identify the main points, highlight the most important sentences, and categorize them into two main arguments. Then, discuss your ideas with your peers.
2. Based on your discussion, summarize each of the two arguments by stating the claim and briefly explaining the reasoning behind it in no more than 100 words.

## Article 2 Excerpt: Andrejevic and Selwyn (2020, pp. 121-124)
### Challenging the Take-Up of Facial Recognition in Schools
The authors raise important questions about diminished notions of pedagogy and consent regarding facial recognition technologies in schools. Beyond these, they argue that several additional issues cast serious doubt on the implementation of such technologies. The following points of contention are discussed:

### 1. The Dehumanising Nature of Facially Focussed Schooling
Facial recognition technologies quantify a student’s face through statistical processes, reducing personal features to numerical values and comparisons. As described by Bousquet (2018), this *mechanistic gaze* abstracts students into statistical images, processed algorithmically without human observation. Unlike human perception, these systems cannot discern complex emotions, leading to a reductive engagement with students. Consequently, students may need to contort their expressions unnaturally to be detected, which can feel dehumanising. Additionally, some students might manipulate expressions to "game the system," further highlighting the distancing and impersonal nature of the technology.

### 2. The Foregrounding of Students’ Gender and Race
Facial recognition technologies often emphasize race and gender in decision-making, raising concerns about discrimination. Stark (2019) notes that these systems arbitrarily categorize individuals based on race and gender, often using racially skewed databases that fail to recognize non-white faces accurately. Even with improved accuracy, sorting students into racialized or gendered categories remains problematic, reviving debunked race "science" and conflating biological traits with social attributes. Stark (2019, p. 53) argues that such technologies inherently incline societies toward racism, exacerbating existing discriminatory practices in schools and beyond.

### 3. The Inescapable Nature of School-Based Facial Recognition
Facial data, unlike other personal data, is inherently tied to an individual, enabling constant surveillance in schools. Students cannot curate or restrict facial data as they might with social media or other interactions. While opt-out options may exist for some systems, non-cooperative facial recognition renders opting out ineffective. School policies, such as dress codes preventing face coverings, further limit students’ ability to avoid surveillance. Moreover, "informed consent" is undermined as systems scan entire areas like classrooms, making opt-in or opt-out approaches impractical for system providers.

### 4. The Elimination of Obscurity
Proponents of facial recognition often claim, *if you have nothing to hide, you have nothing to fear*. However, this overlooks the value of obscurity for some students. Constant surveillance curtails their right to blend into the background or manage visibility in school—a legitimate coping strategy for many. Schools are crucial for developing social identity, and obscurity allows students to control how they are perceived (Gordon et al., 2000). Hartzog and Selinger (2018) describe this as *the normalized elimination of practical obscurity*, undermining students' autonomy in social spaces.

### 5. The Increased Authoritarian Nature of Schooling
Hartzog and Selinger (2018) argue that surveillance via facial recognition is *intrinsically oppressive*. These technologies align with schools’ historical role in regulating and disciplining students, often used to control access and movement. This exacerbates authoritarian tendencies rather than mitigating them, tempting overreach and mission creep. Students may alter their behavior to conform, impeding opportunities for *human flourishing*—a core goal of education. The authors warn that the power of such systems risks systematic abuse and normalizes restrictive conduct within school environments.

### 6. The Cascading Logic of Automation
Facial recognition systems create biometric, geotagged databases through automated data capture. This leads to a *cascading process of automation*, where large databases necessitate automated processing and decision-making. Beyond simple monitoring, these databases enable functions like risk detection, attendance tracking, and expression analysis, often displacing human judgment. While automation offers speed and efficiency, it risks *social de-skilling* by reducing human involvement in understanding student needs and behaviors. This undermines essential socialization processes integral to learning, especially in resource-constrained schools tempted by such technologies.

### 7. The Future Oppression of Marginalised Groups Within Schools
Facial recognition in schools aims to control and standardize student behavior, disproportionately harming those who do not fit into standardized systems. Marginalized groups, such as racial minorities or queer and trans students, are particularly vulnerable. Keyes (2019) highlights how data-driven technologies oppose the fluidity and autonomy valued by queer individuals, relying on rigid norms and categories. Inaccurate or incomplete data profiles for non-conforming students can lead to misrepresentation, ignored issues, or unwarranted assumptions, reinforcing existing hierarchies and deepening discrimination within heteronormative school contexts.