## Page 1

1612 |   
 
Br J Educ Technol. 2025;56:1612–1639.
wileyonlinelibrary.com/journal/bjet
Received: 9 April 2024 | Accepted: 3 November 2024
DOI: 10.1111/bjet.13540  
O R I G I N A L  A R T I C L E
Using AI- supported peer review to enhance 
feedback literacy: An investigation of students' 
revision of feedback on peers' essays
Kai Guo1
 |   Emily Di Zhang2
 |   Danling Li3
 |   Shulin Yu4
© 2024 British Educational Research Association.
1The University of Hong Kong, Hong Kong, 
China
2Shanghai Jiao Tong University, Shanghai, 
China
3The Chinese University of Hong 
Kong(Shenzhen), Shenzhen, China
4University of Macau, Macao SAR, China
Correspondence
Danling Li, The Chinese University of 
Hong Kong (Shenzhen), 2001 Longxiang 
Boulevard, Longgang District, Shenzhen, 
China.
Email: lidanling@cuhk.edu.cn
Funding information
This work was supported by the Research 
Postgraduate Student Innovation Award 
2022/23 and Dissertation Year Fellowship, 
awarded by the Graduate School of the 
University of Hong Kong, to the first author.
Abstract: As a vital learning activity in second lan-
guage (L2) writing classrooms, peer feedback plays 
a crucial role in improving students' writing skills. 
However, student reviewers face challenges in provid-
ing impactful feedback on peers' essays. Low- quality 
peer reviews emerge as a persistent problem, ad-
versely affecting the learning effect of peer feedback. 
To enhance students' peer feedback provision, this 
study introduces EvaluMate, an AI- supported peer 
review system, which incorporates a chatbot named 
Eva, designed to evaluate and provide feedback on 
student reviewers' comments on peers' essays. Forty- 
four Chinese undergraduate students engaged with 
EvaluMate, utilising its features to generate feed-
back on peers' English argumentative essays. Chat 
log data capturing the students' interactions with the 
chatbot were collected, including the comments they 
wrote on peer essays and the feedback offered by the 
chatbot on their comments. The results indicate that 
the integration of AI supervision improved the quality 
of students' peer reviews. Students employed various 
strategies during their comment revision in response 
to AI feedback, such as introducing new points, add-
ing details, and providing illustrative examples, which 
helped improve their comment quality. These findings 
shed light on the benefits of AI- supported peer review 
systems in empowering students to provide more val-
uable feedback on peers' written work.
K E Y W O R D S
peer feedback, artificial intelligence, chatbots, L2 writing, 
feedback literacy, EvaluMate

## Page 2

| 1613
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
INTRODUCTION
Peer feedback, the process of students evaluating and critiquing the work of their peers, 
has been extensively used in second language (L2) writing classrooms (Banister, 2023; 
Lee, 2017; Yu, 2024). It is recognised as an effective strategy for promoting student feedback 
literacy (Carless & Boud, 2018; Nicol et al., 2014). Developing feedback literacy requires 
students to cultivate evaluative judgement, which involves assessing the quality of their own 
work and that of others (McConlogue, 2015; Tai et al., 2018). This skill is a critical component 
of feedback literacy, as highlighted by Carless and Boud (2018).
Although existing research suggests the significant role of peer feedback in supporting 
L2 student writers (Lu et al., 2023; Yu & Lee, 2016), providing feedback can be a com-
plex and demanding task (Latifi et al., 2021; Yu, 2021). Students are more likely to develop 
their feedback literacy through peer feedback if they receive adequate guidance (Carless 
Practitioner notes
What is already known about this topic
• Scholars have extensively investigated diverse pedagogical strategies to en-
hance students' peer feedback provision skills in second language (L2) writing 
classrooms.
• Artificial intelligence (AI) technologies have been utilised to monitor and evaluate 
the peer feedback generated by student reviewers.
• AI- enabled peer feedback evaluation tools have demonstrated the ability to pro-
vide valid assessments of student reviewers' peer feedback.
What this paper adds
• In the context of L2 writing, there is a lack of bespoke AI- enabled peer feedback 
evaluation tools. To address this gap, we have developed an AI- supported peer re-
view system, EvaluMate, which incorporates a large language model- based chat-
bot named Eva. Eva is designed to provide feedback on L2 students' comments 
on their peers' writing.
• While previous studies have primarily focused on assessing the validity of AI- 
enabled peer feedback evaluation tools, little is known about how students incor-
porate AI support into improving their peer review comments. To bridge this gap, 
our study examines not only whether using the system (EvaluMate) can enhance 
the quality of L2 students' peer review comments but also how students respond 
to Eva's feedback when revising their comments.
Implications for practice and/or policy
• The development of the AI- supported peer review system (EvaluMate) introduces 
an innovative pedagogical approach for L2 writing teachers to train and enhance 
their students' peer feedback provision skills.
• Integrating AI supervision into L2 students' peer feedback generation improves the 
quality of comments provided by student reviewers on their peers' writing.
• Students employ various strategies when revising their comments in response to 
Eva's feedback, and these strategies result in varying degrees of improvement 
in comment quality. L2 writing teachers can teach effective revision strategies to 
their students.
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 3

1614 |   
GUO et al.
& Boud, 2018; Min, 2006). Numerous studies have investigated pedagogical approaches to 
enhance students' peer feedback, with some employing artificial intelligence (AI) technolo-
gies to supervise students' peer feedback generation (Darvishi et al., 2024; Guo et al., 2024; 
Han et al., 2023; Jia et al., 2021). These AI tools simulate teachers in providing analysis of 
peer feedback quality and intervene in students' peer feedback creation, showing positive 
effects on peer feedback quality.
Utilising AI to monitor and evaluate student reviewers' peer feedback enables students 
to have better opportunities for self- assessment (Han et al., 2023), allowing them to im-
prove their evaluative skills over time. Incorporating AI supervision into the peer feedback 
generation process requires students to engage in planning, drafting, and revising while 
continuously refining their assessments of work quality (Guo, 2024; Guo et al., 2024). This 
iterative process allows students to generate internal feedback on their progress, with ex-
ternal feedback from AI further supporting students in sharpening their internal evaluations. 
AI evaluation and monitoring would serve as a valuable resource for motivating students 
to share and discuss their judgements, providing ongoing opportunities for enhancing their 
ability to make informed decisions.
However, current research has primarily focused on the effectiveness and outcomes of 
these AI tools. Further investigation is needed into how students integrate AI assessments 
to revise and improve their peer review comments. To address the research gap, the present 
study introduces an AI- supported peer review system called EvaluMate, which integrates 
a large language model- based chatbot to enhance students' peer feedback provision in 
English writing. We aim to examine whether the system improves students' peer feedback 
quality and how students act on the AI supervision. This study offers a novel pedagogical 
approach for writing teachers to develop their students' evaluative judgement and adds to 
our understanding of the potential of AI technologies in scaffolding L2 students' peer feed-
back practices.
LITERATURE REVIEW
Peer feedback provision in L2 writing
Previous studies have demonstrated the significant impact of peer feedback on the learn-
ing of L2 writing, such as increasing awareness of audience and genre (Berggren, 2015; 
Yu, 2019), enhancing writing motivation (Weng et al., 2023), promoting critical thinking 
(Novakovich, 2016) and improving writing competence (Latifi & Noroozi, 2021; Lu et al., 2023; 
Vuogan & Li, 2023). Importantly, earlier research compared peer feedback with teacher 
feedback, finding that although L2 students integrated more teacher feedback, leading to 
more revisions, peer feedback was associated with a higher degree of student autonomy 
(Miao et al., 2006). Additionally, peer feedback resulted in more meaning- based revisions 
and more successful outcomes than teacher feedback, due to effective peer interaction and 
negotiation (Biber et al., 2011; Tsui & Ng, 2010).
Despite the potential benefits of peer feedback for L2 writing, student reviewers often 
encounter various challenges in the process of feedback generation, which is a complex 
task, requiring cognitive and metacognitive skills (Latifi et al., 2021; van Popta et al., 2016). 
Research has indicated that the prevalence of low- quality peer reviews is a consistent issue, 
adversely affecting the impact of peer feedback on the learning process (Han et al., 2023; 
Nelson & Schunn, 2009). From a linguistic perspective, students' still- developing L2 profi-
ciency and understanding of its rhetorical conventions (Allen & Katayama, 2016; Wu, 2019) 
may hinder them from accurately comprehending their peers' writing intentions (Min, 2005; 
Stanley, 1992), identifying language and rhetorical issues, and offering constructive, 
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 4

| 1615
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
high- quality feedback (Wu, 2019; Yu & Lee, 2016). Socio- culturally, L2 students from vary-
ing cultural backgrounds have diverse perceptions of peer interaction and writing (Allaei 
& Connor, 1990), which could adversely affect the peer feedback processes (Hyland & 
Hyland, 2006). For instance, studies have found that L2 students from collectivist cultures 
may hesitate to provide critical comments to preserve interpersonal harmony and mutual 
face- saving (Carson & Nelson, 1996; Nelson & Carson, 1998; Yu & Hu, 2017).
As the problems in peer feedback provision would adversely affect the effectiveness of 
peer feedback activities, it is recognised that training is accorded a vital place in alleviating 
the problems in peer feedback provision and facilitating effective peer feedback (Fan & 
Xu, 2022; Hu, 2005; Rahimi, 2013). Researchers have conducted short- term peer feed-
back training. Some undertook teacher- led demonstration and modelling, such as familia-
rising students with the genre of student essays and the peer review task (Stanley, 1992); 
raising students' awareness of the benefits, problems and solutions of peer feedback 
(Hu, 2005); presenting actual examples of good and poor peer comments (Hu, 2005; Liou & 
Peng, 2009); and modelling the procedures of clarifying writers' intentions, identifying prob-
lems, explaining the nature of problems and making specific suggestions (Hu & Lam, 2010; 
Min, 2005; Rahimi, 2013). Others also attempted teacher- student conferences that discuss 
strategies for giving effective feedback on peer writing and help students modify their com-
ments (Min, 2005; Zhu, 1995).
A perusal of these studies revealed that the training boosted students' favourable atti-
tudes towards peer feedback (Hu, 2005; Kamimura, 2006). Furthermore, the trained stu-
dents provided more specific, relevant and constructive feedback and were engaged in 
giving feedback more than those untrained (Hu & Lam, 2010; Min, 2005). They gave more 
attention to global issues of writing and meaning- based comments than the untrained ones 
(Kamimura, 2006; Rahimi, 2013). Nevertheless, the majority of the training was brief and 
less elaborate due to practical constraints, such as limited class time, heavy curricula to 
cover, and large class sizes, which contradicts the time- consuming training for peer review 
(Hu & Lam, 2010). Elaborate and intensive peer feedback training might overwhelm teach-
ers and make them have insufficient time and energy for other teaching activities. In addi-
tion, following the training, students received minimal immediate and timely support during 
their practical engagement in peer feedback activities.
Using AI technologies to support peer feedback generation
Numerous studies have explored the use of AI technologies as automated writing evaluation 
(AWE) tools to support students in learning writing (for reviews, see Ding & Zou, 2024; Fu, 
Zou, et al., 2024; Li, 2023). These studies primarily utilised AI- enabled AWE tools to evalu-
ate and provide feedback on students' writing, aiming to support essay revision and enhance 
writing performance (eg, Lai, 2010; Wang et al., 2020). Comparatively, fewer studies have 
attempted to implement AI support for peer feedback provision to evaluate and improve the 
quality of student reviewers' peer feedback, and thus improve their ability to provide effective 
peer feedback.
Arguably, AI technologies hold potential for addressing the aforementioned linguistic 
and socio- cultural challenges that student reviewers face when generating feedback on 
their peers' writing. For instance, AI- powered tools can analyse text and offer suggestions 
for grammar, syntax, and vocabulary improvements, allowing students to concentrate on 
higher- order aspects of writing, such as content and organisation (Guo & Wang, 2024; Steiss 
et al., 2024). This assistance can help students overcome limitations related to their L2 pro-
ficiency and comprehension of rhetorical conventions. Furthermore, AI tools can act as me-
diators in the peer feedback process, guiding students on providing constructive feedback 
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 5

1616 |   
GUO et al.
and promoting productive discussions among peers (Darvishi et al., 2024; Han et al., 2023). 
By guiding students through the peer feedback process, AI can help them develop effective 
communication and critical thinking skills, regardless of their cultural background (Liang & 
Wu, 2024; Suriano et al., 2025).
Notably, some studies have examined the validity of automated peer review evaluation 
tools and reported acceptable validity (Xiong et al., 2012; Xiong & Litman, 2011). With a spe-
cial focus on how AI evaluation can improve students' peer feedback quality, several studies 
revealed that automated peer review evaluation tools could enhance students' seriousness 
of peer review (Han et al., 2023) and aid students in providing more specific, targeted, 
high- quality and helpful feedback (Darvishi et al., 2024; Han et al., 2023; Jia et al., 2021). 
However, thus far limited AI- powered tools have been developed and utilised for supporting 
student reviewers' feedback creation in the L2 writing learning context.
Moreover, existing digital tools developed to facilitate peer feedback generation were not 
based on a comprehensive definition of effective feedback features in writing. According to 
the literature (Kerman et al., 2022; Patchan et al., 2016; Wu & Schunn, 2021), writing feed-
back features include (1) affective (inclusion of positive emotions such as praise or compli-
ments and negative emotions such as anger or disappointments), (2) description (summary 
statement of the essay), (3) identification (identification and localisation of the problem in 
the essay), (4) justification (elaborations and justifications of the identified problem) and 
(5) constructive (inclusion of recommendations and action plans for further improvements). 
However, in designing and developing digital tools for supporting peer feedback generation, 
many studies have overlooked essential features, such as neglecting the affective aspect, 
despite its known importance.
In addition, while earlier studies have mainly taken an etic perspective to examine the 
effect of AI supervision on students' peer review comments, few have taken an emic per-
spective to cast light on how students revise their peer review comments in response to 
AI assessments of their comments. An emic perspective will allow researchers to gain a 
deeper understanding of the complex processes of AI- supported peer feedback provision, 
and is essential for informing the development and implementation of AI technologies in 
peer feedback activities.
To address these research gaps, the present study aims to utilise a self- designed AI- 
supported peer review system that captures essential features of peer feedback to provide 
AI evaluation of L2 students' peer review comments, and scrutinise how students act on 
the AI support in their peer feedback generation processes. Specifically, the study seeks to 
address the following two research questions (RQs):
RQ1: To what extent does the quality of students' comments on peers' essays improve 
when using AI feedback on their comments?
RQ2: How do students integrate the feedback received from AI when revising their com-
ments on peers' essays?
METHODS
In this study, we adopted a mixed methods approach, specifically employing a convergent 
parallel design (Creswell & Clark, 2017). In this design, qualitative and quantitative data 
are collected simultaneously, analysed separately and then merged during the interpreta-
tion stage to provide a comprehensive understanding of the research problem. Our focus 
on RQ1 primarily used a quantitative research method. Through quantitative analysis, we 
aimed to measure and quantify the improvement in the quality of students' comments on 
peers' essays when incorporating AI feedback. RQ2, on the other hand, was predominantly 
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 6

| 1617
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
explored using a qualitative research method in our study. Qualitative data collection and 
analysis were employed to delve into the nuances of how students apply AI- generated feed-
back during the revision process of their comments on peers' essays. The division between 
the quantitative and qualitative research methods allows us to separately investigate and 
address each question in a focused manner, ensuring a comprehensive examination of the 
impact of AI feedback on students' comment quality and the process of AI feedback integra-
tion during comment revision. Further details of the research design are presented below.
Context and participants
The study was conducted at a comprehensive university in China that employs English as the 
medium of instruction. A total of 44 undergraduate students, consisting of 13 males and 31 
females, participated in this study. These students came from a diverse range of disciplines 
including humanities and social science, management and economics, and science, tech-
nology, engineering and mathematics (STEM). Their average age was 19.02 years old, with 
a standard deviation of 0.45. All participants demonstrated proficiency in English, with their 
proficiency levels ranging from B2 to C1 according to the Common European Framework 
of Reference for Languages (CEFR), as evidenced by their performance in the college en-
trance exam's English test and their course evaluations by the instructor (the third author). 
Prior to their involvement in the study, informed consent was obtained from all participants.
The students were enrolled in an English for Academic Purposes course taught by the 
third author. The study was conducted within its speech communication module. Throughout 
this module, students were tasked with writing essays and delivering speeches. Peer as-
sessment played an important role in the course, fostering students' abilities to reflect and 
provide constructive critiques. Moreover, the students were receptive and open to using 
technology to enhance their learning experience. The university provided access to AWE 
tools like Grammarly, and the students had prior experience with generative AI tools such 
as ChatGPT. Their familiarity with technology and peer review practices suggested that 
they were comfortable incorporating an AI- assisted peer feedback system into their writing 
and peer review processes. Considering the students' previous experience with technology- 
enhanced learning, the classroom environment was well- suited to support the integration 
and exploration of the AI- assisted approach to peer feedback provision.
EvaluMate, an AI- supported peer review system
To implement AI- supported peer feedback, the researchers developed EvaluMate, an online 
peer review system. This system incorporates AI capabilities and features a large language 
model- based chatbot named Eva, designed to assist students during the peer feedback 
generation process. Figure 1 provides a visual representation of the system's layout, high-
lighting key components such as the writing topic and task prompt located in the top left 
corner. The essay awaiting peer review is displayed in the bottom left corner, while a rat-
ing rubric on content, organisation, and language is provided on the right- hand side for 
students' reference. The rating rubric (see Appendix A) was adapted from Golparvar and 
Abolhasani (2022) and Zhang and Lu (2022).
Figure 2 demonstrates the process in which students evaluate their peers' essays by 
scoring the three aspects (content, organisation and language) and providing comments.
During this process, students can click the “Check my comments” button to request feed-
back from Eva on their review comments. Eva's feedback is based on Kerman et al.'s (2022) 
framework for peer feedback quality evaluation that highlights five important features: 
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 7

1618 |   
GUO et al.
affective, description, identification, justification, and constructive. According to this frame-
work, effective feedback should include positive emotions like praise or compliments, an 
accurate summary of the essay under review, a clear identification and pinpointing of issues 
F I G U R E  1 
Peer review task page (1).
F I G U R E  2 
Peer review task page (2).
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 8

| 1619
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
within the essay, thorough explanations and justifications of identified problems, and con-
structive recommendations and plans for further enhancements. Clicking the “Check my 
comments” button activates a researcher- designed prompt to ChatGPT, a popular large 
language model created by OpenAI (see Appendix B). Figure 3 exemplifies Eva's comments 
and suggestions on a student's review comment:
For a sample of conversations between student users and the chatbot, see Appendix C.
For further details regarding EvaluMate, see Guo (2024).
Data collection
First, the instructor introduced the students to EvaluMate, ensuring a comprehensive under-
standing of its essential functionalities. During this presentation, the instructor explained the 
key features of EvaluMate, empowering the students to proficiently navigate and leverage 
the system to its fullest extent. Notably, to prepare the students for using the rating rubric 
within the system, the following three steps were implemented: (1) an overview of the rubric, 
its purpose, and its importance in ensuring a consistent and objective assessment of peers' 
work; (2) students were given time to review and familiarise themselves with the rubric, its 
categories, and the criteria for each performance level; and (3) each criterion and perfor-
mance level were explained, supplemented by examples and clarifications as necessary. 
Throughout this process, the instructor encouraged the students to seek clarification or ask 
questions in case of any uncertainties.
Subsequently, the students completed an argumentative writing task within the sys-
tem, adapted from the ‘Analyze an Issue’ task, which is designed to assess test takers' 
F I G U R E  3 
Peer review task page (3).
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 9

1620 |   
GUO et al.
argumentative writing skills in the Graduate Record Examination (GRE). The prompt given 
to the students was as follows:
“College students should be encouraged to pursue subjects that interest them 
rather than the courses that seem most likely to lead to jobs.” Discuss the ex-
tent to which you agree or disagree with this statement and provide reasons for 
the position you take. In developing and supporting your position, you should 
include relevant evidence. Write 300–500 words.
To complete the writing task, the students were given a time limit of 40 minutes.
Finally, each student was randomly assigned two peer review tasks on the system. 
Using the chatbot Eva, they were expected to evaluate and improve their comments on 
their peers' essays. It was mandatory for them to seek feedback from Eva at least once 
before submitting their comments. The time limit for the peer review tasks was set to 
20 minutes.
To address the two RQs, we collected chat log data from the 44 students' interactions 
with the chatbot Eva. This data included the comments they wrote and submitted for evalu-
ation, as well as the feedback provided by the chatbot on their comments.
Data analysis
For answering RQ1
To investigate the impact of AI support on the quality of students' comments (RQ1), we 
conducted an assessment of both their initial (the first version that students sent to the 
chatbot for evaluation) and final comments (the last version that students submitted for 
their peers' reference) in each peer review task. We extracted these two versions of 
each participant's comments from the chat log data and organised them in an Excel file. 
Each version is placed in a separate column, with the student's name and task number 
serving as the identifiers. In total, we analysed 88 pairs of initial and final comments (44 
students × 2 tasks). We used a coding scheme developed by Kerman et al. (2022), which 
examined the five essential aspects of student feedback: (1) affective, (2) description, (3) 
identification, (4) justification and (5) constructive (refer to Appendix D for details). Each 
aspect was assigned a score ranging from 0 (poor quality) to 2 (good quality). These 
scores were then summed to determine the overall quality score for each student com-
ment, with a maximum score of 10.
To ensure coding reliability, the first and third authors collaboratively reviewed and fa-
miliarised themselves with the coding scheme. Subsequently, they independently coded a 
subset of 22 pairs of student comments (25% of the data). Cohen's kappa coefficients were 
calculated to assess inter- rater reliability, yielding the following scores: affective (0.91), de-
scription (0.85), identification (0.81), justification (0.87) and constructive (0.92). Any discrep-
ancies were resolved through discussion until a consensus was reached. The remaining 
data were then divided equally between the two authors for independent coding.
Following the coding process, we employed the Shapiro–Wilk test to evaluate the nor-
mality of the data. The assumption of normal distribution was not satisfied, necessitating the 
use of the Wilcoxon signed- rank test for comparing the scores of students' initial and revised 
comments. Accordingly, the Wilcoxon signed- rank test was performed on each of the five 
dimensions of feedback quality, as well as the total score, using IBM SPSS Statistics 29.0. 
Additionally, we employed descriptive statistics to gain insights into the overall trends and 
patterns in students' changing performance in peer feedback.
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 10

| 1621
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
For answering RQ2
To analyse how students implemented the feedback provided by Eva and understand the 
effectiveness or ineffectiveness of their revision attempts on their comment drafts (RQ2), 
we conducted a qualitative, iterative, and inductive coding process. Specifically, following 
the approach adopted by Wang et al. (2020) and Lu et al. (2024), we began by creating two 
Word files: one containing all initial comment drafts and another containing all finally sub-
mitted comments extracted from the chat log data, both ordered by a unique case number 
(88 cases in total). We then used the ‘Compare Documents’ function to generate a file that 
tracked the changes made between the two versions (see Appendix C), including insertions, 
deletions, and other revisions made by the students. Finally, we extracted AI feedback from 
the chat log data and juxtaposed it with the corresponding student comment revisions in an 
Excel file.
The first step in our analysis involved coding whether the students attempted to revise 
their comments based on the actionable AI feedback (i.e. feedback that required students' 
revisions for improvement) they received from Eva. Specifically, we examined whether the 
revisions reflected at least one of the five peer feedback quality aspects. For instance, in 
the case of student comments receiving actionable AI feedback on the affective aspect, we 
looked for revisions where students made an effort to incorporate encouraging and positive 
emotions such as praise or compliments. Out of the 88 cases of peer feedback observed, 
in 8 cases, students received actionable AI feedback on the affective aspect, and 2 of them 
did not make any attempts to revise their initial comments based on the AI feedback. In 83 
cases, students received actionable AI feedback on the descriptive aspect, and 4 of them 
did not make any attempts to revise their initial comments. In 70 cases, students received 
actionable AI feedback on the identification aspect, with 8 cases where no revisions were 
made. In 68 cases, students received actionable AI feedback on the justification aspect, with 
13 cases where no revisions were made. Finally, in 85 cases, students received actionable 
AI feedback on the constructive aspect, and 36 cases did not involve any revisions based 
on the AI feedback they received.
Subsequently, for the comments in which students attempted revisions, we coded the 
extent to which the comments demonstrated improvement in the five aspects, taking into 
account the AI feedback provided. The scoring results from addressing RQ1 were used for 
this step. We categorised student comments as showing no improvement if the attempts 
to implement the AI feedback did not result in any score improvement in a specific aspect. 
Comments were classified as showing slight improvement when the revisions aligned with 
the AI feedback and led to a one- score improvement in the respective aspect. We con-
sidered student comments to show substantive improvement if the revisions significantly 
enhanced a specific aspect of comment quality and resulted in a two- score improvement. 
Overall, we employed four codes: (1) no attempt at implementing given feedback, (2) at-
tempted to implement feedback, but no improvement, (3) slight improvement and (4) sub-
stantive improvement.
Finally, for student revisions that resulted in no improvement, we coded the specific ways 
in which the revisions fell short of achieving improvements. These reasons were represented 
by three codes that indicated the lack of changes (see Appendix E). Additionally, we coded 
student comments where revisions demonstrated attention to the AI feedback and resulted 
in slight or substantive improvement. We arrived at four codes, which represented the suc-
cessful aspects of the revisions (see Appendix E). Each student comment was assigned one 
code, with coders selecting the most applicable code. The first author coded all 88 pairs of 
student comments, while the third author independently double- coded 22 samples (25% of 
the data). The inter- coder agreement, calculated by comparing their coding results, reached 
92%. Any discrepancies were discussed to reach a consensus.
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 11

1622 |   
GUO et al.
RESULTS
To what extent does the quality of students' comments on peers' 
essays improve when using AI feedback on their comments? (RQ1)
After analysing the initial and final versions of student comments, it was found that the 
average length of the initial version was 73.28 words. However, the final version showed a 
significant improvement in length, with an average of 96.85 words. This indicates that, on 
average, each student added 23.57 words to their comments as a result of revising them in 
response to AI feedback.
We used the Wilcoxon signed- rank test to compare the scores of students' initial and final 
comments. As shown in Table 1, the students had significantly higher scores (p < 0.05) in 
all the five dimensions of feedback quality: affective (z = 2.449; p = 0.014; r = 0.185), descrip-
tion (z = 8.230; p = 0.000; r = 0.620), identification (z = 7.117; p = 0.000; r = 0.536), justification 
(z = 5.231; p = 0.000; r = 0.394), and constructive (z = 3.464; p = 0.001; r = 0.261), when they 
revised their comments based on AI feedback. Additionally, the total scores showed a sig-
nificant improvement (z = 8.071; p = 0.000; r = 0.608).
Furthermore, as presented in Table 2, the findings of our descriptive statistics analysis 
show that the students initially excelled in the affective dimension (M = 1.91; SD = 0.29) 
when providing reviewer comments, while their performance was the weakest in the de-
scriptive dimension (M = 0.86; SD = 0.48). Notably, after revising their initial comments 
based on AI feedback, they demonstrated the most significant improvement in the de-
scriptive dimension (M = 1.02; SD = 0.52). Consequently, their final comments exhibited 
the second- highest scores in the descriptive dimension (M = 1.89; SD = 0.32). Additionally, 
it is worth mentioning that the students' constructive dimension scores were among the 
lowest in their initial version (M = 0.95; SD = 0.33) and remained the lowest in their final 
version (M = 1.09; SD = 0.29). The improvement in the constructive dimension was com-
paratively modest (M = 0.14; SD = 0.34) compared to the other dimensions. It is important 
to note that the affective dimension exhibited the smallest improvement, primarily due to 
the students' already proficient performance in this dimension, leaving limited room for 
further enhancement.
TA B L E  1 
Wilcoxon signed- rank test results.
Feedback quality dimensions
Median
z
r
Initial version
Final version
Affective
2.00
2.00
2.449*
0.185
Description
1.00
2.00
8.230*
0.620
Identification
1.00
2.00
7.117*
0.536
Justification
1.00
1.50
5.231*
0.394
Constructive
1.00
1.00
3.464*
0.261
Total
6.00
8.00
8.071*
0.608
*p < 0.05.
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 12

| 1623
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
How do students integrate the feedback received from AI when 
revising their comments on peers' essays? (RQ2)
Students' comment revision outcomes
Table 3 presents the outcomes of student reviewers' comment revisions in response to ac-
tionable AI feedback. First, out of the eight cases where Eva provided actionable feedback 
on the affective aspect, students attempted to revise their comments in six cases. As a 
result, slight improvements were observed. Second, among the 83 cases where Eva pro-
vided actionable feedback on the description aspect, students made an effort to revise their 
comments in 79 cases. Out of these attempts, two did not result in any improvement, while 
64 attempts led to slight improvements. Additionally, 13 attempts resulted in substantive 
improvements. Third, Eva provided actionable feedback on the identification aspect in 70 
cases. Among these, students attempted to revise their comments in 62 cases. Out of these 
attempts, eight did not lead to any improvement, 50 attempts resulted in slight improvements, 
and four attempts resulted in substantive improvements. Fourth, in the case of the justifica-
tion aspect, Eva provided actionable feedback in 68 cases. Out of these, students attempted 
to revise their comments in 55 cases. However, 26 attempts did not yield any improvement. 
On the other hand, 27 attempts led to slight improvements, and two attempts resulted in 
substantive improvements. Finally, out of the 85 cases where Eva provided actionable feed-
back on the constructive aspect, students attempted to revise their comments in 49 cases. 
Unfortunately, 37 attempts did not lead to any improvement. However, 12 attempts resulted 
in slight improvements, although no attempts resulted in substantive improvements.
Comment revision strategies leading to no improvement
Based on an analysis of specific revisions made by the students in cases where no improve-
ment was observed, we identified three strategies commonly adopted by the students. The 
first strategy, observed in the majority of cases (38 instances), involved the students simply 
reiterating their original comments without introducing any new points. This tendency was 
particularly evident in comments related to the constructive aspect. For instance, in case 
11, the student initially suggested, “You are recommended to write a strong rebuttal to make 
your argument stronger.” Despite Eva's encouragement to provide specific action plans for 
improvement, the student failed to offer any new suggestions and merely reiterated their 
original comment, stating, “A stronger rebuttal on the counterargument part will make the 
opposite side of the main claim weaker.”
We also identified a second strategy used by the students in 22 instances. This strategy 
involved the students correcting language errors present in their original comments, such 
TA B L E  2 
Descriptive statistics analysis results.
Feedback quality dimensions
Initial version score 
mean (SD)
Final version 
score mean (SD)
Improvement score 
mean (SD)
Affective
1.91 (0.29)
1.98 (0.15)
0.07 (0.25)
Description
0.86 (0.48)
1.89 (0.32)
1.02 (0.52)
Identification
1.11 (0.53)
1.77 (0.42)
0.66 (0.56)
Justification
1.15 (0.53)
1.50 (0.50)
0.35 (0.52)
Constructive
0.95 (0.33)
1.09 (0.29)
0.14 (0.34)
Total
5.99 (1.25)
8.23 (0.94)
2.24 (0.98)
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 13

1624 |   
GUO et al.
TA B L E  3 
Breakdown of students' improvement scores.
Feedback quality dimensions
Number of instances with 
actionable AI feedback
Number of instances (percentage)
No attempt at 
implementing given 
feedback
No (zero- score) 
improvement
Slight (one- 
score) 
improvement
Substantive 
(two- score) 
improvement
Affective
8
2 (25.00%)
0 (0.00%)
6 (75.00%)
0 (0.00%)
Description
83
4 (4.82%)
2 (2.41%)
64 (77.11%)
13 (15.66%)
Identification
70
8 (11.43%)
8 (11.43%)
50 (71.43%)
4 (5.71%)
Justification
68
13 (19.12%)
26 (38.24%)
27 (39.71%)
2 (2.94%)
Constructive
85
36 (42.35%)
37 (43.53%)
12 (14.12%)
0 (0.00%)
Total
314
63
73
159
19
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 14

| 1625
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
as misspellings and grammatical mistakes. To illustrate, in case 19, the student's initial com-
ment read, “Your structure is clear but it seem that it lacks a conclusion part.” In his revision, 
the student corrected the use of seem to seems. Despite Eva's recommendation to provide 
justifications for the comment, emphasising the significance of a well- written conclusion, the 
student merely focused on the linguistic correction. Consequently, this modification did not 
contribute to an elevated score in the justification dimension.
Similarly, we observed a third strategy employed by the students in 13 instances, which 
involved altering word usage. To illustrate, in case 47, the student's original comment stated, 
“Maybe one more subclaim would be better.” In her revision, the student replaced better 
with more convincing. Despite Eva's recommendation for the student to propose a potential 
subclaim for the author's consideration, aiming to enhance the constructive nature of her 
feedback, the student solely focused on modifying the wording. Consequently, while the 
word change rendered the comment more concise and specific in meaning, it did not result 
in an improved score in the constructive dimension.
Comment revision strategies leading to slight or substantive improvement
Through an analysis of student revisions in cases where slight or substantive improvement 
was observed, we identified four strategies employed by the students in their comment 
revision. The most common strategy, observed in 73 instances, involved the students intro-
ducing new points that were not present in their original comments. For example, in case 
25, Eva prompted the student to provide a summary statement of the evaluated essay to 
demonstrate a comprehensive understanding and encourage the recipient's uptake of the 
feedback. The student responded by including additional information, stating, “The essay 
conveyed the author's endorsement of college students pursuing subjects based on per-
sonal interest. The author substantiated this viewpoint by offering two specific subclaims. 
Furthermore, the author built the argument by incorporating findings from research studies.” 
This addition resulted in a significant two- score improvement in the description dimension.
The students also employed a second strategy of adding details to their original com-
ments, which was observed in 52 instances. For example, in case 7, the student initially 
made a comment stating, “I also found a grammar mistake in the reasoning of Subclaim 
2,” without explicitly specifying the issue. Prompted by Eva, the student revised her original 
comment to provide more clarity, stating, “I also found a grammar mistake in the reasoning 
of Subclaim 2. The phrase ‘there may have…’ should be changed to ‘There may be…’” This 
revision resulted in an increased identification score. Another example can be seen in case 
37, where the student initially suggested, “You can use a more attractive beginning.” Eva 
encouraged the student to explain this comment and provide justification. As a result, the 
student added, “An attractive beginning can make the reader more interested in your writ-
ing.” This addition led to a one- score improvement on the justification dimension.
The third strategy used by the students involved providing specific examples to illustrate their 
original comments. This strategy was observed in 30 instances. For instance, in case 26, the 
student initially commented, “Also, please try to say something positive instead of explaining it 
from a negative aspect.” Upon Eva's suggestion to offer specific action plans for better under-
standing, the student added, “For example, say ‘if people choose their courses based on inter-
est’ instead of ‘if they do not’.” This inclusion of an example resulted in a one- score improvement 
in the constructive dimension. Another instance is found in case 40, where the student's original 
comment stated, “Moreover, ‘many studies’ mentioned in the essay needs an exact citation.” In 
the revision, the student added, “For example, you can search for scientific evidence for your 
statements on the Internet to justify your argument and cite them properly.” This inclusion of an 
example also resulted in a one- score improvement in the constructive dimension.
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 15

1626 |   
GUO et al.
The fourth strategy employed by the students involved transforming words or phrases 
in their original comments into full sentences. This strategy was observed in 23 instances. 
For instance, in case 52, the student initially made a comment that consisted of fragmented 
phrases such as “A spelling error, ‘bewteen’; some mistakes, like ‘college students should 
encouraged’; maybe more reasoning.” During the evaluation, Eva suggested explicitly stat-
ing the location of the identified problems and recommended that the student provide rec-
ommendations for improving the evaluated essay, even though the student had already 
suggested “maybe more reasoning.” In response, the student revised her comment by trans-
forming it into full sentences, stating, “There is a spelling error in the first paragraph. The 
word ‘bewteen’ should be ‘between’. Also, the main claim should be ‘college students should 
be encouraged’ instead of ‘college students should encouraged’. Finally, maybe you can 
add more explanation in your reasoning part to make your subclaim more convincing.” This 
revision enabled Eva to better understand the students’ comments and resulted in increased 
scores in terms of the identification and justification dimensions.
DISCUSSION
This study explored the impact of incorporating AI supervision into L2 students' peer feed-
back creation process. Specifically, we investigated how using AI assistance influenced 
the quality of student reviewers' comments on peers' essays. The findings revealed sev-
eral noteworthy outcomes. First, the students had significantly better performance in all the 
five dimensions of peer feedback, that is affective, description, identification, justification, 
and constructive dimensions. This lends empirical support to researchers' recommenda-
tion of peer feedback training to improve the effectiveness of peer feedback activities (Fan 
& Xu, 2022; Rahimi, 2013), consistent with prior studies highlighting the effectiveness of 
such training (Hu & Lam, 2010; Rahimi, 2013). Second, when engaging with Eva's feedback, 
students demonstrated a tendency to write more detailed comments. This indicates that 
the integration of AI evaluation prompted students to delve deeper into their peer reviews. 
Furthermore, the incorporation of AI supervision resulted in an overall improvement in the 
quality of students' comments, aligning with previous studies in which the benefits of AI in 
scaffolding students' peer feedback were documented (Darvishi et al., 2024; Guo et al., 2024; 
Han et al., 2023; Jia et al., 2021). These improvements were evident in various aspects, in-
cluding the expression of positive emotions, clearer summaries of the evaluated essays, 
explicit identification of problem areas, elaboration and justification of identified issues, and 
the provision of constructive recommendations and action plans for further improvement.
Notably, the descriptive dimension of their comments experienced the most significant 
impact from using AI support. Initially, only a few students provided a statement summaris-
ing the content of the essay being evaluated. After being prompted by Eva, most students 
successfully integrated such summaries into their comments. This demonstrated that a pre-
cise understanding of the peers' writing intentions is a prerequisite for students to identify 
problems and provide meaningful and constructive suggestions that facilitated the feedback 
recipients' uptake of the reviewer comments, concurring with Min (2005) and Stanley (1992), 
in which students' low- quality peer comments derived largely from their misunderstanding 
and over-  or mis- interpretation of the writers' intentions. This further underscores the neces-
sity of including clarifying writers' intentions as the first step in peer feedback training (Hu & 
Lam, 2010; Rahimi, 2013).
Despite these positive outcomes, it is worth noting that the constructive dimension of 
students' comments showed a relatively modest improvement compared to other dimen-
sions. While there was a statistically significant enhancement, it was not as prominent as in 
other aspects, confirming the multidimensional complexity of peer feedback activities (Latifi 
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 16

| 1627
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
et al., 2021; McConlogue, 2015; Nicol, 2010). Although students were capable of identifying 
problems in their peers' essays, they encountered challenges when providing constructive 
recommendations to effectively address these issues. This difficulty might stem from var-
ious barriers, such as students' low self- efficacy (Hu & Lam, 2010) and concerns about 
preserving face (Carson & Nelson, 1996; Nelson & Carson, 1998; Yu & Hu, 2017). Further 
cognitive and metacognitive support may be necessary for students in generating more 
constructive recommendations for their peers' essay improvements.
Furthermore, this study delved into the strategies employed by students during the com-
ment revision process, which had varying degrees of impact on the quality improvement of 
their comments. On one hand, our results indicated that merely reiterating the information 
from their original comments during the revision did not contribute to quality improvement. 
Similarly, focusing solely on surface- level features (eg, word choice and grammatical errors) 
of their original peer review comments rather than effectively acting upon the AI feedback 
did not yield significant enhancements. These findings suggest that a superficial revision 
approach, devoid of substantial changes or deeper analysis, does not lead to substantial 
improvements in comment quality. Students may resort to such an approach due to their 
difficulties with decoding terminology in AI feedback, unwillingness to expend effort or lack 
of feedback literacy (Winstone et al., 2017; Zhang & Mao, 2023). Consequently, further train-
ing is warranted to help students understand and effectively engage with AI feedback when 
revising their peer review comments.
On the other hand, our findings highlighted the positive influence of specific revision strat-
egies. When students introduced new points, added relevant details, provided illustrative 
examples and formulated their comments in full sentences, their revisions resulted in notice-
able quality improvement. These approaches signify a more comprehensive engagement 
with the feedback offered by AI, encompassing a deeper understanding of the essay's con-
tent and a concerted effort to enhance the clarity and coherence of their comments. These 
findings emphasise the importance of adopting meaningful comment revision strategies to 
enhance the overall effectiveness of feedback provision.
CONCLUSION, IMPLICATIONS AND LIMITATIONS
The findings of this study have several theoretical implications for the field of writing in-
struction and peer feedback. First, the study highlights the significance of incorporating AI 
technology, such as Eva, to support students in their feedback provision process. In contrast 
to traditional methods that rely on teacher oversight (eg, Han & Xu, 2020) and peer interac-
tions (eg, Zhang et al., 2024) for monitoring student reviewers' feedback creation, AI plays 
a unique role in facilitating students' peer feedback generation. AI provides immediate and 
personalised assistance, surpassing teachers or peers in aiding students to improve their 
review comments efficiently. The positive impact of AI supervision on the quality of stu-
dents' comments underscores the potential of AI as a valuable tool. This suggests that AI 
can effectively enhance students' abilities to engage thoughtfully and constructively in peer 
review activities, thereby promoting deeper learning, critical thinking skills and evaluative 
judgement capacities (Tai et al., 2018). Second, the study contributes to our understand-
ing of students' comment revision strategies and their influence on comment quality. The 
differentiation between superficial and meaningful revision approaches provides insights 
into the importance of encouraging students to adopt more comprehensive and substantive 
comment revision practices. This knowledge can inform instructional strategies and materi-
als aimed at fostering effective comment revision skills in students, ultimately enhancing the 
overall effectiveness of feedback provision. Third, the study explores how AI can elevate the 
learning experience for L2 learners. By focusing on enhancing feedback provision skills in 
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 17

1628 |   
GUO et al.
peer assessments, the study has uncovered novel avenues for AI integration. While previ-
ous studies typically employ AI tools to assess student essays directly, our approach is 
distinctive: we use AI to evaluate student reviewers' feedback on peer essays. This unique 
application highlights a pioneering method of AI support in writing assessment.
The findings of this study have important implications for educators, curriculum design-
ers, and developers of online peer review systems. First, the integration of AI chatbots, like 
Eva, in online peer review systems can significantly benefit students' feedback provision 
process. Our findings highlight the potential of incorporating AI technology as a support-
ive tool to guide and scaffold students' peer feedback generation. Educators can consider 
implementing AI chatbots in their instructional design to facilitate students' engagement, 
encourage critical thinking, and provide timely and personalised feedback. Additionally, this 
study emphasises the importance of guiding students in adopting effective comment revi-
sion strategies during the feedback creation process. Educators can design explicit instruc-
tional interventions that promote comprehensive and meaningful comment revisions, such 
as introducing new points, incorporating relevant details and examples, and formulating 
comments in full sentences. By providing guidance and opportunities for practice, educators 
can help students develop and refine their comment revision skills, leading to more substan-
tial improvements in the quality of their comments. Importantly, educators should not only 
introduce students to AI technology but also guide them in using it in an ethically responsible 
and accurate manner, which is an essential component of AI literacy (Ng et al., 2021). By 
emphasising ethical considerations in AI usage, such as privacy, bias, transparency and 
accountability in AI systems, educators can foster students' understanding of the ethical im-
plications surrounding AI. This will enable students to make informed decisions about when 
and how to use AI tools and consider factors beyond just technical functionality. Finally, in 
our research, we have specifically employed AI for peer assessment in L2 writing. However, 
we are optimistic about the broader applicability of AI- supported peer feedback across di-
verse educational settings beyond our current focus. We encourage researchers and devel-
opers in other educational domains to adapt and use our approach to explore its efficacy in 
a range of contexts.
This study, while providing valuable insights, is not without its limitations. First, this study 
explored the impact of AI assistance on students' ability to improve the quality of their peer 
feedback. However, it remains unclear whether students can generate equally effective 
feedback without the aid of AI. Future research could further investigate the effects of AI- 
supported peer review on students' feedback skills by employing experimental designs. It 
would be valuable to examine whether students can sustain the production of high- quality 
feedback after using AI support for a period of time. This would provide a deeper under-
standing of the long- term impact of AI assistance on students' feedback generation abilities. 
Second, it is crucial to recognise that this study had an uneven distribution of participants, 
with 13 males and 31 females. Since gender has the potential to influence students' accep-
tance and use of information technologies (Cai et al., 2017), it would be beneficial for future 
research to investigate potential gender differences in the use of AI technologies for peer 
review among students. Specifically, examining whether male and female students exhibit 
divergent patterns in their adoption and perception of AI tools in the context of peer review 
would provide valuable insights. Third, this study investigated students' engagement with AI 
feedback through an analysis of their revisions in response to the AI feedback. It is worth 
noting that this study excluded cases where students did not make any revisions, as it fo-
cused solely on active engagement. However, as suggested by Fu, Yang, et al. (2024), it is 
crucial to explore how students may have engaged with AI feedback before deciding not to 
make revisions, as this ‘invisible engagement’ could provide valuable insights. Therefore, 
future research should consider employing qualitative methodologies like interviews to ex-
amine students' concealed engagement with AI feedback in generating reviewer comments 
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 18

| 1629
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
to gain a comprehensive understanding of their AI feedback utilisation process. Fourth, in 
our prompt design (see Appendix B), we did not explicitly instruct the chatbot to refrain from 
directly revising or reformulating students' comments. Consequently, in our dataset, we en-
countered instances where the chatbot directly revised a student's comments after providing 
revision suggestions. This issue highlights the inherent challenge of predictability and con-
trollability in generative models. It is reasonable to assume that some students might simply 
copy and paste the AI's revisions without actively engaging in the learning process. To 
address this concern, improving the prompt design, such as incorporating clear instructions 
like “do not directly revise my comments, but provide suggestions instead,” could be effec-
tive. By refining the prompt design, we can mitigate the risk of students passively accepting 
AI- generated revisions and foster a more meaningful learning experience. Finally, it will be 
crucial to scrutinise AI- assisted peer feedback from the perspective of the feedback recip-
ients, as a feedback loop completes only when the peer uses the feedback. For instance, 
future research could explore the effects of utilising AI- assisted peer feedback on the writing 
quality of feedback recipients.
FUNDING INFORMATION
This work was supported by the Research Postgraduate Student Innovation Award 2022/23 
and Dissertation Year Fellowship, awarded by the Graduate School of the University of 
Hong Kong, to the first author.
CONFLICT OF INTEREST STATEMENT
There is no potential conflict of interest in this work.
DATA AVAIL ABILIT Y STATEMENT
Data will be made available on request.
ETHICS STATEMENT
The research study has obtained ethical approval from the institutional review board at the 
University of Hong Kong. The research protocol and all associated procedures have been 
carefully reviewed and deemed to comply with ethical standards and guidelines. Participants' 
rights, privacy, confidentiality and well- being have been considered and protected through-
out the study. Informed consent was obtained from all participants prior to their involvement 
in the research.
ORCI D
Kai Guo 
 https://orcid.org/0000-0001-9699-7527 
Emily Di Zhang 
 https://orcid.org/0000-0002-9803-1088 
Danling Li 
 https://orcid.org/0000-0003-0974-1182 
Shulin Yu 
 https://orcid.org/0000-0003-1051-311X 
R E FE R E NCES
Allaei, S., & Connor, U. (1990). Exploring the dynamics of cross- cultural collaboration in writing classrooms. The 
Writing Instructor, 10, 19–28.
Allen, D., & Katayama, A. (2016). Relative second language proficiency and the giving and receiving of written 
peer feedback. System, 56, 96–106. https:// doi. org/ 10. 1016/j. system. 2015. 12. 002
Banister, C. (2023). Exploring peer feedback processes and peer feedback meta- dialogues with learners of ac-
ademic and business English. Language Teaching Research, 27(3), 746–764. https:// doi. org/ 10. 1177/ 13621 
68820 952222
Berggren, J. (2015). Learning from giving feedback: A study of secondary- level students. ELT Journal, 69(1), 
58–70. https:// doi. org/ 10. 1093/ elt/ ccu036
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 19

1630 |   
GUO et al.
Biber, D., Nekrasova, T., & Horn, B. (2011). The effectiveness of feedback for L1- English and L2- writing devel-
opment: A meta- analysis (TOEFL iBT RR-11-05). Educational Testing Service. https:// www. ets. org/ Media/ 
Resea rch/ pdf/ RR-  11- 05. pdf
Cai, Z., Fan, X., & Du, J. (2017). Gender and attitudes toward technology use: A meta- analysis. Computers & 
Education, 105, 1–13. https:// doi. org/ 10. 1016/j. compe du. 2016. 11. 003
Carless, D., & Boud, D. (2018). The development of student feedback literacy: Enabling uptake of feedback. 
Assessment & Evaluation in Higher Education, 43(8), 1315–1325. https:// doi. org/ 10. 1080/ 02602 938. 2018. 
1463354
Carson, J. G., & Nelson, G. L. (1996). Chinese students' perceptions of ESL peer response group interaction. 
Journal of Second Language Writing, 5(1), 1–19. https:// doi. org/ 10. 1016/ S1060 - 3743(96) 90012 - 0
Creswell, J. W., & Clark, V. L. P. (2017). Designing and conducting mixed methods research. Sage Publications.
Darvishi, A., Khosravi, H., Sadiq, S., Gašević, D., & Siemens, G. (2024). Impact of AI assistance on student 
agency. Computers & Education, 210, 104967. https:// doi. org/ 10. 1016/j. compe du. 2023. 104967
Ding, L., & Zou, D. (2024). Automated writing evaluation systems: A systematic review of Grammarly, Pigai, and 
Criterion with a perspective on future directions in the age of generative artificial intelligence. Education and 
Information Technologies, 29, 14151–14203. https:// doi. org/ 10. 1007/ s1063 9- 023- 12402 - 3
Fan, Y., & Xu, J. (2022). Exploring student engagement with peer feedback on L2 writing. Journal of Second 
Language Writing, 50, 100775. https:// doi. org/ 10. 1016/j. jslw. 2020. 100775
Fu, Q. K., Zou, D., Xie, H., & Cheng, G. (2024). A review of AWE feedback: Types, learning outcomes, and impli-
cations. Computer Assisted Language Learning, 37(1–2), 179–221. https:// doi. org/ 10. 1080/ 09588 221. 2022. 
2033787
Fu, X., Yang, C., & Zhang, T. (2024). Visibilizing invisible engagement behind students' no- revision operation to 
written feedback. Language Teaching Research. 1–23. https:// doi. org/ 10. 1177/ 13621 68824 1227666
Golparvar, S. E., & Abolhasani, H. (2022). Unpacking the contribution of linguistic features to graph writing 
quality: An analytic scoring approach. Assessing Writing, 53, 100644. https:// doi. org/ 10. 1016/j. asw. 2022. 
100644
Guo, K. (2024). EvaluMate: Using AI to support students' feedback provision in peer assessment for writing. 
Assessing Writing, 61, 100864. https:// doi. org/ 10. 1016/j. asw. 2024. 100864
Guo, K., Pan, M., Li, Y., & Lai, C. (2024). Effects of an AI- supported approach to peer feedback on university EFL 
students' feedback quality and writing ability. The Internet and Higher Education, 63, 100962. https:// doi. org/ 
10. 1016/j. iheduc. 2024. 100962
Guo, K., & Wang, D. (2024). To resist it or to embrace it? Examining ChatGPT's potential to support teacher 
feedback in EFL writing. Education and Information Technologies, 29, 8435–8463. https:// doi. org/ 10. 1007/ 
s1063 9- 023- 12146 - 0
Han, Y., Wu, W., Liang, Y., & Zhang, L. (2023). Peer grading eliciting truthfulness based on autograder. IEEE 
Transactions on Learning Technologies, 16(3), 353–363. https:// doi. org/ 10. 1109/ TLT. 2022. 3216946
Han, Y., & Xu, Y. (2020). The development of student feedback literacy: The influences of teacher feedback 
on peer feedback. Assessment & Evaluation in Higher Education, 45(5), 680–696. https:// doi. org/ 10. 1080/ 
02602 938. 2019. 1689545
Hu, G. (2005). Using peer review with Chinese ESL student writers. Language Teaching Research, 9, 321–342. 
https:// doi. org/ 10. 1191/ 13621 68805 lr169oa
Hu, G., & Lam, S. T. E. (2010). Issues of cultural appropriateness and pedagogical efficacy: Exploring peer re-
view in a second language writing class. Instructional Science, 38, 371–394. https:// doi. org/ 10. 1007/ s1125 
1- 008- 9086- 1
Hyland, K., & Hyland, F. (2006). State- of- the- art- article: Feedback on second language students' writing. 
Language Teaching, 39, 83–101. https:// doi. org/ 10. 1017/ S0261 44480 6003399
Jia, Q., Cui, J., Xiao, Y., Liu, C., Rashid, P., & Gehringer, A. (2021). ALL- IN- ONE: Multi- task learning BERT mod-
els for evaluating peer assessments. Computation and Language. Retrieved January 28, 2024, from https:// 
arxiv. org/ abs/ 2110. 03895 
Kamimura, T. (2006). Effects of peer feedback on EFL student writers at different levels of English proficiency: A 
Japanese context. TESL Canada Journal, 23(2), 12–39. https:// doi. org/ 10. 18806/ tesl. v23i2. 53
Kerman, N. T., Noroozi, O., Banihashem, S. K., Karami, M., & Biemans, H. J. (2022). Online peer feedback pat-
terns of success and failure in argumentative essay writing. Interactive Learning Environments, 32, 614–626. 
https:// doi. org/ 10. 1080/ 10494 820. 2022. 2093914
Lai, Y. H. (2010). Which do students prefer to evaluate their essays: Peers or computer program. British Journal of 
Educational Technology, 41(3), 432–454. https:// doi. org/ 10. 1111/j. 1467- 8535. 2009. 00959. x
Latifi, S., & Noroozi, O. (2021). Supporting argumentative essay writing through an online supported peer- review 
script. Innovations in Education and Teaching International, 58(5), 501–511. https:// doi. org/ 10. 1080/ 14703 
297. 2021. 1961097
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 20

| 1631
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
Latifi, S., Noroozi, O., & Talaee, E. (2021). Peer feedback or peer feedforward? Enhancing students' argumen-
tative peer learning processes and outcomes. British Journal of Educational Technology, 52(2), 768–784. 
https:// doi. org/ 10. 1111/ bjet. 13054 
Lee, I. (2017). Classroom assessment and feedback in L2 school contexts. Springer.
Li, R. (2023). Still a fallible tool? Revisiting effects of automated writing evaluation from activity theory perspec-
tive. British Journal of Educational Technology, 54(3), 773–789. https:// doi. org/ 10. 1111/ bjet. 13294 
Liang, W., & Wu, Y. (2024). Exploring the use of ChatGPT to foster EFL learners' critical thinking skills from a post- 
humanist perspective. Thinking Skills and Creativity, 54, 101645. https:// doi. org/ 10. 1016/j. tsc. 2024. 101645
Liou, H. C., & Peng, Z. Y. (2009). Training effects on computer- mediated peer review. System, 37, 514–525. 
https:// doi. org/ 10. 1016/j. system. 2009. 01. 005
Lu, Q., Yao, Y., Xiao, L., Yuan, M., Wang, J., & Zhu, X. (2024). Can ChatGPT effectively complement teacher as-
sessment of undergraduate students' academic writing? Assessment & Evaluation in Higher Education, 49, 
616–633. https:// doi. org/ 10. 1080/ 02602 938. 2024. 2301722
Lu, Q., Yao, Y., & Zhu, X. (2023). The relationship between peer feedback features and revision sources mediated 
by feedback acceptance: The effect on undergraduate students' writing performance. Assessing Writing, 56, 
100725. https:// doi. org/ 10. 1016/j. asw. 2023. 100725
McConlogue, T. (2015). Making judgements: Investigating the process of composing and receiving peer feed-
back. Studies in Higher Education, 40, 1495–1506. https:// doi. org/ 10. 1080/ 03075 079. 2013. 868878
Miao, Y., Badger, R., & Zhen, Y. (2006). A comparative study of peer and teacher feedback in a Chinese EFL 
writing class. Journal of Second Language Writing, 15, 179–200. https:// doi. org/ 10. 1016/j. jslw. 2006. 09. 004
Min, H. T. (2005). Training students to become successful peer reviewers. System, 33(2), 293–308. https:// doi. 
org/ 10. 1016/j. system. 2004. 11. 003
Min, H. T. (2006). The effects of trained peer review on EFL students' revision types and writing quality. Journal of 
Second Language Writing, 15(2), 118–141. https:// doi. org/ 10. 1016/j. jslw. 2006. 01. 003
Nelson, G. L., & Carson, J. G. (1998). ESL students' perceptions of effectiveness in peer response groups. 
Journal of Second Language Writing, 7, 113–131. https:// doi. org/ 10. 1016/ S1060 - 3743(98) 90010 - 8
Nelson, M. M., & Schunn, C. D. (2009). The nature of feedback: How different types of peer feedback affect writing 
performance. Instructional Science, 37(4), 375–401. https:// doi. org/ 10. 1007/ s1125 1- 008- 9053- x
Ng, D. T. K., Leung, J. K. L., Chu, S. K. W., & Qiao, M. S. (2021). Conceptualizing AI literacy: An exploratory review. 
Computers and Education: Artificial Intelligence, 2, 100041. https:// doi. org/ 10. 1016/j. caeai. 2021. 100041
Nicol, D. (2010). From monologue to dialogue: Improving written feedback processes in mass higher education. 
Assessment & Evaluation in Higher Education, 35(5), 501–517. https:// doi. org/ 10. 1080/ 02602 93100 3786559
Nicol, D., Thomson, A., & Breslin, C. (2014). Rethinking feedback practices in higher education: A peer review 
perspective. Assessment & Evaluation in Higher Education, 39(1), 102–122. https:// doi. org/ 10. 1080/ 02602 
938. 2013. 795518
Novakovich, J. (2016). Fostering critical thinking and reflection through blog- mediated peer feedback. Journal of 
Computer Assisted Learning, 32(1), 16–30. https:// doi. org/ 10. 1111/ jcal. 12114 
Patchan, M. M., Schunn, C. D., & Correnti, R. J. (2016). The nature of feedback: How peer feedback features 
affect students' implementation rate and quality of revisions. Journal of Educational Psychology, 108(8), 
1098–1120. https:// doi. org/ 10. 1037/ edu00 00103 
Rahimi, M. (2013). Is training students reviewers worth its while? A study of how training influences the quality 
of students' feedback and writing. Language Teaching Research, 17, 67–89. https:// doi. org/ 10. 1177/ 13621 
68812 459151
Stanley, J. (1992). Coaching student writers to be effective peer evaluators. Journal of Second Language Writing, 
1, 217–233. https:// doi. org/ 10. 1016/ 1060- 3743(92) 90004 - 9
Steiss, J., Tate, T., Graham, S., Cruz, J., Hebert, M., Wang, J., Moon, Y., Tseng, W., Warschauer, M., & Olson, 
C. B. (2024). Comparing the quality of human and ChatGPT feedback of students' writing. Learning and 
Instruction, 91, 101894. https:// doi. org/ 10. 1016/j. learn instr uc. 2024. 101894
Suriano, R., Plebe, A., Acciai, A., & Fabio, R. A. (2025). Student interaction with ChatGPT can promote com-
plex critical thinking skills. Learning and Instruction, 95, 102011. https:// doi. org/ 10. 1016/j. learn instr uc. 2024. 
102011
Tai, J., Ajjawi, R., Boud, D., Dawson, P., & Panadero, E. (2018). Developing evaluative judgement: Enabling stu-
dents to make decisions about the quality of work. Higher Education, 76, 467–481. https:// doi. org/ 10. 1007/ 
s1073 4- 017- 0220- 3
Tsui, A. B. M., & Ng, M. M. Y. (2010). Cultural contexts and situated possibilities in the teaching of second lan-
guage writing. Journal of Teacher Education, 61(4), 364–375. https:// doi. org/ 10. 1177/ 00224 87110 364855
van Popta, E., Kral, M., Camp, G., Martens, R. L., & Jan Simons, P. R. (2016). Exploring the value of peer feed-
back in online learning for the provider. Educational Research Review, 20, 24–34. https:// doi. org/ 10. 1016/j. 
edurev. 2016. 10. 003
Vuogan, A., & Li, S. (2023). Examining the effectiveness of peer feedback in second language writing: A meta- 
analysis. TESOL Quarterly, 57(4), 1115–1138. https:// doi. org/ 10. 1002/ tesq. 3178
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 21

1632 |   
GUO et al.
Wang, E. L., Matsumura, L. C., Correnti, R., Litman, D., Zhang, H., Howe, E., Magooda, A., & Quintana, R. (2020). 
eRevis (ing): Students' revision of text evidence use in an automated writing evaluation system. Assessing 
Writing, 44, 100449. https:// doi. org/ 10. 1016/j. asw. 2020. 100449
Weng, F., Ye, S. X., & Xue, W. (2023). The effects of peer feedback on L2 students' writing motivation: An exper-
imental study in China. The Asia- Pacific Education Researcher, 32, 473–483. https:// doi. org/ 10. 1007/ s4029 
9- 022- 00669 - y
Winstone, N. E., Nash, R. A., Rowntree, J., & Parker, M. (2017). ‘It'd be useful, but I wouldn't use it’: Barriers to uni-
versity students' feedback seeking and recipience. Studies in Higher Education, 42(11), 2026–2041. https:// 
doi. org/ 10. 1080/ 03075 079. 2015. 1130032
Wu, Y., & Schunn, C. D. (2021). From plans to actions: A process model for why feedback features. Influence 
feedback implementation. Instructional Science, 49(3), 365–394. https:// doi. org/ 10. 1007/ S1125 1- 021- 
09546 - 5
Wu, Z. (2019). Lower English proficiency means poorer feedback performance? A mixed- methods study. 
Assessing Writing, 41, 14–24. https:// doi. org/ 10. 1016/j. asw. 2019. 05. 001
Xiong, W., & Litman, D. (2011). Automatically predicting peer- review helpfulness. In Proceedings of the 49th 
Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, USA (pp. 
502–507).
Xiong, W., Litman, D., & Schunn, C. (2012). Natural language processing techniques for researching and 
improving peer feedback. Journal of Writing Research, 4, 155–176. https:// doi. org/ 10. 17239/ jowr- 2012. 
04. 02. 3
Yu, S. (2019). Learning from giving peer feedback on postgraduate theses: Voices from master's students in the 
Macau EFL context. Assessing Writing, 40, 42–52. https:// doi. org/ 10. 1016/j. asw. 2019. 03. 004
Yu, S. (2021). Giving genre- based peer feedback in academic writing: Sources of knowledge and skills, difficulties 
and challenges. Assessment & Evaluation in Higher Education, 46(1), 36–53. https:// doi. org/ 10. 1080/ 02602 
938. 2020. 1742872
Yu, S. (2024). Peer assessment in writing instruction. Cambridge University Press.
Yu, S., & Hu, G. (2017). Understanding university students' peer feedback practices in EFL writing: Insights from 
a case study. Assessing Writing, 33, 25–35. https:// doi. org/ 10. 1016/j. asw. 2017. 03. 004
Yu, S., & Lee, I. (2016). Peer feedback in second language writing (2005–2014). Language Teaching, 49(4), 
461–493. https:// doi. org/ 10. 1017/ S0261 44481 6000161
Zhang, T., & Mao, Z. (2023). Exploring the development of student feedback literacy in the second language writ-
ing classroom. Assessing Writing, 55, 100697. https:// doi. org/ 10. 1016/j. asw. 2023. 100697
Zhang, X., & Lu, X. (2022). Revisiting the predictive power of traditional vs. fine- grained syntactic complexity in-
dices for L2 writing quality: The case of two genres. Assessing Writing, 51, 100597. https:// doi. org/ 10. 1016/j. 
asw. 2021. 100597
Zhang, Y., Chen, H., Pi, Z., & Yang, J. (2024). Interactive equality in peer assessment: The impacts on preservice 
teachers' technology- enhanced learning design and feedback uptake. Teaching and Teacher Education, 
138, 104408. https:// doi. org/ 10. 1016/j. tate. 2023. 104408
Zhu, W. (1995). Effects of training for peer response on students' comments and interaction. Written Communication, 
12(4), 492–528. https:// doi. org/ 10. 1177/ 07410 88395 01200 4004
How to cite this article: Guo, K., Zhang, E. D., Li, D., & Yu, S. (2025). Using AI- 
supported peer review to enhance feedback literacy: An investigation of students' 
revision of feedback on peers' essays. British Journal of Educational Technology, 56, 
1612–1639. https://doi.org/10.1111/bjet.13540
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 22

| 1633
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
APPENDIX A
RATING RUBRIC USED BY STUDENTS IN THE PEER FEEDBACK TASKS
Aspect 1: Content
Aspect 2: Organisation
Aspect 3: Language
1 point
Inadequate
The argument is unclear 
or missing. The writer 
fails to provide sufficient 
evidence to support 
the argument. The 
writer does not address 
counterarguments
The essay lacks any 
discernible structure or 
coherence. There are no 
clear transitions between 
ideas or paragraphs
The language is unclear or 
inappropriate, making the 
essay difficult to understand. 
The grammar, syntax, and 
mechanics are consistently 
incorrect or ineffective
2 points
Developing
The argument is 
somewhat clear but lacks 
coherence or consistency. 
The writer provides 
some evidence but does 
not use it effectively to 
support the argument. 
The writer acknowledges 
counterarguments but 
fails to address them 
adequately
The essay has some 
structure, but it is not 
consistent or effective. 
Transitions between 
ideas or paragraphs are 
present, but not always 
clear or effective
The language is somewhat 
appropriate and varied, but 
lacks precision or coherence. 
The grammar, syntax and 
mechanics are inconsistent 
or contain errors that affect 
comprehension
3 points
Proficient
The argument is clear, 
logical and convincing. 
The writer provides 
sufficient evidence to 
support the argument 
and uses it effectively. 
The writer addresses 
counterarguments but 
may not fully refute them
The essay is well- 
structured and coherent, 
with a clear introduction, 
body and conclusion. 
Transitions between 
ideas or paragraphs are 
effective and contribute to 
the overall coherence of 
the essay
The language is mostly 
appropriate, precise and 
varied, and effectively 
conveys the writer's 
ideas. The grammar, 
syntax and mechanics are 
generally correct and do 
not significantly impede 
comprehension
4 points
Accomplished
The argument is highly 
convincing and effectively 
demonstrates the writer's 
point of view. The writer 
provides strong evidence 
to support the argument 
and persuasively uses 
it. The writer addresses 
counterarguments and 
effectively refutes them
The essay is exceptionally 
well- structured and 
coherent, with a clear and 
engaging introduction, 
well- developed body 
paragraphs, and a 
strong conclusion. 
Transitions between 
ideas or paragraphs are 
seamless and contribute 
significantly to the overall 
coherence of the essay
The language is highly 
effective in conveying 
the writer's ideas, and 
demonstrates a sophisticated 
use of vocabulary and syntax. 
The grammar, syntax and 
mechanics are consistently 
correct, and enhance the 
clarity and impact of the 
essay
5 points
Exemplary
The argument is 
exceptionally clear, 
compelling and 
sophisticated. The writer 
provides compelling 
and varied evidence to 
support the argument, 
and effectively integrates 
it into the essay. The 
writer addresses 
counterarguments and 
successfully refutes them 
with strong evidence and 
reasoning
The essay is masterfully 
structured and coherent, 
with a compelling and 
sophisticated introduction, 
seamlessly integrated 
body paragraphs, and 
a powerful conclusion. 
Transitions between 
ideas or paragraphs are 
flawlessly executed and 
contribute to the overall 
impact and effectiveness 
of the essay
The language is exceptionally 
effective, demonstrating 
a mastery of vocabulary, 
syntax, and rhetorical devices 
that enhance the essay's 
impact. The grammar, syntax, 
and mechanics are virtually 
flawless, contributing to the 
essay's exceptional clarity 
and effectiveness
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 23

1634 |   
GUO et al.
APPENDIX B
PROMPT FOR GENERATING AI FEEDBACK
The prompt used to generate ChatGPT's response is as follows:
I am providing feedback on a peer's essay. You provide comments and suggestions on my 
feedback in terms of the five aspects as follows:
1. Affect: Good feedback includes encouraging and positive emotions such as praise 
or compliments.
2. Description: Good feedback includes a summary statement such as description of content 
or taken action to a large extent.
3. Identification: Good feedback includes explicit and localised identification of problems.
4. Justification: Good feedback includes elaborations and justifications of identified 
problems.
5. Constructiveness: Good feedback includes recommendations and action plans for further 
improvements.
Below is my feedback:
[The student's comments typed into the Comment box].
APPENDIX C
AN EXAMPLE OF CONVERSATIONS BETWEEN A STUDENT AND THE CHATBOT 
EVA
Student:
I am providing feedback on a peer's essay. You provide comments and suggestions on my 
feedback in terms of the five aspects as follows:
1. Affect: Good feedback includes encouraging and positive emotions such as praise or 
compliments
2. Description: Good feedback includes a summary statement such as description of content or 
taken action to a large extent
3. Identification: Good feedback includes explicit and localised identification of problem
4. Justification: Good feedback includes elaborations and justifications of identified problem
5. Constructiveness: Good feedback includes recommendations and action plans for further 
improvements
Below is my feedback:
The main problem is that different points were not separated into different paragraphs, which 
makes this article appears to have only one argument. Although the counterargument is 
wonderful, the content is too much
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 24

| 1635
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
Eva:
1. Affect: Your feedback is neutral and does not include any encouraging or positive emotions. 
Consider adding some words of praise or compliments to make your feedback more uplifting 
and motivating for the writer
2. Description: Your feedback provides a clear description of the problem. You point out that 
different points were not separated into different paragraphs, making the article appear to 
have only one argument. This statement effectively summarises the issue at hand
3. Identification: You explicitly identify the problem of not separating different points into 
different paragraphs. This allows the writer to easily understand what specifically needs to be 
addressed in their essay
4. Justification: Your feedback lacks elaboration and justification. It would be beneficial to 
provide further explanation or reasoning as to why separating different points into paragraphs 
is important. How does it impact the clarity and organisation of the essay? By providing more 
justification, the writer can fully understand the reasoning behind your feedback
5. Constructiveness: Your feedback is constructive in the sense that you highlight the 
problem and suggest improvements. However, it would be helpful to include specific 
recommendations and action plans for the writer to follow. How can they go about separating 
their points into paragraphs effectively? Providing clear steps or suggestions for improvement 
will make your feedback more constructive
Student:
I have revised my feedback. You provide further comments and suggestions on my feedback in 
terms of the five aspects as follows:
1. Affect: Good feedback includes encouraging and positive emotions such as praise or 
compliments
2. Description: Good feedback includes a summary statement such as description of content or 
taken action to a large extent
3. Identification: Good feedback includes explicit and localised identification of problem
4. Justification: Good feedback includes elaborations and justifications of identified problem
5. Constructiveness: Good feedback includes recommendations and action plans for further 
improvements
Below is my revised feedback:
It is an excellent argument essay. The content is rich, no grammar mistakes and the logic 
is smooth. The main problem is that different points were not separated into different 
paragraphs, which makes this article appears to have only one argument. Although the 
counterargument is wonderful, the content is too much. My suggestion is to separate your 
views, so that readers can know what you want to say more clearly
Eva:
1. Affect: Your revised feedback now includes positive emotions by describing the essay as 
“excellent” and praising its rich content, lack of grammar mistakes, and smooth logic. This 
adds a more encouraging tone to your feedback
2. Description: Your feedback still includes a clear summary statement by acknowledging the 
essay as an argument essay and highlighting its strengths in terms of content, grammar, and 
logic. This description helps the writer understand the overall impression of their work
3. Identification: You continue to explicitly identify the problem of not separating different points 
into paragraphs. This allows the writer to easily understand what specifically needs to be 
addressed in their essay
4. Justification: Your feedback now includes some elaboration and justification by explaining 
that not separating different points into paragraphs makes the article appear to have only one 
argument. This clarification helps the writer understand the impact of the identified problem 
on the clarity of their essay
5. Constructiveness: Your feedback remains constructive by suggesting separating views into 
paragraphs to enhance clarity for readers. This recommendation provides a specific action 
plan for the writer to follow and improve their essay accordingly
Overall, your revised feedback incorporates more positive emotions, adds some justification 
for the identified problem, and includes a constructive recommendation. Consider further 
expanding on the justification to provide more detailed explanations and providing additional 
recommendations for improvement to make your feedback even more constructive
Note: In this conversation, the student requested Eva to evaluate her feedback on a peer's essay two times. Each time, the 
student simply clicked the “Check my comments” button, which activated a prompt to generate the chatbot's response. On the 
platform interface, the chat history only displayed the sentence “Check my comments” (as shown in Figure 3). Here, we provide 
the actual prompts used for generating the responses.
The screenshot below shows the revisions made by the student in response to Eva's feedback, utilising the ‘Compare 
Documents’ function in Microsoft Word.
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 25

1636 |   
GUO et al.
APPENDIX D
CODING SCHEME FOR ANALYSING STUDENT COMMENT QUALITY (ADOPTED 
FROM KERMAN ET AL. (2022))
Nature of 
feedback
Feedback 
features
Points
Label
Description
Affective
0
Poor—discouraging
The comment included discouraging 
and negative emotions such as anger 
or disappointments
1
Average—neutral/
not mentioned
The comment did not include either 
negative or positive emotions
2
Good—encouraging
The comment included encouraging 
and positive emotions such as praise 
or compliments
Cognitive
Description
0
Poor—not mentioned
The comment did not include 
a summary statement such as 
description of content or taken action
1
Average—mentioned 
to small extent
The comment included a summary 
statement such as description of 
content or taken action but to a small 
extent
2
Good—mentioned to 
a large extent
The comment included a summary 
statement such as description of 
content or taken action to a large 
extent
Identification
0
Poor—not mentioned
The comment did not include explicit 
identification of problem
1
Average—mentioned 
but not localised
The comment included identification 
of problem without localisation of 
identified problem
2
Good—mentioned 
and localised
The comment included explicit and 
localised identification of problem
Justification
0
Poor—not mentioned
The comment did not include 
elaborations and justifications of 
identified problem
1
Average—
mentioned, 
elaborated, but not 
justified
The comment included elaborations 
but not justifications of identified 
problem
2
Good—mentioned, 
elaborated and 
justified
The comment included elaborations 
and justifications of identified problem
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 26

| 1637
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
Nature of 
feedback
Feedback 
features
Points
Label
Description
Constructive
0
Poor—not mentioned
The comment did not include any 
recommendations or action plans for 
further improvements
1
Average—only 
recommendation is 
mentioned
The comment included 
recommendations but not action plans 
for further improvements
2
Good—both 
recommendation 
and action plan are 
mentioned
The comment included 
recommendations and action plans for 
further improvements
APPENDIX E
STUDENT STRATEGIES FOR COMMENT REVISION
Strategy
Description
Example
Number of 
instances 
(percentage)
Original version 
(excerpt)
Final version 
(excerpt)
Case 
no.
No improvement achieved
1 
Repeating 
information
A student repeated 
his/her original 
comments, without 
offering new points
You are 
recommended 
to write a strong 
rebuttal to make 
your argument 
stronger
A strong 
rebuttal on the 
counterargument 
part will make the 
opposite side of 
the main claim 
weaker
11
38 (52.05%)
2 
Correcting 
language 
errors
A student corrected 
language errors 
(eg, spelling and 
grammatical 
mistakes) in his/her 
original comments
It would be much 
better if you can 
summarise the 
research finding 
and give more 
explanation
It would be 
better if you can 
summarise the 
research findings 
and provide more 
explanation
13
22 (30.14%)
3 Changing 
word usage
A student changed 
word usage in 
his/her original 
comments
Maybe one more 
subclaim would be 
better
Maybe adding 
one more 
subclaim 
would be more 
convincing
47
13 (17.81%)
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 27

1638 |   
GUO et al.
Strategy
Description
Example
Number of 
instances 
(percentage)
Original version 
(excerpt)
Final version 
(excerpt)
Case 
no.
Slight or substantive improvement achieved
1 
Introducing 
new points
A student provided 
new points that 
were not present 
in his/her original 
comments
Here is some 
advice: you may 
consider more 
detail about the 
survey you wrote for 
the first subclaim
Here is some 
advice: you may 
consider more 
detail about 
the survey you 
wrote for the 
first subclaim, 
which will make it 
more convincing. 
Also, expand the 
reasoning part 
to have a better 
connection with 
your claims. That 
will help a lot in 
demonstrating 
your ideas. 
Additionally, 
check the 
“Maslow's 
hierarchy of 
needs” to see 
how this material 
can support your 
claim in a more 
fluent, natural way
19
73 (41.01%)
2 Adding 
details
A student added 
details to his/her 
original comments
I also found a 
grammar mistake 
in the reasoning of 
Subclaim 2
I also found a 
grammar mistake 
in the reasoning 
of Subclaim 
2, “there may 
have…” should 
be changed to 
“There may be…”
7
52 (29.21%)
3 Giving 
examples
A student offered 
specific examples to 
help explain his/her 
original comments
Also, please try 
to say something 
positive instead of 
explaining it from a 
negative aspect
Also, please try 
to say something 
positive instead of 
explaining it from 
a negative aspect. 
For example, say 
“if people choose 
their courses 
based on interest” 
instead of “if they 
do not”
26
30 (16.85%)
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## Page 28

| 1639
AI- SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY
Strategy
Description
Example
Number of 
instances 
(percentage)
Original version 
(excerpt)
Final version 
(excerpt)
Case 
no.
4 Using full 
sentences
A student 
transformed words 
or phrases in his/her 
original comments 
into full sentences
A spelling error, 
“bewteen”; 
some mistakes, 
like “college 
students should 
encouraged”; 
maybe more 
reasoning
There is a spelling 
error in the first 
paragraph. The 
word “bewteen” 
should be 
“between”. 
Also, the main 
claim should 
be “college 
students should 
be encouraged” 
instead of “college 
students should 
encouraged”. 
Finally, maybe 
you can add more 
explanation in 
your reasoning 
part to make your 
subclaim more 
convincing
52
23 (12.92%)
 14678535, 2025, 4, Downloaded from https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13540 by Hong Kong Baptist University, Wiley Online Library on [11/09/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License