# Using AI-supported peer review to enhance feedback literacy: An investigation of students' revision of feedback on peers' essays

Kai Guo1, Emily Di Zhang2, Danling Li3, Shulin Yu4

1The University of Hong Kong, Hong Kong, China  
2Shanghai Jiao Tong University, Shanghai, China  
3The Chinese University of Hong Kong (Shenzhen), Shenzhen, China  
4University of Macau, Macao SAR, China

**Correspondence:**  
Danling Li  
The Chinese University of Hong Kong (Shenzhen)  
2001 Longxiang Boulevard, Longgang District  
Shenzhen, China  
Email: lidanling@cuhk.edu.cn

## Abstract

As a vital learning activity in second language (L2) writing classrooms, peer feedback plays a crucial role in improving students' writing skills. However, student reviewers face challenges in providing impactful feedback on peers' essays. Low-quality peer reviews emerge as a persistent problem, adversely affecting the learning effect of peer feedback. To enhance students' peer feedback provision, this study introduces EvaluMate, an AI-supported peer review system, which incorporates a chatbot named Eva, designed to evaluate and provide feedback on student reviewers' comments on peers' essays. Forty-four Chinese undergraduate students engaged with EvaluMate, utilising its features to generate feedback on peers' English argumentative essays. Chat log data capturing the students' interactions with the chatbot were collected, including the comments they wrote on peer essays and the feedback offered by the chatbot on their comments. The results indicate that the integration of AI supervision improved the quality of students' peer reviews. Students employed various strategies during their comment revision in response to AI feedback, such as introducing new points, adding details, and providing illustrative examples, which helped improve their comment quality. These findings shed light on the benefits of AI-supported peer review systems in empowering students to provide more valuable feedback on peers' written work.

**Keywords**: peer feedback, artificial intelligence, chatbots, L2 writing, feedback literacy, EvaluMate

## Practitioner notes

### What is already known about this topic
- Scholars have extensively investigated diverse pedagogical strategies to enhance students' peer feedback provision skills in second language (L2) writing classrooms.
- Artificial intelligence (AI) technologies have been utilised to monitor and evaluate the peer feedback generated by student reviewers.
- AI-enabled peer feedback evaluation tools have demonstrated the ability to provide valid assessments of student reviewers' peer feedback.

### What this paper adds
- In the context of L2 writing, there is a lack of bespoke AI-enabled peer feedback evaluation tools. To address this gap, we have developed an AI-supported peer review system, EvaluMate, which incorporates a large language model-based chatbot named Eva. Eva is designed to provide feedback on L2 students' comments on their peers' writing.
- While previous studies have primarily focused on assessing the validity of AI-enabled peer feedback evaluation tools, little is known about how students incorporate AI support into improving their peer review comments. To bridge this gap, our study examines not only whether using the system (EvaluMate) can enhance the quality of L2 students' peer review comments but also how students respond to Eva's feedback when revising their comments.

### Implications for practice and/or policy
- The development of the AI-supported peer review system (EvaluMate) introduces an innovative pedagogical approach for L2 writing teachers to train and enhance their students' peer feedback provision skills.
- Integrating AI supervision into L2 students' peer feedback generation improves the quality of comments provided by student reviewers on their peers' writing.
- Students employ various strategies when revising their comments in response to Eva's feedback, and these strategies result in varying degrees of improvement in comment quality. L2 writing teachers can teach effective revision strategies to their students.

## Introduction

Peer feedback, the process of students evaluating and critiquing the work of their peers, has been extensively used in second language (L2) writing classrooms (Banister, 2023; Lee, 2017; Yu, 2024). It is recognised as an effective strategy for promoting student feedback literacy (Carless & Boud, 2018; Nicol et al., 2014). Developing feedback literacy requires students to cultivate evaluative judgement, which involves assessing the quality of their own work and that of

Here's the cleaned Markdown:

## AI-Supported Peer Review to Enhance Feedback Literacy

Numerous studies have investigated pedagogical approaches to enhance students' peer feedback, with some employing artificial intelligence (AI) technologies to supervise students' peer feedback generation (Darvishi et al., 2024; Guo et al., 2024; Han et al., 2023; Jia et al., 2021). These AI tools simulate teachers in providing analysis of peer feedback quality and intervene in students' peer feedback creation, showing positive effects on peer feedback quality.

Utilising AI to monitor and evaluate student reviewers' peer feedback enables students to have better opportunities for self-assessment (Han et al., 2023), allowing them to improve their evaluative skills over time. Incorporating AI supervision into the peer feedback generation process requires students to engage in planning, drafting, and revising while continuously refining their assessments of work quality (Guo, 2024; Guo et al., 2024). This iterative process allows students to generate internal feedback on their progress, with external feedback from AI further supporting students in sharpening their internal evaluations. AI evaluation and monitoring would serve as a valuable resource for motivating students to share and discuss their judgements, providing ongoing opportunities for enhancing their ability to make informed decisions.

However, current research has primarily focused on the effectiveness and outcomes of these AI tools. Further investigation is needed into how students integrate AI assessments to revise and improve their peer review comments. To address the research gap, the present study introduces an AI-supported peer review system called EvaluMate, which integrates a large language model-based chatbot to enhance students' peer feedback provision in English writing. We aim to examine whether the system improves students' peer feedback quality and how students act on the AI supervision. This study offers a novel pedagogical approach for writing teachers to develop their students' evaluative judgement and adds to our understanding of the potential of AI technologies in scaffolding L2 students' peer feedback practices.

## Literature Review

### Peer feedback provision in L2 writing

Previous studies have demonstrated the significant impact of peer feedback on the learning of L2 writing, such as increasing awareness of audience and genre (Berggren, 2015; Yu, 2019), enhancing writing motivation (Weng et al., 2023), promoting critical thinking (Novakovich, 2016) and improving writing competence (Latifi & Noroozi, 2021; Lu et al., 2023; Vuogan & Li, 2023). Importantly, earlier research compared peer feedback with teacher feedback, finding that although L2 students integrated more teacher feedback, leading to more revisions, peer feedback was associated with a higher degree of student autonomy (Miao et al., 2006). Additionally, peer feedback resulted in more meaning-based revisions and more successful outcomes than teacher feedback, due to effective peer interaction and negotiation (Biber et al., 2011; Tsui & Ng, 2010).

Despite the potential benefits of peer feedback for L2 writing, student reviewers often encounter various challenges in the process of feedback generation, which is a complex task, requiring cognitive and metacognitive skills (Latifi et al., 2021; van Popta et al., 2016). Research has indicated that the prevalence of low-quality peer reviews is a consistent issue, adversely affecting the impact of peer feedback on the learning process (Han et al., 2023; Nelson & Schunn, 2009). From a linguistic perspective, students' still-developing L2 proficiency and understanding of its rhetorical conventions (Allen & Katayama, 2016; Wu, 2019) may hinder them from accurately comprehending their peers' writing intentions (Min, 2005; Stanley, 1992), identifying language and rhetorical issues, and offering constructive, high-quality feedback (Wu, 2019; Yu & Lee, 2016). Socio-culturally, L2 students from varying cultural backgrounds have diverse perceptions of peer interaction and writing (Allaei & Connor, 1990), which could adversely affect the peer feedback processes (Hyland & Hyland, 2006). For instance, studies have found that L2 students from collectivist cultures may hesitate to provide critical comments to preserve interpersonal harmony and mutual face-saving (Carson & Nelson, 1996; Nelson & Carson, 1998; Yu & Hu, 2017).

Here's the cleaned and normalized Markdown:

## AI-Supported Peer Review to Enhance Feedback Literacy

## Literature Review

By guiding students through the peer feedback process, AI can help them develop effective communication and critical thinking skills, regardless of their cultural background (Liang & Wu, 2024; Suriano et al., 2025).

Notably, some studies have examined the validity of automated peer review evaluation tools and reported acceptable validity (Xiong et al., 2012; Xiong & Litman, 2011). With a special focus on how AI evaluation can improve students' peer feedback quality, several studies revealed that automated peer review evaluation tools could enhance students' seriousness of peer review (Han et al., 2023) and aid students in providing more specific, targeted, high-quality and helpful feedback (Darvishi et al., 2024; Han et al., 2023; Jia et al., 2021). However, thus far limited AI-powered tools have been developed and utilized for supporting student reviewers' feedback creation in the L2 writing learning context.

Moreover, existing digital tools developed to facilitate peer feedback generation were not based on a comprehensive definition of effective feedback features in writing. According to the literature (Kerman et al., 2022; Patchan et al., 2016; Wu & Schunn, 2021), writing feedback features include:

1. Affective (inclusion of positive emotions such as praise or compliments and negative emotions such as anger or disappointments)
2. Description (summary statement of the essay)
3. Identification (identification and localisation of the problem in the essay)
4. Justification (elaborations and justifications of the identified problem)
5. Constructive (inclusion of recommendations and action plans for further improvements)

However, in designing and developing digital tools for supporting peer feedback generation, many studies have overlooked essential features, such as neglecting the affective aspect, despite its known importance.

## Research Questions

The study seeks to address the following two research questions (RQs):

1. To what extent does the quality of students' comments on peers' essays improve when using AI feedback on their comments?
2. How do students integrate the feedback received from AI when revising their comments on peers' essays?

## Methods

In this study, we adopted a mixed methods approach, specifically employing a convergent parallel design (Creswell & Clark, 2017). In this design, qualitative and quantitative data are collected simultaneously, analysed separately and then merged during the interpretation stage to provide a comprehensive understanding of the research problem.

### Context and Participants

The study was conducted at a comprehensive university in China that employs English as the medium of instruction. A total of 44 undergraduate students, consisting of 13 males and 31 females, participated in this study. These students came from a diverse range of disciplines including humanities and social science, management and economics, and science, technology, engineering and mathematics (STEM). Their average age was 19.02 years old, with a standard deviation of 0.45.

### EvaluMate, an AI-supported Peer Review System

To implement AI-supported peer feedback, the researchers developed EvaluMate, an online peer review system. This system incorporates AI capabilities and features a large language model-based chatbot named Eva, designed to assist students during the peer feedback generation process. Eva's feedback is based on Kerman et al.'s (2022) framework for peer feedback quality evaluation that highlights five important features.

## AI-SUPPORTED PEER REVIEW TO ENHANCE FEEDBACK LITERACY

## Framework and System

Effective feedback should include:
- Affective elements (praise or compliments)
- Accurate summary description
- Clear identification of issues
- Thorough explanations and justifications
- Constructive recommendations and plans

The system EvaluMate includes a "Check my comments" button that activates a researcher-designed prompt to ChatGPT. For further details regarding EvaluMate, see Guo (2024).

## Data Collection

The instructor introduced EvaluMate to students through three steps:
1. Overview of the rating rubric and its purpose
2. Time for students to review and familiarize with the rubric
3. Explanation of criteria and performance levels with examples

Students completed an argumentative writing task adapted from the GRE 'Analyze an Issue' task. The prompt was:

> "College students should be encouraged to pursue subjects that interest them rather than the courses that seem most likely to lead to jobs." Discuss the extent to which you agree or disagree with this statement and provide reasons for the position you take. In developing and supporting your position, you should include relevant evidence. Write 300-500 words.

Students had 40 minutes for the writing task. Each student was then randomly assigned two peer review tasks, with 20 minutes per task. They were required to use the chatbot Eva at least once before submitting comments.

## Data Analysis

### Research Question 1

To investigate AI support's impact on comment quality, researchers:
- Analyzed 88 pairs of initial and final comments (44 students × 2 tasks)
- Used Kerman et al. (2022) coding scheme examining five aspects:
  1. Affective
  2. Description
  3. Identification
  4. Justification
  5. Constructive

Each aspect was scored 0-2 points (maximum total: 10 points).

For coding reliability:
- First and third authors independently coded 25% of data
- Cohen's kappa coefficients:
  - Affective: 0.91
  - Description: 0.85
  - Identification: 0.81
  - Justification: 0.87
  - Constructive: 0.92

Analysis methods:
- Shapiro-Wilk test for normality
- Wilcoxon signed-rank test for comparing initial and revised comment scores
- Descriptive statistics for overall trends
- Analysis performed using IBM SPSS Statistics 29.0

## AI-Supported Peer Review to Enhance Feedback Literacy

For answering RQ2, we conducted a qualitative, iterative, and inductive coding process. Following Wang et al. (2020) and Lu et al. (2024), we began by creating two Word files: one containing all initial comment drafts and another containing all finally submitted comments extracted from the chat log data, both ordered by a unique case number (88 cases in total). We then used the 'Compare Documents' function to generate a file that tracked the changes made between the two versions (see Appendix C), including insertions, deletions, and other revisions made by the students. Finally, we extracted AI feedback from the chat log data and juxtaposed it with the corresponding student comment revisions in an Excel file.

The first step in our analysis involved coding whether the students attempted to revise their comments based on the actionable AI feedback (i.e. feedback that required students' revisions for improvement) they received from Eva. Specifically, we examined whether the revisions reflected at least one of the five peer feedback quality aspects. For instance, in the case of student comments receiving actionable AI feedback on the affective aspect, we looked for revisions where students made an effort to incorporate encouraging and positive emotions such as praise or compliments. Out of the 88 cases of peer feedback observed:

- In 8 cases, students received actionable AI feedback on the affective aspect, and 2 did not make revision attempts
- In 83 cases, students received actionable AI feedback on the descriptive aspect, and 4 did not make revision attempts
- In 70 cases, students received actionable AI feedback on the identification aspect, with 8 cases without revisions
- In 68 cases, students received actionable AI feedback on the justification aspect, with 13 cases without revisions
- In 85 cases, students received actionable AI feedback on the constructive aspect, with 36 cases without revisions

Subsequently, for comments where students attempted revisions, we coded the extent of improvement in the five aspects, considering the AI feedback provided. We used the scoring results from RQ1 for this step. We categorized comments as:

1. No attempt at implementing given feedback
2. Attempted to implement feedback, but no improvement  
3. Slight improvement (one-score improvement)
4. Substantive improvement (two-score improvement)

Finally, for revisions resulting in no improvement, we coded specific ways the revisions fell short (three codes, see Appendix E). We also coded comments where revisions showed attention to AI feedback and achieved improvements (four codes, see Appendix E). Each comment received one code. The first author coded all 88 pairs, while the third author independently coded 22 samples (25%). Inter-coder agreement reached 92%, with discrepancies resolved through discussion.

## Results

### To what extent does the quality of students' comments on peers' essays improve when using AI feedback on their comments? (RQ1)

Analysis of initial and final comment versions showed the average length increased from 73.28 words to 96.85 words, an average addition of 23.57 words per comment after AI feedback revisions.

The Wilcoxon signed-rank test comparing initial and final comment scores showed significant improvements (p < 0.05) across all five quality dimensions:

- Affective (z = 2.449; p = 0.014; r = 0.185)
- Description (z = 8.230; p = 0.000; r = 0.620)
- Identification (z = 7.117; p = 0.000; r = 0.536)
- Justification (z = 5.231; p = 0.000; r = 0.394)
- Constructive (z = 3.464; p = 0.001; r = 0.261)

Total scores also showed significant improvement (z = 8.071; p = 0.000; r = 0.608).

| Feedback quality dimensions | Initial version Median | Final version Median | z | r |
|----------------------------|------------------------|---------------------|---|---|
| Affective | 2.00 | 2.00 | 2.449* | 0.185 |
| Description | 1.00 | 2.00 | 8.230* | 0.620 |
| Identification

## AI-Supported Peer Review to Enhance Feedback Literacy

## Students' Comment Revision Outcomes

Table 3 presents the outcomes of student reviewers' comment revisions in response to actionable AI feedback. First, out of the eight cases where Eva provided actionable feedback on the affective aspect, students attempted to revise their comments in six cases. As a result, slight improvements were observed. Second, among the 83 cases where Eva provided actionable feedback on the description aspect, students made an effort to revise their comments in 79 cases. Out of these attempts, two did not result in any improvement, while 64 attempts led to slight improvements. Additionally, 13 attempts resulted in substantive improvements. Third, Eva provided actionable feedback on the identification aspect in 70 cases. Among these, students attempted to revise their comments in 62 cases. Out of these attempts, eight did not lead to any improvement, 50 attempts resulted in slight improvements, and four attempts resulted in substantive improvements. Fourth, in the case of the justification aspect, Eva provided actionable feedback in 68 cases. Out of these, students attempted to revise their comments in 55 cases. However, 26 attempts did not yield any improvement. On the other hand, 27 attempts led to slight improvements, and two attempts resulted in substantive improvements. Finally, out of the 85 cases where Eva provided actionable feedback on the constructive aspect, students attempted to revise their comments in 49 cases. Unfortunately, 37 attempts did not lead to any improvement. However, 12 attempts resulted in slight improvements, although no attempts resulted in substantive improvements.

## Comment Revision Strategies Leading to No Improvement

Based on an analysis of specific revisions made by the students in cases where no improvement was observed, we identified three strategies commonly adopted by the students:

1. The first strategy, observed in the majority of cases (38 instances), involved students simply reiterating their original comments without introducing any new points. This tendency was particularly evident in comments related to the constructive aspect. For instance, in case 11, the student initially suggested, "You are recommended to write a strong rebuttal to make your argument stronger." Despite Eva's encouragement to provide specific action plans for improvement, the student failed to offer any new suggestions and merely reiterated their original comment, stating, "A stronger rebuttal on the counterargument part will make the opposite side of the main claim weaker."

2. The second strategy (22 instances) involved students correcting language errors present in their original comments, such as misspellings and grammatical mistakes. For example, in case 19, the student's initial comment read, "Your structure is clear but it seem that it lacks a conclusion part." In his revision, the student corrected the use of "seem" to "seems". Despite Eva's recommendation to provide justifications for the comment, emphasizing the significance of a well-written conclusion, the student merely focused on the linguistic correction.

3. The third strategy (13 instances) involved altering word usage. For example, in case 47, the student's original comment stated, "Maybe one more subclaim would be better." In her revision, the student replaced "better" with "more convincing". Despite Eva's recommendation to propose a potential subclaim for the author's consideration, the student solely focused on modifying the wording.

## Comment Revision Strategies Leading to Slight or Substantive Improvement

Through analysis of student revisions in cases where slight or substantive improvement was observed, we identified four strategies:

1. The most common strategy (73 instances) involved students introducing new points that were not present in their original comments.

2. Adding details to original comments (52 instances).

3. Providing specific examples to illustrate original comments (30 instances).

[Tables 2 and 3 preserved as in original]

Here's the cleaned Markdown:

The fourth strategy employed by the students involved transforming words or phrases in their original comments into full sentences. This strategy was observed in 23 instances. For instance, in case 52, the student initially made a comment that consisted of fragmented phrases such as "A spelling error, 'bewteen'; some mistakes, like 'college students should encouraged'; maybe more reasoning." During the evaluation, Eva suggested explicitly stating the location of the identified problems and recommended that the student provide recommendations for improving the evaluated essay, even though the student had already suggested "maybe more reasoning." In response, the student revised her comment by transforming it into full sentences, stating, "There is a spelling error in the first paragraph. The word 'bewteen' should be 'between'. Also, the main claim should be 'college students should be encouraged' instead of 'college students should encouraged'. Finally, maybe you can add more explanation in your reasoning part to make your subclaim more convincing." This revision enabled Eva to better understand the students' comments and resulted in increased scores in terms of the identification and justification dimensions.

## Discussion

This study explored the impact of incorporating AI supervision into L2 students' peer feedback creation process. Specifically, we investigated how using AI assistance influenced the quality of student reviewers' comments on peers' essays. The findings revealed several noteworthy outcomes:

1. The students had significantly better performance in all the five dimensions of peer feedback: affective, description, identification, justification, and constructive dimensions. This lends empirical support to researchers' recommendation of peer feedback training to improve the effectiveness of peer feedback activities.

2. When engaging with Eva's feedback, students demonstrated a tendency to write more detailed comments. This indicates that the integration of AI evaluation prompted students to delve deeper into their peer reviews.

3. The incorporation of AI supervision resulted in an overall improvement in the quality of students' comments, aligning with previous studies in which the benefits of AI in scaffolding students' peer feedback were documented.

Notably, the descriptive dimension of their comments experienced the most significant impact from using AI support. Initially, only a few students provided a statement summarising the content of the essay being evaluated. After being prompted by Eva, most students successfully integrated such summaries into their comments. This demonstrated that a precise understanding of the peers' writing intentions is a prerequisite for students to identify problems and provide meaningful and constructive suggestions that facilitated the feedback recipients' uptake of the reviewer comments.

Despite these positive outcomes, it is worth noting that the constructive dimension of students' comments showed a relatively modest improvement compared to other dimensions. While there was a statistically significant enhancement, it was not as prominent as in other aspects, confirming the multidimensional complexity of peer feedback activities.

Furthermore, this study delved into the strategies employed by students during the comment revision process, which had varying degrees of impact on the quality improvement of their comments. On one hand, our results indicated that merely reiterating the information from their original comments during the revision did not contribute to quality improvement. On the other hand, our findings highlighted the positive influence of specific revision strategies. When students introduced new points, added relevant details, provided illustrative examples and formulated their comments in full sentences, their revisions resulted in noticeable quality improvement.

## Conclusion, Implications and Limitations

The findings of this study have several theoretical implications for the field of writing instruction and peer feedback:

1. The study highlights the significance of incorporating AI technology to support students in their feedback provision process.

2. The study contributes to our understanding of students' comment revision strategies and their influence on comment quality.

3. The study explores how AI can elevate the learning experience for L2 learners.

The cleaned Markdown:

## AI Integration in Peer Assessment: Novel Applications and Implications

Previous studies typically employ AI tools to assess student essays directly, but our approach is distinctive: we use AI to evaluate student reviewers' feedback on peer essays. This unique application highlights a pioneering method of AI support in writing assessment.

The findings of this study have important implications for educators, curriculum designers, and developers of online peer review systems. First, the integration of AI chatbots, like Eva, in online peer review systems can significantly benefit students' feedback provision process. Our findings highlight the potential of incorporating AI technology as a supportive tool to guide and scaffold students' peer feedback generation. 

Educators can consider implementing AI chatbots in their instructional design to facilitate students' engagement, encourage critical thinking, and provide timely and personalised feedback. Additionally, this study emphasises the importance of guiding students in adopting effective comment revision strategies during the feedback creation process. Educators can design explicit instructional interventions that promote comprehensive and meaningful comment revisions, such as introducing new points, incorporating relevant details and examples, and formulating comments in full sentences.

By providing guidance and opportunities for practice, educators can help students develop and refine their comment revision skills, leading to more substantial improvements in the quality of their comments. Importantly, educators should not only introduce students to AI technology but also guide them in using it in an ethically responsible and accurate manner, which is an essential component of AI literacy (Ng et al., 2021). By emphasising ethical considerations in AI usage, such as privacy, bias, transparency and accountability in AI systems, educators can foster students' understanding of the ethical implications surrounding AI.

This will enable students to make informed decisions about when and how to use AI tools and consider factors beyond just technical functionality. Finally, in our research, we have specifically employed AI for peer assessment in L2 writing. However, we are optimistic about the broader applicability of AI-supported peer feedback across diverse educational settings beyond our current focus. We encourage researchers and developers in other educational domains to adapt and use our approach to explore its efficacy in a range of contexts.

## Limitations and Future Research

This study, while providing valuable insights, has several limitations:

1. While we explored the impact of AI assistance on students' ability to improve peer feedback quality, it remains unclear whether students can generate equally effective feedback without AI aid. Future research could investigate this through experimental designs.

2. The study had an uneven distribution of participants (13 males, 31 females). Since gender can influence students' acceptance of information technologies (Cai et al., 2017), future research should investigate potential gender differences in AI technology use for peer review.

3. This study investigated students' engagement with AI feedback through revision analysis, excluding cases without revisions. As suggested by Fu, Yang, et al. (2024), exploring this 'invisible engagement' through qualitative methodologies like interviews could provide valuable insights into students' concealed engagement with AI feedback.

Here's the cleaned Markdown:

to gain a comprehensive understanding of their AI feedback utilisation process. Fourth, in our prompt design (see Appendix B), we did not explicitly instruct the chatbot to refrain from directly revising or reformulating students' comments. Consequently, in our dataset, we encountered instances where the chatbot directly revised a student's comments after providing revision suggestions. This issue highlights the inherent challenge of predictability and controllability in generative models. It is reasonable to assume that some students might simply copy and paste the AI's revisions without actively engaging in the learning process. To address this concern, improving the prompt design, such as incorporating clear instructions like "do not directly revise my comments, but provide suggestions instead," could be effective. By refining the prompt design, we can mitigate the risk of students passively accepting AI-generated revisions and foster a more meaningful learning experience. Finally, it will be crucial to scrutinise AI-assisted peer feedback from the perspective of the feedback recipients, as a feedback loop completes only when the peer uses the feedback. For instance, future research could explore the effects of utilising AI-assisted peer feedback on the writing quality of feedback recipients.

## Funding Information

This work was supported by the Research Postgraduate Student Innovation Award 2022/23 and Dissertation Year Fellowship, awarded by the Graduate School of the University of Hong Kong, to the first author.

## Conflict of Interest Statement

There is no potential conflict of interest in this work.

## Data Availability Statement

Data will be made available on request.

## Ethics Statement

The research study has obtained ethical approval from the institutional review board at the University of Hong Kong. The research protocol and all associated procedures have been carefully reviewed and deemed to comply with ethical standards and guidelines. Participants' rights, privacy, confidentiality and well-being have been considered and protected throughout the study. Informed consent was obtained from all participants prior to their involvement in the research.

## ORCID

Kai Guo  
https://orcid.org/0000-0001-9699-7527

Emily Di Zhang  
https://orcid.org/0000-0002-9803-1088

Danling Li  
https://orcid.org/0000-0003-0974-1182

Shulin Yu  
https://orcid.org/0000-0003-1051-311X

## References

Allaei, S., & Connor, U. (1990). Exploring the dynamics of cross-cultural collaboration in writing classrooms. The Writing Instructor, 10, 19–28.

Allen, D., & Katayama, A. (2016). Relative second language proficiency and the giving and receiving of written peer feedback. System, 56, 96–106. https://doi.org/10.1016/j.system.2015.12.002

[References continue...]

## References

Latifi, S., Noroozi, O., & Talaee, E. (2021). Peer feedback or peer feedforward? Enhancing students' argumentative peer learning processes and outcomes. British Journal of Educational Technology, 52(2), 768–784. https://doi.org/10.1111/bjet.13054

Lee, I. (2017). Classroom assessment and feedback in L2 school contexts. Springer.

Li, R. (2023). Still a fallible tool? Revisiting effects of automated writing evaluation from activity theory perspective. British Journal of Educational Technology, 54(3), 773–789. https://doi.org/10.1111/bjet.13294

Liang, W., & Wu, Y. (2024). Exploring the use of ChatGPT to foster EFL learners' critical thinking skills from a posthumanist perspective. Thinking Skills and Creativity, 54, 101645. https://doi.org/10.1016/j.tsc.2024.101645

Liou, H. C., & Peng, Z. Y. (2009). Training effects on computer-mediated peer review. System, 37, 514–525. https://doi.org/10.1016/j.system.2009.01.005

Lu, Q., Yao, Y., Xiao, L., Yuan, M., Wang, J., & Zhu, X. (2024). Can ChatGPT effectively complement teacher assessment of undergraduate students' academic writing? Assessment & Evaluation in Higher Education, 49, 616–633. https://doi.org/10.1080/02602938.2024.2301722

Lu, Q., Yao, Y., & Zhu, X. (2023). The relationship between peer feedback features and revision sources mediated by feedback acceptance: The effect on undergraduate students' writing performance. Assessing Writing, 56, 100725. https://doi.org/10.1016/j.asw.2023.100725

McConlogue, T. (2015). Making judgements: Investigating the process of composing and receiving peer feedback. Studies in Higher Education, 40, 1495–1506. https://doi.org/10.1080/03075079.2013.868878

Miao, Y., Badger, R., & Zhen, Y. (2006). A comparative study of peer and teacher feedback in a Chinese EFL writing class. Journal of Second Language Writing, 15, 179–200. https://doi.org/10.1016/j.jslw.2006.09.004

Min, H. T. (2005). Training students to become successful peer reviewers. System, 33(2), 293–308. https://doi.org/10.1016/j.system.2004.11.003

Min, H. T. (2006). The effects of trained peer review on EFL students' revision types and writing quality. Journal of Second Language Writing, 15(2), 118–141. https://doi.org/10.1016/j.jslw.2006.01.003

Nelson, G. L., & Carson, J. G. (1998). ESL students' perceptions of effectiveness in peer response groups. Journal of Second Language Writing, 7, 113–131. https://doi.org/10.1016/S1060-3743(98)90010-8

Nelson, M. M., & Schunn, C. D. (2009). The nature of feedback: How different types of peer feedback affect writing performance. Instructional Science, 37(4), 375–401. https://doi.org/10.1007/s11251-008-9053-x

Ng, D. T. K., Leung, J. K. L., Chu, S. K. W., & Qiao, M. S. (2021). Conceptualizing AI literacy: An exploratory review. Computers and Education: Artificial Intelligence, 2, 100041. https://doi.org/10.1016/j.caeai.2021.100041

Nicol, D. (2010). From monologue to dialogue: Improving written feedback processes in mass higher education. Assessment & Evaluation in Higher

Here's the cleaned Markdown:

Wang, E. L., Matsumura, L. C., Correnti, R., Litman, D., Zhang, H., Howe, E., Magooda, A., & Quintana, R. (2020). eRevis(ing): Students' revision of text evidence use in an automated writing evaluation system. Assessing Writing, 44, 100449. https://doi.org/10.1016/j.asw.2020.100449

Weng, F., Ye, S. X., & Xue, W. (2023). The effects of peer feedback on L2 students' writing motivation: An experimental study in China. The Asia-Pacific Education Researcher, 32, 473–483. https://doi.org/10.1007/s40299-022-00669-y

Winstone, N. E., Nash, R. A., Rowntree, J., & Parker, M. (2017). 'It'd be useful, but I wouldn't use it': Barriers to university students' feedback seeking and recipience. Studies in Higher Education, 42(11), 2026–2041. https://doi.org/10.1080/03075079.2015.1130032

Wu, Y., & Schunn, C. D. (2021). From plans to actions: A process model for why feedback features. Influence feedback implementation. Instructional Science, 49(3), 365–394. https://doi.org/10.1007/S11251-021-09546-5

Wu, Z. (2019). Lower English proficiency means poorer feedback performance? A mixed-methods study. Assessing Writing, 41, 14–24. https://doi.org/10.1016/j.asw.2019.05.001

Xiong, W., & Litman, D. (2011). Automatically predicting peer-review helpfulness. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, USA (pp. 502–507).

Xiong, W., Litman, D., & Schunn, C. (2012). Natural language processing techniques for researching and improving peer feedback. Journal of Writing Research, 4, 155–176. https://doi.org/10.17239/jowr-2012.04.02.3

Yu, S. (2019). Learning from giving peer feedback on postgraduate theses: Voices from master's students in the Macau EFL context. Assessing Writing, 40, 42–52. https://doi.org/10.1016/j.asw.2019.03.004

Yu, S. (2021). Giving genre-based peer feedback in academic writing: Sources of knowledge and skills, difficulties and challenges. Assessment & Evaluation in Higher Education, 46(1), 36–53. https://doi.org/10.1080/02602938.2020.1742872

Yu, S. (2024). Peer assessment in writing instruction. Cambridge University Press.

Yu, S., & Hu, G. (2017). Understanding university students' peer feedback practices in EFL writing: Insights from a case study. Assessing Writing, 33, 25–35. https://doi.org/10.1016/j.asw.2017.03.004

Yu, S., & Lee, I. (2016). Peer feedback in second language writing (2005–2014). Language Teaching, 49(4), 461–493. https://doi.org/10.1017/S0261444816000161

Zhang, T., & Mao, Z. (2023). Exploring the development of student feedback literacy in the second language writing classroom. Assessing Writing, 55, 100697. https://doi.org/10.1016/j.asw.2023.100697

Zhang, X., & Lu, X. (2022). Revisiting the predictive power of traditional vs. fine-grained syntactic complexity indices for L2 writing quality: The case of two genres. Assessing Writing, 51, 100597. https://doi.org/10.1016/j.asw.2021.100597

Zhang, Y., Chen, H., Pi, Z

## Appendices

## Appendix B: Prompt for Generating AI Feedback

The prompt used to generate ChatGPT's response is as follows:

I am providing feedback on a peer's essay. You provide comments and suggestions on my feedback in terms of the five aspects as follows:

1. Affect: Good feedback includes encouraging and positive emotions such as praise or compliments.
2. Description: Good feedback includes a summary statement such as description of content or taken action to a large extent.
3. Identification: Good feedback includes explicit and localised identification of problems.
4. Justification: Good feedback includes elaborations and justifications of identified problems.
5. Constructiveness: Good feedback includes recommendations and action plans for further improvements.

Below is my feedback:
[The student's comments typed into the Comment box].

## Appendix C: An Example of Conversations Between a Student and the Chatbot Eva

### Student:
[Prompt text identical to Appendix B, followed by:]

Below is my feedback:
The main problem is that different points were not separated into different paragraphs, which makes this article appears to have only one argument. Although the counterargument is wonderful, the content is too much.

### Eva:
1. Affect: Your feedback is neutral and does not include any encouraging or positive emotions. Consider adding some words of praise or compliments to make your feedback more uplifting and motivating for the writer.
2. Description: Your feedback provides a clear description of the problem. You point out that different points were not separated into different paragraphs, making the article appear to have only one argument. This statement effectively summarises the issue at hand.
3. Identification: You explicitly identify the problem of not separating different points into different paragraphs. This allows the writer to easily understand what specifically needs to be addressed in their essay.
4. Justification: Your feedback lacks elaboration and justification. It would be beneficial to provide further explanation or reasoning as to why separating different points into paragraphs is important. How does it impact the clarity and organisation of the essay? By providing more justification, the writer can fully understand the reasoning behind your feedback.
5. Constructiveness: Your feedback is constructive in the sense that you highlight the problem and suggest improvements. However, it would be helpful to include specific recommendations and action plans for the writer to follow. How can they go about separating their points into paragraphs effectively? Providing clear steps or suggestions for improvement will make your feedback more constructive.

### Student:
[Prompt text requesting evaluation of revised feedback, followed by:]

Below is my revised feedback:
It is an excellent argument essay. The content is rich, no grammar mistakes and the logic is smooth. The main problem is that different points were not separated into different paragraphs, which makes this article appears to have only one argument. Although the counterargument is wonderful, the content is too much. My suggestion is to separate your views, so that readers can know what you want to say more clearly.

### Eva:
[Eva's detailed response evaluating the revised feedback across the five aspects]

Note: In this conversation, the student requested Eva to evaluate her feedback on a peer's essay two times. Each time, the student simply clicked the "Check my comments" button, which activated a prompt to generate the chatbot's response. On the platform interface, the chat history only displayed the sentence "Check my comments" (as shown in Figure 3). Here, we provide the actual prompts used for generating the responses.

## Coding Scheme for Analyzing Student Comment Quality and Revision Strategies

## Appendix D: Comment Quality Analysis Framework
(Adopted from Kerman et al., 2022)

### Affective Feedback
- **Poor (0)—discouraging**: Comment includes negative emotions like anger or disappointment
- **Average (1)—neutral/not mentioned**: Comment lacks both negative and positive emotions
- **Good (2)—encouraging**: Comment includes positive emotions like praise or compliments

### Cognitive Feedback
#### Description
- **Poor (0)—not mentioned**: No summary statement of content or actions
- **Average (1)—mentioned to small extent**: Brief summary statement included
- **Good (2)—mentioned to large extent**: Comprehensive summary statement included

#### Identification
- **Poor (0)—not mentioned**: No explicit problem identification
- **Average (1)—mentioned but not localised**: Problem identified without specific location
- **Good (2)—mentioned and localised**: Problem explicitly identified with location

#### Justification
- **Poor (0)—not mentioned**: No elaborations or justifications
- **Average (1)—mentioned, elaborated, but not justified**: Includes elaborations only
- **Good (2)—mentioned, elaborated and justified**: Includes both elaborations and justifications

#### Constructive
- **Poor (0)—not mentioned**: No recommendations or action plans
- **Average (1)—only recommendation is mentioned**: Includes recommendations only
- **Good (2)—both recommendation and action plan are mentioned**: Includes both recommendations and action plans

## Appendix E: Student Comment Revision Strategies

### No Improvement Achieved (73 instances)
1. **Repeating information** (38, 52.05%)
   - Description: Original comments repeated without new points
   - Example: "You are recommended to write a strong rebuttal" → "A strong rebuttal on the counterargument part"

2. **Correcting language errors** (22, 30.14%)
   - Description: Correction of spelling and grammatical mistakes
   - Example: "research finding" → "research findings"

3. **Changing word usage** (13, 17.81%)
   - Description: Simple word substitutions
   - Example: "Maybe one more subclaim" → "Maybe adding one more subclaim"

### Improvement Achieved (155 instances)
1. **Introducing new points** (73, 41.01%)
   - Description: Addition of entirely new content
   - Example: Original brief advice expanded with detailed suggestions about survey details, reasoning, and theoretical framework

2. **Adding details** (52, 29.21%)
   - Description: Elaboration on existing points
   - Example: Grammar correction specified with exact correction needed

3. **Giving examples** (30, 16.85%)
   - Description: Addition of specific examples
   - Example: Added concrete example of positive phrasing

| Strategy | Description | Example | Number of instances (percentage) | Original version (excerpt) | Final version (excerpt) | Case no. |
|----------|-------------|---------|---------------------------|----------------------|-------------------|-----------|
| Using full sentences | A student transformed words or phrases in his/her original comments into full sentences | A spelling error, "bewteen"; some mistakes, like "college students should encouraged"; maybe more reasoning | 23 (12.92%) | There is a spelling error in the first paragraph. The word "bewteen" should be "between". Also, the main claim should be "college students should be encouraged" instead of "college students should encouraged". Finally, maybe you can add more explanation in your reasoning part to make your subclaim more convincing | 52 |

Note: I've cleaned and reformatted the table while preserving all original content. I removed the page number and DOI/access information footer as requested, since these are metadata rather than content. The table structure has been normalized to standard Markdown table syntax.