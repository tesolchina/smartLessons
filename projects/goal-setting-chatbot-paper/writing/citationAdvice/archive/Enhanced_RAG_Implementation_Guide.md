# Enhanced RAG Citation Implementation Guide

*Precise citation recommendations with exact insertion points*


## üö® HIGH PRIORITY Citations (Implement First)

### 1. Srl Theory
**Text to enhance:** "Self-Regulated Learning (SRL) in Higher Education Define SRL as a dynamic process involving cognitiv..."
**Recommended action:** Add foundational SRL citations after defining SRL
**Suggested citations:** Winne & Hadwin (1998), Pintrich (2000), Zimmerman (2013), Panadero (2017)
**Context window:**
```
# Introduction

## 1. Background:
 The Importance of Self-Regulated Learning (SRL) in Higher Education Define SRL as a dynamic process involving cognitive, metacognitive, motivational, and behavioral strategies (Winne & Hadwin, 1998; Pintrich, 2000; Zimmerman, 2013; Winne, 2022). Emphasize the role of SRL in academic achievement and lifelong learning (Kizilcec et al., 2017; Dent & Koenka, 2016; Nussbaumer et al., 2014). Introduce main SRL frameworks, focusing on the cyclical phases model (Zimmerman, 2013; Guan et al., 2024). 2. Goal Setting as a Core SRL Strategy Explain goal setting as a central process in SRL (Pintrich, 2000; Zimmerman, 2013). Describe different types of goals (product/process, proximal/distal, mastery/performance) and the SMART framework (Latham & Locke, 2007; Schunk, 1990; Doran, 1981; Bjerke & Renger, 2017). Summarize evidence that high-quality goal setting enhances learning outcomes (Kizilcec et al., 2017; Bandura & Schunk, 1981; Chang et al., 2013). Highlight persistent problems: students‚Äô self-set goals are often vague and lack actionable criteria (McCardle et al., 2016). 

## 3. Interventions to Support Goal Setting and SRL Review previous intervention approaches:
 web-based systems, behavioral prompts, goal-management tools, and planning strategies (Chang et al., 2013; Scholl et al., 2009; Yeomans & Reich, 2017; Wong et al., 2021). Point out mixed 

# results
, especially in the long-term quality of goal setting (Wong et al., 2021; McCardle et al., 2016). 

## 4. Educational Chatbots and the Rise of AI-Based Support Introduce chatbots as SRL scaffolds:
 compare rule-based and AI-based approaches (Hew et al., 2022; Singh et al., 2019; Ng et al., 2024). Discuss limitations of rule-based chatbots (Hew et al., 2022) and advantages of large language model (LLM)-powered AI chatbots (Ng et al., 2024; Guan et al., 2024). Note the lack of research on how AI-based chatbots affect the quality of self-set goals over sustained periods. 

## 5. Research Gaps and Rationale for the Present Study State the gap:
 insufficient research on (a) how AI-based chatbots improve the quality of students‚Äô self-set goals, (b) temporal trends in goal quality across a semester, and (c) students‚Äô perceptions of chatbot usefulness and ease of use (Guan et al., 2024; Ng et al., 2024; McCardle et al., 2016). 

## 6. 

## Research Questions
 To address these gaps, this study investigates the following questions:
 How can the AI-based customized chatbots change the qualities of students‚Äô self-set studying goals? What are students‚Äô perceived usefulness and ease of use of the AI-based customized chatbot? What are the students ‚Äô perceptions of the AI-based customized chatbots and suggestions for improvement? 

# Review of the related literature
 Self-regulated learning (SRL) is a dynamic process that enables students to actively plan, manage, and control their learning actions to achieve speci
```

### 2. Goal Setting Theory
**Text to enhance:** "SMART framework (Latham & Locke, 2007; Schunk, 1990; Doran, 1981; Bjerke & Renger, 2017). Summarize ..."
**Recommended action:** Verify Doran (1981) citation and add supporting goal-setting research
**Suggested citations:** Doran (1981), Latham & Locke (2007), Schunk (1990), Bandura & Schunk (1981)
**Context window:**
```
. Describe different types of goals (product/process, proximal/distal, mastery/performance) and the SMART framework (Latham & Locke, 2007; Schunk, 1990; Doran, 1981; Bjerke & Renger, 2017). Summarize evidence that high-quality goal setting enhances learning outcomes (Kizilcec et al., 2017; Bandura & Schunk, 1981; Chang et al., 2013). Highlight persistent problems: students‚Äô self-set goals are often vague and lack actionable criteria (McCardle et al., 2016). 

## 3. Interventions to Support Goal Setting and SRL Review previous intervention approaches:
 web-based systems, behavioral prompts, goal-management tools, and planning strategies (Chang et al., 2013; Scholl et al., 2009; Yeomans & Reich, 2017; Wong et al., 2021). Point out mixed 

# results
, especially in the long-term quality of goal setting (Wong et al., 2021; McCardle et al., 2016). 

## 4. Educational Chatbots and the Rise of AI-Based Support Introduce chatbots as SRL scaffolds:
 compare rule-based and AI-based approaches (Hew et al., 2022; Singh et al., 2019; Ng et al., 2024). Discuss limitations of rule-based chatbots (Hew et al., 2022) and advantages of large language model (LLM)-powered AI chatbots (Ng et al., 2024; Guan et al., 2024). Note the lack of research on how AI-based chatbots affect the quality of self-set goals over sustained periods. 

## 5. Research Gaps and Rationale for the Present Study State the gap:
 insufficient research on (a) how AI-based chatbots improve the quality of students‚Äô self-set goals, (b) temporal trends in goal quality across a semester, and (c) students‚Äô perceptions of chatbot usefulness and ease of use (Guan et al., 2024; Ng et al., 2024; McCardle et al., 2016). 

## 6. 

## Research Questions
 To address these gaps, this study investigates the following questions:
 How can the AI-based customized chatbots change the qualities of students‚Äô self-set studying goals? What are students‚Äô perceived usefulness and ease of use of the AI-based customized chatbot? What are the students ‚Äô perceptions of the AI-based customized chatbots and suggestions for improvement? 

# Review of the related literature
 Self-regulated learning (SRL) is a dynamic process that enables students to actively plan, manage, and control their learning actions to achieve specific goals ( Winne&Hadwin, 1998 , Pintrich, 2000 ; Zimmerman , 2013 ; Winne 2022 ). It encompasses cognitive, metacognitive, emotional , motivational, and behavioral processes, allowing learners to initiate and sustain goal-directed activities ( Kizilcec et al , 2017 , Pintrich , 2000 ). Although scholars have developed different models of self-regulated learning, they generally agree that the self-regulated learning process is cyclical and involves setting learning goals and employing study strategies (Panadero, 2017, Guan et al., 2024 ). For example, according to Winne and Hadwin (1998), SRL consists of four stages: identifying and interpreting tasks, setting goals and planning strategies, enacting study activities, and adapting based on experience to improve future study. In the SRL model developed by Zimmerman, SRL is characterized by three cyclical phases: forethought, performance, and self-reflection. During the forethought phase, students set learning goals and plan strategies; in the performance phase, they monitor their progress and apply strategies; and in the self-reflection phase, they evaluate their outcomes and adapt for future efforts ( Zimmerman 2002, Zimmerman , 2013 , Lee et al., 2025 ) . Goal setting is a critical process within self-regulated learning (SRL) ( Pintrich, 2000, Zimmerman , 2013 , Brady et al., 2024). Goals can take different forms ( Latham & Locke, 2007 ) . For example, product goals focus on learning outcomes, whereas process goals emphasize strategies and 

# methods
 for task completion ( Zimmerman and Kitsantas , 1996) . Proximal goals, which are short-term and easier to monitor, provide stronger study motivation than distant goals (Schunk, 1990) . Research with children has shown that p roximal goals can significantly enhance self-efficacy, skill development, and academic outcomes compared to distal goals ( Bandura & Schunk, 1981 ) . The SMART framework (specific, measurable, achievable, realistic, and time-related), introduced by Doran ( Doran, 1981), has become a widely adopted standard for developing effective goals ( Bjerke & Reng
```

### 3. Student Goal Quality
**Text to enhance:** "students‚Äô self-set goals are often vague..."
**Recommended action:** Support claim about vague student goals with research evidence
**Suggested citations:** McCardle et al. (2016), Raluy & Mislang (2022), Brady et al. (2024)
**Context window:**
```
(Kizilcec et al., 2017; Bandura & Schunk, 1981; Chang et al., 2013). Highlight persistent problems: students‚Äô self-set goals are often vague and lack actionable criteria (McCardle et al., 2016). 

## 3. Interventions to Support Goal Setting
```

### 4. Ai Chatbots
**Text to enhance:** "AI-Based Support Introduce chatbots as SRL scaffolds:
 compare rule-based and AI-based approaches (H..."
**Recommended action:** Support AI chatbot capabilities claim with recent research
**Suggested citations:** Ng et al. (2024), Guan et al. (2024), Hew et al. (2025), Lai (2024)
**Context window:**
```
al setting (Wong et al., 2021; McCardle et al., 2016). 

## 4. Educational Chatbots and the Rise of AI-Based Support Introduce chatbots as SRL scaffolds:
 compare rule-based and AI-based approaches (Hew et al., 2022; Singh et al., 2019; Ng et al., 2024). Discuss limitations of rule-based chatbots (Hew et al., 2022) and advantages of large language model (LLM)-powered AI chatbots (Ng et al., 2024; Guan et al., 2024). Note the lack of research on how AI-based chatbots affect the quality of self-set goals over sustained periods. 

## 5. Research Gaps and Rationale for the Present Study State the gap:
 insufficient research on (a) how AI-based chatbots improve the quality of students‚Äô self-set goals, (b) temporal trends in goal quality across a semester, and (c) students‚Äô perceptions of chatbot usefulness and ease of use (Guan et al., 2024; Ng et al., 2024; McCardle et al., 2016). 

## 6. 

## Research Questions
 To address these gaps, this study investigates the following questions:
 How can the AI-based customized chatbots change the qualities of students‚Äô self-set studying goals? What are students‚Äô perceived usefulness and ease of use of the AI-based customized chatbot? What are the students ‚Äô perceptions of the AI-based customized chatbots and suggestions for improvement? 

# Review of the related literature
 Self-regulated learning (SRL) is a dynamic process that enables students to actively plan, manage, and control their learning actions to achieve specific goals ( Winne&Hadwin, 1998 , Pintrich, 2000 ; Zimmerman , 2013 ; Winne 2022 ). It encompasses cognitive, metacognitive, emotional , motivational, and behavioral processes, allowing learners to initiate and sustain goal-directed activities ( Kizilcec et al , 2017 , Pintrich , 2000 ). Although scholars have developed different models of self-regulated learning, they generally agree that the self-regulated learning process is cyclical and involves setting learning goals and employing study strategies (Panadero, 2017, Guan et al., 2024 ). For example, according to Winne and Hadwin (1998), SRL consists of four stages: identifying and interpreting tasks, setting goals and planning strategies, enacting study activities, and adapting based on experience to improve future study. In the SRL model developed by Zimmerman, SRL is characterized by three cyclical phases: forethought, performance, and self-reflection. During the forethought phase, students set learning goals and plan strategies; in the performance phase, they monitor their progress and apply strategies; and in the self-reflection phase, they evaluate their outcomes and adapt for future efforts ( Zimmerman 2002, Zimmerman , 2013 , Lee et al., 2025 ) . Goal setting is a critical process within self-regulated learning (SRL) ( Pintrich, 2000, Zimmerman , 2013 , Brady et al., 2024). Goals can take different forms ( Latham & Locke, 2007 ) . For example, product goals focus on learning outcomes, whereas process goals emphasize strategies and 

# methods
 for task completion ( Zimmerman and Kitsantas , 1996) . Proximal goals, which are short-term and easier to monitor, provide stronger study motivation than distant goals (Schunk, 1990) . Research with children has shown that p roximal goals can significantly enhance self-efficacy, skill development, and academic outcomes compared to distal goals ( Bandura & Schunk, 1981 ) . The SMART framework (specific, measurable, achievable, realistic, and time-related), introduced by Doran ( Doran, 1981), has become a widely adopted standard for developing effective goals ( Bjerke & Renger, 2017) . This framework has been used to help undergraduate students to make learning goals (McCardle et al., 2016, Hew et al., 2022) . Establishing clear objectives and strategies is essential, particularly when learners start a new course (Hew et al., 2025) . Goals serve as 

# reference
 points and standards, enabling learners to make informed decisions about their learning content and 

# methods
 (Pintrich, 2000, Winne, 2022). Research has found that students who set goals tend to achieve greater academic performance compared to those who did not ( Kizilcec et al ., 2017, Garavalia & Gredler, 2002, Brady et al., 2024 ). However, the process of establishing effective learning objectives is often neglected by students in their actual learning activities ( Hew, et al., 202 2). To enhance goal-setting as a critical component of self-regulated learning, a variety of technological and non-technological intervention 

# methods
 have been developed. For instance, g oal-management tools, which enable learners to organize goals hierarchically, allocate resources, and track progress, provide essential support for metacognitive processes such as planning and self-reflection ( Scholl et al., 2009 ). W eb-based portfolio assessment systems (WBPAS) have been shown to effectively support learners in setting, monitoring, and revising their goals, with studies demonstrating that these systems enhance SRL more effectively than traditional paper-based portfolios ( Chang et al., 2013 ). However, these studies don ‚Äô t involve the provision of feedback on the goals by teachers or tutors . Google Docs was used for students to record and share goals with classmates during class, while teachers can monitor students ‚Äô progress and provide feedback outside of class. However, students tended to set goals that were frequently too challenging to accomplish due to a lack of clear guidance on how to set goals in class ( Raluy & Mislang, 2022 ). McCardle et al. (2016) taught college students goal-setting strategies in a learning to learn course, but only found that the weekly study goals submitted by students were often vague, lacking specific learning content, actionable steps, and measurable standards. In addition, t hese patterns persisted over the semester, highlighting the difficulty students face in creating high-quality goals even when exposed to structured learning environments. When teachers did provide feedback on the goals set by students , the process of reviewing individual student goals was time-consuming, resulting in delayed teacher feedback . ( Hew et al., 2025 ). Meanwhile, teachers tended to offer general or commonly recurring advice rather than personalized, tailored feedback ( Raluy & Mislang, 2022 , Hew et al., 2025 ). Human coaches can be effective when they use coaching sessions to help students achieve their goals, but these sessions may be affected by factors such as the physical and mental conditions of both coaches and students ( Terblanche et al., 2022). While helping students set effective learning goals and providing them with timely, reliable, and personalized feedback remains a critical challenge, the advancement of chatbot technology appears to present a novel and promising approach to addressing this issue. Chatbots, also known as conversational agents, are a type of digital system designed to converse with humans through voice or text ( Amin et al., 2022, Smutny & Schreiberova, 2020 , Wollny et al., 2021, Mohamad , et al., 2021). They can be embedded into various software applications, such as digital assistants, online platforms, or accessed through messaging services (Wollny et al., 2021, Mohamad , et al., 2021 ) and provide a dialogue-based user interface ( Mohammed et al., 2023, Singh et al., 2019 ). Chatbots can be divided into two types based on how the responses are generated: retrieval-based chatbots and generative-based chatbots. Retrieval-based chatbots select responses that match users ‚Äô requests from a predefined repository. The retrie val-based chatbot is rule-based if the pre-constructed selection of responses is comprised of human-made rules ( Mohamad et al., 2021 , Guan et al., 2024 ) . The rise of user-friendly chatbot development platforms, such as Danbee AI and Google ‚Äô s Dialogflow, has enabled researchers to create customized, rule-based chatbots for specific purposes without needing programming expertise ( Guo & Li, 2024). Generative-based chatbots are also labeled as AI-based chatbots, which use Machine Translation techniques to convert user input to a response and produce replies after learning from a vast amount of data ( Mohamad et al., 2021 , Guan et al., 2024 ) . Platforms like Poe allow researchers to design tailored AI-based chatbots by crafting prompts, significantly reducing reliance on specialized technical skills and programming expertise ( Guo & Li, 2024) . The first chatbot, Eliza, which was a rule-based chatbot, was invented to undertake psychological interviews (Weizenbaum, 1966, Guan et al., 2024). Since then, chatbots have been developed and applied in diverse fields, such as training and education, technical support, marketing, and technical support (Smutny & Schreiberova, 2020). Recent years have witnessed a growing interest among researchers in utilizing chatbots to tackle educational challenges (Wollny et al., 2021, Ait Baha et al., 2023). They could revolutionize education by assisting teachers, customizing learning experiences , involving students , and gaining a deeper understanding of students ‚Äô behaviors (Kuhail et al., 2022). Researchers have investigated a few pedagogical roles played by educational chatbots, including learning, assisting, mentoring, or monitoring. The learning role of chatbots involves supporting teaching and learning activities by delivering knowledge or skills to students, functioning as an interactive medium for instruction and education (Wollny, et al, 2021, Ait Baha et al., 2023) . They have been utilized across a diverse range of academic disciplines, including 1) humanities and languages, 2) science, technology and mathematics, 3) social sciences, and 4) arts and communication (Smutny & Schreiberova, 2020, Wollny et al., 2021). The assisting role of chatbots refers to their function in simplifying learners' daily tasks and improving accessibility to information, such as providing administrative details or automating routine inquiries, thereby making the learning process more efficient and convenient . The mentoring or monitoring role focuses on student s ‚Äô self-development through reflective and learner-centered interactions, which aims to help students to develop self-regulated learning skills, life skills and learning skills. (Wollny, et al, 2021, Ait Baha et al., 2023). Although the origin of chatbot technology can be traced back to the 1960s (Weizenbaum, 1966), interest in leveraging chatbots to promote self-regulated learning among educational practitioners and researchers has emerged only in recent years (Guan et al., 2024). According to Guan et al. (2024), no studies in the reviewed literature employed chatbots to support all four phases of self-regulated learning (task perception, setting goals and planning, enacting study activities, and adapting based on experience to improve future study) as defined by Winne and Hadwin (1998). Nearly 60% of them focused on supporting one single SRL phase while 40% of them addressed two phases. Among them, five articles incorporated chatbots to assist students in setting learning goals and developing study plans (Guan et al., 2024). Rule-based chatbots, which operate on predefined rules, have been used to assist students in clarifying their goals through structured conversations before an online course starts (Hew et al., 2021, 2022, Du et al., 2021) . The three studies have been conducted to explore the role of rule-based chatbots in facilitating students ‚Äô goal-setting processes within online learning environments. The chatbots were integrated into the University ‚Äô s Moodle learning management system via Dialogflow Messenger, allowing seamless access for students. The core design centered on the SMART goal-setting framework, which guided the development of five conversational questions that prompted students to define their learning goals before the course began. For each question, students were presented with multiple-choice options (A, B, C) to facilitate their responses. Based on their selections, the chatbot provided relevant recommendations (Hew et al., 2021, 2022, Du et al., 2021). The goal-setting chatbot or the SRL chatbot developed in the three studies offered significant benefits by helping students clarify and structure their learning goals through a guided, SMART-based framework, thereby raising awareness of effective self-regulated learning strategies (Hew et al., 2021, 2022, Du et al., 2021); their 24/7 availability and teacher-designed, personalized recommendations enhanced perceived usefulness and provided accessible support (hew et al., 2022). However, these rule-based chatbots faced limitations, including restricted response options, a lack of true intelligent and natural conversation flow, with students requesting more choices, faster and more intelligent responses (Hew et al., 2021, 2022). This is because rule-based chatbots operate on pre- established rules, offering limited responses to user queries (Singh et al., 2019 , Hew et al., 2022, Ng et al., 2024 ). Furthermore, their one-time use at the course outset was seen as insufficient, as students desired ongoing interaction for goal tracking, progress reminders throughout the learning journey (Hew et al., 2021, 2022, Du et al., 2021). These findings highlight the inherent limitations of rule-based systems and the need for more advanced chatbot technologies. E ducational chatbots, powered by natural language processing (NLP) and artificial intelligence , have emerged as promising tools for enhancing SRL processes, including goal-setting (Guan et al., 2024 , Ng et al., 2024 ). Unlike rule-based systems, AI-based chatbots leverage large language models (LLMs) to provide adaptive, context-sensitive, and personalized interactions (Ng et al., 2024 , Lai
```

### 5. Student Goal Quality
**Text to enhance:** "students‚Äô self-set goals, (b) temporal trends in goal quality across a semester, and (c) students‚Äô p..."
**Recommended action:** Support claim about vague student goals with research evidence
**Suggested citations:** McCardle et al. (2016), Raluy & Mislang (2022), Brady et al. (2024)
**Context window:**
```
ent Study State the gap:
 insufficient research on (a) how AI-based chatbots improve the quality of students‚Äô self-set goals, (b) temporal trends in goal quality across a semester, and (c) students‚Äô perceptions of chatbot usefulness and ease of use (Guan et al., 2024; Ng et al., 2024; McCardle et al., 2016). 

## 6. 

## Research Questions
 To address these gaps, this study investigates the following questions:
 How can the AI-based customized chatbots change the qualities of students‚Äô self-set studying goals? What are students‚Äô perceived usefulness and ease of use of the AI-based customized chatbot? What are the students ‚Äô perceptions of the AI-based customized chatbots and suggestions for improvement? 

# Review of the related literature
 Self-regulated learning (SRL) is a dynamic process that enables students to actively plan, manage, and control their learning actions to achieve specific goals ( Winne&Hadwin, 1998 , Pintrich, 2000 ; Zimmerman , 2013 ; Winne 2022 ). It encompasses cognitive, metacognitive, emotional , motivational, and behavioral processes, allowing learners to initiate and sustain goal-directed activities ( Kizilcec et al , 2017 , Pintrich , 2000 ). Although scholars have developed different models of self-regulated learning, they generally agree that the self-regulated learning process is cyclical and involves setting learning goals and employing study strategies (Panadero, 2017, Guan et al., 2024 ). For example, according to Winne and Hadwin (1998), SRL consists of four stages: identifying and interpreting tasks, setting goals and planning strategies, enacting study activities, and adapting based on experience to improve future study. In the SRL model developed by Zimmerman, SRL is characterized by three cyclical phases: forethought, performance, and self-reflection. During the forethought phase, students set learning goals and plan strategies; in the performance phase, they monitor their progress and apply strategies; and in the self-reflection phase, they evaluate their outcomes and adapt for future efforts ( Zimmerman 2002, Zimmerman , 2013 , Lee et al., 2025 ) . Goal setting is a critical process within self-regulated learning (SRL) ( Pintrich, 2000, Zimmerman , 2013 , Brady et al., 2024). Goals can take different forms ( Latham & Locke, 2007 ) . For example, product goals focus on learning outcomes, whereas process goals emphasize strategies and 

# methods
 for task completion ( Zimmerman and Kitsantas , 1996) . Proximal goals, which are short-term and easier to monitor, provide stronger study motivation than distant goals (Schunk, 1990) . Research with children has shown that p roximal goals can significantly enhance self-efficacy, skill development, and academic outcomes compared to distal goals ( Bandura & Schunk, 1981 ) . The SMART framework (specific, measurable, achievable, realistic, and time-related), introduced by Doran ( Doran, 1981), has become a widely adopted standard for developing effective goals ( Bjerke & Renger, 2017) . This framework has been used to help undergraduate students to make learning goals (McCardle et al., 2016, Hew et al., 2022) . Establishing clear objectives and strategies is essential, particularly when learners start a new course (Hew et al., 2025) . Goals serve as 

# reference
 points and standards, enabling learners to make informed decisions about their learning content and 

# methods
 (Pintrich, 2000, Winne, 2022). Research has found that students who set goals tend to achieve greater academic performance compared to those who did not ( Kizilcec et al ., 2017, Garavalia & Gredler, 2002, Brady et al., 2024 ). However, the process of establishing effective learning objectives is often neglected by students in their actual learning activities ( Hew, et al., 202 2). To enhance goal-setting as a critical component of self-regulated learning, a variety of technological and non-technological intervention 

# methods
 have been developed. For instance, g oal-management tools, which enable learners to organize goals hierarchically, allocate resources, and track progress, provide essential support for metacognitive processes such as planning and self-reflection ( Scholl et al., 2009 ). W eb-based portfolio assessment systems (WBPAS) have been shown to effectively support learners in setting, monitoring, and revising their goals, with studies demonstrating that these systems enhance SRL more effectively than traditional paper-based portfolios ( Chang et al., 2013 ). However, these studies don ‚Äô t involve the provision of feedback on the goals by teachers or tutors . Google Docs was used for students to record and share goals with classmates during class, while teachers can monitor students ‚Äô progress and provide feedback outside of class. However, students tended to set goals that were frequently too challenging to accomplish due to a lack of clear guidance on how to set goals in class ( Raluy & Mislang, 2022 ). McCardle et al. (2016) taught college students goal-setting strategies in a learning to learn course, but only found that the weekly study goals submitted by students were often vague, lacking specific learning content, actionable steps, and measurable standards. In addition, t hese
```

### 6. Student Goal Quality
**Text to enhance:** "students face in creating high-quality goals even when exposed to structured learning environments. ..."
**Recommended action:** Support claim about vague student goals with research evidence
**Suggested citations:** McCardle et al. (2016), Raluy & Mislang (2022), Brady et al. (2024)
**Context window:**
```
le standards. In addition, t hese patterns persisted over the semester, highlighting the difficulty students face in creating high-quality goals even when exposed to structured learning environments. When teachers did provide feedback on the goals set by students , the process of reviewing individual student goals was time-consuming, resulting in delayed teacher feedback . ( Hew et al., 2025 ). Meanwhile, teachers tended to offer general or commonly recurring advice rather than personalized, tailored feedback ( Raluy & Mislang, 2022 , Hew et al., 2025 ). Human coaches can be effective when they use coaching sessions to help students achieve their goals, but these sessions may be affected by factors such as the physical and mental conditions of both coaches and students ( Terblanche et al., 2022). While helping students set effective learning goals and providing them with timely, reliable, and personalized feedback remains a critical challenge, the advancement of chatbot technology appears to present a novel and promising approach to addressing this issue. Chatbots, also known as conversational agents, are a type of digital system designed to converse with humans through voice or text ( Amin et al., 2022, Smutny & Schreiberova, 2020 , Wollny et al., 2021, Mohamad , et al., 2021). They can be embedded into various software applications, such as digital assistants, online platforms, or accessed through messaging services (Wollny et al., 2021, Mohamad , et al., 2021 ) and provide a dialogue-based user interface ( Mohammed et al., 2023, Singh et al., 2019 ). Chatbots can be divided into two types based on how the responses are generated: retrieval-based chatbots and generative-based chatbots. Retrieval-based chatbots select responses that match users ‚Äô requests from a predefined repository. The retrie val-based chatbot is rule-based if the pre-constructed selection of responses is comprised of human-made rules ( Mohamad et al., 2021 , Guan et al., 2024 ) . The rise of user-friendly chatbot development platforms, such as Danbee AI and Google ‚Äô s Dialogflow, has enabled researchers to create customized, rule-based chatbots for specific purposes without needing programming expertise ( Guo & Li, 2024). Generative-based chatbots are also labeled as AI-based chatbots, which use Machine Translation techniques to convert user input to a response and produce replies after learning from a vast amount of data ( Mohamad et al., 2021 , Guan et al., 2024 ) . Platforms like Poe allow researchers to design tailored AI-based chatbots by crafting prompts, significantly reducing reliance on specialized technical skills and programming expertise ( Guo & Li, 2024) . The first chatbot, Eliza, which was a rule-based chatbot, was invented to undertake psychological interviews (Weizenbaum, 1966, Guan et al., 2024). Since then, chatbots have been developed and applied in diverse fields, such as training and education, technical support, marketing, and technical support (Smutny & Schreiberova, 2020). Recent years have witnessed a growing interest among researchers in utilizing chatbots to tackle educational challenges (Wollny et al., 2021, Ait Baha et al., 2023). They could revolutionize education by assisting teachers, customizing learning experiences , involving students , and gaining a deeper understanding of students ‚Äô behaviors (Kuhail et al., 2022). Researchers have investigated a few pedagogical roles played by educational chatbots, including learning, assisting, mentoring, or monitoring. The learning role of chatbots involves supporting teaching and learning activities by delivering knowledge or skills to students, functioning as an interactive medium for instruction and education (Wollny, et al, 2021, Ait Baha et al., 2023) . They have been utilized across a diverse range of academic disciplines, including 1) humanities and languages, 2) science, technology and mathematics, 3) social sciences, and 4) arts and communication (Smutny & Schreiberova, 2020, Wollny et al., 2021). The assisting role of chatbots refers to their function in simplifying learners' daily tasks and improving accessibility to information, such as providing administrative details or automating routine inquiries, thereby making the learning process more efficient and convenient . The mentoring or monitoring role focuses on student s ‚Äô self-development through reflective and learner-centered interactions, which aims to help students to develop self-regulated learning skills, life skills and learning skills. (Wollny, et al, 2021, Ait Baha et al., 2023). Although the origin of chatbot technology can be traced back to the 1960s (Weizenbaum, 1966), interest in leveraging chatbots to promote self-regulated learning among educational practitioners and researchers has emerged only in recent years (Guan et al., 2024). According to Guan et al. (2024), no studies in the reviewed literature employed chatbots to support all four phases of self-regulated learning (task perception, setting goals and planning, enacting study activities, and adapting based on experience to improve future study) as defined by Winne and Hadwin (1998). Nearly 60% of them focused on supporting one single SRL phase while 40% of them addressed two phases. Among them, five articles incorporated chatbots to assist students in setting learning goals and developing study plans (Guan et al., 2024). Rule-based chatbots, which operate on predefined rules, have been used to assist students in clarifying their goals through structured conversations before an online course starts (Hew et al., 2021, 2022, Du et al., 2021) . The three studies have been conducted to explore the role of rule-based chatbots in facilitating students ‚Äô goal-setting processes within online learning environments. The chatbots were integrated into the University ‚Äô s Moodle learning management system via Dialogflow Messenger, allowing seamless access for students. The core design centered on the SMART goal-setting framework, which guided the development of five conversational questions that prompted students to define their learning goals before the course began. For each question, students were presented with multiple-choice options (A, B, C) to facilitate their responses. Based on their selections, the chatbot provided relevant recommendations (Hew et al., 2021, 2022, Du et al., 2021). The goal-setting chatbot or the SRL chatbot developed in the three studies offered significant benefits by helping students clarify and structure their learning goals through a guided, SMART-based framework, thereby raising awareness of effective self-regulated learning strategies (Hew et al., 2021, 2022, Du et al., 2021); their 24/7 availability and teacher-designed, personalized recommendations enhanced perceived usefulness and provided accessible support (hew et al., 2022). However, these rule-based chatbots faced limitations, including restricted response options, a lack of true intelligent and natural conversation flow, with students requesting more choices, faster and more intelligent responses (Hew et al., 2021, 2022). This is because rule-based chatbots operate on pre- established rules, offering limited responses to user queries (Singh et al., 2019 , Hew et al., 2022, Ng et al., 2024 ). Furthermore, their one-time use at the course outset was seen as insufficient, as students desired ongoing interaction for goal tracking, progress reminders throughout the learning journey (Hew et al., 2021, 2022, Du et al., 2021). These findings highlight the inherent limitations of rule-based systems and the need for more advanced chatbot technologies. E ducational chatbots, powered by natural language processing (NLP) and artificial intelligence , have emerged as promising tools for enhancing SRL processes, including goal-setting (Guan et al., 2024 , Ng et al., 2024 ). Unlike rule-based systems, AI-based chatbots leverage large language models (LLMs) to provide adaptive, context-sensitive, and personalized interactions (Ng et al., 2024 , Lai, 2024 ) . AI-based chatbots have been proven effective in fostering students ‚Äô self-regulated learning due to their flexibility and personalized recommendations (Ng et al., 2024). Hew et al (2025) developed a large language model (LLM) based chatbot system, the GoalPlanMentor, to support students ‚Äô self-regulated learning in online courses. Given the challenges of providing timely and personalized feedback to students in online learning, the research first developed GoalPlanMentor with two key agents: GoalPlanDetectAgent (GPDA) and GoalPlanAwareTeachingAgent (GPATA). The GPDA recognizes students' goals and plans from chat logs using a Memory-Augmented Prompt and stores them in databases, while the GPATA interacts with students, helps them reflect on and revise goals and plans, and guides them through weekly learning tasks. The study then conducted experiments with 25 Asian students in an eight-week education course. 

# Results
 showed high agreement between the system's and human coders' detection of students' goals and plans, high-quality feedback from the system, and students' positive perception of its usefulness. Higher perceived usefulness in goal-setting was associated with better learning achievements (Hew et al., 2025). Despite their efforts , there is little research on the long-term impact of AI-based chatbots on the quality of students‚Äô self-set goals. This study adopts a novel approach by developing an AI-based customized chatbot using system prompts to help students learn about how to set SMART goals. Development of the AI-Based Customized Chatbot Recent chatbot construction platforms such as Poe allowed researchers to build AI-based chatbots easily by simply design ing prompts , eliminating the need to apply complex technical skills (Guo & Li, 2024 ). In our study, a customized AI-based goal-setting chatbot was developed using Bytewise, a platform for building customized chatbots . The platform uses an application programming interface (API) to process both developer-crafted system prompts and user intputs, delivering AI-gerneated responses that create an interactive and efficient learning experience (Wang, 2024). The chatbot was programmed to record conversations, which were accessible to the owners for analysis and feedback purposes. The system prompts were designed to guide students through the process of setting SMART goals via dialogues, because according to the principles of multimedia leanrning, conversational presentation of materials enhances comphrehension ( Mayer, 2017) . The system prompts were designed by the course teacher, also the first writer of the paper. The interaction intentionally begins with friendly greeting to establish a positive, relaxing tone for the learning experience. After that, the chatbot provides studnets with a menu offering three choices, allowing studnets to choose the path that best suits their immediate need (Fig 1.) The first option features a teach-test-show structure, which explains the goal-setting SMART framework (Doran, 1981) first. It then requires the student to actively summarize the concepts, reinforcing knowledge retention through recall. Finally, it provides a concrete example for 

# reference
, modeling what a good outcome looks like. The second option is designed for active application of the SMART framework learned earlier. It transforms the chatbot from an information source into an interactive tutor. The student practices creating their own goal, and the chatbot provides personalized, AI-generated suggestions from improvement, facilitating a iterative refinement process (Fig. 2). The last option is designed for efficiency and support, providing students with study resources they need to achieve their goals. After completing any option, the student can choose another action or revisit previous ones, thus creating a continuous and supportive learning loop (Fig 3.). Fig. 1 The conversation between the goal-setting chatbot and students Fig. 2 The personlized feedback generated by the chatbot Fig. 3 How the chatbot interacts with students 

# Methods
 This study employed a convergent mixed-

# methods
 design (Creswell & Plano Clark, 2018) to provide a comprehensive understanding of how an AI-based chatbot influences the quality of undergraduate students' self-set learning goals and their perceptions of the tool. Quantitative and qualitative data were collected concurrently, analyzed separately, and then integrated during the interpretation phase to triangulate findings and develop a more nuanced analysis. Participants and Context The participants consisted of 40 first-year undergraduate engineering students enrolled in a 15-week "English for Academic and General Purposes" course at a large, research-intensive public university in Southwest China. The course, which is structured in two phases (Weeks 3‚Äì7 and 8‚Äì17), aims to develop students' English proficiency and core academic skills, including lecture comprehension, note-taking, academic 

# discussion
, and presentations. The course instructor, who was responsible for designing and implementing the goal-setting chatbot intervention, is also the first author of this study. Process of data collection This research employed a longitudinal data collection strategy over the 15-week semester to capture the development of students' goal-setting practices. Multiple forms of data were gathered, including chatbot interaction logs, students' initial baseline goals, revised goals immediately following the chatbot intervention, and subsequent goal submissions at Weeks 8, 11, and 14 to assess retention. Other qualitative data consisted of written reflections comparing pre- and post-intervention goals (Reflection 1), a reflection on the goal-setting process (Reflection 2), and focus group interviews with 15 students stratified by engagement level. The quantitative dataset includes participants‚Äô ratings on a five-point Likert scale assessing perceived usefulness and ease of use of the chatbot. An overview of the data collection procedure is presented in Table 1. During Week 3, the course‚Äôs intended learning outcomes were introduced to provide students with a clear framework for their studies. This orientation was designed to reduce ambiguity in approaching learning tasks, a challenge noted by Winne & Hadwin (1998). 38 students then submitted their initial study goals aligned with the intended learning outcomes to provide a baseline against which subsequent goal development could be measured. Subsequently, the chatbot intervention was introduced. The chatbot guided students through a structured three-path menu to support SMART goal-setting. The Learn path provided instruction and an example of the SMART framework (Doran, 1981). The Apply path allowed students to practice writing goals and receive personalized feedback, creating a reinforced learning loop. The Resources path offered tailored study material recommendations. Following the intervention, 39 students submitted their revised goals alongside a written reflection (reflection 1). This reflection required them to compare their pre- and post-intervention goals and to articulate the chatbot‚Äôs role in guiding them to set SMART goals. In week 4, 39 students completed a five-point Likert scale survey adapted from Davis (1989) to measure their perceptions of the chatbots‚Äô usefulness and ease of use. The instrument comprised six items measuring perceived usefulness‚Äîthe degree to which students believed the chatbot would enhance their goal-setting performance‚Äîand four items measuring perceived ease of use‚Äîthe extent to which they found the system effortless to operate. These constructs are central to technology adoption (Davis, 1989). In this study, perceived usefulness reflected students‚Äô assessment of the chatbot‚Äôs effectiveness in improving goal-setting, while ease of use captured their perceptions of its simplicity and intuitiveness. To assess the long-term retention and organic adoption of the SMART framework, 36, 32, and 25 students submitted goals at Weeks 8, 11, and 14 without further mandatory use of the chatbot or requirement to use the SMART framework, thereby allowing us to observe their autonomous goal-setting behavior. This design allowed for the examination of the intervention‚Äôs sustained impact on students' natural goal-setting behaviors. The timeframe was selected to provide periodic check-in points for students to monitor and regulate their progress, a practice highlighted as critical in SRL models (Zimmerman, 2008). Moreover, setting short-term goals has been shown to promote greater use of study strategies compared to long-term goals (Seijts & Latham, 2001). In week 15, all participants completed a goal-setting reflection. Students answered questions such as ‚ÄúHow did the chatbot help you make your study goals?‚Äù ‚ÄúHow did the chatbot help you study English?‚Äù and ‚ÄúWhat suggestions can you provide for improving the chatbot?‚Äù (reflection 2). Conversation records from students‚Äô interactions with the goal-setting chatbot, such as utterance turns, can be analyzed to assess the students‚Äô behavioral engagement (Hew et al., 2022). In week 18, we extracted and analyzed two engagement metrics from the chatbot interaction logs: the number of utterance turns and the word count of student prompts. Based on their engagement metrics, participants were stratified into high, medium, and low engagement tiers, from which three focus groups of five students each were selected to participate in the focus group interview. These semi-structured interviews were designed to gain deeper insight into students‚Äô experiences with the chatbot, including their perceptions of its utility in goal-setting and their recommendations for its enhancement. Conducted and audio-recorded in Chinese, the interviews were then translated and transcribed into English. Table 1 Overview of data sources and collection Week Procedure Data Collected for Analysis 3 a. 

# Introduction
 of course learning outcomes b. Submission of initial study goals Baseline study goals 3 a. Chatbot intervention (Learn, Apply, Resources paths) b. Submission of revised goals and Reflection 1 1. Revised study goals (Post-intervention) 2. Written Reflection 1 (Comparison of pre/post goals & chatbot's role) 4 Completion of the chatbot use survey Rating a 5-point Likert Scale 8 Submission of study goals (without mandated chatbot use) Study Goals (Week 8) 11 Submission of study goals (without mandated chatbot use) Study Goals (Week 11) 14 Submission of study goals (without mandated chatbot use) Study Goals (Week 14) 15 Submission of final goal-setting reflection (Reflection 2) Written Reflection 2 (Overall reflection) 18 a. access chatbot interaction logs b. Focus group interviews 1. engagement level metrics such as utterance turns and word count of student prompts 2. Interview transcripts (3 groups of 5 students each, stratified by engagement level) Data Analysis To address the first 

## research question
 ‚ÄúHow can AI-based chatbots change the qualities of students‚Äô self-set study goals?‚Äù, we analyzed students' self-set goals using the SMART framework (Doran, 1981). Goals were collected at five time points: as a baseline, after the chatbot intervention in Week 3, and subsequently in Weeks 8, 11, and 

## 14. Each goal was coded for the presence and clarity of the five SMART criteria:
 Specific, Measurable, Achievable, Relevant, and Time-bound. Based on the number of criteria met, goals were categorized into six types: SMART-0 through SMART-5 (Table 2) . The lowest tier, SMART-0 type, represents an intention with no SMART elements, while the highest tier, SMART-5 type, includes all five SMART elements. As the levels increase, more SMART elements are integrated. For example, a SMART-1 goal incorporates just one element, such as the "Relevant" aspect of wanting to improve writing skills in the SMART-1 type in the table. A SMART-2 goal adds a second element; the goal to practice writing twice a week is both Relevant and Time-bound . However, it still remains deficient because it lacks specificity (what kind of writing?), is not measurable (how many passages?), and therefore its achievability cannot be properly assessed. To ensure coding reliability, two researchers collaboratively coded all goals, resolving any discrepancies through 

# discussion
 to reach consensus. The frequency of each goal type was calculated across time points, and trends in goal quality were analyzed to identify patterns of change and improvement over the semester. Table 

## 2. C oding Goals by SMART Criteria:
 A Six-Tiered Exemplification Goal types Examples SMART-0 Type Set tasks in advance. SMART-1 Type Improve my writing skills (SMART-1-R, including the element ‚Äú relevant ‚Äù ) SMART-2 Type Writing passages for practice twice a week (SMART-2-RT, including ‚Äú Relevant and Time bound ‚Äù ); Practice my listening by watching TED videos (SMART-2-SR, including ‚Äú Specific and Relevant ‚Äù ) SMART-3 Type I plan to watch TED videos every day to practice my listening skills (SMART-3-SRT, including ‚Äú Specific, Relevant, and Time-bound ‚Äù ); I plan to accumulate vocabulary for 30 minutes every morning (SMART-3-RMT, including ‚Äú Relevant, Measurable, and Timebound ‚Äù ) SMART-4 Type On Monday, learn 15 English words. (SMART-4-MART, including ‚Äú Measurable, Achievable, Relevant, and Time-bound ‚Äù ) Engage in a 30-minute 

# discussion
 session with my partner, focusing on topics related to my coursework or recent learning. (SMART-4-SMAR, including ‚Äú Specific, Measurable, Achievable, and Relevant ‚Äù ) SMART-5 Type Monday: Listen to a TED Talk or podcast about renewable energy, focusing on identifying key points and any relevant technical terms used (20 mins). (SMART-5, including all five elements) To address the second 

## research question
, ‚ÄúWhat are students‚Äô perceived usefulness and ease of use of the AI-based customized chatbot?‚Äù, we administered a five-point Likert scale adapted from Davis (1989). Responses ranged from 1 ( strongly disagree ) to 5 ( strongly agree ), and the instrument included items measuring both perceived usefulness (the extent to which students believed the chatbot improved their goal-setting performance) and perceived ease of use (the degree to which they found the system effortless to operate) (Davis, 1989). Data were analyzed using SPSS to calculate mean scores for each construct. Cronbach‚Äôs alpha was computed to assess the internal consistency and reliability of the scale. To gain deeper insights into student perceptions and improvement suggestions, and to address both the second and third 

## research questions
, we collected student self-reflections and conducted semi-structured interviews. Reflection prompts included: ‚ÄúWhat differences do you observe between your original and revised goals?‚Äù, ‚ÄúHow did the chatbot help you learn to set SMART goals?‚Äù, ‚ÄúIs the AI-based chatbot useful for helping you set study goals? Why or why not?‚Äù, and ‚ÄúWhat suggestions can you provide for improving the chatbot?‚Äù Interview questions explored similar themes, such as ‚ÄúHow can the chatbot help you set goals?‚Äù, ‚ÄúIs the AI-based chatbot useful for helping you set study goals? Why or why not?‚Äù, ‚ÄúDo you have any suggestions for improving the chatbot?‚Äù Thematic analysis (Braun & Clarke, 2006, 2020) was conducted on the reflection responses and interview transcripts. To ensure inter-coder reliability, two researchers collaboratively coded the data, discussing and resolving any discrepancies through consensus. This process yielded a set of codes and emergent themes that provided rich, nuanced insights into students‚Äô experiences and perceptions. These themes directly informed the interpretation of the second and third 

## research questions
. Finally, a convergent mixed-

# methods
 design was employed, with quantitative and qualitative data integrated at the interpretation stage. Quantitative findings, such as shifts in goal-setting patterns and survey 

# results
 on perceived usefulness and ease of use, were systematically compared and contrasted with the qualitative themes derived from interviews and reflections to develop a comprehensive understanding of the 

# results
. 

# Results
 Changes in Goal-Setting Engagement The 

# introduction
 of the AI-based chatbot was associated with an immediate and sustained increase in both the quantity and quality of student goal-setting (Table 2 ). The number of participants submitting goals remained high throughout the semester, starting at 38 pre-intervention (Week 3), increasing to 39 immediately post-intervention, and gradually declining to 25 by Week 15, a trend that largely tracked overall goal-setting participation, and the observed decrease in week 15 can likely be attributed to students reallocating their focus toward preparing for final examinations in their other courses. The total word count of goal submissions increased dramatically from 1,069 at baseline to 4,855 immediately after the intervention, with subsequent totals remaining substantially above pre-intervention levels. This pattern was mirrored in the mean length of goal statements, which rose from 9.2 words at baseline to 26.2 post-intervention, and continued to grow, reaching 34.9 words by Week 15. Similarly, the mean number of goals per participant increased from 3.1 at baseline to 4.7 post-intervention, and further to 6.4 by Week 15, indicating that students not only elaborated more on their goals but also set more of them over time. Table 2: Student Goal-Setting Metrics Before and After Intervention Metric Pre-Intervention Baseline (Week 3) Post-intervention (Week 3) Post-intervention (Week 8) Post-intervention (Week 11) Post-intervention (Week 15) Number of participants who submitted goals 38 39 36 32 25 Total word count of goal submissions 1069 4855 5706 3538 5556 Total number of goal statements 116 185 171 163 159 Mean length of goal statements (words) 9.2 26.2 33.4 21.7 34.9 Mean number of goals per participant 3.1 4.7 4.8 5.1 6.4 Improvement in Goal Quality: SMART Criteria Analysis of goal statements according to the SMART framework yielded evidence of substantial improvement in goal quality (Table 3 ). Prior to the intervention, only 16.38% of goals met all five SMART criteria (SMART-5), while 63% were classified as SMART-0, SMART-1, or SMART-2, indicating vague or incomplete goals. Students may set vague goals when they lack a clear understanding of the learning expectations (McCardle, et al., 2016). To ensure students understood what they were supposed to learn, we introduced the intended learning outcomes clearly at the beginning of the course. Thus, a gap between students ‚Äô internal understanding and their ability to express it with precision can explain the poor quality of their initial goals, a finding supported by McCardle et al (2016). Following the chatbot intervention, the proportion of fully SMART goals (SMART-5) rose sharply to 68.11%, and this improvement was largely sustained or enhanced through Week 15, when 84.28% of goals met all SMART criteria. The proportion of low-quality goals (SMART-0 and SMART-1) dropped precipitously to less than 3% post-intervention and was completely eliminated by Week 15. Following the intervention, the total share of intermediate-level categories (SMART-2 to SMART-4) also declined over time, dropping from 37.07% to 29.19% in week 3 and continuing to fall to 15.73% by week 15, reflecting a shift toward more complete and effective goal statements. This pattern demonstrates both the immediate and long-term effectiveness of the chatbot intervention in fostering high-quality goal-setting. Table 3: Distribution and Changes in SMART Goals Patterns Before and After Intervention Goals before intervention (week 3) Goals after intervention (week 3) Week 8 Week 11 Week 15 Total number of goals 116 185 171 163 159 Percentage of SMART-0 type 1.72% 0.54% 0.58% 0.00% 0.00% Percentage of SMART-1 type 44.83% 2.16% 2.92% 0.61% 0.00% Percentage of SMART-2 type 16.38% 4.86% 8.19% 3.68% 2.52% Percentage of SMART-3 type 12.93% 6.49% 9.36% 9.20% 6.92% Percentage of SMART-4 type 7.76% 17.84% 14.62% 7.98% 6.29% Percentage of SMART-5 type 16.38% 68.11% 64.33% 78.53% 84.28% Behavioral Engagement with the Chatbot Metrics of behavioral engagement with the chatbot indicated active and substantive student interaction (Table 4 ). 39 students used the goal-setting chatbot to help them make study goals. On average, students engaged in 11.95 utterance turns (SD = 9.27) per chatbot session, with a range from 2 to 46 turns, reflecting variability in the depth of interaction. The word count per student also varied widely (from 2 to 560 words), with a mean of 136.95 (SD = 166.98). The chatbot ‚Äô s engagement data shows that 20 students engaged with the chatbot within 10 turns. Engagement extended to 30 turns for 16 students, while three students were particularly active, with one achieving 30 turns, another 34, and a third 46. An analysis of user prompt length revealed that 26 students used fewer than 130 words in total, and another 10 stayed between 130 and 500 words. Conversely, three students provided more than 500 words, at 535, 543, and 560, respectively. Table 4: utterance turns and user words N Minimum Maximum Mean Std. Deviation Conversation turns 39 2 46 11.95 9.265 User words 39 2 560 136.95 166.978 Student Perceptions of Usefulness and Ease of Use Students reported highly positive perceptions of both the usefulness and the ease of use of the AI-based chatbot for goal-setting (Table 5 ). All items related to perceived usefulness received mean ratings above 4.3 on a 5-point scale, with the highest ratings for making the goal-setting process easier (M = 4.67, SD = 0.84) and more effective (M = 4.62, SD = 0.71). Students also strongly agreed that the chatbot helped them keep making SMART goals in subsequent weeks (M = 4.54, SD = 0.82) and that it was overall useful in their studies (M = 4.38, SD = 0.94). Perceived ease of use was similarly rated highly, with mean values ranging from 4.44 to 4.56. Students found the chatbot easy to use (M = 4.56, SD = 0.82), user-friendly (M = 4.49, SD = 0.97), and were able to recover from errors easily (M = 4.44, SD = 0.94). The questionnaire demonstrated high internal reliability (Cronbach‚Äôs Œ± = 0.962 for usefulness, 0.919 for ease of use). Table 5: Perceived usefulness and ease of use of the goal-setting chatbot Item N Mean(SD) usefulness Using the chatbot helped me to set my goals and study plans. 39 4.51 (.82) Using the chatbot helped me to make SMART goals and plans. 39 4.49 (.85) Using the chatbot helped me to keep making SMART goals and plans during the past weeks. 39 4.54 (.82) The chatbot made the goal-setting process easier. 39 4.67 (.84) The chatbot made the goal-setting process more effective. 39 4.62 (.71) Overall, the chatbot was useful in my study. 39 4.38 (.94) Ease of use It was easy to use the chatbot. 39 4.56 (.82) The chatbot acted as anticipated. 39 4.44 (.88) Despite encountering errors while using the chatbot, I was able to recover and continue easily. 39 4.44 (.94) Overall, the chatbot was user-friendly. 39 4.49 (.97) Students‚Äô perceptions and experiences of chatbot-facilitated goal setting To gain a deeper understanding of students‚Äô experiences and perceptions, a thematic analysis (Braun & Clarke, 2006, 2020) was conducted on qualitative data gathered from open-ended surveys and interviews. The analysis followed a structured process of identifying initial codes, grouping them into initial themes, and finally synthesizing them into overarching final themes. This process ensured a comprehensive and nuanced interpretation of student feedback. The resulting themes not only elucidate the qualitative aspects of student engagement but also correlate strongly with the quantitative findings on perceived usefulness and ease of use, providing a multi-faceted view of the chatbot interventions. Four themes emerged as a 

# result
 of the analysis. These themes are also juxtaposed with the relevant quantitative data to provide a foundation for integrated 

# discussion
 (Table 6) . Facilitation of SMART Goal Formulation by Students A dominant theme to emerge from the qualitative analysis was the chatbot‚Äôs pivotal role as an educational scaffold, explicitly teaching students the principles and practical application of SMART goal-setting. Before interacting with the chatbot, many students reported only a vague or superficial understanding of how to formulate effective goals. This is evident in the poor quality of the initial goals set before the intervention. Echoing our findings, researchers also noted that students tended to set goals that were overly ambitious or too vague, rendering them ultimately unachievable ( Raluy & Mislang, 2022 ). However, t he chatbot‚Äôs structured, dialogic approach provided ongoing guidance that demystified the process, gradually equipping students with the capacity to create goals that were specific, measurable, achievable, relevant, and time-bound. For many, the learning journey began with uncertainty or even confusion about what constituted a ‚Äúgood‚Äù goal. As one student reflected: ‚ÄúTo be honest, I didn't really understand SMART goals at the beginning, and I was quite unclear about how to make detailed plans in many aspects. However, with the robot's help, I now understand that when setting goals in the future, for example, I need to make them measurable and time-bound. The robot taught me some principles for planning. It also provided templates and guidance on how to make SMART goals.‚Äù (interview, Nie) The chatbot‚Äôs educational value was reinforced by its use of concrete examples and structured frameworks. By offering sample goals and breaking down complex objectives into actionable steps, the chatbot enabled students to better internalize the components of SMART goals. As another student explained: ‚ÄúThe plans that the chatbot provides for me always have a general framework. By reviewing the plans it gives me, I can get a rough idea of how to structure my own planning, such as what my goals should be, what specific details to include, and how to make them measurable. Through the examples it provides, I can also learn a lot, which helps me develop the ability to create SMART goals.‚Äù (interview, Chen) Crucially, the chatbot‚Äôs interactive process extended beyond one-way instruction. Students described how the iterative exchange , with the chatbot providing feedback, posing clarifying questions, and prompting for revision , fostered deeper learning and refinement of their own goal-setting practices: ‚ÄúThe chatbot taught me how to set SMART goals by giving me examples and explaining what SMART goals are. Through interaction and practical exercises, it helped me refine and optimize my goals. Through these 

# methods
, I learned how to set effective SMART goals.‚Äù (interview, Huang) Importantly, the skills and strategies acquired through the chatbot were not confined to the immediate academic context. Several students reported transferring their newfound goal-setting abilities to extracurricular domains, demonstrating the chatbot‚Äôs impact on broader self-regulation and life skills. As one student summarized:‚ÄúI learned how to set SMART goals, mastered this skill, and have since applied it in various club activities.‚Äù (interview, Qin) After SMART goal training, use of the chatbot was optional. Among the students who chose to use it, we identified two primary interaction approaches for goal-setting (Figure 4). The first is a collaborative co-creation process, where students actively draft an initial plan themselves and then submit it to the chatbot for refinement and enhancement. For instance, one student described this iterative 

# method
: ‚ÄúI usually first write a rough weekly plan myself, then input this plan into the chat window and send it to the chatbot, asking it to help me develop a smarter version.‚Äù (interview, Jiang). The second common approach involves input-based generation, where users provide the chatbot with detailed descriptions of their academic needs, goals, preferences, and challenges, relying on the system to generate fully-formed and personalized plans. As one participant noted, ‚ÄúI will input information about a weak area I usually struggle with, what I want to improve, and which aspects I've already done well in, then let the chatbot generate a day-by-day plan for me.‚Äù (interview, Dai). Fig. 4 Two Paths to a SMART Plan: Collaborative and AI-Generated Collectively, these narratives underscore the chatbot‚Äôs function not just as a technological tool, but as an instructional partner‚Äîguiding, modeling, and reinforcing effective goal-setting habits that students could internalize and apply autonomously. This theme aligns with the observed quantitative improvements in goal quality and highlights the potential of AI-driven scaffolding to foster metacognitive skills critical for lifelong learning. Facilitation of Personalized Learning through Rich Resource Provision and Adaptive support A central finding of this study was the chatbot's pivotal role in facilitating a personalized learning experience by synergizing its vast repository of learning resources with its capacity for adaptive academic support. Students perceived the chatbot not merely as an information tool, but as an active agent that curated materials and tailored strategies to their unique academic backgrounds, learning paces, and specific goals. The provision of rich and relevant learning resources formed the foundational element of this personalized experience. Students reported that the chatbot could effectively bridge information gaps by offering access to a wide array of materials, from academic essays to specialized study guides. (ËøôÈáåÈúÄË¶ÅÊâæÊñáÁåÆÊù•ÊîØÊåÅAI-based chatbots‰∏∫‰ªÄ‰πàËÉΩÊèê‰æõa wide array of resources‰ª•Âèä‰∏∫‰ªÄ‰πàËÉΩÂÆûÁé∞personalization) This capability was particularly valued for its contextual relevance, as noted by a student in a specialized field: ‚ÄúFor example, when I told it I'm majoring in Electronic Information, it provided me with a lot of materials highly relevant to this field.‚Äù (interview, Ji) Others emphasized the sheer volume and utility of these resources, with one student stating, ‚Äúits vast knowledge base allows it to answer a wide range of questions, providing me with many valuable 

# reference
 opinions‚Äù (reflection 1, Liu), and another adding, ‚Äúchatbots offer me various resources such as study materials, online courses and academic essays. It‚Äôs a good way to shorten the information gap‚Äù (reflection 1, Chen). Beyond resource curation, the chatbot‚Äôs ability to provide adaptive and personalized support was a critical component. This was evidenced by its role in crafting customized study plans that responded to individual needs and progress. Students highlighted how the chatbot could analyze their inputs to generate structured, manageable, and personally tailored strategies:‚ÄúThrough interactions, a chatbot can analyze a student's progress and struggles, allowing it to offer personalized study plans and learning strategies. This tailored approach helps in effectively addressing the unique challenges faced by each student.‚Äù (reflection 1, Huang) . The depth of this personalization was further elaborated by another participant: ‚ÄúThe Chatbot can assist in creating a study plan by offering several advantages. It can provide personalized scheduling based on my learning pace and preferences, help me stay organized and manage my time effectively, and offer reminders for upcoming tasks or exams.‚Äù (reflection 1, Hu) . However, achieving this high degree of personalization was not without its challenges. The process was inherently interactive and contingent upon detailed user input. Several students reported that they needed to invest significant effort in articulating their needs clearly and specifically to unlock the most relevant support and resources: ‚ÄúI need to spend extra effort to describe my requirements in great detail and with specificity each time, which can be a bit cumbersome.‚Äù (interview, Chen) . ‚ÄúRegarding personalization, I feel that you need to set a premise or condition with it first‚Äîit will only provide personalized responses under certain conditions.‚Äù (interview, Nie) . This indicates that while the chatbot is powerfully equipped to facilitate personalized learning, its effectiveness is maximized through a collaborative process where the user actively defines their parameters. 3 . Provision of Reliable Support Unconstrained by Time or Location Students widely valued the chatbot for its reliability, autonomous learning support and operational efficiency. These advantages were contrasted with the limitations inherent in human-based support systems . The first significant finding was the students' strong appreciation for the chatbot's consistency and accuracy, which was directly framed as a remedy to human variability. This finding aligns with existing research suggesting that the effectiveness of pre-scheduled human coaching can be compromised by logistical constraints, as sessions must occur at a fixed time even if either the coach or participan is not in an optimal physical or mental state to engage productively (Terblanche, et al., 2022) . In contrast, our p articipants highlighted that the quality of support was not subject to the mood, energy levels, or subjective biases of a human tutor, a perception that established the chatbot as a uniquely trustworthy and impartial resource. This theme of dependable objectivity was eloquently summarized by one student, Guan, who elaborated: ‚ÄúChatbots provide consistent advice and information, which is critical in educational settings. They ensure that the guidance given is not influenced by human factors like mood or fatigue, which can sometimes affect the quality of advice provided by human advisors.‚Äù (Reflection 1, Guan) This reflection underscores a student desire for standardized, equitable support, suggesting that the non-human nature of the interaction is itself a key benefit that fosters confidence and reduces anxiety about receiving inconsistent information. Beyond reliability, the chatbot‚Äôs 24/7 functionality was identified as a critical feature that empowers self-directed learning. Students reported that this constant availability dismantles temporal barriers to education, allowing them to take control of their learning rhythms and seek help at their most teachable moments, regardless of institutional schedules. This finding is supported by existing literature; as Hew et al. (2022) discover, chatbots enhanced learning by offering students immediate, around-the-clock support and feedback without the time and location constraints inherent in human teacher availability. Thus, the technology facilitates a more autonomous, student-centered learning model, where educational support is seamlessly integrated into the individual's workflow. Huang's reflection captures the essence of this empowerment: ‚ÄúFirstly, chatbots are available 24/7, providing students the flexibility to seek assistance at any time that suits their schedule. This constant availability can be particularly beneficial for students who may have irregular schedules or those who find themselves needing help outside of normal tutoring or office hours.‚Äù (Reflection 1, Huang) This around-the-clock access was not merely described as a convenience, but as a fundamental shift in the support paradigm, creating a perpetual safety net that encourages continuous engagement and accommodates diverse student lifestyles and commitments. Reduction of Planning Burden and Resultant Motivation Enhancement A nother significant finding of this study pertains to the chatbot's positive impact on students' psychological engagement with the goal-setting process. Specifically, the chatbot was found to significantly reduce the cognitive and logistical burden associated with planning, which in turn enhanced students' motivation to set and pursue their academic goals. (ËøôÈáåÈúÄË¶ÅÂºïÁî®ÂÅöËÆ°Âàístart from scratchÂæàÈöæÔºâ S tudents reported that the act of creating and refining study plans independently was often a daunting, time-consuming, and demotivating task. The chatbot alleviated this burden by acting as an automated planning assistant, generating structured and detailed plans based on user input. This reduction in effort was frequently cited as a direct catalyst for increased motivation. One student explicitly made this connection, stating: ‚ÄúCreating plans on my own definitely takes time, and I often lack the motivation to revise them. With the help of a chatbot, I only need to provide my requirements, and it will basically give me a well-structured plan that I can simply fine-tune. This reduces the difficulty of planning and increases my motivation.‚Äù (interview, Dai) The motivational benefit extended beyond mere convenience. Students described how the chatbot‚Äôs ability to break down overwhelming objectives into manageable, step-by-step tasks made their goals feel more achievable. This process fostered a sense of competence and progress, which is a key driver of intrinsic motivation. The chatbot‚Äôs role in providing clear, immediate feedback and structure transformed planning from a source of anxiety into a motivating force, as highlighted by a student: ‚ÄúChatbot's modified plan has clear goals, strong rationality, and strong feasibility. The new plan helped me clarify my goals, allocate tasks reasonably, and gave me more motivation.‚Äù (reflection 1, Wang) Furthermore, the chatbot was seen as a tool that could empower typically disengaged learners. By offering a low-stakes, non-judgmental interface, it helped to lower the barriers to seeking academic help, potentially motivating even reticent students to engage more actively in their learning: ‚ÄúLastly, it has the potential to motivate reticent students, who may be hesitant to engage with their educators, to become more communicative.‚Äù (reflection 1, Qin) Table 6 : Juxtaposition of Quantitative Data and Qualitative Themes: The Chatbot's Impact on Goal Setting Quantitative Data Thematic Analysis (Qualitative Findings) How the Qualitative Data Explains the Quantitative Trend Goal Qua lity: SMART-5 Goals: Increased from 16.38% (Pre -intervention ) to 84.28% (Week 15). Low-Quality Goals (SMART-0/1): Decreased from 46.5% to 0% . Teaches SMART Goal Formulation Students learned the principles through examples, feedback, and refinement from the chatbot, internalizing the framework and applying it beyond the course. The massive improvement in goal quality is directly explained by students' reports of the chatbot acting as an instructional tool. It explicitly taught them how to transform vague intentions into specific, measurable, achievable, relevant, and time-bound plans. Perceived Usefuln
```

### 7. Goal Setting Theory
**Text to enhance:** "SMART goal-setting framework, which guided the development of five conversational questions that pro..."
**Recommended action:** Verify Doran (1981) citation and add supporting goal-setting research
**Suggested citations:** Doran (1981), Latham & Locke (2007), Schunk (1990), Bandura & Schunk (1981)
**Context window:**
```
em via Dialogflow Messenger, allowing seamless access for students. The core design centered on the SMART goal-setting framework, which guided the development of five conversational questions that prompted students to define their learning goals before the course began. For each question, students were presented with multiple-choice options (A, B, C) to facilitate their responses. Based on their selections, the chatbot provided relevant recommendations (Hew et al., 2021, 2022, Du et al., 2021). The goal-setting chatbot or the SRL chatbot developed in the three studies offered significant benefits by helping students clarify and structure their learning goals through a guided, SMART-based framework, thereby raising awareness of effective self-regulated learning strategies (Hew et al., 2021, 2022, Du et al., 2021); their 24/7 availability and teacher-designed, personalized recommendations enhanced perceived usefulness and provided accessible support (hew et al., 2022). However, these rule-based chatbots faced limitations, including restricted response options, a lack of true intelligent and natural conversation flow, with students requesting more choices, faster and more intelligent responses (Hew et al., 2021, 2022). This is because rule-based chatbots operate on pre- established rules, offering limited responses to user queries (Singh et al., 2019 , Hew et al., 2022, Ng et al., 2024 ). Furthermore, their one-time use at the course outset was seen as insufficient, as students desired ongoing interaction for goal tracking, progress reminders throughout the learning journey (Hew et al., 2021, 2022, Du et al., 2021). These findings highlight the inherent limitations of rule-based systems and the need for more advanced chatbot technologies. E ducational chatbots, powered by natural language processing (NLP) and artificial intelligence , have emerged as promising tools for enhancing SRL processes, including goal-setting (Guan et al., 2024 , Ng et al., 2024 ). Unlike rule-based systems, AI-based chatbots leverage large language models (LLMs) to provide adaptive, context-sensitive, and personalized interactions (Ng et al., 2024 , Lai, 2024 ) . AI-based chatbots have been proven effective in fostering students ‚Äô self-regulated learning due to their flexibility and personalized recommendations (Ng et al., 2024). Hew et al (2025) developed a large language model (LLM) based chatbot system, the GoalPlanMentor, to support students ‚Äô self-regulated learning in online courses. Given the challenges of providing timely and personalized feedback to students in online learning, the research first developed GoalPlanMentor with two key agents: GoalPlanDetectAgent (GPDA) and GoalPlanAwareTeachingAgent (GPATA). The GPDA recognizes students' goals and plans from chat logs using a Memory-Augmented Prompt and stores them in databases, while the GPATA interacts with students, helps them reflect on and revise goals and plans, and guides them through weekly learning tasks. The study then conducted experiments with 25 Asian students in an eight-week education course. 

# Results
 showed high agreement between the system's and human coders' detection of students' goals and plans, high-quality feedback from the system, and students' positive perception of its usefulness. Higher perceived usefulness in goal-setting was associated with better learning achievements (Hew et al., 2025). Despite their efforts , there is little research on the long-term impact of AI-based chatbots on the quality of students‚Äô self-set goals. This study adopts a novel approach by developing an AI-based customized chatbot using system prompts to help students learn about how to set SMART goals. Development of the AI-Based Customized Chatbot Recent chatbot construction platforms such as Poe allowed researchers to build AI-based chatbots easily by simply design ing prompts , eliminating the need to apply complex technical skills (Guo & Li, 2024 ). In our study, a customized AI-based goal-setting chatbot was developed using Bytewise, a platform for building customized chatbots . The platform uses an application programming interface (API) to process both developer-crafted system prompts and user intputs, delivering AI-gerneated responses that create an interactive and efficient learning experience (Wang, 2024). The chatbot was programmed to record conversations, which were accessible to the owners for analysis and feedback purposes. The system prompts were designed to guide students through the process of setting SMART goals via dialogues, because according to the principles of multimedia leanrning, conversational presentation of materials enhances comphrehension ( Mayer, 2017) . The system prompts were designed by the course teacher, also the first writer of the paper. The interaction intentionally begins with friendly greeting to establish a positive, relaxing tone for the learning experience. After that, the chatbot provides studnets with a menu offering three choices, allowing studnets to choose the path that best suits their immediate need (Fig 1.) The first option features a teach-test-show structure, which explains the goal-setting SMART framework (Doran, 1981) first. It then requires the student to actively summarize the concepts, reinforcing knowledge retention through recall. Finally, it provides a concrete example for 

# reference
, modeling what a good outcome looks like. The second option is designed for active application of the SMART framework learned earlier. It transforms the chatbot from an information source into an interactive tutor. The student practices creating their own goal, and the chatbot provides personalized, AI-generated suggestions from improvement, facilitating a iterative refinement process (Fig. 2). The last option is designed for efficiency and support, providing students with study resources they need to achieve their goals. After completing any option, the student can choose another action or revisit previous ones, thus creating a continuous and supportive learning loop (Fig 3.). Fig. 1 The conversation between the goal-setting chatbot and students Fig. 2 The personlized feedback generated by the chatbot Fig. 3 How the chatbot interacts with students 

# Methods
 This study employed a convergent mixed-

# methods
 design (Creswell & Plano Clark, 2018) to provide a comprehensive understanding of how an AI-based chatbot influences the quality of undergraduate students' self-set learning goals and their perceptions of the tool. Quantitative and qualitative data were collected concurrently, analyzed separately, and then integrated during the interpretation phase to triangulate findings and develop a more nuanced analysis. Participants and Context The participants consisted of 40 first-year undergraduate engineering students enrolled in a 15-week "English for Academic and General Purposes" course at a large, research-intensive public university in Southwest China. The course, which is structured in two phases (Weeks 3‚Äì7 and 8‚Äì17), aims to develop students' English proficiency and core academic skills, including lecture comprehension, note-taking, academic 

# discussion
, and presentations. The course instructor, who was responsible for designing and implementing the goal-setting chatbot intervention, is also the first author of this study. Process of data collection This research employed a longitudinal data collection strategy over the 15-week semester to capture the development of students' goal-setting practices. Multiple forms of data were gathered, including chatbot interaction logs, students' initial baseline goals, revised goals immediately following the chatbot intervention, and subsequent goal submissions at Weeks 8, 11, and 14 to assess retention. Other qualitative data consisted of written reflections comparing pre- and post-intervention goals (Reflection 1), a reflection on the goal-setting process (Reflection 2), and focus group interviews with 15 students stratified by engagement level. The quantitative dataset includes participants‚Äô ratings on a five-point Likert scale assessing perceived usefulness and ease of use of the chatbot. An overview of the data collection procedure is presented in Table 1. During Week 3, the course‚Äôs intended learning outcomes were introduced to provide students with a clear framework for their studies. This orientation was designed to reduce ambiguity in approaching learning tasks, a challenge noted by Winne & Hadwin (1998). 38 students then submitted their initial study goals aligned with the intended learning outcomes to provide a baseline against which subsequent goal development could be measured. Subsequently, the chatbot intervention was introduced. The chatbot guided students through a structured three-path menu to support SMART goal-setting. The Learn path provided instruction and an example of the SMART framework (Doran, 1981). The Apply path allowed students to practice writing goals and receive personalized feedback
```

### 8. Goal Setting Theory
**Text to enhance:** "SMART goals. In week 4, 39 students completed a five-point Likert scale survey adapted from Davis (1..."
**Recommended action:** Verify Doran (1981) citation and add supporting goal-setting research
**Suggested citations:** Doran (1981), Latham & Locke (2007), Schunk (1990), Bandura & Schunk (1981)
**Context window:**
```
 their pre- and post-intervention goals and to articulate the chatbot‚Äôs role in guiding them to set SMART goals. In week 4, 39 students completed a five-point Likert scale survey adapted from Davis (1989) to measure their perceptions of the chatbots‚Äô usefulness and ease of use. The instrument comprised six items measuring perceived usefulness‚Äîthe degree to which students believed the chatbot would enhance their goal-setting performance‚Äîand four items measuring perceived ease of use‚Äîthe extent to which they found the system effortless to operate. These constructs are central to technology adoption (Davis, 1989). In this study, perceived usefulness reflected students‚Äô assessment of the chatbot‚Äôs effectiveness in improving goal-setting, while ease of use captured their perceptions of its simplicity and intuitiveness. To assess the long-term retention and organic adoption of the SMART framework, 36, 32, and 25 students submitted goals at Weeks 8, 11, and 14 without further mandatory use of the chatbot or requirement to use the SMART framework, thereby allowing us to observe their autonomous goal-setting behavior. This design allowed for the examination of the intervention‚Äôs sustained impact on students' natural goal-setting behaviors. The timeframe was selected to provide periodic check-in points for students to monitor and regulate their progress, a practice highlighted as critical in SRL models (Zimmerman, 2008). Moreover, setting short-term goals has been shown to promote greater use of study strategies compared to long-term goals (Seijts & Latham, 2001). In week 15, all participants completed a goal-setting reflection. Students answered questions such as ‚ÄúHow did the chatbot help you make your study goals?‚Äù ‚ÄúHow did the chatbot help you study English?‚Äù and ‚ÄúWhat suggestions can you provide for improving the chatbot?‚Äù (reflection 2). Conversation records from students‚Äô interactions with the goal-setting chatbot, such as utterance turns, can be analyzed to assess the students‚Äô behavioral engagement (Hew et al., 2022). In week 18, we extracted and analyzed two engagement metrics from the chatbot interaction logs: the number of utterance turns and the word count of student prompts. Based on their engagement metrics, participants were stratified into high, medium, and low engagement tiers, from which three focus groups of five students each were selected to participate in the focus group interview. These semi-structured interviews were designed to gain deeper insight into students‚Äô experiences with the chatbot, including their perceptions of its utility in goal-setting and their recommendations for its enhancement. Conducted and audio-recorded in Chinese, the interviews were then translated and transcribed into English. Table 1 Overview of data sources and collection Week Procedure Data Collected for Analysis 3 a. 

# Introduction
 of course learning outcomes b. Submission of initial study goals Baseline study goals 3 a. Chatbot intervention (Learn, Apply, Resources paths) b. Submission of revised goals and Reflection 1 1. Revised study goals (Post-intervention) 2. Written Reflection 1 (Comparison of pre/post goals & chatbot's role) 4 Completion of the chatbot use survey Rating a 5-point Likert Scale 8 Submission of study goals (without mandated chatbot use) Study Goals (Week 8) 11 Submission of study goals (without mandated chatbot use) Study Goals (Week 11) 14 Submission of study goals (without mandated chatbot use) Study Goals (Week 14) 15 Submission of final goal-setting reflection (Reflection 2) Written Reflection 2 (Overall reflection) 18 a. access chatbot interaction logs b. Focus group interviews 1. engagement level metrics such as utterance turns and word count of student prompts 2. Interview transcripts (3 groups of 5 students each, stratified by engagement level) Data Analysis To address the first 

## research question
 ‚ÄúHow can AI-based chatbots change the qualities of students‚Äô self-set study goals?‚Äù, we analyzed students' self-set goals using the SMART framework (Doran, 1981). Goals were collected at five time points: as a baseline, after the chatbot intervention in Week 3, and subsequently in Weeks 8, 11, and 

## 14. Each goal was coded for the presence and clarity of the five SMART criteria:
 Specific, Measurable, Achievable, Relevant, and Time-bound. Based on the number of criteria met, goals were categorized into six types: SMART-0 through SMART-5 (Table 2) . The lowest tier, SMART-0 type, represents an intention with no SMART elements, while the highest tier, SMART-5 type, includes all five SMART elements. As the levels increase, more SMART elements are integrated. For example, a SMART-1 goal incorporates just one element, such as the "Relevant" aspect of wanting to improve writing skills in the SMART-1 type in the table. A SMART-2 goal adds a second element; the goal to practice writing twice a week is both Relevant and Time-bound . However, it still remains deficient because it lacks specificity (what kind of writing?), is not measurable (how many passages?), and therefore its achievability cannot be properly assessed. To ensure coding reliability, two researchers collaboratively coded all goals, resolving any discrepancies through 

# discussion
 to reach consensus. The frequency of each goal type was calculated across time points, and trends in goal quality were analyzed to identify patterns of change and improvement over the semester. Table 

## 2. C oding Goals by SMART Criteria:
 A Six-Tiered Exemplification Goal types Examples SMART-0 Type Set tasks in advance. SMART-1 Type Improve my writing skills (SMART-1-R, including the element ‚Äú relevant ‚Äù ) SMART-2 Type Writing passages for practice twice a week (SMART-2-RT, including ‚Äú Relevant and Time bound ‚Äù ); Practice my listening by watching TED videos (SMART-2-SR, including ‚Äú Specific and Relevant ‚Äù ) SMART-3 Type I plan to watch TED videos every day to practice my listening skills (SMART-3-SRT, including ‚Äú Specific, Relevant, and Time-bound ‚Äù ); I plan to accumulate vocabulary for 30 minutes every morning (SMART-3-RMT, including ‚Äú Relevant, Measurable, and Timebound ‚Äù ) SMART-4 Type On Monday, learn 15 English words. (SMART-4-MART, including ‚Äú Measurable, Achievable, Relevant, and Time-bound ‚Äù ) Engage in a 30-minute 

# discussion
 session with my partner, focusing on topics related to my coursework or recent learning. (SMART-4-SMAR, including ‚Äú Specific, Measurable, Achievable, and Relevant ‚Äù ) SMART-5 Type Monday: Listen to a TED Talk or podcast about renewable energy, focusing on identifying key points and any relevant technical terms used (20 mins). (SMART-5, including all five elements) To address the second 

## research question
, ‚ÄúWhat are students‚Äô perceived usefulness and ease of use of the AI-based customized chatbot?‚Äù, we administered a five-point Likert scale adapted from Davis (1989). Responses ranged from 1 ( strongly disagree ) to 5 ( strongly agree ), and the instrument included items measuring both perceived usefulness (the extent to which students believed the chatbot improved their goal-setting performance) and perceived ease of use (the degree to which they found the system effortless to operate) (Davis, 1989). Data were analyzed using SPSS to calculate mean scores for each construct. Cronbach‚Äôs alpha was computed to assess the internal consistency and reliability of the scale. To gain deeper insights into student perceptions and improvement suggestions, and to address both the second and third 

## research questions
, we collected student self-reflections and conducted semi-structured interviews. Reflection prompts included: ‚ÄúWhat differences do you observe between your original and revised goals?‚Äù, ‚ÄúHow did the chatbot help you learn to set SMART goals?‚Äù, ‚ÄúIs the AI-based chatbot useful for helping you set study goals? Why or why not?‚Äù, and ‚ÄúWhat suggestions can you provide for improving the chatbot?‚Äù Interview questions explored similar themes, such as ‚ÄúHow can the chatbot help you set goals?‚Äù, ‚ÄúIs the AI-based chatbot useful for helping you set study goals? Why or why not?‚Äù, ‚ÄúDo you have any suggestions for improving the chatbot?‚Äù Thematic analysis (Braun & Clarke, 2006, 2020) was conducted on the reflection responses and interview transcripts. To ensure inter-coder reliability, two researchers collaboratively coded the data, discussing and resolving any discrepancies through consensus. This process yielded a set of codes and emergent themes that provided rich, nuanced insights into students‚Äô experiences and perceptions. These themes directly informed the interpretation of the second and third 

## research questions
. Finally, a convergent mixed-

# methods
 design was employed, with quantitative and qualitative data integrated at the interpretation stage. Quantitative findings, such as shifts in goal-setting patterns and survey 

# results
 on perceived usefulness and ease of use, were systematically compared and contrasted with the qualitative themes derived from interviews and reflections to develop a comprehensive understanding of the 

# results
. 

# Results
 Changes in Goal-Setting Engagement The 

# introduction
 of the AI-based chatbot was associated with an immediate and sustained increase in both the quantity and quality of student goal-setting (Table 2 ). The number of participants submitting goals remained high throughout the semester, starting at 38 pre-intervention (Week 3), increasing to 39 immediately post-intervention, and gradually declining to 25 by Week 15, a trend that largely tracked overall goal-setting participation, and the observed decrease in week 15 can likely be attributed to students reallocating their focus toward preparing for final examinations in their other courses. The total word count of goal submissions increased dramatically from 1,069 at baseline to 4,855 immediately after the intervention, with subsequent totals remaining substantially above pre-intervention levels. This pattern was mirrored in the mean length of goal statements, which rose from 9.2 words at baseline to 26.2 post-intervention, and continued to grow, reaching 34.9 words by Week 15. Similarly, the mean number of goals per participant increased from 3.1 at baseline to 4.7 post-intervention, and further to 6.4 by Week 15, indicating that students not only elaborated more on their goals but also set more of them over time. Table 2: Student Goal-Setting Metrics Before and After Intervention Metric Pre-Intervention Baseline (Week 3) Post-intervention (Week 3) Post-intervention (Week 8) Post-intervention (Week 11) Post-intervention (Week 15) Number of participants who submitted goals 38 39 36 32 25 Total word count of goal submissions 1069 4855 5706 3538 5556 Total number of goal statements 116 185 171 163 159 Mean length of goal statements (words) 9.2 26.2 33.4 21.7 34.9 Mean number of goals per participant 3.1 4.7 4.8 5.1 6.4 Improvement in Goal Quality: SMART Criteria Analysis of goal statements according to the SMART framework yielded evidence of substantial improvement in goal quality (Table 3 ). Prior to the intervention, only 16.38% of goals met all five SMART criteria (SMART-5), while 63% were classified as SMART-0, SMART-1, or SMART-2, indicating vague or incomplete goals. Students may set vague goals when they lack a clear understanding of the learning expectations (McCardle, et al., 2016). To ensure students understood what they were supposed to learn, we introduced the intended learning outcomes clearly at the beginning of the course. Thus, a gap between students ‚Äô internal understanding and their ability to express it with precision can explain the poor quality of their initial goals, a finding supported by McCardle et al (2016). Following the chatbot intervention, the proportion of fully SMART goals (SMART-5) rose sharply to 68.11%, and this improvement was largely sustained or enhanced through Week 15, when 84.28% of goals met all SMART criteria. The proportion of low-quality goals (SMART-0 and SMART-1) dropped precipitously to less than 3% post-intervention and was completely eliminated by Week 15. Following the intervention, the total share of intermediate-level categories (SMART-2 to SMART-4) also declined over time, dropping from 37.07% to 29.19% in week 3 and continuing to fall to 15.73% by week 15, reflecting a shift toward more complete and effective goal statements. This pattern demonstrates both the immediate and long-term effectiveness of the chatbot intervention in fostering high-quality goal-setting. Table 3: Distribution and Changes in SMART Goals Patterns Before and After Intervention Goals before intervention (week 3) Goals after intervention (week 3) Week 8 Week 11 Week 15 Total number of goals 116 185 171 163 159 Percentage of SMART-0 type 1.72% 0.54% 0.58% 0.00% 0.00% Percentage of SMART-1 type 44.83% 2.16% 2.92% 0.61% 0.00% Percentage of SMART-2 type 16.38% 4.86% 8.19% 3.68% 2.52% Percentage of SMART-3 type 12.93% 6.49% 9.36% 9.20% 6.92% Percentage of SMART-4 type 7.76% 17.84% 14.62% 7.98% 6.29% Percentage of SMART-5 type 16.38% 68.11% 64.33% 78.53% 84.28% Behavioral Engagement with the Chatbot Metrics of behavioral engagement with the chatbot indicated active and substantive student interaction (Table 4 ). 39 students used the goal-setting chatbot to help them make study goals. On average, students engaged in 11.95 utterance turns (SD = 9.27) per chatbot session, with a range from 2 to 46 turns, reflecting variability in the depth of interaction. The word count per student also varied widely (from 2 to 560 words), with a mean of 136.95 (SD = 166.98). The chatbot ‚Äô s engagement data shows that 20 students engaged with the chatbot within 10 turns. Engagement extended to 30 turns for 16 students, while three students were particularly active, with one achieving 30 turns, another 34, and a third 46. An analysis of user prompt length revealed that 26 students used fewer than 130 words in total, and another 10 stayed between 130 and 500 words. Conversely, three students provided more than 500 words, at 535, 543, and 560, respectively. Table 4: utterance turns and user words N Minimum Maximum Mean Std. Deviation Conversation turns 39 2 46 11.95 9.265 User words 39 2 560 136.95 166.978 Student Perceptions of Usefulness and Ease of Use Students reported highly positive perceptions of both the usefulness and the ease of use of the AI-based chatbot for goal-setting (Table 5 ). All items related to perceived usefulness received mean ratings above 4.3 on a 5-point scale, with the highest ratings for making the goal-setting process easier (M = 4.67, SD = 0.84) and more effective (M = 4.62, SD = 0.71). Students also strongly agreed that the chatbot helped them keep making SMART goals in subsequent weeks (M = 4.54, SD = 0.82) and that it was overall useful in their studies (M = 4.38, SD = 0.94). Perceived ease of use was similarly rated highly, with mean values ranging from 4.44 to 4.56. Students found the chatbot easy to use (M = 4.56, SD = 0.82), user-friendly (M = 4.49, SD = 0.97), and were able to recover from errors easily (M = 4.44, SD = 0.94). The questionnaire demonstrated high internal reliability (Cronbach‚Äôs Œ± = 0.962 for usefulness, 0.919 for ease of use). Table 5: Perceived usefulness and ease of use of the goal-setting chatbot Item N Mean(SD) usefulness Using the chatbot helped me to set my goals and study plans. 39 4.51 (.82) Using the chatbot helped me to make SMART goals and plans. 39 4.49 (.85) Using the chatbot helped me to keep making SMART goals and plans during the past weeks. 39 4.54 (.82) The chatbot made the goal-setting process easier. 39 4.67 (.84) The chatbot made the goal-setting process more effective. 39 4.62 (.71) Overall, the chatbot was useful in my study. 39 4.38 (.94) Ease of use It was easy to use the chatbot. 39 4.56 (.82) The chatbot acted as anticipated. 39 4.44 (.88) Despite encountering errors while using the chatbot, I was able to recover and continue easily. 39 4.44 (.94) Overall, the chatbot was user-friendly. 39 4.49 (.97) Students‚Äô perceptions and experiences of chatbot-facilitated goal setting To gain a deeper understanding of students‚Äô experiences and perceptions, a thematic analysis (Braun & Clarke, 2006, 2020) was conducted on qualitative data gathered from open-ended surveys and interviews. The analysis followed a structured process of identifying initial codes, grouping them into initial themes, and finally synthesizing them into overarching final themes. This process ensured a comprehensive and nuanced interpretation of student feedback. The resulting themes not only elucidate the qualitative aspects of student engagement but also correlate strongly with the quantitative findings on perceived usefulness and ease of use, providing a multi-faceted view of the chatbot interventions. Four themes emerged as a 

# result
 of the analysis. These themes are also juxtaposed with the relevant quantitative data to provide a foundation for integrated 

# discussion
 (Table 6) . Facilitation of SMART Goal Formulation by Students A dominant theme to emerge from the qualitative analysis was the chatbot‚Äôs pivotal role as an educational scaffold, explicitly teaching students the principles and practical application of SMART goal-setting. Before interacting with the chatbot, many students reported only a vague or superficial understanding of how to formulate effective goals. This is evident in the poor quality of the initial goals set before the intervention. Echoing our findings, researchers also noted that students tended to set goals that were overly ambitious or too vague, rendering them ultimately unachievable ( Raluy & Mislang, 2022 ). However, t he chatbot‚Äôs structured, dialogic approach provided ongoing guidance that demystified the process, gradually equipping students with the capacity to create goals that were specific, measurable, achievable, relevant, and time-bound. For many, the learning journey began with uncertainty or even confusion about what constituted a ‚Äúgood‚Äù goal. As one student reflected: ‚ÄúTo be honest, I didn't really understand SMART goals at the beginning, and I was quite unclear about how to make detailed plans in many aspects. However, with the robot's help, I now understand that when setting goals in the future, for example, I need to make them measurable and time-bound. The robot taught me some principles for planning. It also provided templates and guidance on how to make SMART goals.‚Äù (interview, Nie) The chatbot‚Äôs educational value was reinforced by its use of concrete examples and structured frameworks. By offering sample goals and breaking down complex objectives into actionable steps, the chatbot enabled students to better internalize the components of SMART goals. As another student explained: ‚ÄúThe plans that the chatbot provides for me always have a general framework. By reviewing the plans it gives me, I can get a rough idea of how to structure my own planning, such as what my goals should be, what specific details to include, and how to make them measurable. Through the examples it provides, I can also learn a lot, which helps me develop the ability to create SMART goals.‚Äù (interview, Chen) Crucially, the chatbot‚Äôs interactive process extended beyond one-way instruction. Students described how the iterative exchange , with the chatbot providing feedback, posing clarifying questions, and prompting for revision , fostered deeper learning and refinement of their own goal-setting practices: ‚ÄúThe chatbot taught me how to set SMART goals by giving me examples and explaining what SMART goals are. Through interaction and practical exercises, it helped me refine and optimize my goals. Through these 

# methods
, I learned how to set effective SMART goals.‚Äù (interview, Huang) Importantly, the skills and strategies acquired through the chatbot were not confined to the immediate academic context. Several students reported transferring their newfound goal-setting abilities to extracurricular domains, demonstrating the chatbot‚Äôs impact on broader self-regulation and life skills. As one student summarized:‚ÄúI learned how to set SMART goals, mastered this skill, and have since applied it in various club activities.‚Äù (interview, Qin) After SMART goal training, use of the chatbot was optional. Among the students who chose to use it, we identified two primary interaction approaches for goal-setting (Figure 4). The first is a collaborative co-creation process, where students actively draft an initial plan themselves and then submit it to the chatbot for refinement and enhancement. For instance, one student described this iterative 

# method
: ‚ÄúI usually first write a rough weekly plan myself, then input this plan into the chat window and send it to the chatbot, asking it to help me develop a smarter version.‚Äù (interview, Jiang). The second common approach involves input-based generation, where users provide the chatbot with detailed descriptions of their academic needs, goals, preferences, and challenges, relying on the system to generate fully-formed and personalized plans. As one participant noted, ‚ÄúI will input information about a weak area I usually struggle with, what I want to improve, and which aspects I've already done well in, then let the chatbot generate a day-by-day plan for me.‚Äù (interview, Dai). Fig. 4 Two Paths to a SMART Plan: Collaborative and AI-Generated Collectively, these narratives underscore the chatbot‚Äôs function not just as a technological tool, but as an instructional partner‚Äîguiding, modeling, and reinforcing effective goal-setting habits that students could internalize and apply autonomously. This theme aligns with the observed quantitative improvements in goal quality and highlights the potential of AI-driven scaffolding to foster metacognitive skills critical for lifelong learning. Facilitation of Personalized Learning through Rich Resource Provision and Adaptive support A central finding of this study was the chatbot's pivotal role in facilitating a personalized learning experience by synergizing its vast repository of learning resources with its capacity for adaptive academic support. Students perceived the chatbot not merely as an information tool, but as an active agent that curated materials and tailored strategies to their unique academic backgrounds, learning paces, and specific goals. The provision of rich and relevant learning resources formed the foundational element of this personalized experience. Students reported that the chatbot could effectively bridge information gaps by offering access to a wide array of materials, from academic essays to specialized study guides. (ËøôÈáåÈúÄË¶ÅÊâæÊñáÁåÆÊù•ÊîØÊåÅAI-based chatbots‰∏∫‰ªÄ‰πàËÉΩÊèê‰æõa wide array of resources‰ª•Âèä‰∏∫‰ªÄ‰πàËÉΩÂÆûÁé∞personalization) This capability was particularly valued for its contextual relevance, as noted by a student in a specialized field: ‚ÄúFor example, when I told it I'm majoring in Electronic Information, it provided me with a lot of materials highly relevant to this field.‚Äù (interview, Ji) Others emphasized the sheer volume and utility of these resources, with one student stating, ‚Äúits vast knowledge base allows it to answer a wide range of questions, providing me with many valuable 

# reference
 opinions‚Äù (reflection 1, Liu), and another adding, ‚Äúchatbots offer me various resources such as study materials, online courses and academic essays. It‚Äôs a good way to shorten the information gap‚Äù (reflection 1, Chen). Beyond resource curation, the chatbot‚Äôs ability to provide adaptive and personalized support was a critical component. This was evidenced by its role in crafting customized study plans that responded to individual needs and progress. Students highlighted how the chatbot could analyze their inputs to generate structured, manageable, and personally tailored strategies:‚ÄúThrough interactions, a chatbot can analyze a student's progress and struggles, allowing it to offer personalized study plans and learning strategies. This tailored approach helps in effectively addressing the unique challenges faced by each student.‚Äù (reflection 1, Huang) . The depth of this personalization was further elaborated by another participant: ‚ÄúThe Chatbot can assist in creating a study plan by offering several advantages. It can provide personalized scheduling based on my learning pace and preferences, help me stay organized and manage my time effectively, and offer reminders for upcoming tasks or exams.‚Äù (reflection 1, Hu) . However, achieving this high degree of personalization was not without its challenges. The process was inherently interactive and contingent upon detailed user input. Several students reported that they needed to invest significant effort in articulating their needs clearly and specifically to unlock the most relevant support and resources: ‚ÄúI need to spend extra effort to describe my requirements in great detail and with specificity each time, which can be a bit cumbersome.‚Äù (interview, Chen) . ‚ÄúRegarding personalization, I feel that you need to set a premise or condition with it first‚Äîit will only provide personalized responses under certain conditions.‚Äù (interview, Nie) . This indicates that while the chatbot is powerfully equipped to facilitate personalized learning, its effectiveness is maximized through a collaborative process where the user actively defines their parameters. 3 . Provision of Reliable Support Unconstrained by Time or Location Students widely valued the chatbot for its reliability, autonomous learning support and operational efficiency. These advantages were contrasted with the limitations inherent in human-based support systems . The first significant finding was the students' strong appreciation for the chatbot's consistency and accuracy, which was directly framed as a remedy to human variability. This finding aligns with existing research suggesting that the effectiveness of pre-scheduled human coaching can be compromised by logistical constraints, as sessions must occur at a fixed time even if either the coach or participan is not in an optimal physical or mental state to engage productively (Terblanche, et al., 2022) . In contrast, our p articipants highlighted that the quality of support was not subject to the mood, energy levels, or subjective biases of a human tutor, a perception that established the chatbot as a uniquely trustworthy and impartial resource. This theme of dependable objectivity was eloquently summarized by one student, Guan, who elaborated: ‚ÄúChatbots provide consistent advice and information, which is critical in educational settings. They ensure that the guidance given is not influenced by human factors like mood or fatigue, which can sometimes affect the quality of advice provided by human advisors.‚Äù (Reflection 1, Guan) This reflection underscores a student desire for standardized, equitable support, suggesting that the non-human nature of the interaction is itself a key benefit that fosters confidence and reduces anxiety about receiving inconsistent information. Beyond reliability, the chatbot‚Äôs 24/7 functionality was identified as a critical feature that empowers self-directed learning. Students reported that this constant availability dismantles temporal barriers to education, allowing them to take control of their learning rhythms and seek help at their most teachable moments, regardless of institutional schedules. This finding is supported by existing literature; as Hew et al. (2022) discover, chatbots enhanced learning by offering students immediate, around-the-clock support and feedback without the time and location constraints inherent in human teacher availability. Thus, the technology facilitates a more autonomous, student-centered learning model, where educational support is seamlessly integrated into the individual's workflow. Huang's reflection captures the essence of this empowerment: ‚ÄúFirstly, chatbots are available 24/7, providing students the flexibility to seek assistance at any time that suits their schedule. This constant availability can be particularly beneficial for students who may have irregular schedules or those who find themselves needing help outside of normal tutoring or office hours.‚Äù (Reflection 1, Huang) This around-the-clock access was not merely described as a convenience, but as a fundamental shift in the support paradigm, creating a perpetual safety net that encourages continuous engagement and accommodates diverse student lifestyles and commitments. Reduction of Planning Burden and Resultant Motivation Enhancement A nother significant finding of this study pertains to the chatbot's positive impact on students' psychological engagement with the goal-setting process. Specifically, the chatbot was found to significantly reduce the cognitive and logistical burden associated with planning, which in turn enhanced students' motivation to set and pursue their academic goals. (ËøôÈáåÈúÄË¶ÅÂºïÁî®ÂÅöËÆ°Âàístart from scratchÂæàÈöæÔºâ S tudents reported that the act of creating and refining study plans independently was often a daunting, time-consuming, and demotivating task. The chatbot alleviated this burden by acting as an automated planning assistant, generating structured and detailed plans based on user input. This reduction in effort was frequently cited as a direct catalyst for increased motivation. One student explicitly made this connection, stating: ‚ÄúCreating plans on my own definitely takes time, and I often lack the motivation to revise them. With the help of a chatbot, I only need to provide my requirements, and it will basically give me a well-structured plan that I can simply fine-tune. This reduces the difficulty of planning and increases my motivation.‚Äù (interview, Dai) The motivational benefit extended beyond mere convenience. Students described how the chatbot‚Äôs ability to break down overwhelming objectives into manageable, step-by-step tasks made their goals feel more achievable. This process fostered a sense of competence and progress, which is a key driver of intrinsic motivation. The chatbot‚Äôs role in providing clear, immediate feedback and structure transformed planning from a source of anxiety into a motivating force, as highlighted by a student: ‚ÄúChatbot's modified plan has clear goals, strong rationality, and strong feasibility. The new plan helped me clarify my goals, allocate tasks reasonably, and gave me more motivation.‚Äù (reflection 1, Wang) Furthermore, the chatbot was seen as a tool that could empower typically disengaged learners. By offering a low-stakes, non-judgmental interface, it helped to lower the barriers to seeking academic help, potentially motivating even reticent students to engage more actively in their learning: ‚ÄúLastly, it has the potential to motivate reticent students, who may be hesitant to engage with their educators, to become more communicative.‚Äù (reflection 1, Qin) Table 6 : Juxtaposition of Quantitative Data and Qualitative Themes: The Chatbot's Impact on Goal Setting Quantitative Data Thematic Analysis (Qualitative Findings) How the Qualitative Data Explains the Quantitative Trend Goal Qua lity: SMART-5 Goals: Increased from 16.38% (Pre -intervention ) to 84.28% (Week 15). Low-Quality Goals (SMART-0/1): Decreased from 46.5% to 0% . Teaches SMART Goal Formulation Students learned the principles through examples, feedback, and refinement from the chatbot, internalizing the framework and applying it beyond the course. The massive improvement in goal quality is directly explained by students' reports of the chatbot acting as an instructional tool. It explicitly taught them how to transform vague intentions into specific, measurable, achievable, relevant, and time-bound plans. Perceived Usefulness: Very high (All items >4.3/5). Made process easier: M = 4.67 Made process more effective: M = 4.62 Ease of Use: M = 4.56 Provides High-Quality & Reliable Support The chatbot was valued for its 24/7 availability, instant responses, and consistent, unbiased support, unlike human tutors. Enables Personalization of Goals Students praised the ability to tailor plans to their major, schedule, and pace. The high survey scores are a direct 

# result
 of the student experience. Their positive perceptions of usefulness and ease of use were formed because the tool was reliably available, delivered instant, high-quality , and personalized support . Students developed two approaches that helped them use the chatbot to set goals effectively . Goal Quantity: Total Goals: Increased from 116 to 159 . Mean Goals/Participant: Increased from 3.1 to 6.4 . Goal Elaboration: Mean Words/Goal: Increased from 9.2 to 34.9 . Increases Motivation & Reduces Planning Burden By generating structured plans, the chatbot reduced the cognitive load and frustration of planning, which increased motivation to set and pursue goals. The dramatic increase in the number and elaborateness of goals submitted is the behavioral outcome of reduced burden. Because the chatbot made planning easier and less time-consuming, students were motivated to set more, and more detailed, goals. Toward an improved chatbot: student suggestions Based on their experiences, students proposed key improvements to enhance the chatbot's intelligence and usability. A major suggestion was to equip the chatbot with a memory or context-awareness function to avoid repetitive exchanges. Students noted the frustration of having to repeatedly input the same information, as one explained: ‚ÄúWhen making plans, I mentioned my current English level to the chatbot once, but the next time it creates a plan, it forgets and I have to repeat the information‚Äù (interview, Jiang). To address this, they proposed the chatbot store user data to build a persistent profile, enabling more personalized support ‚Äúto reduce the need for clarifying my needs repeatedly‚Äù (interview, Yu). Students also advocated for enriched interactivity and proactive features. They recommended moving beyond plain text outputs to include visually structured formats, with one student suggesting enhancements so ‚Äúthe final output is presented in a specific format‚Äîsuch as a table or other structured layout‚Äù (interview, Qin). There was also strong support for integrated reminder systems, as students expressed a desire for a feature ‚Äúto send reminder notifications at specific times, prompting me to complete my goals‚Äù (interview, Xu). These enhancements aim to transform the chatbot from a reactive tool into a more dynamic and intuitive learning partner. However, alongside these suggestions for functional improvement, a note of caution emerged regarding the chatbot's potential impact on cognitive development. Several students voiced concerns that an over-reliance on the chatbot's pre-structured plans could potentially hinder the development of their own critical thinking and autonomous planning abilities. One student starkly observed, ‚ÄúI can follow the plan easily without using my brain‚Äù (reflection 1, Huang), highlighting the risk of passive compliance. Another student warned of the danger of dependency, stating, ‚Äúmaybe let us become dependent and lose our abilities to think independently‚Äù (reflection 1, Zheng). This sentiment underscores a critical challenge for chatbot design: how to balance the provision of efficient, intelligent support with the imperative to foster, rather than substitute, the user's own metacognitive and strategic thinking skills. Therefore, future iterations must be designed not only to be smarter and more helpful but also to actively encourage users to critique, adapt, and ultimately internalize the planning process. 

# Discussion
 The present study employed a convergent mixed-

# methods
 design to investigate the impact of an AI-based chatbot on undergraduate students' self-set learning goals and their perceptions of the tool. The quantitative and qualitative 

# results
 converge to paint a compelling picture: the AI chatbot served as a powerful instructional scaffold, leading to immediate and sustained improvements in the quality of students' goal-setting, while also being perceived as highly useful and easy to use. The following 

# discussion
 integrates these findings to address the three 

## research questions
, situating them within the broader literature on SRL and educational technology. The Chatbot as an Effective Instructional Scaffold for Acquiring SMART Goal-Setting Skills The most striking finding of this study pertains to the dramatic improvement in the quality of students' self-set goals, directly addressing RQ1. The quantitative data (Table 3) show a profound shift from vague goals to fully articulated SMART goals, with the proportion of SMART-5 goals rising from 16.38% at baseline to 84.28% to 68.11% immediately after intervention. The figure has risen to 84.28% by Week 15. This was not a transient effect but a sustained change, evidenced by the high-quality goals students continued to set autonomously weeks after the initial intervention. The qualitative data provide the crucial explanation for this shift: the chatbot functioned as an effective teacher of the SMART framework. Students reported entering the interaction with only " a n unclear understanding " (Interview, Nie) of effective goal-setting. Through its structured, multi-path design‚Äîoffering instruction, examples, and, most importantly, iterative feedback‚Äîthe chatbot provided clarity on how to formulate SMART goals . As one student noted, it " taught me how to set SMART goals by giving me examples and explaining what SMART goals are. Through interaction and practical exercises, it helped me refine and optimize my goals " (Interview, Huang). This aligns with the principles of multimedia learning (Mayer, 2017), suggesting that the conversational presentation of the material enhanced comprehension and skill acquisition. This finding extends the existing literature on educational goal-setting chatbots. While previous research using rule-based chatbots successfully introduced goal-setting (Hew et al., 2021, 2022; Du et al., 2021), their reliance on multiple-choice options limited the depth of learning and was frequently cited as a limitation by students. Our study demonstrates that an AI-based chatbot, capable of processing open-ended input and generating personalized, constructive feedback, can move beyond mere awareness-raising to facilitate a deeper internalization of complex SRL strategies. It effectively addresses the persistent problem identified by McCardle et al. (2016) and Raluy & Mislang (2022) , where students' goals remain vague even after instruction, by providing a low-stakes, interactive environment for practice and refinement. Facilitating Personalized and Autonomous Learning The study's 

# results
 also provide a clear answer to RQ2, concerning students' perceived usefulness and ease of use. The exceptionally high quantitative ratings (all items >4.3/5, Table 5) for both constructs are richly explained by the qualitative themes of personalization, reliability, and autonomy. Students highly valued the chatbot‚Äôs ability to provide personalized learning support. They reported that it could tailor resource recommendations to their specific majors (e.g., "it provided me with a lot of materials highly relevant to my major " - Interview, Ji) and generate adaptive study plans based on their described needs and pace. This study effectively resolves the limitation noted by Hew et al. (202 1 ) and Hew et al. (202 2 ) Ôºå who found that students needed more personalized guidance than what was offered by a limited set of predefined options. This perceived personalization is a key advantage of LLM-based chatbots over their rule-based predecessors (Ng et al., 2024; Lai, 2024) and is a likely driver of the high perceived usefulness scores. However, students also aptly noted that this personalization was contingent on their own input, requiring them to "describe my requirements in great detail" (Interview, Chen). This highlights a collaborative human-AI process rather than fully automated personalization. Furthermore, the chatbot was praised for its reliability and 24/7 availability, themes that directly contrast with the limitations of human support systems. Students appreciated that the chatbot‚Äôs support was "not influenced by human factors like mood or fatigue" (Reflection 1, Guan), a finding that echoes concerns about the variability of human coaches (Terblanche et al., 2022). This constant availability empowered students to engage in self-directed learning, allowing them to "seek assistance at any time that suits their schedule" (Reflection 1, Huang), thus supporting autonomous SRL behaviors in a way that aligns with the expectations of university students (Hew et al., 2022). Reducing Cognitive Load and Enhancing Motivational Engagement The integration of data reveals how the chatbot transformed the goal-setting process from a daunting task into a motivating activity, further addressing RQ1 and RQ

## 2. Quantitatively, this is seen in the increased behavioral engagement:
 students not only set higher-quality goals but also set more of them (mean goals per participant increased from 3.1 to 6.4) and elaborated on them in greater detail (mean word count per goal rose from 9.2 to 34.9). Qualitatively, students explicitly stated that the chatbot "reduces the difficulty of planning and increases my motivation" (Interview, Dai). By specifying the structure of the SMART plans students should make and providing relevant study materials , the chatbot significantly reduced the cognitive load associated with starting from a blank slate (ËøôÈáåÈúÄË¶ÅËÆ∫ÊñáÊîØÊíë) . This reduction in extraneous cognitive load allowed students to channel their efforts into refining and customizing their plans, thereby enhancing their sense of competence and self-efficacy‚Äîkey drivers of motivation in SRL models (Zimmerman, 2013) . The chatbot‚Äôs role thus evolved from a simple tool to a motivational partner that made the forethought phase of SRL less burdensome and more engaging. Navigating the Tension: Between Empowerment and Over-Reliance Addressing RQ3, the study uncovers a critical, nuanced insight into student perceptions: a strong appreciation for the chatbot's support coexists with a thoughtful caution about its potential drawbacks. Student feedback yielded insightful recommendations for augmenting the chatbot's capabilities, chiefly by endowing it with memory and context-awareness to eliminate repetitive information inputs (e.g., "the next time it creates a plan, it forgets and I have to repeat the information" - Interview, Jiang), and by incorporating proactive features such as reminder systems‚Äîa finding that aligns with prior research by Hew et al. (2022). These suggestions point to a desire for a more seamless and integrated coaching experience. More importantly, students voiced a sophisticated concern about over-reliance, warning that it "may be let us become dependent and lose our abilities to think independently" (Reflection 1, Zheng). This insight is crucial for the future design of educational AI. It underscores the delicate balance between providing support and fostering dependency. The ultimate aim of SRL interventions is to make the scaffold eventually unnecessary (Winne & Hadwin, 1998). Therefore, future iterations of such chatbots must be designed not only to be smarter but also to strategically fade their support, perhaps by increasingly prompting students to self-evaluate their plans rather than providing ready-made solutions, thus ensuring they develop and retain their metacognitive skills. Practical Implications The findings of this study offer several actionable implications for educators, instructional designers, and institutions seeking to harness AI for enhancing self-regulated learning. This study provides a replicable model for integrating AI chatbots into academic courses. Instructors can adopt a similar scaffolded approach by first explicitly teaching learning outcomes and the value of goal-setting. Designing a chatbot with clear, structured pathways that guide students from knowledge acquisition to active application is crucial, as this structure reduces cognitive load and ensures meaningful engagement. Furthermore, encouraging sustained use by embedding goal-setting checkpoints throughout the semester allows students to practice and refine their skills autonomously. This model extends beyond language learning and can be adapted to any discipline with clear, measurable learning objectives, such as STEM, business, or the humanities, by customizing the chatbot's examples and resource bank to the specific domain. A key consideration involves balancing automation with human support to create an effective hybrid feedback system. The AI chatbot demonstrated here efficiently handles the initial, labor-intensive task of providing immediate, personalized feedback on goal quality for all students, a process that would be prohibitively time-consuming for a single instructor (ÂºïÁî®ÊñáÁåÆÔºâ . This automation enhances scalability and reduces teacher workload, freeing up instructor time for higher-order interventions. However, the human teacher's role remains irreplaceable for designing pedagogical prompts, addressing complex motivational issues, and providing nuanced content-specific advice. The optimal model leverages AI for scalable, immediate feedback on procedural tasks while reserving human instructor time for more sophisticated pedagogical support. Finally, educators and developers should leverage the behavioral data and user feedback generated by chatbot interactions for ongoing system improvement. The interaction logs provide invaluable analytics on student engagement patterns and common stumbling blocks. This data can be used to iteratively improve the chatbot's design based on direct student suggestions and to inform broader curriculum design. For instance, if analytics reveal widespread student difficulty with a specific SMART criterion, this can signal a need for instructor-led review or support in that area, making the chatbot a valuable diagnostic tool for aligning teaching strategies with learner needs. Limitations and Future Directions While the 

# results
 are promising, this study has limitations that point toward fruitful avenues for future research. The primary limitations concern the study's generalizability. The findings are based on a relatively small sample of first-year engineering students at a single university, which limits their applicability to broader, more diverse populations across different cultural contexts, academic levels, and disciplines. Furthermore, the current research focused exclusively on the goal-setting phase of self-regulated learning. Future work should explore how AI chatbots can be designed to support additional SRL phases, such as monitoring progress, facilitating help-seeking, and guiding self-reflection, to provide more comprehensive learning support. Several technical and ethical considerations must also be addressed in future iterations. The potential for AI hallucination necessitates the development of safeguards to ensure the accuracy of the feedback provided. Data privacy is paramount when handling student interactions, and the accessibility of proprietary models remains a barrier for widespread adoption. Future development could explore using retrieval-augmented generation (RAG) to ground the chatbot‚Äôs responses in vetted educational materials, improving accuracy and reducing fabrications. Based on student feedback and technological possibilities, several exciting directions emerge for future research. Developing chatbots with memory functions would enable more personalized and context-aware interactions, eliminating the need for students to repeat information. Integrating visual outputs, reminder systems, and community features could significantly enhance usability and motivation. Finally, exploring how the chatbot can be integrated into a broader ecosystem that includes peer and teacher feedback would create a powerful, multi-layered support system for learners. 

# Conclusion
 In 

# conclusion
, this study makes a significant contribution to the field of AI-driven educational support by demonstrating the efficacy of a customized chatbot in improving the quality of students' self-set learning goals. The research provides robust evidence that an AI chatbot can serve as an effective instructional scaffold, making the process of learning to set SMART goals more manageable, motivating, and effective. The mixed-

# methods
 approach offers a nuanced understanding of both the quantitative improvements in goal quality and the qualitative reasons behind students' positive perceptions, particularly valuing the tool's personalization, reliability, and ability to reduce cognitive load. The significance of this work lies in its practical blueprint for integrating conversational AI into education to foster core self-regulated learning skills. It moves beyond simple task automation and presents a viable model for a hybrid feedback system that synergizes the scalability of AI with the expertise of human instructors. The findings highlight the potential for AI not to replace teachers, but to empower them by handling routine tasks, thereby freeing them to focus on higher-impact teaching activities. By providing a tool that makes the foundational skill of goal-setting more accessible, this research has the potential to impact not only academic achievement but also the development of lifelong learning competencies, marking an important step forward in the responsible and effective use of AI in education. 

# References
 Winne, P. H., &Hadwin, A. F. (1998). Studying as self-regulated engagement in learning. in metacognition in educational theory and practice. Metacognition in Educational Theory and Practice, 277‚Äì304. Pintrich, P. R. (2000). The role of goal orientation in self-regulated learning. In: Handbook of self-regulation(pp. 451‚Äì502). Elsevier.mmer Zimmerman, B. J. (2013). From cognitive modeling to self-regulation: A social cognitive career path. Educational Psychologist, 48(3), 135‚Äì147. https:// doi. org/ 10. 1080/ 00461 520. 2013. 794676 Winne, P. H. (2022). Modeling self-regulated learning as learners doing learning science: How trace data and learning analytics help develop skills for self-regulated learning. Metacognition and Learning, 1‚Äì19. Kizilcec, R. F., P¬¥erez-Sanagust√≠n, M., & Maldonado, J. J. (2017). Self-regulated learning strategies predict learner behavior and goal attainment in Massive Open Online Courses. Computers & Education, 104, 18‚Äì

## 33. HYPERLINK "https:
//doi.org/10.1016/j.compedu.2016.10.001" https://doi.org/10.1016/j.compedu.2016.10.001 Doran, George T. (1981). There‚Äôs a S.M.A.R.T. way to write management‚Äôs goals and objectives. Management 
```

## ‚ö†Ô∏è MEDIUM PRIORITY Citations (Implement Second)

### 1. Chatbot Education
**Text to enhance:** "Chatbots and the Rise of AI-Based Support Introduce chatbots as SRL scaffolds:
 compare rule-based a..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 2. Educational Technology
**Text to enhance:** "perceived usefulness and ease of use..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 3. Chatbot Education
**Text to enhance:** "Chatbots, also known as conversational agents, are a type of digital system designed to converse wit..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 4. Chatbot Education
**Text to enhance:** "chatbots, including learning, assisting, mentoring, or monitoring. The learning role of chatbots inv..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 5. Chatbot Education
**Text to enhance:** "chatbots refers to their function in simplifying learners' daily tasks and improving accessibility t..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 6. Chatbot Education
**Text to enhance:** "chatbots to assist students in setting learning goals and developing study plans (Guan et al., 2024)..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 7. Educational Technology
**Text to enhance:** "perceived usefulness and provided accessible support (hew et al., 2022). However, these rule-based c..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 8. Chatbot Education
**Text to enhance:** "chatbots, powered by natural language processing (NLP) and artificial intelligence , have emerged as..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 9. Chatbot Education
**Text to enhance:** "chatbots on the quality of students‚Äô self-set goals. This study adopts a novel approach by developin..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 10. Chatbot Education
**Text to enhance:** "chatbots‚Äô usefulness and ease of use. The instrument comprised six items measuring perceived usefuln..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 11. Educational Technology
**Text to enhance:** "perceived usefulness‚Äîthe degree to which students believed the chatbot would enhance their goal-sett..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 12. Educational Technology
**Text to enhance:** "perceived usefulness reflected students‚Äô assessment of the chatbot‚Äôs effectiveness in improving goal..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 13. Educational Technology
**Text to enhance:** "perceived usefulness and ease of use..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 14. Educational Technology
**Text to enhance:** "perceived usefulness (the extent to which students believed the chatbot improved their goal-setting ..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 15. Educational Technology
**Text to enhance:** "perceived usefulness and ease of use..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 16. Educational Technology
**Text to enhance:** "perceived usefulness received mean ratings above 4.3 on a 5-point scale, with the highest ratings fo..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 17. Educational Technology
**Text to enhance:** "Perceived usefulness and ease of use..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 18. Educational Technology
**Text to enhance:** "perceived usefulness and ease of use..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 19. Chatbot Education
**Text to enhance:** "chatbots‰∏∫‰ªÄ‰πàËÉΩÊèê‰æõa wide array of resources‰ª•Âèä‰∏∫‰ªÄ‰πàËÉΩÂÆûÁé∞personalization) This capability was particularly val..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 20. Chatbot Education
**Text to enhance:** "Chatbots provide consistent advice and information, which is critical in educational settings. They ..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 21. Chatbot Education
**Text to enhance:** "chatbots enhanced learning by offering students immediate, around-the-clock support and feedback wit..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 22. Chatbot Education
**Text to enhance:** "chatbots are available 24/7, providing students the flexibility to seek assistance at any time that ..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 23. Educational Technology
**Text to enhance:** "Perceived Usefulness: Very high (All items >4.3/5). Made process easier: M = 4.67 Made process more ..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 24. Chatbot Education
**Text to enhance:** "chatbots. While previous research using rule-based chatbots successfully introduced goal-setting (He..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 25. Educational Technology
**Text to enhance:** "perceived usefulness and ease of use..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 26. Educational Technology
**Text to enhance:** "perceived usefulness scores. However, students also aptly noted that this personalization was contin..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

### 27. Chatbot Education
**Text to enhance:** "chatbots must be designed not only to be smarter but also to strategically fade their support, perha..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 28. Chatbot Education
**Text to enhance:** "chatbots into academic courses. Instructors can adopt a similar scaffolded approach by first explici..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 29. Chatbot Education
**Text to enhance:** "chatbots can be designed to support additional SRL phases, such as monitoring progress, facilitating..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 30. Chatbot Education
**Text to enhance:** "chatbots with memory functions would enable more personalized and context-aware interactions, elimin..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 31. Chatbot Education
**Text to enhance:** "chatbots to support student goal setting and social presence in fully online activities: learner eng..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 32. Chatbot Education
**Text to enhance:** "chatbots support self-regulated learning..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 33. Chatbot Education
**Text to enhance:** "chatbots: A systematic review. Expert Systems with Applications, 184(), 

## 115461. HYPERLINK "http..."
**Recommended action:** Add comprehensive chatbot education review citations
**Suggested citations:** Hew et al. (2022), Singh et al. (2019), Wollny et al. (2021), Guan et al. (2024)

### 34. Educational Technology
**Text to enhance:** "Perceived usefulness and ease of use..."
**Recommended action:** Reference Technology Acceptance Model (Davis, 1989)
**Suggested citations:** Chang et al. (2013), Scholl et al. (2009), Davis (1989)

## üìù Implementation Instructions

### Phase 1: Critical Citations (Do First)
1. **SRL Definition Enhancement**
   - Locate: First mention of 'Self-regulated learning is a dynamic process'
   - Action: Add '(Winne & Hadwin, 1998; Pintrich, 2000; Zimmerman, 2013)' after the definition
   - Why: Establishes theoretical foundation immediately

2. **Student Goal Quality Claims**
   - Locate: Claims about students setting 'vague goals'
   - Action: Add '(McCardle et al., 2016; Raluy & Mislang, 2022)' to support empirical claims
   - Why: Provides evidence for the problem statement

3. **AI Chatbot Capabilities**
   - Locate: Claims about AI chatbot advantages over rule-based systems
   - Action: Add '(Ng et al., 2024; Guan et al., 2024; Hew et al., 2025)' for recent evidence
   - Why: Supports technological claims with current research

### Phase 2: Supporting Citations
- Add Davis (1989) for Technology Acceptance Model references
- Add comprehensive chatbot education reviews where appropriate
- Enhance goal-setting theory sections with additional supporting research

## üéØ Quality Assurance Checklist
- [ ] All high-priority citations implemented
- [ ] Claims about student behavior supported by research
- [ ] Theoretical frameworks properly cited
- [ ] Technology claims backed by recent studies
- [ ] Methodology appropriately referenced
